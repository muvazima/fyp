@&#MAIN-TITLE@&#A novel approach to the low cost real time eye mouse

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We show the impact of the eye mouse performance depend on modified pupil center corneal reflection(PCCR) method


                        
                        
                           
                           We analyze various major reasons to make an eye gaze tracking system to be a system have a lower performance.


                        
                        
                           
                           We propose a solution reducing the complexities of the previous system, so that proposed system may produce a great performance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Accessibility

Eye gaze system

Low cost

Real time embedded system

@&#ABSTRACT@&#


               
               
                  The PCCR (Pupil Center Corneal Reflection) method became dominant for finding human's diverse eye gaze directions through the research on the eye tracking technology that has been done for a very long period of time. The initial studies on the eye tracking technology were related to the general human interface for operating equipment and devices, then it has been promoted to the field of various purposes such as a market research in a recent study analyzing customer's behaviors. In particular, a real time eye gaze tracking system is most important for many HCI applications including stereoscopic synthesis, intend extraction, behavior analysis and etc. In order to make an eye gaze tracking system to be real time, the system must have an efficient pupil detection algorithm and ambience-independent image processing as well as reduced complexity, small size and number of circuit components. This paper proposes a method for getting clean images compared to the previous systems to reduce image processing overhead. Because it also helps reducing the number of image frames to be dropped during the image processing, the proposed method can provide a sufficient performance even on a low cost hardware system by reducing the transmission traffic.
               
            

@&#INTRODUCTION@&#

The number of ALS (amyotrophic lateral sclerosis, also known as Lou Gehrig's disease) patients in Korea is estimated to be about 1200 and over 2,000,000 physically disabled people in the whole world. ALS begins with irregular limb weakness, body-wide tremors and/or speech difficulty. Unfortunately, ALS patients, sometimes within months, lose the ability to move, eat, drink, speak and breathe. Their life can sometimes be prolonged through the use of a feeding tube and respirator. The term ‘ALS’ can be expressed what is called ‘life in a glass coffin’, which means death comes within 2 to 4years. Particularly, ALS strikes anyone, anywhere and anytime without warning. Fifteen new cases are diagnosed each day in the United States alone, the same are diagnosed with multiple sclerosis. However, ALS patients die usually within a few years [1].

This rapidly progressive disease attacks motor neurons in the spinal cord and brain, and there is no known cure for it at this time. The combined number of patients difficult to move their body by accident or illness is getting bigger. Therefore, a potential demand of the input pointing devices utilizing the eyes is increasing because they can be used instead of their hands, speech or other methods to interact. Around 30,000 potential patients just in Japan use computer with specially designed input devices and, of course, their economic circumstances are also all different depending on the individuals. In case of the commercially used eye-based mouse for example, it is difficult for many people to try because of its expensive cost.

To resolve above problem, this paper introduces a missionary project called Low Cost Real-time Eye Gaze System, a hardware-based pointing device framework for ALS patients that is composed of a remote type eye mouse hardware and firmware, a HID (Human Interface Device) driver software for the operating system and an application software suit. This hardware-based pointing device framework can be implemented with low cost materials via a convenient-to-deploy assembling environment. This framework was designed as a result of the VOC (Voice of Customers) from the ALS patients including potential candidates, as well as analytical needs and desktop research results. The implemented system was proved that it can excellently provide functionality of a conventional pointing device through a practical usability test. It is expected that this system can provide an IT experience even for ALS patients so that they can interact with the world, which was not achievable before (Fig. 1
                     ).

The implemented eye mouse has a performance adorable harmonized software and hardware. In this paper, we broadly handle the system organization, and focus on the reduced complexity of the system for a low cost real time system.

Gaze-tracking has been used in studies dating as far back as 1935 and 1967. During these early days of eye tracking, systems were cumbersome, invasive and not very accurate. However, with recent advancements in eye tracking technology, we can have a system that is remove, non-encumbering, non-invasive and accurate to within 0.5–2.0° accuracy but expensive. It has been focused for a very long period, in the Initiated approach was the recognition of human's interests and then recently, the focus is getting moved to the searching purposes and behaviors for various fields such as marketing research, detecting of napping at driver's seat, and etc [2–6].

Initial studies of the eye tracking are starting from the interface for operating the equipment and devices, and then, it has been promoted in the field of various purposes. More recently, the movement has been trying to apply to the personal device such as a PC, mobile devices, TV, tablet and etc. Thus, this kind of trend has come out and showed off the UX as an eye tracking technology at CES2012 by Microsoft Company.

Most modern approaches, including remote gaze estimation, are based on the analysis of eye features, head features extracted from video frames. The most common approach to remote POG (Point of Gaze) uses the estimates of the centers of the pupil and on or more corneal reflection. It is called PCCR (Pupil center corneal reflection) system, it uses the corneal reflection image, generally called from 1st to 4th purkinje image or pupil and glint image, in other words, virtual images of the pair of the light source in front of surface of the cornea [7].

When light falls on the eye, part of it is reflected back, through the pupil, in a very narrow bean pointing towards light source. If a light source is located very close to a camera optical axis, a very bright pupil is grabbing, otherwise dark pupil does. If this two grabbed image is subtracted, these two images of subtraction are left by the pupil and glints.

The proposed PCCR eye gaze tracking prototype system is shown in Fig. 2
                        , it is a prototype model for several experimental setting to find the optimum images. This system is composed of various parts in the camera, and experiments were carried out by configuring the system to match the optimum distance through the experiment by placing on both sides of the reflected light LED. Eye-tracking systems use the difference vector (P-CR) between the pupil position (P) and the corneal reflection (CR) to determine the gaze vector [8,9].

It is important that setting up the eye camera and performing a good calibration routine are just as important as the design of the system for the disabled [10]. In this paper, to obtain POG direction, this system uses photometric InfraLED reflections on the surface of the cornea. In appropriate situation, one or more glint lights (dark pupil) and full reflection light (bright pupil) from retina by illuminated light near the optical axis of the eyes. It is an important performance parameter to get clear images from PCCR system because it depends on fast and exact calculation of each image by stable clear image grabbing.

The proposed system uses a CMOS digital imaging sensor and a PC for the image processing instead of standalone FPGA. It processes 640×480 progressive scan frames at a 60 frame for second rate. Any CPU based implementation of real-time image-processing algorithm has two major bottlenecks, that is a data transfer bandwidth and sequential data processing rate. After a frame is grabbed and moved to memory, the CPU can sequentially process the pixels. Instead of FPGA, we used note PC and then USB port that bandwidth is limited between Image capturing device and the PC.

One of the important criteria using eye tracking system in the most input devices is the accuracy of the eye gaze. There are several affecting accuracy factors when using these operations with their eyes, but the operation performance of the human eye depends on the tracking resolution of the eye-tracker, the size of the display, resolution, distance, UI/UX design and etc. For example, the head motion eye has the disadvantage of close relationship with the human face and person. When the object on the LCD screen moves quickly, eye tracking offset occurs. Thus, eye tracking system is the expansion of the freedom of movement, but there is actually a limit to the resolution of the camera and FOV of the eye tracking system. To resolve this problem, we need the most appropriate system configuration depending on the purpose of the eye tracking system.

In a view of the price, the remote stationary eye mouse is around 8k–40k US dollars for physically disabled people. This type of mouse is used as an input device for people suffering from ALS, however, despite expensive price, the stationary remote eye mouse is used in the specific field because it is difficult to use and control by practice. In other words, when people use their eyes as a means of access information by eye gaze tracking, there is some discrepancy in the usability surface. It means that the eyes must be used not only getting information but also controlling some devices or equipment. As people use other parts of your body, ALS patient uses their eyes instead of their hands or bodies because there is no better function to replace.

Thus, the performance factors for eye gaze tracking as shown in Fig. 2 are very important to the ALS patient because they cannot communicate with others without the eye gaze tracking system comfortably designed for them. Especially, there exist some factors influence on this eye mouse is the efficient hardware performance. Usability evaluation is also a criterion of the eye tracking system, so a sufficient evaluation process for accurate usability is necessary for the comfortable use.

The consideration of the movement related lighting condition always exists with consistency during their operation with expensive price, and simultaneous input and output. At the same time, the following up the accuracy of system performance despite of the head movement, eyelids, noises and etc. In general, because general human's eye is untrained as a input device like mouse, there is a limitation of accuracy, portions of ease of use, to be verified based on the quantitative assessment manner so hard move freely like he is intended.

In addition, there are environmental luminance, the FOV of the IR LED, the distance between pupil and glint IR LED, the light strength of the IR LED, the bright of the camera lens, a macro lens suitable for taking photographs unusually close to the subject, etc. Most of all, it is important that all these things are able to be harmonized like a gear. On the surface of the cornea, only this well harmonized hardware system finds the POG direction very well. It is used for searching the pupil, various algorithms have been used for finding a pupil by using a reflection lights from retina or other algorithm.

The proposed hardware is shown in Fig. 3
                        , and the image acquisition process is shown in Fig. 4
                        . To acquire an image from the nature, light should be projected (exposed) to an object. The reflected light from the object will cause an image sensor to be charged with an amount of electrons (or others) that is proportional to the light exposed. The amount of charged electrons is measured as a voltage form and then an ADC (Analog–Digital Converter) digitizes it. (In some cases, it is not digitalized and timely pumped into the CVBS frame.) The digitalized image data will be buffered and consequently and timely converted into an analog signal to construct a standard CVBS signal.

As shown in Figs. 5–6
                        
                        , the CVBS (Composite Video Baseband Signal) signal was defined to transfer 2D image through a single physical line or a single RF (Radio Frequency) channel in an analog form. A video clip for one second consists of 30 (in case of NTSC standard, 25 in case of PAL standard) subsequent multiple still image frames. Each image frame is followed by a VSYNC (vertical synchronizer) signal to separate the image frames from each other. A 2D still image is also compounded with multiple 1D raster scanned horizontal line images. These line images are timely connected one by one to construct one train signal that is transferred as one continuous analog signal. As a delimiter between the line image signals, each line image is also followed by an HSYNC (horizontal synchronizer) signal. A video receiver composes moving images frame by frame from a training line image signal by detecting HSYNC and VSYNC signals.

As shown in Figs. 7–9
                        
                        
                        , to make exposure of the sequential image frames equally balanced and stable, the exposure time should be synchronized with the timing of the image framing. Because the line images and the synchronizing signals are related with a natural image cut (shot) that was captured earlier than the time the line images and synchronizing signals appear, the exposing time should be precisely optimized to achieve clear images.

The lighting system of the eye mouse turns infrared LEDs on and off alternatively to minimize computation overhead. This means that an exact synchronization method is required between the light system and the camera module. This is very important because the amount of exposed light may depend on the ambient situations due to lack of light in case of the out-of-synchronization. Equalization of the exposure is directly related to the synchronization.

Right exposure of the image sensor is one of the most important matters to acquire a clear image and hence extract accurate gaze tracking information in the eye mouse system. Intensity of the light source affects the contrast ratio of a captured image and a weaker or stronger intensity light causes performance degradation of the gaze tracking algorithm. An adaptive thresholding algorithm for intensity control of the light source can enhance the performance of the eye tracking system. In case of a long distance eye tracking system that uses the PCCR, in particular, it is difficult or impossible in many cases to detect the position of the pupil when the light source is not stable or fatigue. Exact exposure time is crucial for image quality because two light sources (infrared LEDs) blink alternatively.

A digital video frame and its related signals are interpreted at two different viewpoints: transmitter's and receiver's. The operation of a video receiver is clear because it can compose a video stream line by line and frame by frame whenever it detects HSYNC and VSYNC. However, the operation of the video transmitter is somewhat complex. One cut of a natural image projected to the image sensor is 2 dimensional, which is captured at a specific point of time. After captured, it should be separated into multiple line images of many pixels that are positioned into different time points in a linear digital image signal. This may cause an exposure problem. In case of continuous lighting, it doesn't cause the amount of light for each line or frame to steeply divert.

In case that a flashing light is used, the times that the light turns on and off critically affect the amount of exposure of the light to each line or frame. This means that the times of turning on and off should be exactly synchronized to the timing of the lines and frames in the train signal. If it is not achieved, the receiver will show dappled images each of which has a darker or brighter horizontal band in different vertical positions and also they flow up and down irregularly.

In a usual lighting system, LED bright control is performed by PWM (pulse width modulation) function. However, PWM may cause out-of-synchronization problem between LED light and image sensor exposure time in the PCCR system and therefore it needs a contiguous lighting source. For a contiguous lighting source, the amount of exposure can be controlled through an electric current adjustment by varying the value of the feedback resistor for the LED driver, which controls the strength of the LED light.

Under a dark environment, the brightness will not be enough for the image sensor to take images clearly. In an outdoor environment, the surrounding light from the Sun causes an over exposure and the LED light should get brighter and the brightness will reach the maximum level allowed for health of the human eyes in an extreme environment. A dynamic range of the exposure for the image sensor needs to be set within these two critical end points.

The practical amount of the current flowing into the LED light block is circuit-dependent. If the maximum range of the current is once decided, the number of steps can be selected as convenience. By selecting 4 steps of different value for the feedback resistor from 0.1kΩ to 10kΩ, the current flow makes the brightness from 3.5mW/sr to 35mW/sr, for example.

The LED bright control alone for the exposure strength of the image sensor means that the LED light is getting into a sharp strong impulse flash long with the extreme increment of the biased amount of the surrounding light. In case of a critical situation, the higher LED brightness may cause harm to the human eyes. Therefore, the instantaneous light strength of the LED block should be limited to a regulated level. This means that the total amount of the light lacks for an enough exposure for the image sensor.

The lack of exposure can be compensated by broadening the duration of the LED light turned on. It is very difficult to synchronize the turn-on time of the LED light to the exposure of the image sensor for a clear digitizing of an image frame because a usual image sensor runs in a free-running mode. So, the trigger for the turn-on signal can only be synchronized to the VSYNC signal. The rising edge of the VSYNC signal is sampled in order to control the operation of LED light.

As shown in Figs. 10–11
                        
                        , each one of every two VSYNC signals is captured to control one centered pupil light and two sided glint lights, alternatively. This approach cannot assure the synchronization between the exposure lighting time and the image digitizing time. However, adjusting the starting point of the LED light turned on makes the exposure to move through and approach to the exact point of an image matured.

The method calculating based on the time is impossible to create accurate Glint and Pupil light. Because it is not consider getting clean images.

The pupil LED at the center of the camera module makes an image that has a dot at the center of the captured pupil and the two glint LEDs at both sides make an image that has two horizontally aligned dots close to the round of the captured pupil. If the two exposure durations are overlapped or too close to each other, the resulted image has three dots as if all LEDs get turned on at the same time. This causes much overhead to the tracking algorithm. Therefore, it is highly important to exactly control the exposure time and level for both light sources. As shown in Fig. 10, D12 and D11 are the clock signals for pupil and glint image capturing precisely. It is possible to obtain a clear image in a sufficient exposure environment using 4 step current level but cannot be obtained in a insufficient environment. Instead, we propose interrupt method additionally as shown in Fig. 11.

Eq. (1) is the mathematical model for the adaptive exposure control equation. The Subsections 4.3 and 4.4 are basic knowledge about the camera shutter and aperture functions. Based on the two subsections, adaptive value for the proposed system is recommended by resistor setting.
                        
                           (1)
                           
                              
                                 E
                                 
                                    T
                                    min
                                 
                              
                              <
                              
                                 E
                                 L
                              
                              −
                              
                                 E
                                 S
                              
                              <
                              
                                 E
                                 
                                    T
                                    MAX
                                 
                              
                           
                        
                     where E
                     
                        L
                      is the controlled LED exposure level, E
                     
                        S
                      is the surround exposure level, and 
                        
                           E
                           
                              T
                              min
                           
                        
                      and 
                        
                           E
                           
                              T
                              MAX
                           
                        
                      are minimum and maximum exposure levels, respectively, allowed for correct image acquisition on the chosen image sensor.

The factors 
                        
                           E
                           
                              T
                              min
                           
                        
                      and 
                        
                           E
                           
                              T
                              MAX
                           
                        
                      are both image sensor dependent and much related to the sensitivity depth of it, which should be experimentally measurable or given by manufacturers. The factor E
                     
                        S
                      is the surrounding brightness of the working environment. If the implemented system is for in-door use only, the range is reasonable, though the range becomes too wide if it is used outdoor under the bright sunlight. So, it needs a policy to decide the range of the value E
                     
                        S
                     .
                        
                           (2)
                           
                              
                                 E
                                 L
                              
                              =
                              
                                 
                                    ∫
                                    
                                       t
                                       1
                                    
                                    
                                       t
                                       2
                                    
                                 
                                 
                              
                              n
                              C
                              ⋅
                              
                                 i
                                 LED
                              
                              
                                 t
                              
                              d
                              t
                              ,
                              
                                 t
                                 1
                              
                              <
                              
                                 t
                                 2
                              
                              and
                              
                                 t
                                 1
                              
                              ≠
                              
                                 t
                                 2
                              
                           
                        
                     where n is the number of LEDs, C is light-current coefficient per LED and i
                     
                        LED
                     (t) is the current flowing into the LED block.

The factors t
                     1 and t
                     2 are the start and the end time for exposure by the LED block, the difference of which is the duration between the LED block turns on and off. If a flash of exposure starts at t
                     1 over the image sensor, an image will be fully matured after a reasonable amount of time and captured by a set of Analog to Digital Converters (ADC) with a Sample and Hold (S&H) circuit into a digital image frame. This digitized image frame is reformed into a part of CVBS signal.

@&#EXPERIMENTAL RESULTS@&#

To acquire an image from each person, we set up the proposed system that is 1680×1050 resolution, 27-inch monitor, 90cm distance between the human eye and proposed eye gaze tracking system. The test process includes seeing the monitor for a few seconds, recording/calculating the true value with filtering method and displaying the results. After that, the test can check the eye gaze tracking point of the proposed system. The test result on four people is displayed in Figs. 12–15
                     
                     
                     
                     .


                     Fig. 12 shows the result of a skilled individual who has used the system three times or more. Fig. 12(a) is the mean filter output of the eye gaze tracking system that performs 12-point calibration while Fig. 12(b) uses a median filter to get accuracy of about 1 or 2° respectively. The eyes stayed for the fixation time of 300ms, which is deemed valid data. Otherwise, the incoming measurement value is ignored. Fig. 13 shows the result of the users who experienced the system for the first time. Their eye gaze sometimes went beyond the monitor though they became familiar with it soon and used it well. Person 1 and 2 also showed accuracy of about 1 or 2° and Person 3 and 4 also presented accuracy of 2° or lower in Figs. 14 and 15. This accuracy occurred at the corners of the monitor while it was especially difficult to produce accuracy on the left top corner.

It is considered that the result was produced due to the error as big as the circle diameter that happened during gazing at the monitor in the calibration process, while the circle became smaller in the process of guiding during calibration. It is deemed that the result occurred due to the error as big as the circle diameter in the process of gazing at it. It is expected that the error can be reduced down to 1° or smaller if the gazer is guided to be able to see the circle bigger during calibration. When the distance is 65cm between the monitor and the gazer, 1° is about 1.1cm resolution on the monitor. The experiment found out that even those with physical disability who had experienced the system before could not use it easily and preferred the resolution of 2° or more because 1° was too sensitive. The experiment had four subjects to illustrate the data. Fig. 12(a) shows output of the eye gaze tracking system during calibration and Fig. 12(b) shows the median filter data. Person 1 experienced the system more than three times, person 2 did twice or more while person 3 and 4 used it for the first time. If it is assumed that the system is designed for the Lou Gehrig's patients, the test result on those without disability without fixing their head shows the result of 2° or lower. As for the mean filter, the output is affected by the noise since the noise component is included in the average calculation. However, as for the median filter, the noise is filtered before being selected and does not affect the output. The result shows that the median filter brought even more accurate result than the mean filter though the median filter took more time. Nevertheless, the system already reduced the calculation amount and secured enough room for calculation. Thus, no problem occurred to use the proposed modified median filter.

@&#CONCLUSIONS@&#

This paper suggests the solution for the disabled who can use their eyes only. The modified PCCR-based eye gaze tracking system is created for the former that provided VOC-based convenient mouse function to help their PC use. The standard UVC camera-based driver is used to grab images so that installation of a special driver is unnecessary. After that, a hardware is built that will offer two consecutive images repeatedly that represent the bright pupil and dark pupil to find the glint through differentiating the images and eventually create the algorithm that will find the bright pupil by finding the blob candidates around the glint.

In this paper, we first addressed the proposed PCCR eye gaze system that helps people who cannot freely manipulate objects, particularly digital devices including personal computers, freely. The person with disabilities sometimes would like to access their environment with their own intention without other supporting like other normal people. Conclusionly, the proposed system can be used instead of arm and hand as an eye gaze mouse of the disabled.

Second, we present an interrupt method that reduces the complexity on calculation of images for eye gaze tracking system. This method identifies the interrupt timing of getting clean images so it need not capture all the images anymore when the proposed system is on activation.

Third, we applied an additive method in the eye gaze tracking system called timer method. The interrupt method can be used in the sufficient exposure mode, instead, in the various exposure modes, the timer method is used for getting clean images. Thus, we evaluate that this proposed system can give a clean image than the previous system.

@&#REFERENCES@&#

