@&#MAIN-TITLE@&#Vision-based action recognition of earthmoving equipment using spatio-temporal features and support vector machine classifiers

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present a computer vision based method for equipment action recognition.


                        
                        
                           
                           Our vision-based method is based on a multiple binary SVM classifier and spatio-temporal features.


                        
                        
                           
                           A comprehensive real-world video dataset of excavator and truck actions is presented.


                        
                        
                           
                           We achieve accuracies of 86.33% and 98.33% for excavator and truck action classes.


                        
                        
                           
                           The presented method can be used for construction activity analysis using long sequences of videos.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Computer vision

Action recognition

Construction productivity

Activity analysis

Time-studies

Operational efficiency

@&#ABSTRACT@&#


               
               
                  Video recordings of earthmoving construction operations provide understandable data that can be used for benchmarking and analyzing their performance. These recordings further support project managers to take corrective actions on performance deviations and in turn improve operational efficiency. Despite these benefits, manual stopwatch studies of previously recorded videos can be labor-intensive, may suffer from biases of the observers, and are impractical after substantial period of observations. This paper presents a new computer vision based algorithm for recognizing single actions of earthmoving construction equipment. This is particularly a challenging task as equipment can be partially occluded in site video streams and usually come in wide variety of sizes and appearances. The scale and pose of the equipment actions can also significantly vary based on the camera configurations. In the proposed method, a video is initially represented as a collection of spatio-temporal visual features by extracting space–time interest points and describing each feature with a Histogram of Oriented Gradients (HOG). The algorithm automatically learns the distributions of the spatio-temporal features and action categories using a multi-class Support Vector Machine (SVM) classifier. This strategy handles noisy feature points arisen from typical dynamic backgrounds. Given a video sequence captured from a fixed camera, the multi-class SVM classifier recognizes and localizes equipment actions. For the purpose of evaluation, a new video dataset is introduced which contains 859 sequences from excavator and truck actions. This dataset contains large variations of equipment pose and scale, and has varied backgrounds and levels of occlusion. The experimental results with average accuracies of 86.33% and 98.33% show that our supervised method outperforms previous algorithms for excavator and truck action recognition. The results hold the promise for applicability of the proposed method for construction activity analysis.
               
            

@&#INTRODUCTION@&#

Equipment activity analysis, the continuous process of benchmarking, monitoring, and improving the proportion of time construction equipment spend on different construction activities, can play an important role in improving construction productivity. A combination of detailed assessment and continuous improvement can help minimize idle times, improves operational efficiency [1–5], saves time and money [6], and results in a reduction of fuel use and emissions for construction operations [7,8]. Through systematic implementation and reassessment, activity analysis can also extend equipment engine life and provide safer environments for equipment operators and workers.

Despite the benefits of activity analysis in identifying areas for improvement, an accurate and detailed assessment of work in-progress requires an observer to record and analyze the entire equipment’s actions for every construction operation. Such manual tasks can be time-consuming, expensive, and prone to errors. In addition, due to the intra-class variability on how construction tasks are typically carried out, or in the duration of each work step, it is often necessary to record several cycles of operations to develop a comprehensive analysis of operational efficiency. Not only the traditional time-studies are labor intensive, but they also require a significant amount of time to be spent on manually analyzing data. The monotonous data analysis process can also affect the quality of the process as a result of the physical limitations or biases of the observer. Without a detailed activity analysis, it is unfeasible to investigate the relationship between the activity duty cycles vs. productivity, or fuel use and emissions [9]. There is a need for a low-cost, reliable, and automated method for activity analysis that can be widely applied across all construction projects. This method either vision-based or non-vision based, needs to remotely and continuously analyze equipment’s actions and provide detailed field data on their performance.

Over the past few years, cheap and high-resolution video cameras, extensive data storage capacities, and the availability of Internet connection on construction sites have enabled capturing and streaming construction videos on a truly massive scale. Detailed and dependent video streams provide a transformative potential for gradually and inexpensively sensing actions of construction equipment, enabling construction companies to remotely analyze operational details and in turn assess productivity, emissions, and safety of their operations [10]. To date, the application of existing site video streams for automated performance assessment is still untapped and unexploited by researchers in most parts.

Here, we address a key challenge: action recognition; i.e., determining various actions equipment performs over time. While in the past few years several studies have looked into these areas (Section 2), many challenging problems still remain unsolved. As a step forward, this paper focuses on the problem of recognizing single actions of earthmoving equipment from site video streams. Fig. 1
                      shows examples of the actions of an excavator and a dump truck operation, wherein the excavator performs a cycle of digging, hauling (swinging with a full bucket), dumping, and swinging (with an empty bucket) and the truck performs a cycle of filling, moving, and dumping.

Given videos taken by a fixed camera with small lateral movements (caused by wind or small ground vibrations), clutter, and moving equipment, the task is to automatically and reliably identify and categorize such actions. This paper presents an algorithm that aims to account for these scenarios. As such, the state-of-art research in this area is first overviewed. Next, a set of open research problems for the field are discussed, including action recognition under different camera viewpoints within dynamic construction sites. The specific focus of the proposed method and its details are then described. Also, a comprehensive dataset and a set of validation methods that can be used in the field for development and benchmarking of future algorithms are provided. The perceived benefits and limitations of the proposed method in the form of open research challenges are presented.

In most state-of-the-art practices, the collection and analysis of the site performance data are not yet automated. The significant amount of information required to be manually collected may (1) adversely affect the quality of the analysis, resulting in subjective reports [11,12] and (2) minimize opportunities for continuous monitoring which is a necessary step for performance improvement [11–14]. Hence, many critical decisions may be made based on this inaccurate or incomplete information, ultimately leading to project delays and cost overruns.

In recent years, a number of research groups have focused on developing techniques to automatically assess construction performance. The main goal of these methods is to support improvement of operational efficiency and minimize idle times. Several studies such as [1–4] emphasize on the importance of a real-time resource tracking for improving construction performance. To address this need, different tracking technologies such as barcodes and RFID tags [14–19], Ultra WideBand (UWB) [20–22,61–63,65,68], 3D range imaging cameras [21,23], global and local positioning systems(GPS) [21,23,64], and computer vision techniques [24,25,66,67,69–75] have been tested to provide tracking data for onsite construction resources. While dominantly used for tracking construction material, they have also been used in locating workers and recording the sequence of their movement necessary to complete a task; e.g., [2,6,24–28,59,60,72–76]. For the task of performance monitoring, there is a need for detailed data on activities of construction equipment and workers, which makes a low-cost vision-based method, an alternative appealing solution; particularly because a low-cost single camera (e.g., $40–100 Wi-Fi HD camera) can potentially be used for (1) recognizing activities of multiple equipment and workers for both performance monitoring and safety analysis purposes and (2) minimizing the need for sophisticated on-board telemetric sensory for each equipment (or other sensory mentioned above for each worker) which can come at a higher cost.

Despite a large number of emerging works in the area of human action recognition for smart online queries or robotic purposes and their significance for performance assessment on construction sites, this area has not yet been thoroughly explored in the Architecture/Engineering/Construction (AEC) community. The work in [13] is one of the first in this area, which presented a vision-based tracking model for monitoring a tower crane bucket in concrete placement operations. Their proposed method is focused on action recognition of crane buckets and hence it cannot be directly applied to earthmoving operations. In a more recent work, Gong and Caldas [2] proposed an action recognition method based on an unsupervised learning algorithm and showed promising results. However, generalizing the applicability of unsupervised learning models for unstructured construction sites can be challenging. In this paper we show that a supervised learning method may provide better performance in the equipment action recognition task. Zou and Kim [6] also presented an image-processing approach that automatically quantifies the idle time of a hydraulic excavator. The approach uses color information for detecting motion of equipment in 2D and thus can be challenged by changes of scene brightness and camera viewpoint. Also for performance assessment purposes, detailed data beyond idle/non-idle times can be very beneficial. Others such as Rezazadeh Azar et al. [73,74] benefit from location data for recognizing detecting activities and show promising performance. Such methods may require to be learned for every single site and mainly focus on detecting activities based on the location of the equipment. As such it is still challenging to differentiate between actions within a cycle (e.g., digging vs. hauling actions for an excavator) from location features alone.

In the computer vision community, there are a large number of researches in the area of person recognition and pose estimation [29–34,46,47,59,60]. The results of these algorithms seem to be both effective and accurate and in some cases [32] they can also track deformable configurations which can be very effective for action recognition purposes. A number of approaches adopted visual representations based on spatio-temporal points [35,36]. This can be combined with discriminative classifiers (e.g., SVMs) [37,38], semi-latent topic models [39], or unsupervised generative models [40,41]. Other methods have shown the use of temporal structures for recognizing actions using Bayesian networks and Markov models [42,43], and the incorporation of spatial structures [44]. To leverage the power of local features, [40] introduced a new unsupervised model to learn and recognize the spatial–temporal features. Savarese et al. [45] introduced correlations that describe co-occurrences of code words within spatio-temporal neighborhoods. While not directly applicable, certain elements of all of these works can be effectively used to create new methods suitable for equipment action recognition.

Previous research on sensor-based or vision-based approaches has primarily focused on location tracking of workers and equipment. In practice, when faced with the requirement for continuous benchmarking and monitoring of construction operations, techniques that can support automated identification of construction actions as supplementary modules can be beneficial. Site video streams offer great potential for benchmarking and monitoring both location and action of construction resources. Current overall limitations of the state of the art computer vision approaches in action recognition for construction “activity analysis” are as follows:
                           
                              1.
                              Lack of comprehensive datasets of action recognition of various construction equipment which capture various actions of the equipment from almost all possible viewpoints, under various illumination and background clutter conditions;

Lack of automated techniques that can detect articulated actions of construction equipment and workers plus their body posture necessary for performance assessments. Majority of vision-based approaches focus on recognizing simple actions where people are asked to perform distinct and single action in a rather more controlled environment; e.g., walking, jogging, running and boxing (please see the definition of simple actions in [77]). In contrast to these actions wherein similar body posture is repeated multiple times in a single-action video, construction equipment actions are not repeated and each atomic-action video only contain one instance of the action;

Assuming a prior knowledge of starting temporal points for each action within a sequence. Without a proper knowledge of these starting points, a time-series of actions cannot be formed for further construction activity analysis;

None of the existing techniques look into simultaneous recognition of multiple actions, rather they look into simultaneous action recognition per single class of objects. For example, in pedestrian tracking, the focus is to detect a group action (i.e., multiple people conducting the same action such as walking) as opposed to multiple individual actions of pedestrians (i.e., one pedestrian walking, the other running, the other hand waving).

None of the existing approaches take a holistic approach to benchmarking, monitoring, and visualization of performance information. Without a proper visualization, it will be difficult for practitioners to control the excessive impacts of performance deviations. In addition, understanding the severity levels of performance deviations will not be easy.

There is a need for techniques that can support automation of the entire process of benchmarking, monitoring, and control of performance deviations by identifying the sequence of resource actions, and determining idle/non-idle periods. Timely and accurate performance information brings awareness on project specific issues and empowers practitioners to take corrective actions, avoid delays, and minimize excessive impacts due to low operational efficiency [48]. In this paper, we address two of these limitations with the following contributions: (1) We introduce a new dataset for benchmarking and evaluating the performance of action recognition algorithms for commonly used earthmoving equipment (dump trucks and excavators) and (2) We propose an algorithm for vision-based analysis of articulated actions of single earthmoving equipment. The proposed algorithm is presented in the following section.

Our action recognition approach fits within an overall strategy depicted in Fig. 2
                     . This strategy consists of equipment detection, tracking, and action recognition modules from each video stream. The idea here is that tracking module can isolate each equipment and as result action recognition module can be focused on single equipment action recognition.

Given a collection of site video streams collected from fixed cameras, our research objective is to (1) automatically learn different classes of earthmoving equipment actions present in the video dataset and (2) apply the model to perform action recognition in new video sequences. In this sense, the fundamental research question that we attempt to answer is the following: “Given a video which is segmented to contain only one execution of an equipment action, how we can correctly and automatically classify the video into its action category?” Answering this question will help us formulate new models over the temporal domain that will be able to detect the transitions between actions of interest within each video as well as the duration of these actions. Our proposed approach which is inspired by the work of [35,40,49] is illustrated in Fig. 3
                     .

Our method can cope with some limited camera motion. Specifically, as shown later in the paper we have validated our approach on videos recorded with a camera mounted on a tripod which has been subject to natural wind forces. However, one cannot assume the camera is completely static since there may be undesired motions due to external forces or example caused by strong wind. Also, the videos are expected to contain typical dynamic construction foregrounds and backgrounds that can generate motion clutter. In the training stage of our proposed method, it is assumed that each video only contains one action of particular equipment. This assumption is relaxed at the full testing stage, where the proposed method can handle observations cluttered by the presence of other equipment performing various actions.

To represent all possible motion patterns for earthmoving equipment, a comprehensive video dataset for various actions is created. These videos, each containing single equipment performing only one action are initially labeled. First for each video, the local space–time regions are extracted using the spatio-temporal interest point detector [35]. A Histogram of Oriented Gradients (HOG) descriptors [30] is then computed from each interest point. These local region descriptors are then clustered into a set of representative spatio-temporal patterns, each called a code word. The set of these code words, from now on, is called a codebook. The distribution of these code words is learned using a multi-class one-against-all Support Vector Machine (SVM) classifier. The learned model will then be used to recognize equipment action classes in new video sequences. In the following each step is discussed in detail.

There are several choices in the selection of visual features to describe actions of equipment. In general, there are three popular types of visual features: static features based on edges and limb shapes [50], dynamic features based on optical flow measurements [31], and spatio-temporal features obtained from local video patches [35,36,51,52]. Spatio-temporal features are shown to be useful in the articulated human action categorization [40]. Hence, in our method, videos are represented as collections of spatio-temporal features by extracting space–time interest points. To do so, it is assumed that during video recording, lateral movements do exist but are minimal. Our interest points are defined around the local maxima of a response function. To obtain the response, similar to [35,40] we apply 2D Gaussian and separable linear 1D Gabor filters as follows:
                           
                              (1)
                              
                                 R
                                 =
                                 
                                    
                                       (
                                       I
                                       ⊗
                                       g
                                       ⊗
                                       
                                          
                                             h
                                          
                                          
                                             ev
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       (
                                       I
                                       ⊗
                                       g
                                       ⊗
                                       
                                          
                                             h
                                          
                                          
                                             od
                                          
                                       
                                       )
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        where I(x, y, t) is the intensity at location (x, y, t) of a video sequence, g(x, y, σ) is the 2D Gaussian kernel applied along the spatial dimensions, 
                           
                              
                                 
                                    h
                                 
                                 
                                    ev
                                 
                              
                              (
                              t
                              ;
                              τ
                              ,
                              ω
                              )
                           
                         and 
                           
                              
                                 
                                    h
                                 
                                 
                                    od
                                 
                              
                              (
                              t
                              ;
                              τ
                              ,
                              ω
                              )
                           
                         are the quadrature pairs of the 1D Gabor filter which are applied temporally.
                           
                              (2)
                              
                                 g
                                 (
                                 x
                                 ,
                                 y
                                 ,
                                 σ
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                       π
                                       
                                          
                                             σ
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 exp
                                 
                                    
                                       
                                          -
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      2
                                                   
                                                
                                                +
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                             
                                                2
                                                
                                                   
                                                      σ
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    
                                       h
                                    
                                    
                                       eV
                                    
                                 
                                 (
                                 t
                                 ;
                                 τ
                                 ,
                                 ω
                                 )
                                 =
                                 -
                                 cos
                                 (
                                 2
                                 π
                                 t
                                 ω
                                 )
                                 ×
                                 exp
                                 
                                    
                                       
                                          -
                                          
                                             
                                                t
                                             
                                             
                                                2
                                             
                                          
                                          /
                                          
                                             
                                                τ
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    
                                       h
                                    
                                    
                                       od
                                    
                                 
                                 (
                                 t
                                 ;
                                 τ
                                 ,
                                 ω
                                 )
                                 =
                                 -
                                 sin
                                 (
                                 2
                                 π
                                 t
                                 ω
                                 )
                                 ×
                                 exp
                                 
                                    
                                       
                                          -
                                          
                                             
                                                t
                                             
                                             
                                                2
                                             
                                          
                                          /
                                          
                                             
                                                τ
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        The two parameters σ and τ correspond to the spatial and temporal scales of the detectors respectively. Similar to [35,40], in all cases, ω
                        =4/τ is used, and hence the response function R is limited to only two input parameters (i.e., σ and τ). In order to handle multiple scales of the equipment in the 2D video streams, the detector is applied across a set of spatial and temporal scales. To simplify the process, in the case of spatial scale changes, the detector is only applied using one scale and thus the codebook is used to encode all scale changes that are introduced and observed in the video dataset; i.e., our video dataset contains multiple spatial scales of each equipment for training purposes. It is noted in [35,40] that any 2D video region with an articulated action can induce a strong response to the function R. This is due to the spatially distinguishing characteristics of actions, and as a result those 2D regions that undergo pure translational motion or do not contain spatially distinguishing features will not induce strong responses. The space–time interest points are small video neighborhoods extracted around the local maxima of the response function. Each neighborhood is called a cuboid and contains the local 3D video volume that contributed to the response function (3rd dimension is time). The size of the cuboid is chosen to be six times the detection scales along each dimension (6σ
                        ×6σ
                        ×6τ). To obtain a descriptor for each cuboid, a Histogram of Gradients (HOG) [37] is then computed. The detailed process is as follows:

At first, the normalized intensity gradients on x and y directions are calculated and the cuboid is smoothed at different scales. Here the normalized intensity gradients are representing the normalized changes of the average intensities, and the 2D Gaussian smoothing is conducted using the response function R. The gradient orientations are then locally histogrammed to form a descriptor vector. The size of the descriptor is equal to (the number of spatial bins in the cuboid)×(the number of temporal bins)×(the number of gradient direction bins). In our case, this descriptor size is (3×3)×2×10=180. In addition to the application of HOG descriptors, histograms of optical flow [53] was also considered. As validated in Section 4, the HOG descriptor results in superior performance. Fig. 4
                         shows an example of interest points detected for an excavator’s ‘digging’ action class. Each small box represents a detected spatio-temporal interest point. Fig. 5
                         shows an example of the HOG descriptor for one of the interest points from the excavator’s digging action class.

In order to learn the distribution of spatio-temporal features in a given video, first a set of HOG descriptors corresponding to all detected interest points in the entire training video dataset is generated. Using the k-means clustering algorithm and the Euclidean distance as the clustering metric, the descriptors of the entire training dataset are clustered into a set of code words. The result of this process is a codebook that associates a unique cluster membership with each detected interest point. Hence, each video is represented as a distribution of spatio-temporal interest points belonging to different code words. Fig. 6
                         illustrates the action codebook formation process. A total of 350 cluster centers are considered for the best action recognition performance. The effect of the codebook size (the number of code word, also the number of clusters) on the action classification accuracy is explored in Section 4.4.3 of this paper.

To train the learning model of the action categories, a multi-class one-against-all Support Vector Machine (SVM) classifier is used. The SVM is a discriminative machine learning algorithm which is based on the structural risk minimization induction principle [54]. In this work, it was hypothesized that traditional classifiers such as Naïve Bayes [55] or unsupervised learning methods such as probabilistic Latent Semantic Analysis (pLSA) [56] may not obtain the best recognition performance. For equipment action classification, the number of samples per class can be limited and consequently these methods tend to result in over-fitting. In the following, the multiple SVM classifier are briefly introduced. For validation, the performance of the proposed algorithm for learning equipment action classes is compared to Naïve Bayes and pLSA in Section 4.3 and the hypothesis for application of a multiple one-against-all supervised SVM classifier is validated.

To classify all K action categories, we adopt K one-against-all SVM classifiers [57], so that video instances associated category (k) are within the same class and the rest of the videos are in another. For example, one of the binary SVM classifiers decides whether a new excavator video belongs to the ‘Digging’ or ‘non-Digging’ action classes. Given N labeled training data {xi
                           ,
                           yi
                           }, i
                           =1,…,
                           N; yi
                           
                           ∈{0,1}, xi
                           
                           ∈
                           Rd
                           , wherein xi
                            is the distribution of the spatio-temporal interest points for each video (i) with d dimensions (occurrence histograms of visual words), and yi
                            is the binary action class label, the SVM classifier aims at finding an optimal hyper-plane wTx
                           +
                           b
                           =0 between the positive and negative samples. We assume there is no prior knowledge about the distribution of the action class videos. We use a conventional learning approach to SVM which seeks to optimize the following:
                              
                                 (5)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         min
                                                      
                                                      
                                                         w
                                                         ,
                                                         b
                                                      
                                                   
                                                
                                                
                                                   
                                                      1
                                                   
                                                   
                                                      2
                                                   
                                                
                                                ‖
                                                w
                                                
                                                   
                                                      ‖
                                                   
                                                   
                                                      2
                                                   
                                                
                                                +
                                                C
                                                
                                                   
                                                      
                                                         ∑
                                                      
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      
                                                         N
                                                      
                                                   
                                                
                                                
                                                   
                                                      ξ
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                subject to
                                                :
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      i
                                                   
                                                
                                                (
                                                w
                                                ·
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                +
                                                b
                                                )
                                             
                                          
                                          
                                             
                                                ⩾
                                                1
                                                -
                                                
                                                   
                                                      ξ
                                                   
                                                   
                                                      i
                                                   
                                                
                                                
                                                for
                                                
                                                1
                                                ,
                                                …
                                                ,
                                                N
                                             
                                          
                                       
                                       
                                          
                                          
                                             
                                                
                                                   
                                                      ξ
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ⩾
                                                0
                                                
                                                for
                                                
                                                1
                                                ,
                                                …
                                                ,
                                                N
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

In this formula, C represents a penalty constant which is determined by cross-validation. For each action classifier, the classification decision score is stored. Among all classifiers, the one which results in the highest classification score is chosen as the equipment action class and the outcome of each video’s classification is labeled accordingly.

Before testing our algorithm, it was important to assemble a comprehensive action recognition dataset. The new dataset accounts for variability in form and shape of construction equipment, different camera viewpoints, different lighting conditions, and static and dynamic occlusions. As a first step, our dataset includes combination of excavators and dump trucks for five types of excavator actions (i.e., moving, digging, hauling [swing with full bucket], swinging [empty bucket], and dumping) and three types for dump truck actions (i.e., moving, filling, and dumping) for three types of excavators (manufacturers: Caterpillar, Komatsu, and Kobelco) and three types of dump trucks (manufacturers: Caterpillar, Trex, and Volvo). This dataset – in which each video only contains one equipment performing a single action – was generated using videos collected over the span of 6months. To ensure various types of backgrounds and level of occlusions, the videos were collected from five different construction projects (i.e., two building and three infrastructure projects). Due to various possible appearances of equipment, particularly, their actions from different views and scales in a video frame, as shown in Fig. 7
                        , several cameras were set up in two 180° semi-circles (each camera roughly 45° apart from one another) at the training stage. The different distances of these two semi-circles from the equipment, enables the equipment actions to be videotaped at two different scales (full and half high definition video frame heights). Combined with the strategy used to encode spatial scale in the codebook, all possible scales are considered. Overall a total of 150–170 training videos were annotated and temporally segmented for each action of equipment (overall 895 videos for four and three action classes of excavators and dump trucks). Each video has different durations, and hence various possible temporal scales for each action are introduced into the training dataset. Table 1
                         summarizes the technical characteristic of our dataset.

To quantify and benchmark the performance of the action recognition algorithm, we plot the Precision–Recall curves and study the Confusion Matrix. These metrics are extensively used in the Computer Vision and Information Retrieval communities as set-based measures; i.e., they evaluate the quality of an unordered set of data entries. More details can be found in [78,79].

@&#EXPERIMENTAL RESULTS@&#

In this following section, we first present the experimental results from our proposed algorithm. Then, in the subsequent sections, we test the efficiency of our approach for the recognition task on various model parameters; i.e., feature detection, feature descriptors, codebook sizes, and finally the machine learning classifier.

For the excavator and dump truck actions datasets, which contain 626 and 233 short single-action sequences respectively, the interest points were extracted and the corresponding spatio-temporal features described using the procedure described in Section 3.1. Some sample video frames from different equipment actions with scale, viewpoint, and background changes are shown in Fig. 8
                        .

The detector parameters are set to σ
                        =1.5 and τ
                        =3 and Histograms of Gradients (HOG) are used to describe the feature points. Some examples of the detected spatio-temporal feature patches are shown in Fig. 9
                        . Each row represents the number of video frames that are used to describe the feature. In order to form the codebook, 350 code words (k-means cluster centers) were selected and the spatio-temporal features for each video were assigned to these code words. The outcome is the codebook histogram for each video. Next, we learn and recognize the equipment action classes using the multi-class linear SVM classifiers. To solve Eq. (5), we use the libSVM [58] and set the kernel type to C-SVC. For each action classifier, a decision score is learned. Comparing these decision scores enables the most appropriate action class to be assigned to each video.

In order to test the efficacy of our approach for the action recognition task, we divided the action dataset into training and testing sequences with a ratio of 3–1 and computed the confusion matrix for evaluation. This process of splitting training and testing videos is randomly conducted for five times, and the average accuracy values are reported for the confusion matrix. The algorithms were implemented in Linux 64bit Matlab on an Intel Core i7 workstation laptop with 8GB of RAM.

For excavator action recognition, Fig. 10
                        a shows that the largest confusion happens between ‘hauling’ and ‘swinging’ action classes. This is consistent with our intuition that both these actions are visually similar (hauling: bucket full vs. swinging: bucket empty). Hence we combined these actions classes assuming that in longer video sequences, the order of equipment actions can help easily distinguish them from one another; i.e., hauling can only happen after digging is detected. Fig. 10b shows the recognition performance for excavators when three action classes are considered. Another significant confusion occurs between ‘digging’ and ‘dumping’ action classes. These actions are also visually similar (bucket getting closer or farther from the excavator arms). Fig. 11
                         shows the decision values for each binary action classification. In each subfigure, the horizontal axis represents the entire video dataset (see Table 1 for the action label of each video) and the vertical axis shows the action classification score. The hyper-plane in each individual binary classification is automatically learned through the binary linear SVM classifier (shows as a red
                           3
                           For interpretation of color in Fig. 11, the reader is referred to the web version of this article.
                        
                        
                           3
                         line). The most appropriate action class for each video is selected by comparing the decision values from the binary classification results and choosing the label which returns the highest classification score (see the scores in Fig. 11d–f).


                        Fig. 12
                         shows the precision–recall curves for the action classification of the excavator and truck testing dataset using the multi-class binary SVM classifiers. The combined hauling and swinging action class for the excavator action recognition and the moving class for the dump truck action recognition have the best average performances.

Several example features from the testing sequences in both truck and excavator video datasets are shown in Fig. 13
                        . In this figure, each spatio-temporal feature patch is automatically color coded with the corresponding action category. Also note that in several videos of truck and excavator datasets (e.g., Fig. 13a-4, b-2, and b-3), the spatio-temporal features from partially occluded equipment are detected and categorized. Fig. 13 also shows that the truck action corresponding to “filling” is similar to the one recognized for the excavator as “dumping”. Since the main objective here was to correctly and automatically classify only one execution of an equipment action into its appropriate action category, this does not create any confusion in our action recognition strategy. Rather as mentioned in the overall strategy, once equipment is detected and localized in 2D, the spatio-temporal features within the bounding box of the equipment are used for action recognition. In this case, the bounding boxes of excavator and truck will have an overlap based on their configuration and the camera viewpoint.

In our experiments, given the size of the training datasets for excavators and trucks, the computational time for training all SVM classifiers for both equipment datasets was benchmarked as 5.2h. This computation time was measured on a desktop workstation using a single CPU core with an implementation in MATLAB. The computational time for recognizing a single equipment action for each video – which typically has a length of about 5–12s – was in order of 3–5s.

In the following subsections, we test the effect of the feature detection parameters, the type and size of the feature descriptor, the codebook sizes, and various machine learning algorithms on the average performance of the action classification. The best parameters are selected based on reasonable accuracy and computational times, and were presented in Section 4.2.

The effect of the two parameters σ and τ, which correspond to the spatial and temporal scales of the detectors, was tested under different parameters for the excavators’ actions. For this experiment the algorithm uses HOG descriptors, codebook of size 350, and the multi-class SVM classifier. Fig. 14
                            shows that the combination of σ
                           =1.5 and τ
                           =3 results in the highest average accuracy. This means that the temporal scales of features can have a higher influence on average action classification accuracy.

Two types of feature descriptors: (1) HOG and (2) Histograms of Optical Flow (HOF) were tested to determine which one results in a better performance. As illustrated in Fig. 15
                           , while considering the codebook size to be 350, σ
                           =1.5 and τ
                           =3, the HOG descriptors with 180 bins show better performance in comparison to HOF descriptors with 198 bins.

As explained in 3.2 through the k-means algorithm, the distance of each descriptor to each codebook center was computed, and a codebook membership was given to each HOG descriptor. To generate the best codebook, the effect of the codebook size on the average accuracy of the multi-class binary SVM kernels is also studied. Fig. 16
                            shows the classification accuracy vs. the codebook size for the excavator video datasets. As observed, codebook histograms with the size of 350 code words result in the highest action classification accuracy. While for the case of 600 bins, a similar level of accuracy is observed, nonetheless to minimize the computation time, the codebook with 350 codewords is selected.

We have also studied the impact of using different supervised and unsupervised machine learning algorithms. Particularly the multiple linear SVM proposed in our algorithm is compared with Naïve Bayes and pLSA unsupervised algorithms proposed in [2]. As observed in Fig. 17
                           , the performance of the multiple linear SVM is superior to the competing algorithms. This is consistent with our intuition that in the case of construction equipment and their actions where intra-class variably is significant, the supervised SVM classifier algorithm should perform better.

This study presented the first comprehensive video dataset for action recognition of excavator vs. dump truck in earthmoving operations. It also presents a new method that can automatically classify a video which contains only one execution of an equipment action into its appropriate action category. The average accuracy of the action classification obtained for both excavator and dump truck video datasets is 86.33% and 98.33% respectively. This performance is comparative to the state-of-the-art in both computer vision and AEC communities [2,40]. In particular it shows that the multiple binary SVM classifier outperform generative pLSA or Naïve Bayes classifiers used in the state-of-the-art methods. This is both due to over-fitting, as well as the poor performance of the unsupervised methods in segmenting the codebooks into distinct action classes which is due to the large intra-class variability in visual appearance of equipment actions. The presented results show the robustness of the proposed method to dynamic changes of illumination, viewpoint, camera resolution, and scale changes as well as static and dynamic occlusions. The minimal spatial resolution of the equipment in the videos [in the range of (∼80–190)×(∼80–190) pixels per equipment] promises the applicability of the proposed method for existing site video cameras.

While this paper presented some initial steps towards processing site video streams for the purpose of action recognition, several critical challenges remain. Some of the open research problems for our community include:
                        
                           •
                           
                              Action recognition in long video sequences: Recognizing equipment actions in long sequences of video is a difficult task as (1) the duration of actions are not pre-determined and (2) the starting point of actions are unknown. The action recognition algorithm presented in this paper is only capable of accurately recognizing actions when the starting point and duration of each action are known as prior knowledge. To automatically and accurately recognize the starting point and the duration of each equipment action, more work is needed on the temporal detection of each action’s starting points and duration with reasonable accuracy.


                              Multiple equipment tracking and localization: Action recognition for multiple equipment, requires precise 2D detection, tracking, and localization of equipment in the video streams. Robust detection, tracking, and 2D localization could also enable tracking trajectory of equipment in 3D which could be beneficial for proximity analysis purposes. It further enables the action recognition to be limited to certain regions in the video streams, further minimizing the effect of noise caused by (1) lateral movement of the camera, (2) dynamic motions of foreground (e.g., grass or vegetation) or background (e.g., offsite pedestrians or moving vehicles), and finally (3) spatio-temporal features detected around the moving shadow of the working equipment.


                              Variability in equipment types and models: Accuracy of action recognition is an important concern for applications such as equipment productivity assessment. As a result, comprehensive dataset of all types and models of equipment from all possible viewpoints is required for model training purposes. The dataset presented in this work only includes two types of equipment from six different manufacturers. Development of larger datasets is still needed.


                              Detection of idle times: In this paper, it is assumed that the idle times can be easily distinguished in cases where no spatio-temporal features are detected or there are detected in low numbers. Given typical non-working short time periods between equipment actions and possible noise in site video streams, it is important to conduct further studies to investigate the reasonable time periods and minimal spatio-temporal features that can be considered as idle times.

Finally, from a practical perspective, it is most likely that a combination of multiple sensing technologies is necessary answer all practical issues toward automating action recognition. For instance, it is challenging for vision systems to operate in foggy environments or under very heavy rain. In this paper, we presented a vision-based approach that can provide a low-cost solution that can be used under many normal operation conditions, which can be complementary to GPS and telemetric sensory. If the vision-based method is robust, the proposed method – as a complementary solution – will also be reliable to provide more detailed assessment on site activities. In terms of site infrastructure monitoring, our proposed method of vision-based analysis requires cameras to be installed over tripods and be located rather closer to the operation (∼20–100m range). In this sense, similar to the current practice of surveying, the field engineers or the operators of such technologies, would be able to locate the single camera or multiple cameras in locations where most of the operation can be observed. Locating surveying cameras for many operations is a common practice that can also be directly applied to our vision-based solution.

@&#CONCLUSION@&#

This paper presents a new method for action recognition of earthmoving equipment from fixed video cameras. The experimental results with average accuracies of 86.33% and 98.33% for excavator and truck action recognition respectively hold the promise for applicability of the proposed method for automated construction activity analysis. The robustness of the proposed approach to variations in size and type of construction equipment, camera configuration, lighting condition or presence of occlusions further strengthens the proposed method. Successful execution of the proposed research has potential to transform the way construction operations are currently being monitored. Construction operations will be more frequently assessed through an inexpensive and easy to install solution, thus relieving construction companies from the time-consuming and subjective task of manual method analysis of construction operation or installation of expensive location tracking and telematics devices.

The current model is capable of recognizing single actions of the construction equipment for a given video captured from varying viewpoints, scales, and illuminations. In order to provide a comprehensive method for automated productivity analysis, future work will include action recognition in long video sequences, multiple equipment tracking and localization, detection of idle times, and improving the dataset for better consideration of possible variability in equipment type and model. As part of a larger research project, these are currently being explored.

@&#ACKNOWLEDGEMENTS@&#

The authors would like to thank the Virginia Tech Department of Planning, Design and Construction, as well as Holder and Skanska construction companies for providing access to their jobsites for a comprehensive data collection. The support of RAAMAC lab’s current and former members, Chris Bowling and David Cline, Hooman Rouhi, Hesham Barazi, Daniel Vaca, Marty Johnson, Nour Dabboussi, and Moshe Zelkowicz is also appreciated. The work is supported by grant from Institute of Critical Technologies and Applied Science at Virginia Tech. The work is also partly supported by “el Patrimonio Autonomo Fondo Nacional de Financiamiento para la Ciencia, la Tecnologia y la Innovacion, Francisco Jose De Caldas” under Contract RC No. 0394-2012 with Universidad del Norte.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.aei.2013.09.001.


                     
                        
                           Supplementary video 1
                           
                        
                     
                     
                        
                           Supplementary Figure 1
                           
                        
                     
                  

@&#REFERENCES@&#

