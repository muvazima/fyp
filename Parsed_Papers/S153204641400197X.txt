@&#MAIN-TITLE@&#Automated curation of gene name normalization results using the Konstanz information miner

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           False positive removal after gene recognition and normalization.


                        
                        
                           
                           Integration of existing annotations, acronym resolution and classification.


                        
                        
                           
                           Based on well established technologies in pharmaceutical industry.


                        
                        
                           
                           Improvement in precision while keeping recall high.


                        
                        
                           
                           We propose to use existing gene taggers to create corpora for common problematic cases.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Gene name normalization

KNIME

Text mining

Acronym expansion

Gene specific F-score

@&#ABSTRACT@&#


               
               
                  Background
                  Gene name recognition and normalization is, together with detection of other named entities, a crucial step in biomedical text mining and the underlying basis for development of more advanced techniques like extraction of complex events. While the current state of the art solutions achieve highly promising results on average, performance can drop significantly for specific genes with highly ambiguous synonyms. Depending on the topic of interest, this can cause the need for extensive manual curation of such text mining results. Our goal was to enhance this curation step based on tools widely used in pharmaceutical industry utilizing the text processing and classification capabilities of the Konstanz Information Miner (KNIME) along with publicly available sources.
               
               
                  Results
                  
                     F-score achieved on gene specific test corpora for highly ambiguous genes could be improved from values close to zero, due to very low precision, to values >0.9 for several cases. Interestingly the presented approach even resulted in an increased F-score for genes showing already good results in initial gene name normalization. For most test cases, we could significantly improve precision, while retaining a high recall.
               
               
                  Conclusions
                  We could show that KNIME can be used to assist in manual curation of text mining results containing high numbers of false positive hits. Our results also indicate that it could be beneficial for future development in the field of gene name normalization to create gene specific training corpora based on incorrectly identified genes common to current state of the art algorithms.
               
            

@&#INTRODUCTION@&#

@&#BACKGROUND@&#

Gene mention detection and subsequent normalization is a crucial step in text mining applications within the biomedical and pharmaceutical domain. Together with detection of other named entities it is the driver of more recent tasks in natural language processing like event extraction [1,2]. The performance of gene normalization is usually measured by the F-score reached on the document set developed for the BioCreative II gene normalization task [3]. Geno by Wermter et al. [4] and GNAT by Hakenberg et al. [5] are probably the best approaches currently available and reach an F-score of about 86% on the BioCreative II corpus. Recently, Li et al. even reported experimental results showing an F-score>90% on this data set [6]. From a practical perspective, two additional aspects have to be considered. First of all, those values present an average between genes, for which the approach works extraordinarily well, and genes where precision and recall are way below those numbers. Secondly, even an F-score of 86% might result in a potentially high number of information that is missed vs. a still significant number of false positive hits that has to be curated manually. Apart from gene names that also stand for other biomedical entities like diseases or cell lines, the problem is mainly caused by ambiguous abbreviations. For example the synonym “SAH” for gene id 6296, might not appear as a problem in a very broad context. However, in the context of “hemorrhage” a huge number of false positive hits for SAH – being the abbreviation of “subarachnoid hemorrhage” – might hide more relevant results. Such problems will not be detected, when testing against a set like BioCreative II, as the problematic gene might be mentioned with a low frequency or not at all, as in the case of SAH. The aim of this study was to propose a method for filtering these typical false positive hits, based on tools widely used in pharmaceutical industry. Most uses cases for gene mention detection and gene name normalization in scientific literature still rely on articles that are listed in PubMed [7]. Therefore, a lot of manually curated information is publicly available. This still holds true to a certain extent when full text articles are investigated. Many articles can be mapped to their citation in PubMed and thus again to the manual annotation, e.g. via the digital object identifier (DOI) [8] using tools like the National Center for Biotechnology Information (NCBI) ID converter [9]. A direct use of available human-curated information is not feasible for the aforementioned gene name normalization algorithms that should be applicable to any kind of text. However, as our approach is designed to solve a practical problem based on PubMed listed articles, this manual annotations are indeed a valuable source of information. As described above, the problem usually comprises a large number of abstracts containing the same ambiguous synonym of a single gene. Datasets like BioCreative II are not suitable to simulate that situation. We therefore manually created training and test data sets meeting this requirement. We chose the Konstanz Information Miner (KNIME) [10] as the underlying platform for implementation as it is freely available but is also used within the pharmaceutical industry. Thus, it allows for easy integration of our workflow in existing data analysis pipelines. It provides a graphical interface where tabular data is handled via a series of interconnected “nodes” that wrap functions to process the incoming data. These nodes are organized within workflows, i.e. sets of connected nodes usually starting with a node for data retrieval or reading, followed by data processing nodes and completed by output or visualization nodes in the end. KNIME is already widespread mainly within chemistry for dealing with large datasets and several extensions for various chemical tasks have been developed over time. Only very recently an extension for text processing has been added [11] and enables KNIME to be used in natural language processing tasks. We also focused in this study on using publicly available resources wherever possible. Similar to the approach by Wermter et al. [4] it was our intention to minimize the necessary manual effort to curate any kind of dictionaries. We developed a workflow consisting of two major steps incorporating and extending some concepts introduced earlier in gene normalization algorithms [12]. Step 1 aims at the identification of unambiguously true positive or clearly false positive hits by rather strict string matching to dictionaries, resolution of abbreviations and identification of existing manual annotation in the public domain. In step 2, a support vector machine classifier for a specific gene is trained with the two sets created in step 1 in order to classify the remaining hits. In contrast to other approaches, we also use the context of gene specific false positives in addition to simple filtering of results containing blacklisted terms.

@&#RELATED WORK@&#

Gene name normalization usually comprises several steps: Recognition of possible gene mentions, generation of candidate pairs for normalization and disambiguation, i.e. choosing the correct normalized form. Additionally, most algorithms include a filtering step to remove the aforementioned false positive hits. Li et al. [6] use a Wikipedia based filter to remove mentions of gene families, which were considered false positives in gene normalization tasks, as it is not possible to assign a unique identifier. Wermter et al. [4] employed a similar approach, but enriched the Wikipedia derived blacklist with several MeSH (Medical Subject Heading) [13] terms and protein family as well as complex annotations taken from the GENIA corpus [14]. These filter steps, however, were mostly introduced to meet the requirements of the task for comparability, but exclusion of hits for protein families does not remove the more problematic false positive hits described above. Hu et al. [12] take the Wikipedia based filter only as an addition to a machine learning approach, generating a confidence score for each gene mention. They combine this method with a final quality assessment via cosine similarity of context vectors. In contrast to those approaches, GNAT by Hakenberg et al. [5,15] introduces filtering already before the disambiguation step. Their algorithm includes resolution of ambiguous abbreviations in a training data set, both locally and on abstract level, to calculate likelihoods of hits being false positive dependent on their local context. They additionally also employ blacklists for unspecific gene names. Other than protein families, the lists also include amino acids, diseases, tissues and species. Finally, GNAT contains a heuristic approach, to accept gene synonyms that are common English words or biomedical terms only when an additional unambiguous synonym is present. A common scheme among these filter steps is that false positive hits are simply discarded and the information contained within their context is not used in a gene specific way. In addition to these existing false positive filters, we propose to include a classification step that explicitly uses the context of typical false positives for a specific gene.

@&#METHODS@&#

The first step in our workflow was re-introduction of already available manual annotations linking articles to gene information, as provided by the NCBI and the Universal Protein Resource (UniProt) [16]. We also made use of the corresponding MeSH (Medical Subject Heading) annotation corresponding to the gene of interest. While those sources are far from complete, they should comprise a reliable dataset for identification of correct gene mentions. This part of the curation workflow, as shown in Fig. 1
                         also included slightly fuzzy dictionary matching of correct gene names via a Lucene index [17] in order to identify articles clearly mentioning the candidate gene. On the other hand, we identified false positive hits at that point via blacklists and acronym expansion.

After these steps, the initial text mining result set had been split into three parts: Set 1 comprises a set of abstracts where the gene was identified correctly (assuming that manual curation in the employed databases was correct) and set 2 should contain only clearly false positive hits. For set 3, however, no decision had been made yet. Therefore the second round of disambiguation makes use of built-in functionalities of KNIME and several publicly available extensions. Context vectors were created from the abstracts in set 1 and 2 to train a gene specific support vector machine (SVM) classifier. This classifier was used to create datasets 3.1 and 3.2, as shown in the diagram in Fig. 1 comprising “probably true positive” or “probably false positive hits”, respectively.

Linguamatics I2E version 4.1 [18] was used to initially detect possible mentions of a gene of interest within PubMed abstracts. I2E allows for setting a threshold for disambiguation. This threshold was set to the lowest possible value for this initial step in order to increase recall to a maximum. We intentionally ignored the reduced precision and thus a high number of false positive hits had been included.

The gene dictionary used at several points in the curation workflow was derived from the synonyms of the human, mouse and rat gene variants as included in the NCBI gene database [19]. These synonyms were extracted from the columns “description”, “full name from nomenclature authority” and “other designation” in the “gene_info” file. No further modifications to the terms had been made, apart from exclusion of synonyms that are abbreviations on its own, like the synonym SAH, for the gene with the identifier 6296.

Two obviously helpful resources for PubMed abstracts are the MeSH annotations and the gene2pubmed mapping available at the NCBI [20]. We additionally used the manually created mapping between proteins, gene symbols and corresponding PubMed identifiers (PMIDs) contained within the UniProt database [16]. The PMIDs of gene mention containing articles were identified and mapped to those lists. Moreover, the articles contained in these lists were used to identify the most frequent MeSH terms associated with literature about the gene of interest. From this subset terms sharing some string similarity (scaled Levenshtein distance>0.9) with the synonyms in the gene dictionary were chosen. Otherwise very general and widely used MeSH terms like e.g. “Human” tend to be ranked highest. For genes sharing the same abbreviation for their official gene symbol and the corresponding protein entry on Uniprot, the MeSH Supplementary Concept “GeneSymbol protein, human” was also considered to be a correct manual annotation, as it is likely that an unambiguous connection between this gene and only one protein can be assumed. The subset of abstracts containing correctly identified genes was enriched with abstracts containing these MeSH annotations.

An additional mapping of PMIDs to the gene2pubmed entries for bacterial and viral genes was used in order to move those articles into the false positive set. Especially highly ambiguous synonyms like p40 or gp55 that already refer to several different human genes, additionally also refer to viral and bacterial genes. Thus this step helps to reduce false positives without the risk of losing a correctly identified human gene.

Acronyms were expanded in several steps. In a first step, the text is searched for known meanings of the detected acronym. This blacklist was compiled from several public domain sources. These include Allie [21,22], a database developed by Integrated Database Project, MEXT, Japan and a list compiled using the AcroTagger algorithm [23] on MEDLINE abstracts.

To expand additional acronyms, regular expressions were generated at runtime to resolve patterns like e.g. abbreviations followed by brackets or abbreviations in brackets. For the former case, the text within brackets was considered to be the meaning of the acronym when all letters of the acronym were contained (numbers, articles and short prepositions were ignored). Several possible alternatives were considered for the latter case. Simple matching between the starting letters of words preceding or following the brackets and the acronym letters was employed first. Matching hits were assumed to be correct interpretations of the acronym. This step also allowed for matches containing a lower number of words than inferred by the abbreviation similar to the search for head patterns in the BADREX approach [24]. If the regular expression did not match, the last word preceding or the first word following the bracket was examined and considered correct if the letters of the acronym had been found in the correct order. This helped to resolve acronyms like MAX, that usually stands for words like “maximum” or ClU for “5-chlorouracil”.

We also used the gene dictionary described above to identify articles where the correct long form of a gene abbreviation was mentioned. Indexing was performed via the KNIME Table Indexer node which allows for searching using a Lucene based query syntax [17]. A slightly fuzzy Lucene query had been employed (Lucene query: GeneSynonym∼0.95). This still allowed for matching small writing differences in rather long synonyms. For example, “dehydro-epiandrosterone preferring” or a misspelling like “dehydorepiandrosterone preferring” would still be accepted when “dehydroepiandrosterone preferring” was the query phrase. However, for shorter synonyms only exact matches were accepted. If a correct hit for the gene had been found, a previously identified blacklist entry was ignored. This is especially important for those genes whose synonymous abbreviations were derived from related disorders. In those cases, ambiguous and unambiguous synonyms of a gene can be found within in the abstract, with different meanings. For acronyms resolved via a regular expression, the identified expanded form was additionally cross-checked for string similarity with gene dictionary entries and not considered a false positive hit if this similarity was high, defined by a scaled Levenshtein distance>0.5 [25] and the abstract was included in the “unclassified” subset 3 for further classification.

Another practically oriented addition to the workflow is the reduction of confidence values, calculated by the subsequent classification step described below, for articles mentioning plant related MeSH terms (like e.g. Arabidopsis). A positive classification bias is instead calculated for articles mentioning the official human gene symbol within a comma separated list of potential gene symbols. This kind of lists is often found in gene expression studies. For these publications, however, both our filter steps would fail. In most cases there is neither MeSH annotation for all genes with altered expression nor an expanded form of the gene symbol contained in the abstract. The context based classification will also fail, as the context is not gene specific.

These lists were identified by the regular expression

/([A-Z0-9-⧹s⧹(⧹)(orf)(rs)]+⧹s∗,⧹s∗){2,}[A-Z0-9-⧹s⧹(⧹)(orf)(rs)]+⧹s∗,?⧹s∗(and [A-Z0-9-⧹s⧹(⧹)(orf)(rs)]+)?/

The expression for a single list item [A-Z0-9-⧹s⧹(⧹)(orf)(rs)]+ takes into account that most human gene symbols contain only uppercase letters, numbers and hyphens. Thus one or more of these characters (indicated by the “+” after the square brackets) is required. It also allows for whitespaces (⧹s) and brackets within the symbol. A typical example of such lists can be found in the article with PMID: 21152965 (“PICALM, CR1, CLU, PCK1, and ZNF224”). These lists also tend to contain genomic locations (so called “open reading frames”) which are typically notated using upper case letters, numbers and the sequence “orf”, e.g. “TDP2, ACOT13, C6orf62, FAM65B, and CMAHP” in PMID: 24509779. We furthermore included the sequence “rs” in our regular expression to identify lists containing single nucleotide polymorphisms (SNPs) whose nomenclature usually follows the standard “rs” (for “reference SNP”) followed by a sequence of digits (e.g. PMID: 24042540: rs1464510-LPP, rs1881457-IL13, rs2104286-IL2RA). The overall expression matches lists containing two or more items ({2,}) followed by a comma and optional whitespaces (⧹s∗,⧹s∗) and another item only optionally followed by a comma or whitespaces (⧹s∗,?⧹s∗). The last part of the regular expression contains the string “and” to allow for the typical ending of comma separated lists in English language, distinguishing the last bullet point by an “and” as seen in the first two examples. This entire group (indicated by the braces) is optional in order to match lists without this ending, like in the third example.

After detection of the list, the official gene symbol was searched within the identified section, allowing only for hyphens or white-spaces between letters and numbers (e.g. MOXD1, MOXD 1 and MOXD-1).

All abstracts were processed via the KNIME text processing nodes. The sub-workflow comprised the standard nodes contained within in this package for part-of-speech tagging, bag-of-word creation, punctuation removal and the built-in list for English stop word filtering. We additionally filtered out words shorter than 3 characters and applied the Porter stemming algorithm [26]. However, no dedicated biomedical named entity recognition step was included. Up to 10 keywords per abstract were subsequently determined using the KeyGraph [27] implementation included in the KNIME text processing package. Vectors were created using the most abundant keywords in set 1 and 2 and acronyms that had been recognized as possible gene mentions were excluded. This was important in order to classify depending on the context of ambiguous acronyms only. For classification of set 3, a support vector machine classifier was trained employing the vectors created from set 1 and 2. We made use of the Weka integration for KNIME and the model was trained utilizing the libsvm [28] implementation within this package, and in particular utilizing the AdaBoostM1 node which additionally wraps classifier training in a boosting algorithm [29]. We chose the nu-SVM algorithm [30] from the various options available in libsvm, and thus, classification is not possible if set 1 or 2 are empty. However, this is likely to happen for rather small datasets, e.g. for genes that have not been deeply investigated, yet. Although our approach was mainly aimed at datasets that are relatively large due to a high number of false positive hits, we implemented fallback solutions for these harder to classify small sets. As a first fallback option the one-class SVM algorithm [30] was used, when at least one of the two sets was populated. This algorithm allows for SVM classification if only one class is available. In this case the algorithm needs to choose whether a hit belongs to this class or not in contrast to other versions that are used to choose between various classes. If this classification failed as well, we did at least a simple search for the official gene symbol with the modifications described above for identification in lists in Section 2.6. This is a plausible fallback option from a practical point of view, as we are probably looking at rather unknown genes in that case. Therefore, researchers might have found them incidentally (e.g. in expression analyses) and thus use the official gene name, and not a trivial name established in their lab or community. The latter matching step for official gene symbols was also used in cases where all abstracts were classified as false positive hits, assuming that this is rather unlikely in a larger dataset and thus this step might help to increase recall.

Publicly available corpora were not suitable to simulate the problem of removing a huge number of false positives for a single gene. We therefore created a training and test corpus using Linguamatics I2E to retrieve PubMed abstracts mentioning any synonym of one single gene, ignoring any calculated confidence whether a synonym might indeed stand for a gene. The synonyms included all abbreviations for a specific gene contained in the NCBI gene database and all long forms of the gene and protein names listed there with some modifications introduced by I2E. This procedure was chosen to make sure that false positive hits were included in the datasets. The genes were chosen from real-world examples where those genes appeared as problematic. We verified that these genes were underrepresented in the BioCreative II dataset. In the manual curation step, a gene was accepted as long as a human, mouse or rat variant of the gene was mentioned.

The training dataset comprised the following genes:
                           
                              •
                              gene id 1075 (ambiguous synonyms: PLS, HMS, PALS, JP, JPD, 6132 abstracts);

gene id 84867 (STEP, 460 abstracts);

gene id 4149 (MAX, 1202 abstracts).

The test dataset comprised the following genes:
                           
                              •
                              gene id 1191 (CLU,CLI, 3965 abstracts);

gene id 6296 (SAH,SA, 923 abstracts);

gene id 26002 (MOX, 391 abstracts);

gene id 6822 (DHEAS, STD, 1008 abstracts);

gene id 6742 (SSBP,198 abstracts).

In order to obtain additional information about the overall performance of the two filter steps, we created a set of PubMed abstracts, for 862 random genes from real-world examples, containing at least 200 abstracts per gene. This set was created as described above but not curated manually and only used to investigate the average proportion of abstracts in set 1, 2 and 3 after the first filter step.

@&#RESULTS AND DISCUSSION@&#

The individual steps of the curation workflow were chosen and thresholds for fuzzy string matching and SVM classification were tuned using the highly ambiguous genes in the training dataset described in Section 2.8. F-scores were calculated according to Eq. (1).
                           
                              (1)
                              
                                 F
                                 =
                                 2
                                 
                                    
                                       precision
                                       ·
                                       recall
                                    
                                    
                                       precision
                                       +
                                       recall
                                    
                                 
                              
                           
                        
                     

Precision and recall in Eq. (1) were defined as shown in Eqs. (2) and (3):
                           
                              (2)
                              
                                 precision
                                 =
                                 
                                    
                                       true positives
                                    
                                    
                                       true positives
                                       +
                                       false positives
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 recall
                                 =
                                 
                                    
                                       true positives
                                    
                                    
                                       true positives
                                       +
                                       false negatives
                                    
                                 
                              
                           
                        
                     

For testing, we picked several examples of genes that occurred in text mining results focused around mapping of genes to diverse topics. Those genes were characterized by resulting in rather high numbers of false positive hits compared to correct hits in these result sets. Due to this setup, the calculated F-scores are gene specific and cannot be directly compared to F-scores calculated for example in the BioCreative shared tasks. In order to investigate possible improvements achievable with our workflow, we calculated precision, recall and F-score with several different settings in I2E, either optimized for one parameter (precision or recall) and a specific gene or a value working equally well for different genes. For the three training data sets our KNIME based approach achieved F-scores of 0.96 (gene id: 1075), 0.94 (gene id: 84867) and 0.92 (gene id: 4149).

The results for precision, recall and overall performance on the test data sets are summarized in Table 1
                        .

The strength of the automated curation approach can be seen especially in cases where the initial gene name normalization fails largely. We tested the approach on the gene with identifier 6296. Unfortunately, this gene is named SAH in older publications, and the synonym should thus not be excluded generally. However more recent publications do not use this synonym anymore. Limiting our search to publications from the years 2011 and 2012, we found 1258 potential occurrences of the gene. However, 1257 times SAH was mentioned, referring to something like the disorders “subarachnoid hemorrhage”, ”severe alcoholic hepatitis” or the substance “S-adenosyl-homocysteine”. Even when setting the threshold in I2E to optimized precision, we ended up with 303 hits, at least including the one correct mention. With the standard setting, the correct hit was even hidden in 975 hits. These false positive hits are often created by sections like “SAH stimulates tyrosine kinase pathway to increase intracellular Ca2+” in the abstract with PMID:9886354 or “SAH inhibits isoprenylcysteine carboxyl methyltransferase” in the fulltext article with PubMed Central ID 1395315 where it would be even for a human curator almost impossible to tell, whether SAH refers to a gene/protein in these sections or not. After automated curation, however, only the correct hit was left. Another example was the gene with identifier 26002, where most false positive hits are caused by the synonym MOX, also referring to the antibiotic moxifloxacin. In this case, we could improve from an F-score of 0.15 up to a value of 0.91.

Interestingly, the automated curation could also improve overall performance in cases where initial quality was already comparably good or at least average. For the gene 1191 (CLU, CLI) we could increase the F-score from 0.95 to 0.98 and for gene 6742 (SSBP) from 0.80 to 0.88. It should be noted that for both genes precision was significantly higher after automated curation, when compared to the I2E results alone tuned to a comparable recall (Table 1).

After the first filter step (i.e. acronym expansion, mapping of manual annotations and string matching of correct synonyms and blacklists) we observed an average proportion of 31% (median) of abstracts in set 3, i.e. unclassified. An important finding was, that for 418 out of the 862 genes in this set, the number of abstracts in set 1 or 2 was clearly biased into one direction (>10-fold). The subsequent classification might thus tend to favor the over-represented class. This behavior should be beneficial, as e.g. a low number of false positive hits in the first step, might indicated that the gene is rather unproblematic and most mentions are probably true.

The SVM classification step alone achieved an average F-score of 0.64 with a precision of 0.58 and a recall of 0.72 on subset 3 for the 9 genes investigated in this publication. It has to be noted, that “easy” to classify gene mentions had been already tackled by the first filter step, and the classifier only deals with the remaining harder to identify examples.

@&#LIMITATIONS@&#

The first step of our approach relies partially on the availability of manually curated data. Therefore, e.g. a low number of entries in the gene2pubmed file for a specific gene could be potentially problematic when a large number of mentions is detected for this gene. We created a subset from the large 862 gene dataset, containing only genes characterized by a relatively low number of gene2pubmed entries, while a potential gene mention was detected in a large number of abstracts (⩾100∗number of gene2pubmed entries). We found 48 genes in our dataset fulfilling these criteria. Interestingly, this seems to be a good indicator of genes, with a highly ambiguous synonymous abbreviation. Thus, a much higher number of hits had been sorted out by the acronym resolution step, shown by a clearly increased average size of set 2 (false positives) from 8% to 67%. We are therefore confident that a low availability of manually curated data can be compensated by the other steps in many cases.

For gene identifier 6822, we found a clear limitation of the approach, as we did not reach better results than the original gene name normalization in terms of recall. This was due to the fact that the classification step failed for most abstracts, as the major false positive hit DHEAS, used as abbreviation for the hormone dehydroepiandrosterone, was also found in abstracts mentioning other synonyms of the gene, as the encoded protein (sulfotransferase) is indeed processing this compound [31]. Nevertheless, when tuning the initial gene recognition step to maximum precision, we still reach better precision (1 vs. 0.4), while hardly sacrificing recall (0.59 vs. 0.62, or 22 correct hits vs. 23), resulting in an F-score of 0.75 (vs.0.49) after automated curation (Table 1). Therefore our goal of reducing false positive hits without reducing recall was almost reached even in that case.

The slightly fuzzy string matching in identification of the possible expanded forms of gene acronyms results in an additional intrinsic limitation: The automated curation approach does not improve the performance of the initial gene name normalization with regard to several members of a protein family, e.g. “estrogen receptor 1” would not be additionally distinguished from “estrogen receptor 2” if initial disambiguation had failed. The algorithm would assume both suggested options are a true positive hit, as long as “estrogen receptor” is mentioned.

We did a cross-check for the chosen examples with a different gene name recognition and normalization system. Due to its public availability we did a search within the evexdb database [32] for the genes 6822, 26002 and 6296. This database uses state of the art gene mention recognition via BANNER [33] and gene name normalization via GenNorm [34]. We found that essentially the same false positive hits had been detected for those genes, as in our setting. Actually none of the detected regulation events for 26002 and 6296 was a true positive hit, independent of the given confidence, when accessing the database in December 2013. For 6822, among 8 detected regulation events only one contained a correct hit for the sulfotransferase, while the others were actually describing events for the hormone DHEAS. These results confirm that the examples identified in this study are realistic problems occurring in state of the art gene name normalization that can be addressed by our approach.

@&#CONCLUSIONS@&#

We could show that KNIME and its recently added text processing capabilities can be used to drastically reduce the number of false positive hits for gene name normalization after possible hits had been narrowed down to a smaller set of genes. The presented workflow should be particularly helpful for usually problematic gene names causing many false positive hits due to ambiguous acronyms. Even if the secondary classification step fails, the initial processing step comprising acronym expansion and dictionary matching, can help to reduce the size of the remaining dataset that has to be curated manually. Thus, this approach can be a time saver in manual curation tasks or might even reveal interesting information that might have been ignored otherwise due to being obfuscated in a huge number of false positive hits, as shown for the example SAH. This study also demonstrates that it could be helpful for the further development of algorithms for gene name normalization to use current state of the art approaches in order to identify common errors and setup solutions to specifically deal with these issues. Our automated curation approach could also be helpful in the easier creation of such novel training data sets.

Matthias Zwick is an employee of Boehringer Ingelheim Pharma GmbH & Co. KG which is a research-based pharmaceutical company that discovers, develops, manufactures, and markets prescription medicines for humans and animals.

@&#ACKNOWLEDGMENTS@&#

We thank Dr. Katrin Fundel-Clemens for compiling parts of the blacklist of expanded abbreviations and Dr. Thorsten Schweikardt for very valuable discussions during workflow development.

@&#REFERENCES@&#

