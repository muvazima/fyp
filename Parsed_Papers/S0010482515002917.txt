@&#MAIN-TITLE@&#An experimental comparison of feature selection methods on two-class biomedical datasets

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Ten feature selection methods are compared using stability and similarity measures.


                        
                        
                           
                           Univariate FS perform better than multivariate FS for high dimensional datasets.


                        
                        
                           
                           Multivariate FS slightly outperform univariate FS for complex and smaller datasets.


                        
                        
                           
                           Most stable appears to be entropy based FS.


                        
                        
                           
                           FS yielding the highest prediction performance are MRMR and Bhattacharyya distance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Feature selection

Stability

Classification performance

Univariate FS

Multivariate FS

@&#ABSTRACT@&#


               
               
                  Feature selection is a significant part of many machine learning applications dealing with small-sample and high-dimensional data. Choosing the most important features is an essential step for knowledge discovery in many areas of biomedical informatics. The increased popularity of feature selection methods and their frequent utilisation raise challenging new questions about the interpretability and stability of feature selection techniques. In this study, we compared the behaviour of ten state-of-the-art filter methods for feature selection in terms of their stability, similarity, and influence on prediction performance. All of the experiments were conducted on eight two-class datasets from biomedical areas. While entropy-based feature selection appears to be the most stable, the feature selection techniques yielding the highest prediction performance are minimum redundance maximum relevance method and feature selection based on Bhattacharyya distance. In general, univariate feature selection techniques perform similarly to or even better than more complex multivariate feature selection techniques with high-dimensional datasets. However, with more complex and smaller datasets multivariate methods slightly outperform univariate techniques.
               
            

@&#INTRODUCTION@&#

Classification tasks in which the number of features is much larger than the number of subjects are becoming more and more abundant in many research areas. High dimensional datasets frequently occur in text processing, combinatorial chemistry or bioinformatics. These datasets can contain tens of thousands of features while having available only hundreds (or usually less than hundred) samples. High dimensionality can negatively impact the performance of classifier by increasing risk of over-fitting and prolonging the computational time. Moreover, there are the applications where one's intention is to identify a small group of features that may be descriptive for some phenomenon or may shed the light on some underlaying process.

There are two approaches to reduce the dimensionality of dataset: feature extraction and feature selection [1–3]. In case of the feature extraction, the new basis is chosen for the data and new features are derived from the original input. On the other hand, the main goal of the feature selection techniques is to reduce effects of high dimensionality on dataset and to find a subset of features from the entire feature set that can efficiently describe the data. Reducing the dimensionality of data helps to avoid the effects of the curse of dimensionality 
                     [4] that seriously degrades the ability of learning algorithms to develop robust models. As the reduced subset is usually significantly smaller than the set of the input features, the computation time of subsequent analysis is greatly reduced. In this study, we will focus only on feature selection.

When facing the issue of choosing a feature selection (FS) algorithm for some machine learning problem, the usual approach is first to try the simple univariate techniques and if these do not provide desired result move on and try more complex FS methods. In fact, there is no procedure or systematic approach for choosing the most suitable FS method for particular problem. The increasing number of FS techniques that are indeed very effective and sophisticated makes the problem of selecting the most suitable FS even more apparent [5–7]. The only available guidelines are previous experiences and comparative studies from the literature [8,9]. When evaluating the suitability of FS method we are concerned with two aspects: (i) stability of FS i.e. how the output of FS algorithm changes when the data change [10] and (ii) how efficiently data are described by the selected subset of features, i.e. what is the influence on prediction accuracy. The motivation for evaluating stability comes from the domain experts requiring a small set of discriminatory features that are robust to variations in the training dataset [10,11].

There are several studies comparing FS techniques from different aspects, however the literature on the subject is rather limited. The first study introducing the term stability of FS that also evaluated stability of five FS methods was done by Kalousis in [10]. Similarly, Molous et al. [12] evaluated stability of five univariate FS techniques. These studies focus mainly on univariate methods and do not analyze new and more advanced techniques. A deeper analysis of the performance of feature selection in high-dimensional setting with focus on stability was provided by [13], however, here the authors again limited investigation to the univariate FS methods. Recently, Haury et al. [14] compared several FS methods on datasets for breast cancer prognosis finding that complex wrapper and embedded methods generally do not outperform univariate filter methods. Similar results, confirming competitive performance of the simple univariate techniques were presented by Lai et al. [15]. On the contrary, Wang in [16] proposed the method involving reduced exhaustive search outperforming simple filter methods. These studies emphasize the fact that feature selection still does not have a unique solution and comparative studies are important for understanding FS methodology.

Other authors analyzed the similarity of the different FS methods and attempted to make a link between stability and influence of FS on prediction performance [17–19]. Within this study, we provide extensive comparison of filter methods for FS that have not been analyzed before on different binary biomedical databases.

When evaluating the stability of FS one needs to consider appropriate stability measure and procedure. It was shown that some stability measures are not suitable for large datasets and variable subset size [11,20]. This and other aspect of feature selection are described in several theoretical surveys such as [21–23]. We used two established and robust stability measures Kuncheva index and consistency index.

In this paper, we compare ten frequently used FS methods from stability, similarity and prediction performance point of view. Several FS techniques that have been introduced only recently and show promising performance are included in analysis. We used four different state-of-the-art classifiers and eight binary datasets to ensure robustness and reliability of the obtained features.

In general, feature selection algorithms can be categorized as filter, wrapper or embedded method. The filter methods select subset of entire feature set as a preprocessing step, independently of chosen learning algorithm. The wrapper methods utilize the learning algorithm of interest as a black box to score subsets of features according to their predictive power. The embedded methods perform variable selection in the process of training and are usually specific to the given learning machines [1,14,24].

The filter methods are faster and provide better generalization than wrapper or embedded methods since they act independently of the learning algorithm [25]. For the purpose of this work all studied methods are applied as the filter methods, i.e. input to FS is a matrix of features data for a set of samples and output is a subset of features of a user-defined size d. The feature subset is then used to build predictor.

We compared ten popular feature selection algorithms: univariate FS (t-test, Bhattacharyya distance, ANOVA and entropy), tree-based ensemble FS [26], least absolute shrinkage and selection operator (LASSO) [27,28], minimum redundancy maximum relevance (including both mutual information difference (MID) and mutual information quotient (MIQ) schemes [29], iterative Relief (iRelief) [30] and linear support vector machine (SVM). All FS used in this study were implemented in Python using Scikit 
                     [31], MLPY 
                     [32] and SciPy libraries.

The univariate FS approaches rank each feature individually, not taking into account any feature dependencies. This is equivalent to doing a single-feature selection in which, according to some authors, many good features can be removed [1]. On the contrary, several studies confirm that the univariate techniques perform as good as more complex multivariate techniques [14,22]. In particular, we considered the two sample t-test and ANOVA that are among the most widely used techniques in microarray studies. Additionally, the Bhattacharyya distance and relative entropy were used to calculate a distance between the distributions of the two groups

Instead of implementing single FS algorithm, ensemble techniques apply several weak learners that contribute to the final decision. Ensemble FS are suitable for small sample dataset. They provide stable solutions and are robust to over-fitting [33]. We utilized extremely randomized trees as base learners [26].

Randomized LASSO or stability selection is based on subsampling in combination with selection algorithms [28]. The LASSO is computed on each resampling. It penalizes the absolute value of the coefficients that leads to shrunking of some coefficients to zero, which means that the features associated with those coefficients are eliminated.

The first stage of mRMR, the maximum relevance method, selects the best individual features correlated to target classification variable [29]. The features selected according to the maximum relevance method could have a large redundancy. In order to remove redundancy among the features, the minimum redundancy condition is introduced. MID and MIQ represent schemes to combine the relevance and redundancy that are defined using Mutual Information (MI) [29].

Iterative Relief algorithm [30] was developed on the basis of Relief – heuristic FS algorithm [34]. Iterative Relief elegantly handles the issues of Relief when the nearest neighbors of the samples defined in the original measurement space are inappropriate and eliminates the issues with the presence of the outliers and noisy data.

Finally, linear SVM was used to reduce a number of features through selection of non-zero coefficients. Particularly we used scikit-learn [31] implementation of the LIBLINEAR library [35].

For the purpose of our study, we used six high dimensional microarray datasets and two smaller biomedical datasets for differential diagnosis of Parkinson's disease. Our aim was to evaluate diverse databases while stay focused on task of binary classification of datasets where a number of features is higher than a number of samples.

The overview information about dataset are summarized in Table 1
                     . The B2006 dataset [36] is a three-class dataset, however, we followed the approach of the authors of the original paper [36] and pooled class of Crohn's disease and ulcerative colitis together. Similarly to the previous case, G1999 dataset [37] is considered as a two-class dataset obtained by merging ALL-T and ALL-B together. As a result, all datasets used in this study represent binary class classification problem. We focused on binomial datasets since task of binary classification is the most frequent classification task, considering that even some of the multiclass classification problems are often solved by reducing single multiclass classification task into multiple binary classification problems.

All datasets used in this study are publicly available or can be obtained from their owners, so our results can be compared and reproduced by other authors. The further details about datasets together with baseline classification results can be found in references provided in Table 1.

In this section we present and discuss the most significant experimental results. First, we concentrate on stability of the feature selection techniques and analyse how stability behaves for the different sizes d of the selected feature subsets. Then, we evaluate similarity among feature subsets selected by the different FS methods and finally we examine the results of predictive performance of selected feature subset.

The stability of FS algorithm was defined by Kalousis [10] as the robustness of the feature preferences it produces to differences in training sets drawn from the same generating distribution. Stability is an indicator of feature selection reproducibility. The instability of FS technique reduces confidence in the importance of the selected features. A high stability of FS is crucial for the reliable results and equally important as the high classification accuracy when evaluating the FS performance.

There are several reasons for instability of feature selection. One possible cause is that majority of FS algorithms are designed without consideration of stability aspects, and aim for selecting a minimal subset of the features with the highest classification accuracy [21,44]. Another cause is existence of the multiple sets of true markers, i.e. it is possible that for given data there are many markers that are highly correlated with the data. Finally, it is known that very high dimensionality combined with a small sample size causes serious problems in machine learning resulting in algorithm instability. It has been shown that at least thousands of samples are needed to achieve stable feature selection [45].

To measure the stability two aspects have to be considered: the testing procedure and stability measure. We sampled a set of all features to form the system of K feature subsets. Prior to each sampling, the original dataset was randomly permuted and each subset contains 80 % of samples of the original dataset.

In recent years there has been a number of different stability measures implemented to assess robustness of feature selection techniques [10,46,20,47,48,11]. Each of these measures expresses slightly different aspect of the problem, however, there are some properties that are common for the group of measures. Based on these properties, measures can be divided according to the evaluation scope to feature-focused and subset-focused; according to the importance assigned to feature exclusion to selection-registering and selection-exclusion registering, and according to their ability to cope with differently sized subset we differentiate between subset-size-biased and subset-size-unbiased [11]. Since the measures from different groups tend to provide complementary information we decided to use Kuncheva index κ 
                        [20] (subset-focused, exclusion-registering) and weighted consistency index CW 
                        [11] (feature-focused, selection-registering). The κ and CW are defined as follows.

Let 
                           F
                           =
                           {
                           
                              
                                 
                                    f
                                 
                                 
                                    1
                                 
                              
                           
                           ,...,
                           
                              
                                 
                                    f
                                 
                                 
                                    c
                                 
                              
                           
                           }
                         be the set of all features of cardinality 
                           |
                           F
                           |
                           =
                           c
                         and 
                           S
                           =
                           {
                           
                              
                                 S
                              
                              
                                 1
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 
                                    S
                                 
                                 
                                    K
                                 
                              
                           
                           }
                         be a system of K feature subsets, obtained by applying K times particular FS algorithm on different samplings of data set. Let S
                        
                           id
                         and S
                        
                           jd
                         be the subset of features 
                           
                              
                                 S
                              
                              
                                 id
                              
                           
                           ,
                           
                              
                                 
                                    S
                                 
                                 
                                    jd
                                 
                              
                           
                           ⊂
                           F
                        , where 
                           |
                           
                              
                                 
                                    S
                                 
                                 
                                    id
                                 
                              
                           
                           |
                           =
                           |
                           
                              
                                 
                                    S
                                 
                                 
                                    jd
                                 
                              
                           
                           |
                           =
                           d
                        .

Then, Kuncheva index 
                           κ
                           (
                           S
                           )
                         for a system 
                           S
                           =
                           {
                           
                              
                                 S
                              
                              
                                 1
                                 d
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 
                                    S
                                 
                                 
                                    Kd
                                 
                              
                           
                           }
                        , for fixed subset size 
                           d
                           ≤
                           c
                         is defined as
                           
                              (1)
                              
                                 κ
                                 (
                                 S
                                 )
                                 =
                                 
                                    
                                       2
                                    
                                    
                                       K
                                       (
                                       K
                                       −
                                       1
                                       )
                                    
                                 
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       K
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       =
                                       i
                                       +
                                       1
                                    
                                    
                                       K
                                    
                                 
                                 
                                    
                                       |
                                       
                                          
                                             S
                                          
                                          
                                             id
                                          
                                       
                                       ∩
                                       
                                          
                                             S
                                          
                                          
                                             jd
                                          
                                       
                                       |
                                       ·
                                       c
                                       −
                                       
                                          
                                             d
                                          
                                          
                                             2
                                          
                                       
                                    
                                    
                                       d
                                       (
                                       c
                                       −
                                       d
                                       )
                                    
                                 
                                 .
                              
                           
                        
                     

Similarly, let 
                           N
                           =
                           
                              
                                 
                                    ∑
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    K
                                 
                              
                           
                           
                              
                                 S
                              
                              
                                 i
                              
                           
                         be the total number of occurrences of any feature in 
                           S
                         and Ψ
                        
                           f
                         be the number of occurrences of the feature 
                           f
                           ∈
                           F
                         in system 
                           S
                        . Weighted consistency index CW is defined as follows [11]:
                           
                              (2)
                              
                                 CW
                                 (
                                 S
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          f
                                          ∈
                                          F
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             Ψ
                                          
                                          
                                             f
                                          
                                       
                                    
                                    
                                       N
                                    
                                 
                                 ·
                                 
                                    
                                       
                                          
                                             Ψ
                                          
                                          
                                             f
                                          
                                       
                                       −
                                       1
                                    
                                    
                                       K
                                       −
                                       1
                                    
                                 
                                 .
                              
                           
                        
                     

Both measures introduced above are subset-size-biased, i.e. not suitable for a comparison of the FS methods yielding systems of different size. We employ relative weighted consistency index CW
                        
                           rel
                         to suppress the influence of subset size in system on the measure final value [11]. CW
                        
                           rel
                         is obtained by adjusting CW on its minimal CW
                        
                           min
                         and maximal CW
                        
                           max
                         possible values as
                           
                              (3)
                              
                                 
                                    
                                       CW
                                    
                                    
                                       rel
                                    
                                 
                                 (
                                 S
                                 )
                                 =
                                 
                                    
                                       CW
                                       (
                                       S
                                       )
                                       −
                                       
                                          
                                             CW
                                          
                                          
                                             min
                                          
                                       
                                       (
                                       N
                                       ,
                                       K
                                       ,
                                       F
                                       )
                                    
                                    
                                       
                                          
                                             CW
                                          
                                          
                                             max
                                          
                                       
                                       (
                                       N
                                       ,
                                       K
                                       )
                                       −
                                       
                                          
                                             CW
                                          
                                          
                                             min
                                          
                                       
                                       (
                                       N
                                       ,
                                       K
                                       ,
                                       F
                                       )
                                    
                                 
                              
                           
                        Introducing 
                           D
                           =
                           N
                           
                              mod
                           
                           (
                           c
                           )
                         and 
                           H
                           =
                           N
                           
                              mod
                           
                           (
                           K
                           )
                         the CW
                        
                           rel
                         becomes [49]
                        
                           
                              (4)
                              
                                 
                                    
                                       CW
                                    
                                    
                                       rel
                                    
                                 
                                 (
                                 S
                                 )
                                 =
                                 
                                    
                                       c
                                       (
                                       N
                                       −
                                       D
                                       +
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                f
                                                ∈
                                                F
                                             
                                          
                                       
                                       
                                          
                                             Ψ
                                          
                                          
                                             f
                                          
                                       
                                       (
                                       
                                          
                                             Ψ
                                          
                                          
                                             f
                                          
                                       
                                       −
                                       1
                                       )
                                       )
                                       −
                                       
                                          
                                             N
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       
                                          
                                             D
                                          
                                          
                                             2
                                          
                                       
                                    
                                    
                                       c
                                       (
                                       
                                          
                                             H
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       K
                                       (
                                       N
                                       −
                                       H
                                       )
                                       −
                                       D
                                       )
                                       −
                                       
                                          
                                             N
                                          
                                          
                                             2
                                          
                                       
                                       +
                                       
                                          
                                             D
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

In this section we concentrate on stability of the feature selection techniques and analyse how stability behaves for the different sizes d of the selected feature subsets.

The stability analysis was first performed in terms of κ and CW evaluation. For each dataset, for the different FS techniques, K=500 subsets were generated by random drawing 80 % of subjects from particular database. For each of them FS was performed to select d=100 features from the whole subset. The κ and CW were obtained according to (1) and (2), respectively. The results of analysis are collected in Table 2
                         for κ and Table 3
                         for CW measure. As we can see, the scores for two selected stability metrics, Kuncheva index and weighted consistency index, are consistent, providing some initial confidence in the obtained results.

When comparing the results for different FS techniques, entropy based FS achieved the highest stability values for majority of the investigated databases, followed by other univariate techniques (ANOVA FS, Bhattacharyya distance based FS and t-test FS). Only difference is D2013 database where the highest stability score is obtained for LASSO and mRMR-MID FS techniques.

As a further step, we evaluate FS stability as the function of the number of selected features. To suppress the influence of subset size we use CW
                        
                           rel
                         metric to determine the FS stability in this case. As can be seen from Fig. 1
                        , most of the FS stabilities observed for the investigated databases exhibit very similar trend. The CW
                        
                           rel
                         score is increasing for a low number of features and achieving constant value from 35 to 50 features. Slightly different behavior occurs for the case of the D2013 dataset, where CW
                        
                           rel
                         continues to increase in linear fashion. However, the prediction accuracies for this database reported by the previous studies are significantly lower than for other investigated databases, indicating low prediction potential of used features and high complexity of database.

The stability scores in Fig. 1 are in agreement with our previous results, showing the highest stability for univariate FS methods with entropy based FS clearly outperforming other techniques. Only difference can be seen in Fig. 1(b) where t-test, ANOVA and Bhattacharyya univariate FS techniques manifest higher stability than entropy-based FS. On the other hand, mRMR-MIQ algorithm performs very poorly in terms of FS stability, especially for a low number of selected features.

The stability measures introduced in the previous sections can be used to assess the internal stability of FS process. These are referred to as intrameasures and evaluate single system properties [11]. The stability measure can be also viewed in different context, i.e. assessing the similarity between the outputs of the different FS methods. In this case we use intermeasures or similarity measures [11,18]. Assessing the similarity between the outputs of different FS methods provides information about diversity of the FS methods and directly compare their output.

Analogously to Kuncheva index and CW, we define intersystem Kuncheva index 
                           I
                           κ
                         and intersystem weighted consistency index ICW. Let 
                           
                              
                                 S
                              
                              
                                 m
                              
                           
                           =
                           {
                           
                              
                                 S
                              
                              
                                 1
                              
                              
                                 m
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 
                                    S
                                 
                                 
                                    K
                                 
                                 
                                    m
                                 
                              
                           
                           }
                         be a system of K feature subsets, obtained by applying K times particular FS algorithm on different samplings of the dataset, where m denotes the indice of two compared systems. The intersystem Kuncheva index between two FS methods is defined as follows:
                           
                              (5)
                              
                                 I
                                 κ
                                 (
                                 
                                    
                                       S
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       S
                                    
                                    
                                       2
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       |
                                       
                                          
                                             F
                                          
                                          
                                             Thr
                                          
                                          
                                             1
                                          
                                       
                                       ∩
                                       
                                          
                                             F
                                          
                                          
                                             Thr
                                          
                                          
                                             2
                                          
                                       
                                       |
                                       ·
                                       c
                                       −
                                       
                                          
                                             d
                                          
                                          
                                             2
                                          
                                       
                                    
                                    
                                       d
                                       (
                                       c
                                       −
                                       d
                                       )
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    F
                                 
                                 
                                    Thr
                                 
                                 
                                    1
                                 
                              
                           
                           ⊂
                           F
                         and 
                           
                              
                                 
                                    F
                                 
                                 
                                    Thr
                                 
                                 
                                    2
                                 
                              
                           
                           ⊂
                           F
                         are sets of Thr features with the highest number of occurrences in 
                           
                              
                                 S
                              
                              
                                 1
                              
                           
                         or 
                           
                              
                                 S
                              
                              
                                 2
                              
                           
                        , respectively.

Similarly, the intersystem weighted consistency [11]:
                           
                              (6)
                              
                                 ICW
                                 (
                                 
                                    
                                       S
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       S
                                    
                                    
                                       2
                                    
                                 
                                 )
                                 =
                                 1
                                 −
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          f
                                          ∈
                                          F
                                       
                                    
                                 
                                 −
                                 
                                    
                                       
                                          w
                                       
                                       
                                          f
                                       
                                    
                                 
                                 |
                                 
                                    
                                       
                                          
                                             
                                                Ψ
                                             
                                             
                                                f
                                             
                                             
                                                1
                                             
                                          
                                       
                                       
                                          
                                             
                                                K
                                             
                                             
                                                1
                                             
                                          
                                       
                                    
                                    −
                                    
                                       
                                          
                                             
                                                Ψ
                                             
                                             
                                                f
                                             
                                             
                                                2
                                             
                                          
                                       
                                       
                                          
                                             
                                                K
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                                 |
                                 ,
                              
                           
                        where
                           
                              (7)
                              
                                 
                                    
                                       w
                                    
                                    
                                       f
                                    
                                 
                                 =
                                 
                                    
                                       max
                                       (
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         Ψ
                                                      
                                                      
                                                         f
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         K
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                
                                             
                                          
                                          ,
                                          
                                             
                                                
                                                   
                                                      
                                                         Ψ
                                                      
                                                      
                                                         f
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         K
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             g
                                             ∈
                                             F
                                          
                                       
                                       max
                                       (
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         Ψ
                                                      
                                                      
                                                         g
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         K
                                                      
                                                      
                                                         1
                                                      
                                                   
                                                
                                             
                                          
                                          ,
                                          
                                             
                                                
                                                   
                                                      
                                                         Ψ
                                                      
                                                      
                                                         g
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         K
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       )
                                    
                                 
                                 .
                              
                           
                        
                     

For the ease of interpretation we limit the discussion in this section only to a representative case of the G1999 database. Similarity analyses of the feature selection methods observed for other databases exhibit similar trend and are available in Online Supplementary Material. G1999 database was selected as the representative case because it is well-known and frequently used database, moreover, it represents trade-off between huge databases such as B2006, C2006a/b and smaller databases D2013 or T2014.

Similarly to the experiments on FS stability, we applied FS algorithms to select 100 features from each of the 500 perturbed versions of dataset. Every perturbed dataset contains 80% randomly selected samples of the original dataset.


                        Fig. 2
                         depicts frequencies of selected features against feature index for G1999 database and various FS methods. It can be seen that certain features are selected more frequently than the others depending on the used FS method. Additionally, it is visible that selection frequency patterns of some methods are more similar to each other (e.g. t-test, ANOVA, Bhattacharyya). In contrast, output of iRelief FS method is clearly different from other patterns. To analytically express the similarity of the FS outputs, we employ similarity measures 
                           I
                           κ
                         and ICW. We use different similarity measures to ensure robustness of the results and to provide different views on the similarity of FS techniques. Comparing Tables 4 and 5
                        
                        , it can be seen that both similarity measures, 
                           I
                           κ
                         and ICW, characterize similar trend and similarity performance of FS methods.

As can be expected, t -test and ANOVA procedure show high degree of similarity. Another method that generates highly similar results to ANOVA or t-test is Bhattacharyya distance, indicating similarity among these univariate methods. On the contrary, the method that gives the most dissimilar results, i.e. entropy FS, turned out to be very stable in our previous experiment. Interestingly, the two mRMR methods, mRMR-MID and mRMR-MIQ, which share same theoretical background and differ only in the scheme that combines relevance and redundancy appears to be quite dissimilar with 
                           I
                           κ
                           =
                           0.66
                         and ICW=0.75.

Besides the FS stability, another important aspect when assessing performance of FS is goodness of prediction. To measure predictive performance of FS we evaluate four supervised classification algorithms trained and tested on the data restricted to selected feature subset. Particularly, we tested support vector machines, adaptive boosting ensemble classifier (Ada), random forests (RF) and deep belief network (DBN) classifier. These four classifiers represent different approaches to problem of supervised machine learning. SVM and RF are probably the most frequently used classifiers. Both classifiers repeatedly achieve leading prediction performance in different domains. We also included AdaBoost classifier that is known for its robustness against overfitting and as such is expected to perform well in high dimensional scenarios. Finally, last considered classifier is based on DBN. DBNs are relatively a new approach that has emerged in recent years and have shown potential to represent complicated structures in the data. There are not many studies that adopt DBN as supervised classifier therefore we believe that our results can also contribute to DBN research. A brief description of classifiers and their settings are given below or in references therein.

We assess the performance of the classifiers by the area under ROC curve (AUC) metric and use 10-fold cross-validation (CV) scheme.

The underlying idea of SVM classifier is to calculate a maximal margin hyperplane separating two classes of the data. To learn non-linearly separable functions, the data are implicitly mapped to a higher dimensional space by means of a kernel function. New samples are classified according to the side of the hyperplane they belong to. We used Radial Basis Function (RBF) kernel [50]. The RBF kernel is defined as
                              
                                 (8)
                                 
                                    K
                                    (
                                    x
                                    ,
                                    
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          e
                                       
                                       
                                          (
                                          −
                                          ∥
                                          x
                                          −
                                          
                                             
                                                x
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                ∥
                                             
                                             
                                                2
                                             
                                          
                                          )
                                          /
                                          2
                                          
                                             
                                                γ
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           where γ controls the width of RBF function.

The parameters kernel gamma γ and penalty parameter C were optimized using grid search of possible values. Specifically, we searched over the grid 
                              (
                              C
                              ,
                              γ
                              )
                            defined by the product of the sets 
                              C
                              =
                              [
                              
                                 
                                    2
                                 
                                 
                                    −
                                    10
                                 
                              
                              ,
                              
                                 
                                    2
                                 
                                 
                                    −
                                    9
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    2
                                 
                                 
                                    6
                                 
                              
                              ,
                              
                                 
                                    2
                                 
                                 
                                    7
                                 
                              
                              ]
                           , 
                              γ
                              =
                              [
                              
                                 
                                    2
                                 
                                 
                                    −
                                    7
                                 
                              
                              ,
                              
                                 
                                    2
                                 
                                 
                                    −
                                    6
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    2
                                 
                                 
                                    6
                                 
                              
                              ,
                              
                                 
                                    2
                                 
                                 
                                    7
                                 
                              
                              ]
                           .

AdaBoost belongs to the important family of ensemble methods known as boosting. The key idea behind boosting techniques is to use ensemble methods to combine weak classifiers in order to build a strong learner. AdaBoost is an iterative boosting algorithm constructing a strong classifier as a linear combination of weak classifiers, each performing at least above chance level. As weak classifiers we used decision trees [51]. Similarly to SVM, we searched grid of possible classifier settings to find optimal performance. The grid was determined by the product of the sets 
                              
                                 
                                    n
                                 
                                 
                                    e
                                 
                              
                              =
                              [
                              50
                              ,
                              100
                              ,
                              200
                              ]
                            (maximum number of estimators at which boosting is terminated), 
                              
                                 
                                    n
                                 
                                 
                                    split
                                 
                              
                              =
                              [
                              1
                              ,
                              2
                              ,
                              3
                              ,
                              5
                              ,
                              10
                              ]
                            (the number of features to consider when looking for the best split) and 
                              
                                 
                                    n
                                 
                                 
                                    depth
                                 
                              
                              =
                              [
                              1
                              ,
                              2
                              ,
                              3
                              ,
                              5
                              ,
                              10
                              ]
                            (the maximum depth of the tree).

A drawback associated with decision trees classifiers is their high variance. In order to improve the stability decision forest methodology [52] was proposed and later further improved by Breiman [53] to provide integrated form of random forest classifier. Random forest classifier is an ensemble technique that uses an ensemble of unpruned decision trees, each built on a bootstrap sample of the training data using randomly selected subset of variables. We considered different parameter configurations for the values of 
                              
                                 
                                    n
                                 
                                 
                                    tree
                                 
                              
                              =
                              [
                              200
                              ,
                              500
                              ,
                              1000
                              ]
                            (a number of trees to build), 
                              
                                 
                                    m
                                 
                                 
                                    depth
                                 
                              
                              =
                              [
                              1
                              ,
                              2
                              ,
                              3
                              ,
                              5
                              ,
                              10
                              ]
                            (the maximum depth of the tree) and 
                              
                                 
                                    m
                                 
                                 
                                    split
                                 
                              
                              =
                              [
                              1
                              ,
                              2
                              ,
                              3
                              ,
                              5
                              ,
                              10
                              ]
                            (minimum number of samples required to split an internal node).

It has been shown that deep architectures have potential to better represent function than the shallow ones [54,55]. The deep belief network (DBN) is formed by stacking restricted Boltzman machines at the top of each other and train them in the greedy manner. The training strategy for DBNs may hold great promise as a principle to find solution for the problem of training deep networks. Upper layers of a DBN represent more abstract concepts that explain the input observation, whereas lower layers extract low-level features from data. They learn simpler concepts first and then use them to build and learn more abstract concepts. It is relatively difficult to finely train DBN and there are plenty of parameters to choose from. We experimented only with a limited number of parameters. The number of units in the input layer and the output layer was set to number of features and number of classes, respectively. The number of units in the middle layers was varied through 
                              n
                              =
                              [
                              100
                              ,
                              300
                              ,
                              500
                              ,
                              1000
                              ]
                            and 
                              m
                              =
                              [
                              100
                              ,
                              300
                              ,
                              500
                              ]
                            with number of epochs 
                              
                                 
                                    n
                                 
                                 
                                    e
                                 
                              
                              =
                              [
                              50
                              ,
                              100
                              ]
                           .

We first evaluate the accuracy of selected subset of features by different FS methods. Here, the accuracy refers to AUC prediction performance reached by classifier when trained on the d features selected by FS method. We used tenfold CV scheme, where the original dataset was split into training data (90 %) and testing data (10 %). The classification process was repeated a total 100 times and the AUC was averaged.

We test the AUC of feature subsets with size d=100 obtained by four different classifiers. Table 6
                         shows the mean accuracies for all eight datasets used in this study. The highest classification performance averaged over all datasets (AUC=89.3 %) was achieved by two different approaches. The first is the pipeline of linear SVM as FS and Adaboost classifier and the second Bhattacharyya distance performing FS and SVM applied for classification. However, examining the results of linear SVM, it can be seen that it does not perform very well with classifiers other than Adaboost. On the other hand, mRMR-MIQ shows very good performance with all classifiers and provide the highest AUC in 11 out of 32 investigated scenarios. So, the FS methods achieving consistently high score in conjunction with various classifiers evaluated on eight different datasets are Bhattacharyya distance based FS and mRMR-MIQ.

The Adaboost classifier consistently gives good results and achieved the highest average AUC score. We therefore choose it as a default classification algorithm for further assessment of AUC. In order to reduce computational time for this experiment we did not perform grid search for optimal value of the parameters, but used parameters that proved to achieve the best performance in the previous experiment setting n
                        
                           e
                        =200, 
                           
                              
                                 n
                              
                              
                                 split
                              
                           
                           =
                           1
                         and 
                           
                              
                                 n
                              
                              
                                 depth
                              
                           
                           =
                           1
                        . Fig. 3
                         shows AUC as a function of number of features for eight different databases. As we can see the behavior of AUC with the increasing number of features included in classifier is similar for most of the FS methods. The AUC slightly increases with an incrementing number of features and then keeps constant value. These results confirm our previous observation, presented in Table 6, showing Bhattacharyya distance FS and mRMR-MIQ performing the best with other FS following. Similarly to Table 6, iRelief methods achieve the lowest AUC for most of the databases. The exceptions are more complex databases such as T2003, D2013 or T2014 where iRelief performs comparably to others. While for high-dimensional databases univariate FS methods performs comparably or even better than more sophisticated methods, for smaller and more complex databases the AUC of univariate methods drops below AUC of other FS methods. This is clearly visible especially in Fig. 3(g).

@&#CONCLUSIONS@&#

In this paper, we compared a group of ten FS methods. Eight public biomedical databases were used to evaluate influence of FS on prediction performance, stability of FS and similarity of FS outputs. The results indicate that univariate FS methods are more stable than multivariate methods. The explanation of this observation lies in the fact that the majority of mutlivariate methods are designed to minimize redundancy in selected subset of features. Therefore, for high dimensional databases, where many features are correlated or functionally related, it results in unstable behavior. For databases with smaller number of features, as in our case D2013 and T2014 database, stability of the mutlivariate FS techniques approaches or even surpasses the stability of univariate methods. It should be noted that D2013 and T2014 are not only the smallest databases in terms of number of a features, but also the most challenging to correctly predict class label.

Comparing prediction performance of FS methods it is clear that low stability of FS does not result in low prediction performance. This is clearly visible for mRMR-MIQ performing rather poorly in terms of stability but achieving excellent prediction performance. Overall, if we are about to select one stable method that additionally provides high classification performance, we recommend Bhattacharyya distance based FS providing balanced results.

To evaluate classification performance four state-of-the art classifiers were used. We intentionally included also DBN classifier that was introduced only recently and there are only few studies utilizing this classifier. Basically, all classifiers provided the competitive results; with RF giving slightly lower prediction performance than three other.

Even though we used mainly biomedical databases, since a topic of feature selection is frequently encountered in this field, results and conclusions can be applicable also to other fields where FS techniques are a part of decision pipeline.

None declared.

@&#ACKNOWLEDGEMENTS@&#

The described research was performed in laboratories supported by the SIX project; the registration number CZ.1.05/2.1.00/03.0072, the operational program Research and Development for Innovation. Peter Drotár was supported by the project CZ.1.07/2.3.00/30.0005 of Brno University of Technology. This work was supported by National Sustainability Program under Grant LO1401 and the Scientific Grant Agency of the Ministry of Education, science, research and sport of the Slovak Republic under the Contract no. 1/0766/14.

Supplementary data associated with this paper can be found in the online version at doi:10.1016/j.compbiomed.2015.08.010.


                     
                        
                           Application 1
                           
                        
                     
                  

@&#REFERENCES@&#

