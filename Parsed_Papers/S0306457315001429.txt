@&#MAIN-TITLE@&#A semantics and image retrieval system for hierarchical image databases

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Proposes automatic semantics and image retrieval system for hierarchical databases.


                        
                        
                           
                           System uses < 1/3 search space to retrieve semantics, and finally similar images.


                        
                        
                           
                           ≈ 77% semantic retrieval accuracy on ImageNet. Uses ≈ 4% space to retrieve images.


                        
                        
                           
                           System reports precision of 0.78 @ 20 and 0.67 @ 100 images on categorized WANG.


                        
                        
                           
                           The study explores adequacy of visual signatures set used to represent a semantic.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Content based image retrieval

Semantic assignment

Clustering

Visual search space

Indexing of visual features

@&#ABSTRACT@&#


               
               
                  This work presents a content based semantics and image retrieval system for semantically categorized hierarchical image databases. Each module is designed with an aim to develop a system that works closer to human perception. Images are mapped to a multidimensional feature space, where images belonging a semantic are clustered and indexed to acquire its efficient representation. This helps in handling the existing variability or heterogeneity within this semantic. Adaptive combinations of the obtained depictions are utilized by the branch selection and pruning algorithms to identify some closer semantics and select only a part of the large hierarchical search space for actual search. So obtained search space is finally used to retrieve desired semantics and similar images corresponding to them. The system is evaluated in terms of accuracy of the retrieved semantics and precision-recall curves. Experiments show promising semantics and image retrieval results on hierarchical image databases. The results reported with non-hierarchical but categorized image databases further prove the efficacy of the proposed system.
               
            

@&#INTRODUCTION@&#

During last two decades image retrieval emerged as a promising technology which also received a widespread interest of media. Researchers from various fields such as image processing, pattern recognition, computer vision, psychology, and many more have joined hands together and put their sincere efforts into understanding the real world implications, applications, and constraints in image retrieval. Many techniques were developed as a result of their collaborative work (Datta et al., 2008; Deselaers et al., 2008; Liu et al., 2007; Vassilieva, 2009). Initially, image retrieval systems worked on a text-based framework, then came the content-based image retrieval (CBIR), and now the focus has been shifted to automatic image annotation (AIA). In CBIR systems, image contents are visually interpreted for retrieval and a visual feature space is generated to search similar images. These systems try to emulate human perception through visual distances in the form of similarity measures defined on color, texture, and shape features. The major challenges in this domain are image database for testing, feature extraction and indexing techniques along with distance measures corresponding to human-visual system, query processing, user friendliness, and visual semantic gap (Datta et al., 2008). In such a state of affairs, till now CBIR community does not have universally acceptable algorithms to characterize human vision in the context of object recognition and image retrieval. It is time to think about more sophisticated and robust models and algorithms to index, retrieve, organize, and interact with image data.

Most of the existing CBIR systems/approaches limit their contributions to one or two of these issues. In an attempt to effectively address these issues, this work proposes a prototype for a full-fledged semantics and image retrieval system working on a large image database. The system is developed especially for semantically categorized hierarchical image databases and is fully automatic irrespective of the size of the database. Semantic based categorization of an image database results in a hierarchical tree structure with categories and subcategories of images at various levels. Visual features of images in such a database form a semantic based hierarchical search space on which the proposed system works. The core of the system consists of an effective way of traversing the hierarchical search space (to reduce the search space/time), clustering performed within categories (to reduce semantic gap), and indexing of clusters (to further reduce the search time). A user friendly interface is developed for experimentation. Experiments are performed with two sets of query images (inside and outside database) on a hierarchical image database (e.g., ImageNet), and also on a flat database (e.g., WANG), to analyze the efficacy of the system for semantic assignment as well as image retrieval.

The manuscript is organized as follows. Section 2 presents a brief review of CBIR systems. Section 3 presents the proposed content based semantics and image retrieval system and discusses its modules including feature extraction, clustering of image data, indexing, and adaptive branch selection on clustered and indexed data. Results are discussed in Section 4 and Section 5. Finally, Section 6 concludes the work.

@&#RELATED WORK@&#

CBIR systems map images to a multidimensional space formed with color, texture, and shape features (Deselaers et al., 2008). A suitable similarity function is used to measure the visual similarity of two images. The choice of features may depend on the application and the database used for retrieval. Research shows that the systems using a combination of multiple visual features are more accurate in terms of precision and recall (Hiremath & Pujari, 2007; Lin et al., 2009; Wang et al., 2011).

Blobworld (Carson et al., 2002), CIRES (Iqbal & Aggarwal, 2002), MARS (Rui et al., 1997), MIT’s Photobook (Pentland et al., 1994), NeTra (Ma & Manjunath, 1999), QBIC (Niblack et al., 1993), SIMPLIcity (Wang et al., 2001), VisualSEEk (Smith & Chang, 1997), WebSEEk (Chang et al., 1997) are some well-known CBIR systems. Some unnamed systems are also available in literature (ElAlami, 2011; Lin et al., 2009; Shrivastava & Tyagi, 2015; Youssef, 2012). These systems focus on image retrieval solely to improve the efficiency of CBIR system. The most popular among these, e.g., IBM’s QBIC (Query By Image Content) applies all three features and uses R*-trees for enhanced speed. It accepts queries based on example images, user-constructed sketches or/and selected color, and texture patterns (Niblack et al., 1993). Blobworld (Carson et al., 2002) first segments images to find regions that have an associated color and texture descriptors indexed using a tree. At the time of querying, instead of entire image, description of a few regions is used to facilitate feasible large-scale retrieval. Similarly, NeTra identifies homogeneous regions and computes their color, texture, shape, and spatial location information. It provides the flexibility to specify properties of the region of interest, and the system searches for images with the similar regions in the database (Ma & Manjunath, 1999). VisualSEEk is unique as it provides joint color/spatial querying capability. It uses color set back-projection technique to extract color regions. The colors, sizes, spatial locations, and relationships of regions are used to compare images (Smith & Chang, 1997). SIMPLIcity (Semantics sensitive Integrated Matching for Picture LIbraries) also represents an image as a set of regions, which are characterized by color, texture, shape, and location (Wang et al., 2001). The system first classifies an image into semantic category and then applies semantically-adaptive feature extraction and searching methods. This categorization narrows down the searching range and helps in enhanced retrieval. CIRES (Content-based Image REtrieval System) combines image structure with color and texture features without putting much emphasis on the segmentation or detailed object descriptions (Iqbal & Aggarwal, 2002). Another visual information system, WebSEEk is designed for Web-based environments. It collects, analyzes, indexes, and searches visual information on the Web (Chang et al., 1997). These systems do not put much emphasis on relevance feedback to enhance the performance. However, MARS (Multimedia Analysis and Retrieval System) uses an integrated relevance feedback architecture to address semantic gap and human perception. Feedback mechanism is implemented at feature representation as well as similarity measure levels to improve efficiency (Rui et al., 1997). Photobook (Pentland et al., 1994) works on any commonly used color, texture, and shape features. It uses an interactive learning agent to select and combine features for a given task, and also provides a library of matching algorithms.

Many of these systems do not make use of clustering (ElAlami, 2011; Lin et al., 2009; Shrivastava & Tyagi, 2015; Youssef, 2012), but a few like Blobworld, MARS, and QBIC used clustering to determine regions within an image, i.e., segmentation. Some CBIR systems, like SemQuery (Sheikholeslami et al., 2002), RIME (Chang et al., 1999), CLUE (Chen et al., 2005) have used clustering to group images prior to image retrieval. Researchers have also utilized clustering for the purpose of image annotation and indexing (Datta et al., 2008; Ober et al., 2007), and object discovery (Tuytelaars et al., 2010). Clustering is also applied on videos to automatically learn actions and then use them for action categorization and localization (Jiang et al., 2013; Niebles et al., 2008).

In order to retrieve images resembling to human perception, SemQuery (Sheikholeslami et al., 2002), RIME (Chang et al., 1999), and CLUE (Chen et al., 2005) used the concept of clustering successfully. SemQuery is a semantics-based clustering and indexing approach that uses a set of heterogeneous features which are merged by means of a hierarchical clustering. RIME (Replicated IMage dEtector), on other hand detects unauthorized images copying on WWW and is able to identify all near replicas of an image. An unsupervised learning based technique, CLUE (cluster-based retrieval of images by unsupervised learning) dynamically applies a graph-theoretic clustering algorithm to images which are retrieved in response to the query. It is a general approach that can be embedded in any CBIR systems for an improved performance. A topic-oriented semi-supervised and probability-based document clustering is proposed to facilitate users in forming groups according to their needs (Karthikeyan & Aruna, 2013). Using two kinds of indexing keys (i.e., color and spatial information), some high potential candidates are identified from a large database; and a quad modeling method is used to form a weight matrix to finally retrieve similar images. Another attempt to make the system work according to user requirements proposed several folding-based algorithms to cluster web image search results (Alamdar & Keyvanpour, 2014). Efficiency is improved by focusing on the preferences of clusters representative selection, fuzziness, weight, cluster refinement, and utilization of hierarchical algorithm.

Humans easily classify images on the basis of semantics. The semantic gap is the lack of correlation between high-level image semantics and low-level features of images. Development of universally acceptable algorithms to reduce semantic gap and characterize human vision for object recognition and image retrieval are still in progress. Researchers have tried to deal with this issue in a number of ways, but the obtained performance is not of that level as is achieved by any of the human annotation system (Hanbury, 2008; Singhai & Shandilya, 2010; Wang, 2011; Zhang et al., 2012). The state-of-the-art techniques to reduce the ‘semantic gap’ are as follows (Liu et al., 2007): (1) object ontology: easy to design and works fine with small databases; (2) machine learning algorithms: used extensively on flat image databases; (3) relevance feedback: used in combination with other options usually and proved to be effective due to users involvement; (4) semantic template generation: a promising and practical approach in semantic-based image retrieval; and (5) fusion of textual and visual content of images: relies on the accuracy of metadata, but integration of both the contents is a challenging issue.

Many CBIR systems either use these ways or their combinations to reduce the semantic gap. For a small database of 5000 Corel images, an image retrieval system is developed using the concept of object ontology (Mezaris et al., 2003). Final query results are generated using a relevance feedback mechanism. A few works have also used the concept of semantic templates (Cheng et al., 1998; Smith & Li, 1999; Zhuang et al., 1999). Learning algorithms for limited number of concepts are frequently used to reduce semantic gap (Carneiro et al., 2007; Chen et al., 2012; Fan et al., 2008; Li & Wang, 2008; Qi et al., 2009; Tsai & Hung, 2008). Recently, an efficient attempt is made by utilizing large deep convolutional neural networks to classify 1.2 million images into 1000 different classes (Krizhevsky et al., 2012). A few works have also explored the fusion of textual and visual content of images (Liu et al., 2011; Wang et al., 2008). In the last few years researchers have combined the fusion of textual and visual contents of images with the machine learning algorithms (Feng & Lapata, 2013; Gao et al., 2013; Wang et al., 2014; Wu et al., 2013; Yu et al., 2014a; 2014b). Most of these systems consider domain specific flat image databases. However, it is observed that the usage of structured vocabulary leads to better results (Tousch et al., 2012).

It follows from the discussion that all essential components of a content based semantic and image retrieval system must work in synchrony to enable it to perform closer to human perception. In the past few years the concept of structured vocabulary came into existence. This work utilizes a semantically categorized hierarchical image database (ImageNet) as a substitute of structured vocabulary, and results in an efficient content based semantic and image retrieval system. The system works on commonly used color histograms (Sural et al., 2002), color moments (Stricker & Orengo, 1995), gabor texture (Manjunath & Ma, 1996), and pseudo-Zernike moments (Wang et al., 2011) shape features. The variability and broadness covered by a semantic is managed by using a variant of an agglomerative hierarchical clustering algorithm that can automatically identify groups within a semantic in the visual space. All the identified clusters within a semantic are arranged in the form of a Vantage point trees (Fu et al., 2000) to facilitate effective traversal of the search space. Instead of traversing each and every semantic in a hierarchical image database at the time of query input, only a few semantics are visited and further explored in search of semantics and finally, the image retrieval. The branch selection and appropriate pruning algorithms are employed in the developed system to facilitate the selection of the required semantics.

The block diagram of the proposed content based semantics and image retrieval system is shown in Fig. 1
                     . Visual features of images in the semantically categorized database considering categories/sub-categories as nodes in the image tree are extracted. Correlation between low-level visual features and high-level semantics of images belonging to a node is established by assigning a visual prototype/signature to each node of the image tree. Clustering of visual features of images belonging to a node is done to get some groups within this semantic. A few prominent groups, referred as dominant clusters here, are identified to form visual signatures representing the semantic of this node. Each node has a set of visual signatures corresponding to each of the identified dominant cluster. Linear traversal of these visual signatures is time consuming. Therefore, for each node these signatures are arranged using three Vantage point (VP) trees, corresponding to color, texture, and shape features. Visual features of images and visual signatures of nodes are stored through off-line procedure.

The process of semantics and image retrieval starts with the extraction of color, texture, and shape features of a query image. An adaptive branch selection on this clustered and indexed data is developed in this work. It uses visual features of the query image and stored visual signatures of nodes to select and search a few subtrees at each level of the categorized image database. The resulting search space is further pruned with Extended OR-Inc pruning algorithm (Pandey et al., 2015). Retrieval module utilizes the final search space to retrieve general semantics, specific semantics, and the associated images corresponding to the query image. All the results are presented to the user by means of a simple interface as shown in Fig. 2
                     .

Each image in the database is first mapped to the multidimensional feature space formed with color, texture, and shape features (Deselaers et al., 2008). The most common color, texture, and shape features found in literature are used for effective representation of categories in visual space. Their respective details are given in Table 1. Color features are extracted using HSV color space at global as well as local levels by dividing an image into five regions (central ellipsoidal and four surrounding regions) (Sural et al., 2002). At global level, a 68 dimensional color histogram and 9 dimensional color moment feature is obtained. But at local level, histograms and moments corresponding to five image regions are concatenated to form a 5 × 68 dimensional color histogram and a 5 × 9 dimensional color moments feature vector.
                     

Gabor filter constructed with Gabor function as the mother wavelet is the most frequently used texture feature. Suitable dilations and rotations of Gabor function give a self-similar filter dictionary. Gabor filter with four scales and six orientations are used here. Mean (m) and standard deviation (σ) of the magnitude of wavelet transform coefficients give 48 dimensional texture feature vector. Rotation invariant and less sensitive to noise Pseudo-Zernike moments having multilevel representation capability are used as shape features. Moments (
                           
                              
                                 A
                                 ^
                              
                              
                                 i
                                 j
                              
                           
                        ) are calculated for a non-negative number 
                           
                              i
                              =
                              5
                           
                         with | j | ≤ i, to get a 21 dimensional shape feature vector.

It is observed that not only the visual features of different categories but also the visual features in a category vary a lot. For example, images of Animal most likely overlap with images of Tree and Person in visual space. It gives rise to inter-semantic gap as most of the pictures either show animals in the natural environment containing trees etc. or people take their images with animals. Also, visual space enclosed within a category can be viewed as the union of visual spaces corresponding to different groups within a category. Again, visual space of Animal consists of visual spaces of Bird, Dog, Fish, Bear, etc and results in intra-semantic gap. To reduce such overlap of semantics in visual space, clustering of images within categories and sub-categories is done. The concept of ‘dominant clusters’ is used for assigning visual signatures to categories and sub-categories which results in their better representation in visual space. Adaptive selection of branches using this clustered data helps to reduce the semantic gap and facilitates more accurate semantic assignment to the query images.

The low level features of images within each node (representing category/sub-category) are clustered to get some visually separable groups within it. A variant of hierarchical agglomerative clustering algorithm built on the information loss (Pandey & Khanna, 2014) is used and is given in Fig. 3
                        . Initially, each image is put in one cluster. Each cluster is denoted by a representative image, as given in Eq. (3) which may or may not change after each step. After getting the features of images, a distance matrix (D) containing distance between each image pair is constructed as given in Eq. (1).

Using this matrix, two representative images or clusters at the minimum distance are identified and grouped in the bottom-up manner using rules listed in Table 2
                        . A loss of information is calculated for each cluster, which is the sum of distances between the cluster representative image and other images contained in the cluster. New clusters are formed such that the overall loss of information (I), as given in Eq. (4), is minimal. The process of merging two nearest clusters continues till all images belong to a single cluster. To get the terminating point, the loss of information and proportional reduction in error (PRE) values are examined. PRE is the ratio of increase in (I) to the previous (I) at each iteration (Rokach & Maimon, 2005). Iteration just before the one with maximum PRE would signify the end of cluster forming or merging process. The output consists of some clusters formed with most of the images in the database. Any remaining image is associated with a cluster whose representative image is closest to it.

                           
                              •
                              
                                 Distance matrix
                                 
                                    
                                       (1)
                                       
                                          
                                             
                                                
                                                   
                                                      D
                                                      
                                                         [
                                                         P
                                                         ]
                                                      
                                                      
                                                         [
                                                         Q
                                                         ]
                                                      
                                                      =
                                                      
                                                         {
                                                         
                                                            
                                                               
                                                                  
                                                                     1
                                                                     ,
                                                                  
                                                               
                                                               
                                                                  
                                                                     
                                                                        if
                                                                     
                                                                     
                                                                     P
                                                                     =
                                                                     Q
                                                                  
                                                               
                                                            
                                                            
                                                               
                                                                  
                                                                     
                                                                        d
                                                                        
                                                                           (
                                                                           P
                                                                           ,
                                                                           Q
                                                                           )
                                                                        
                                                                     
                                                                     ,
                                                                  
                                                               
                                                               
                                                                  
                                                                     otherwise
                                                                  
                                                               
                                                            
                                                         
                                                         }
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 Here, d
                                 (P, Q) is the distance between two images, P and Q , and is calculated by weighted summation of the three distances as in Eq. (2).

                                    
                                       (2)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         d
                                                         
                                                            (
                                                            P
                                                            ,
                                                            Q
                                                            )
                                                         
                                                      
                                                      =
                                                      
                                                         w
                                                         C
                                                      
                                                      *
                                                      
                                                         d
                                                         
                                                            C
                                                            
                                                               (
                                                               P
                                                               ,
                                                               Q
                                                               )
                                                            
                                                         
                                                      
                                                      +
                                                      
                                                         w
                                                         T
                                                      
                                                      *
                                                      
                                                         d
                                                         
                                                            T
                                                            
                                                               (
                                                               P
                                                               ,
                                                               Q
                                                               )
                                                            
                                                         
                                                      
                                                      +
                                                      
                                                         w
                                                         S
                                                      
                                                      *
                                                      
                                                         d
                                                         
                                                            S
                                                            
                                                               (
                                                               P
                                                               ,
                                                               Q
                                                               )
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 where d
                                 
                                    C(P, Q), d
                                 
                                    T(P, Q), and d
                                 
                                    S(P, Q) are color, texture, and shape distances between images P and Q; and wC, wT
                                 , and wS
                                  are weights (equal in this work) assigned to color, texture, and shape distances, respectively.


                                 Representative image
                              

Let, there are C clusters, and μi
                                  is the mean of ith
                                  cluster with |Ci
                                 | images. The representative image, Clstrimgi
                                 , for this cluster is calculated as in Eq. (3).

                                    
                                       (3)
                                       
                                          
                                             
                                                
                                                   
                                                      C
                                                      l
                                                      s
                                                      t
                                                   
                                                
                                                
                                                   
                                                      r
                                                      i
                                                      m
                                                      
                                                         g
                                                         i
                                                      
                                                      =
                                                      a
                                                      r
                                                      g
                                                      m
                                                      i
                                                      
                                                         n
                                                         
                                                            
                                                               c
                                                               i
                                                            
                                                            ∈
                                                            
                                                               C
                                                               i
                                                            
                                                         
                                                      
                                                      
                                                         (
                                                         
                                                            d
                                                            
                                                               (
                                                               
                                                                  μ
                                                                  i
                                                               
                                                               ,
                                                               
                                                                  c
                                                                  i
                                                               
                                                               )
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                
                                                   
                                                      
                                                         where
                                                      
                                                      
                                                      
                                                         μ
                                                         i
                                                      
                                                      =
                                                      
                                                         1
                                                         
                                                            |
                                                            
                                                               C
                                                               i
                                                            
                                                            |
                                                         
                                                      
                                                      
                                                         ∑
                                                         
                                                            
                                                               c
                                                               i
                                                            
                                                            ∈
                                                            
                                                               C
                                                               i
                                                            
                                                         
                                                      
                                                      
                                                         c
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              


                                 Loss of information
                                 
                                    
                                       (4)
                                       
                                          
                                             
                                                
                                                   
                                                      I
                                                      =
                                                   
                                                
                                                
                                                   
                                                      
                                                         ∑
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         C
                                                      
                                                      
                                                         (
                                                         
                                                            
                                                               |
                                                               
                                                                  C
                                                                  i
                                                               
                                                               |
                                                            
                                                            
                                                               
                                                                  ∑
                                                                  
                                                                     i
                                                                     =
                                                                     1
                                                                  
                                                                  C
                                                               
                                                               
                                                                  |
                                                                  
                                                                     C
                                                                     i
                                                                  
                                                                  |
                                                               
                                                            
                                                         
                                                         *
                                                         I
                                                         n
                                                         f
                                                         o
                                                         L
                                                         o
                                                         s
                                                         
                                                            s
                                                            i
                                                         
                                                         )
                                                      
                                                      *
                                                      log
                                                      
                                                         (
                                                         
                                                            
                                                               |
                                                               
                                                                  C
                                                                  i
                                                               
                                                               |
                                                            
                                                            
                                                               
                                                                  ∑
                                                                  
                                                                     i
                                                                     =
                                                                     1
                                                                  
                                                                  C
                                                               
                                                               
                                                                  |
                                                                  
                                                                     C
                                                                     i
                                                                  
                                                                  |
                                                               
                                                            
                                                         
                                                         *
                                                         I
                                                         n
                                                         f
                                                         o
                                                         L
                                                         o
                                                         s
                                                         
                                                            s
                                                            i
                                                         
                                                         )
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                
                                                   
                                                      
                                                         where
                                                      
                                                      
                                                      I
                                                      n
                                                      f
                                                      o
                                                      L
                                                      o
                                                      s
                                                      
                                                         s
                                                         i
                                                      
                                                      =
                                                      
                                                         ∑
                                                         
                                                            
                                                               c
                                                               i
                                                            
                                                            ∈
                                                            
                                                               C
                                                               i
                                                            
                                                         
                                                      
                                                      D
                                                      
                                                         [
                                                         C
                                                         l
                                                         s
                                                         t
                                                         r
                                                         i
                                                         m
                                                         
                                                            g
                                                            i
                                                         
                                                         ]
                                                      
                                                      
                                                         [
                                                         
                                                            c
                                                            i
                                                         
                                                         ]
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              


                                 Assigning clusters to remaining images
                              

Each unassigned image is compared with the representative images of identified clusters and assigned to a cluster whose representative image is the closest.

Considering visual signatures of all identified clusters in a category/node to represent its semantic is not preferable as it would not only increase the system load unnecessarily but also degrade the performance. Therefore, the clusters actually representing the semantics, i.e., dominant clusters, are supposed to be identified. A dominant cluster should be compact and well-populated. Minimum, maximum, and average intra-cluster distances are explored to get such clusters. The minimum intra-cluster distance stands for the strongest link within a cluster. The maximum intra-cluster distance is the maximum separation. The average intra-cluster distance shows the bonding among the cluster members. The maximum and the average intra-cluster distances should be as small as possible for a cluster to be compact. Based on these concepts, the measure given in Eq. (5) is formulated to get such clusters automatically.

                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             D
                                             
                                                C
                                                i
                                             
                                             =
                                             
                                                
                                                   m
                                                   a
                                                   
                                                      x
                                                      
                                                         p
                                                         ,
                                                         q
                                                         ∈
                                                         
                                                            C
                                                            i
                                                         
                                                      
                                                   
                                                   
                                                      d
                                                      
                                                         (
                                                         p
                                                         ,
                                                         q
                                                         )
                                                      
                                                   
                                                   +
                                                   
                                                      1
                                                      
                                                         
                                                            |
                                                            
                                                               C
                                                               i
                                                            
                                                            |
                                                         
                                                         ·
                                                         
                                                            |
                                                            
                                                               C
                                                               i
                                                            
                                                            −
                                                            1
                                                            |
                                                         
                                                      
                                                   
                                                   
                                                      ∑
                                                      
                                                         p
                                                         ,
                                                         q
                                                         ∈
                                                         
                                                            C
                                                            i
                                                         
                                                      
                                                   
                                                   
                                                      d
                                                      
                                                         (
                                                         p
                                                         ,
                                                         q
                                                         )
                                                      
                                                   
                                                
                                                
                                                   |
                                                   
                                                      C
                                                      i
                                                   
                                                   |
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where Ci
                         is the ith
                         cluster, p, q represent its members.

The numerator value should be small for a cluster to be compact whereas the denominator should be large for a well-populated cluster. Smaller DC values indicate desired clusters. Clusters whose DC value satisfies a threshold are chosen as dominant clusters representing a node. Visual signatures corresponding to each dominant cluster would form a set of visual signatures assigned to this node.

After clustering, each node has an associated set of visual signatures formed with the identified dominant cluster to represent its semantics in visual space. Although clustering reduces semantic gap, but linear traversal of these clusters is time consuming. Therefore, it is required to arrange the visual signatures of a node so that it may help to reduce the search time without much degrading the performance of the system. In view of this goal, all the identified clusters within a semantic are arranged in the form of a Vantage-point (VP) tree to facilitate effective traversal of the search space.

Generally, vector space and metric space indexes are used in content-based retrieval systems. Vector space index represents an object in a d-dimensional vector space and tries to preserve the distances among the objects. The resulting d-dimensional points can be arranged using index methods like quadtrees, kd-trees, R-trees, etc. These indexes do not perform search on the original data, thus information loss is natural as distances can be preserved to the some extent only. Also, their performance depends on the number of dimensions and degrades with increasing number of dimensions. Irrespective of the actual representation of objects, metric space indexes rely solely on pairwise distances and tries to capture the metric structure of the search space. These indexes (like List methods, Voronoi regions, and especially VP tree) are well-suited to solve nearest-neighbor queries (Fu et al., 2000). The proposed work selects ‘n’ nearest nodes from query image at each level of the hierarchical search space to retrieve semantics of a query image, and therefore VP trees are utilized to index visual signatures of dominant clusters.

VP tree is a binary space partitioning tree that builds the index directly based on the distance. The method first chooses a point (the “vantage point”) and divides the data points into two parts. One part consists of points that are nearer to the vantage point, and remaining points form the other part. Recursive use of this method partitions the data into smaller sets. The partitioning mechanism of VP tree is shown in Fig 4
                        . The scheme is applied to arrange dominant clusters belonging to a semantic.

Each dominant cluster has an associated color, texture, and shape visual signature. Three VP trees, corresponding to color, texture, and shape signatures are used to arrange these clusters. As data points are clusters, therefore instead of randomly choosing the first vantage point to be the root, the measure in Eq. (5) is utilized. The visual signature of a cluster having the minimum value of DC measure within a node is selected as a root for the corresponding VP tree. Color VP tree is build by using the color visual signature of the cluster identified as root. The distance between root and color visual signature of other clusters is computed. Median of these distances (M) is used to get left and right children. This procedure is repeated till all visual signatures are indexed. Similarly, the other two VP trees, i.e., on texture and shape are build with texture and shape visual signatures of the clusters.

The structure of each entry in a VP tree is shown in Fig 5
                        .

The information in each existing entry is utilized to traverse the resulting VP tree by checking some conditions. A track of all 
                           
                              c
                              l
                              u
                              s
                              t
                              e
                              r
                              _
                              i
                              d
                           
                        s traversed in each of the three VP trees is maintained. For further calculations, visual signatures corresponding to these 
                           
                              c
                              l
                              u
                              s
                              t
                              e
                              r
                              _
                              i
                              d
                           
                        s would only be considered by the branch selection and pruning algorithms. The following criteria are used to choose either left or right child, or both children of a node:

                           
                              1.
                              Compute the similarity, say Sc
                                 , between the visual feature of the query image and visual signature of a cluster selected as vantage point.

If 
                                    
                                       
                                          S
                                          c
                                       
                                       <
                                       
                                          (
                                          m
                                          a
                                          x
                                          _
                                          l
                                          e
                                          f
                                          t
                                          −
                                          M
                                          /
                                          2
                                          )
                                       
                                       ,
                                    
                                  traverse the left child; else if 
                                    
                                       
                                          S
                                          c
                                       
                                       >
                                       
                                          (
                                          m
                                          i
                                          n
                                          _
                                          r
                                          i
                                          g
                                          h
                                          t
                                          +
                                          M
                                          /
                                          2
                                          )
                                       
                                       ,
                                    
                                  traverse the right child; else traverse both the children.

The algorithm developed for adaptive selection of branches using clustered and indexed data is given in Fig. 6
                        . Instead of considering all dominant clusters identified for a node, only a few clusters depending on the traversal of each of the three VP trees are used. For each feature, distance between the query image and dominant clusters is computed. Also, weighted summation of color, texture, and shape distances corresponding to each dominant cluster is obtained. The cluster having minimum weighted sum distance is selected. Color, texture, and shape distances of the query image with this cluster are considered as the respective distances between the query image and this node. Following this procedure, three sorted lists corresponding to the three distances are prepared at each level and analyzed to select a few subtrees ‘n’ out of ‘N’ available subtrees. Performance of the system may be fine-tuned with the value of n chosen.


                        Extended OR-Inc Pruning: The proposed branch selection using clustered and indexed data returns nodes depending on the height and width of the subtrees chosen in Step 3. Although algorithm is capable of reducing the search space still all selected nodes are not possible candidates to be searched for semantic assignment to the query image. Further pruning of this search space for retaining good nodes while discarding the bad nodes would help in finding accurate image semantic. A good node is the one that lies on the path leading to the node containing images semantically similar to the query image, while a bad node leads to either a different path in the same subtree or a different subtree. Although, it is not possible to discard all the bad nodes, but the soft pruning may help in retaining good nodes. Soft pruning removes only the bad node from the path and not the entire subtree following it. Several pruning algorithms are discussed in Pandey et al. (2015) of which Extended OR-Inc Pruning was considered as one of the best options and followed in this work. The nodes retained by the pruning algorithm are used to assign specific semantics to the query image and also to get the required images.

Pruning algorithm makes use of distances corresponding to the dominant visual feature. Dominant feature of a subtree is the feature which gives top rank to this subtree. Dqsi
                         is the distance (already calculated) between query image and ith
                         node of the subtree (having Ns
                         nodes) w.r.t. dominant feature. Dmean and Dmed are mean and median of Dqs. Extended mean distance (Dmeanx) and extended median distance (Dmedx) are also computed as given in Eq. (6). In Extended OR algorithm all those nodes are retained for which Dqs ≤ Dmeanx OR Dqs ≤ Dmedx.

                           
                              (6)
                              
                                 
                                    
                                       
                                          
                                             D
                                             m
                                             e
                                             a
                                             n
                                             x
                                          
                                       
                                       
                                          
                                             =
                                             D
                                             m
                                             e
                                             a
                                             n
                                             +
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   N
                                                   s
                                                
                                             
                                             
                                                (
                                                
                                                   |
                                                   D
                                                   q
                                                   
                                                      s
                                                      i
                                                   
                                                   −
                                                   D
                                                   m
                                                   e
                                                   a
                                                   n
                                                   |
                                                
                                                /
                                                
                                                   N
                                                   s
                                                
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             D
                                             m
                                             e
                                             d
                                             x
                                          
                                       
                                       
                                          
                                             =
                                             D
                                             m
                                             e
                                             d
                                             +
                                             m
                                             e
                                             d
                                             i
                                             a
                                             n
                                             
                                                (
                                                
                                                   |
                                                   D
                                                   q
                                                   
                                                      s
                                                      i
                                                   
                                                   −
                                                   D
                                                   m
                                                   e
                                                   d
                                                   |
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

After this the ‘Inc’ part of the algorithm explores parentchild relationship. At any level of the hierarchy if the distance of the query image increases with the child node as compared to the parent node for more than half of the features then the child node is removed for further consideration.

Nodes satisfying the condition imposed by Extended OR-Inc pruning are used to assign general and specific semantics to the query image. General semantics are those subtrees which are selected by the branch selection at the top level of the categorized database. To get specific semantics, ranks of nodes selected at each level of the hierarchical database is explored. In this work, ranks of nodes are calculated corresponding to three features individually. A rank is obtained by simply arranging the nodes in increasing order of distances, i.e., semantics of a node at rank 1 is the most desirable semantic. There are three ranks available for each surviving node in the search space. To get a single list, color, texture and shape ranks are fused using CombSUM fusion (Wu et al., 2006). The final top three semantics are then assigned as the required specific semantic to the query image. At the time of image retrieval, only those images are considered that belong to the nodes corresponding to retrieved specific semantics. Linear matching is done and the top ten images are retrieved.

@&#RESULTS AND DISCUSSION@&#

The performance of the proposed system is evaluated in terms of retrieved semantics as well as retrieved images. All experiments are performed on a system with Core 2 Quad processor, 8GB RAM and 500GB hard disk.

Although concept of ontology generation is explored for improved textual queries and annotations, but in CBIR systems hierarchies are not used commonly. WordNet, a rich lexical database follows ontology of concepts built on a semantic structure and is opted for a lot of applications. Instead of WordNet, a semantically categorized hierarchical image database built upon the backbone of WordNet, i.e., ImageNet is used in this work (Deng et al., 2009). For each of its synsets, images corresponding to the set of WordNet synonyms are collected from several image search engines in various languages. ImageNet 2011 Winter Release has more than 14,000 K images for nearly all object classes categorized by 21,841 synsets. Fig. 7 shows the structure of ImageNet hierarchy and some representative images for Fabric category.

Due to limited computing resources a subset of ImageNet, with 3,32,000 images from 365 synsets belonging to the most common 11 categories is used and shown in Table 3
                        
                         is used for experimentation. The proposed system is exhaustively tested with two sets of query images for ImageNet database. Set-1 of query images consists of four subsets automatically formed by randomly selecting 5% of images from each synset of test database. Average performance of the system on these four subsets is considered to overcome any sort of bias.

The experiments are also performed on commonly used WANG database for comparison purpose. Images of ten categories of WANG are arranged at the top level to form a categorized structure. Set-2 of 36,500 query images is collected through (Google image search engine, 2015). It consists of 100 images for each synset under the categories shown in Table 3. For WANG database each of its image is used as query image. The system automatically assigns semantics to query images and checks it against its predefined semantics. No manual intervention is required during performance evaluation as human subjectivity may affect the understanding of correlation between visual and semantic similarity.

Output of the system after ‘Branch Selection Algorithm’ @n = 3 for the query image Sport, athletic → Skating → n00448466_58 is shown in Fig. 8
                        (a). The algorithm successfully retrieves the desired Sport, athletic synset. It also reduces the actual search space (11 subtrees, 365 synsets) to only 3 subtrees with 29 synsets. Fig. 8(b) and (c) show the search space after first and second level of pruning. ‘First Level Pruning Extended OR’ retains 24 synsets which is further reduced to 9 by ‘Second Level Pruning Inc’. Finally, these 9 synsets are arranged in the increasing order of distances from the query image and the top three synsets are selected to generate the suggested list of semantics.

The performance of the system is expressed in terms of accuracy that denotes the selection of desired subtree in terms of percentage. Table 4
                         compares the performance of Pandey et al. (2015) and the proposed system on 11 subtrees of ImageNet for 
                           
                              n
                              =
                           
                         3 which prunes the search space by 75%. In this work, the parameter 
                           
                              N
                              =
                              11
                           
                         therefore 
                           
                              n
                              =
                              
                                 ⌈
                                 N
                                 /
                                 4
                                 ⌉
                              
                              =
                              
                                 ⌈
                                 2.75
                                 ⌉
                              
                              =
                              3
                           
                        . The table helps to understand pros and cons of introducing clustering and indexing of visual signatures of nodes in the image tree. The following discussion is based on the average values obtained for the parameters with Branch Selection Algorithm (BSA) (Pandey et al., 2015) and Cluster and indexing based Branch Selection Algorithm (IndxCBSA) for query images of Set-1 and Set-2.


                        Set-1: For 11 hierarchies tested with Branch Selection Algorithm (Pandey et al., 2015), more than 70% accuracy is achieved for 7 hierarchies. Also, it out-performs if a query image is from Appliance (93.4%), but opposite is the case if the image is from Fabric or Sports, Athletic ( ≈ 30%). This happens due to the nature of the images that constitutes these categories. The proposed system incorporating the concept of clustering gives considerably improved results especially for the two poorly performing hierarchies. It improves accuracy from 32.22% to 57.32% for Fabric and 30.71% to 54.39% for Sports, Athletic. Although a little drop in accuracy (0.59%) is observed for Flower. For the remaining categories the improvement is clearly visible. For seven hierarchies Appliance (0.15%), Vegetable (0.6%), Tree (4%), Animal (5.29%), Structure (6.72%) Person (7.42%) and Geological Formation (7.63%) the improvement is less than 10%, and for hierarchy Fruit a significant improvement of 13% is observed.

Comparing the algorithms in terms of search space and time, then Branch Selection Algorithm (Pandey et al., 2015) uses 12.85% of search space (in terms of selected nodes) and takes about 20 s to retrieve the semantic. The proposed system uses more search space and also an increase in semantic retrieval time is observed. Although search space increases by 1.71% only but search time increases from 19.26 sec to 30.60 sec. This happens because BSA performs a single comparison of the query image and visual signature of a node/category, but IndxCBSA compares query image with ‘c’ dominant clusters traversed in VP trees. That is, Pandey et al. (2015) used 47 synsets to retrieve semantics of a query image with accuracy of 68.12% on an average. The proposed system uses 53 synsets and reports an overall average accuracy of 76.61% to retrieve image semantics. In all, using approximately 1.7% more search space the proposed system significantly improves the retrieval accuracy by 8.5%.


                        Set-2: For query images from Set-2 as well, the Branch Selection Algorithm (Pandey et al., 2015) outperforms if an image is from Appliance, and under-performs for query images from Fabric (35.40%). Also, accuracy of more than 65% is reported for 4 hierarchies only. Incorporating the concept of clustering and indexing considerably improves the obtained accuracy values and successfully reports more than or approximately 65% accuracy for 9 out of 11 hierarchies. For Structure the reported accuracy is 1% less than 65%, i.e., 64%. For the only remaining hierarchy, Fabric, although the accuracy is improved by 13.77% but the final reported accuracy is 49.17%. Here also, the proposed system uses 2.04% more search space than Pandey et al. (2015), but improves the overall accuracy of image semantics retrieval by 7.32%.

The observations made for both the sets of query images are similar. Thus, it can be summarized that adaptive selection of branches with clustered and indexed visual signatures, i.e., IndxCBSA, gives a better trade-off between accuracy and semantics retrieval time as compared to Branch Selection Algorithm (Pandey et al., 2015).

Performance of an image retrieval system depends on its capability to retrieve images similar to that of a query image. In information retrieval, the performance of the developed system is evaluated on its probability of getting relevant images, i.e., precision and recall. Precision is the percentage of retrieved instances that are relevant, while recall is the percentage of relevant instances that are retrieved (Müller et al., 2001). Average performance of the system over a large set of queries is considered to draw precision-recall curve for overall evaluation of system performance. Average precision at each standard recall level is computed across all queries. The precision-recall graphs obtained for 11 hierarchies of ImageNet with query images from Set-1 and Set-2 are shown in Fig. 9
                        (a)–(k). The precision–recall graph illustrates system performance at multiple operating points. These graphs are used here to compare the performance of the system for different semantics. Curves closer to the upper right-hand corner of the graph (where recall and precision are maximized) indicate the best performance. The exact slope of the curve may vary between systems, but slope of the precision-recall curve is downward from left to right. This typical nature of curve is in support of the observation that when more relevant images are retrieved (i.e., recall increases), more non-relevant images would also be retrieved (i.e., precision decreases). This general inverse relationship between recall and precision remains.

The graphs shown in Fig. 9(a)–(k) are obtained for images retrieved from the search space retained after the application of IndxCBSA followed by ‘Ext OR- Inc’ pruning. IndxCBSA selects on an average 14.65% of the search space which gets further pruned by 76.35%, and hence the actual search space used to retrieve relevant images is approximately 3.5% only. Moreover, precision and recall values are calculated automatically without inclusion of any information related to the obvious relationship existing among the categories in this very small search space. To understand this consider the example given in Fig. 8 for the query image Sport, athletic → Skating → n00448466_58. The system retrieves 9 synsets from Fabric, Sports and Person hierarchies. Although the image belongs to Sports, but Person semantic cannot be treated as irrelevant in this case. Also, Fabric contains images where a model is posing for a dress, and the search space may contain those images which are again not irrelevant for this query image. Similarly, Fig 2 shows the retrieval results for the query image from Geological Formation → Natural elevation for 
                           
                              n
                              =
                              3
                              ,
                           
                        
                        
                           
                              N
                              =
                              11
                           
                        . This image is of a tree planted somewhere at the mountain top and is thus placed under Natural elevation synset. It is clearly visible that an object which is being focused in this image is a tree. The proposed system is able to retrieve both; the desired and correlated general semantics (i.e., Geological Formation and Tree) but when it comes to specific semantics retrieval, the synsets belonging to Tree (Conifer, Pine tree, and Pinon, pinyon) are identified as more relevant for retrieval. As stated earlier, images are retrieved from so obtained search space. For this query image, the search space consists of most of the synsets from Tree semantic. Looking at the top ten images shown in Fig 2, the reported precision is more than 90%. While calculating precision and recall values these types of correlations are not considered as it demands for some kind of manual intervention, and in the present state the proposed system is fully automatic. Thus, an image retrieved is considered relevant only if its semantic is same as that of the query image in ImageNet otherwise it is an irrelevant retrieval. For this query image the automatic process reports 0% precision and recall values because out of all the retrieved images not a single image belongs to the actual desired semantic Geological Formation. A few more problematic images similar to the above discussed scenario are shown in Table 5
                        . A quick look of the images and semantics assigned to those by the proposed system as compared to the semantics assigned in ImageNet reveals the accuracy and usefulness of the proposed system.

All the graphs in Fig. 9 are plotted under these conditions which actually give the retrieval performance on the lower side. These graphs also show that for any of the hierarchies considered, the performance of the proposed system is approximately same for Set-1 (images within database) and Set-2 (images outside database collected through Google) of query images. Fig. 9(l) shows a little better average performance of the system for Set-2 as compared to that for Set-1. This further indicates that the semantics considered for experimentation are properly and effectively represented in visual space through commonly used visual features and proposed techniques. Overall, the system shows promising image retrieval results considering the size of the database used for experimentation and the actual search space used for retrieval.

For some of the query images, Table 6
                         summarizes the general and specific semantics derived with IndxCBSA for the ImageNet database considered in this work with 
                           
                              n
                              =
                              3
                           
                         followed by ‘Ext OR-Inc’ pruning. The search space pruned is also mentioned for each query image. The remaining search space is used to retrieve relevant images, and each row shows the top five retrieved images corresponding to the query images.

To compare the proposed system with other works (ElAlami, 2011; Lin et al., 2009; Shrivastava & Tyagi, 2015; Youssef, 2012), WANG is considered. WANG contains 1000 images (100 images for each of the 10 categories) of Africa, Beach, Buildings, Buses, Dinosaurs, Elephants, Flowers, Horses, Mountains, and Food categories. It is categorized at the top level and each image is used as query image. The reported values show that the developed system performs well on a flat non-hierarchical image database.

Performance of the system is compared with other CBIR systems existing in literature (ElAlami, 2011; Lin et al., 2009; Shrivastava & Tyagi, 2015; Youssef, 2012). With the aim of eliminating the usage of fusion and normalization techniques, Shrivastava and Tyagi (2015) presented a three stage system. The first stage retrieves a fixed number of images based on the color histogram. Relevance is further improved by matching the Gabor texture and Fourier descriptor based shape features of the retrieved images respectively. ElAlami (2011) utilized genetic algorithm to optimize 3D color histogram and Gabor texture features to simplify the image retrieval process. The most relevant feature set is extracted through two successive functions, preliminary and deeply reduction. Lin et al. (2009) proposed an image retrieval system, which uses a combination of color co-occurrence matrix, difference between pixels of scan pattern, and color histogram for k-means to describe different image content. It has also adopted sequential forward feature selection to simplify computation of image retrieval. Youssef (2012) proposed a curvelet-based scheme utilizing dominant color and texture features. Dominant colors are extracted using an enhanced region-based vector codebook sub-band clustering; and curvelets are fine tuned to capture accurate directional features.

The precision and recall values are calculated at the rate of 20 retrieved images for each of the 1000 WANG images. Precision and recall values of query images in a category are then averaged to get the corresponding average precision and average recall. Fig. 10
                      compares these systems on the basis of average precision and average recall with the number of retrieved images varying from 20 to 100. From Table 7
                      it is clear that the proposed system successfully achieves a better overall performance as compared to ElAlami (2011); Lin et al. (2009); Shrivastava and Tyagi (2015). As compared to the system proposed by Youssef (2012), the proposed system performs better for Africa and Elephants categories only. Also, on an average the proposed system performs a little lesser when precision and recall are calculated @20 in Table 7. Ideally, precision increases with an increase in recall if relevant images are present in the database. In this situation a system which shows least drop in precision with increasing recall is better. On increasing the number of retrieved images, the proposed system proves to be better. Drop in precision with an increase in recall means that the system is retrieving more number of irrelevant images although there are relevant images present in the database. Looking at Fig. 10 it is clearly visible that the proposed system outperforms other systems with an increase in the number of images retrieved. The graphs reveal the superiority of the proposed system in maintaining good precision and recall values.

The semantics and image retrieval system proposed in this manuscript is fully automatic and does not require any manual intervention or user feed back to do the specified task. Clustering of images belonging to a category and identification of dominant clusters help to address the variability and broadness covered by this semantic. As a result, now a semantic/category is assigned a set of visual signatures corresponding to the dominant clusters it contains. This results in the reduced overlap of categories in visual space which enables the system to perform closer to human perception. The proposed system assigns acceptable semantics in many cases. This reveals that a set of visual signatures used to represent a semantic in hierarchical image database are performing well. The proposed system also shows good image retrieval performance considering the portion of image database used for retrieval. Average execution time of the proposed system is approximately 31 seconds, which is high for an online search. However, considering the present computational facility used with this size of database, the results are encouraging. In the real time environment, it is proposed to execute algorithm at the server for substantial reduction in its execution time.

Although the focus of this work is to design an automatic system, but inclusion of user feedback would help to enhance the performance of the proposed system. In case system fails to select proper semantics, user feedback may help to backtrack and select the appropriate ones. Image features and visual signatures of nodes in the database are generated offline, and new images are not inserted during experimentation. In real life, database may be kept in the updated mode. This insertion intensiveness can be easily handled online. Insertion of an image requires computation of its features, updating the set of visual signatures and the indexes of the node to which this image is added. Also the proposed system supports only image based queries, in future a keyword based search can also be incorporated.

@&#REFERENCES@&#

