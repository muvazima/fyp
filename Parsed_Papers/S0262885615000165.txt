@&#MAIN-TITLE@&#Near laser-scan quality 3-D face reconstruction from a low-quality depth stream

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We infer a very accurate 3-D face model for a freely moving user from a single depth camera.


                        
                        
                           
                           Using unwrapped cylindrical 2-D images enables us to use simple 2-D image processing algorithms to process the 3-D information.


                        
                        
                           
                           We use a combination of spatial smoothing and temporal integration for noise removal.


                        
                        
                           
                           A robust rejection method produces reliable results in the presence of facial expression changes and partial occlusions.


                        
                        
                           
                           Our system runs online and in real-time.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Kinect

3D reconstruction

Face modeling

@&#ABSTRACT@&#


               
               
                  We propose a method to produce near laser-scan quality 3-D face models of a freely moving user with a low-cost, low resolution range sensor in real-time. Our approach does not require any prior knowledge about the geometry of a face and can produce faithful geometric models of any star-shaped object. We use a cylindrical representation, which enables us to efficiently process the 3-D mesh by applying 2-D filters.
                  We use the first frame as a reference and incrementally build the model by registering each subsequent cloud of 3-D points to the reference using the ICP (Iterative Closest Point) algorithm implemented on a GPU (Graphics Processing Unit). The registered point clouds are merged into a single image through a cylindrical representation. The noise from the sensor and from the pose estimation error is removed with a temporal integration and a spatial smoothing of the successively incremented model. To validate our approach, we quantitatively compare our model to laser scans, and show comparable accuracy.
                        1
                     
                     
                        1
                        This paper extends the method presented in [15].
                     
                  
               
            

@&#INTRODUCTION@&#

Accurate 3-D face modeling with affordable sensors offers many potential applications in biometrics, as in [20], or the gaming industries. Therefore, many researchers in computer vision and graphics have focused on this problem and proposed methods from structured lighting systems, multiple images, videos, and even a single image [1,42,22,36,2].

Getting an accurate 3-D face model from low-quality 3-D cameras, such as the PrimeSense [31] camera, is a challenging problem. These cameras can provide both a standard RGB image and a depth image containing the 3-D information at 30 frames per second in VGA format. However, the quality of any single frame is not sufficient to generate reasonable 3-D models. The sensor computes a depth map based on the triangulation principle given correspondences between stored pattern and projected pattern. Hence, depth data near boundaries can be very noisy, as evaluated in [4,32]. A simple averaging in time is not sufficient.

In this paper, we leverage the recent developments of inexpensive 3-D sensor technologies. We propose to generate 3-D models of a user's face from the depth video stream of a low-cost range sensor online and in real-time.

In order to compensate for the noisy depth data, we propose to use several poses, accumulate and refine the information through time. Prior to any processing, we reduce the noise and blur the discontinuities of the raw input data by applying a bilateral filter [38]. Then, we automatically segment the head region.

The first frame, containing a near frontal face is set as a reference, and is used to generate a 3-D point cloud. Given a new frame, we convert the depth map into a 3-D point cloud and register it with respect to the reference point cloud.

To accumulate multiple views from a moving face, we propose to use unwrapped cylindrical 2-D images in canonical form, which is a well-known technique to represent a 3-D face [40,22]. This method enables us to perform 2-D image-based operations to filter out noisy input, instead of complex 3-D mesh processing. Also, a running mean is performed on every pixel for temporal integration and a bilateral filter [38] is used for spatial smoothing. Fig. 1
                      shows the overview of our approach.

Building a set of unwrapped cylindrical 2-D images enables us to add the information on previously occluded parts and refine the poor information on the edges. However, any error in the 3-D pose estimation would impact the model. To minimize the added noise and increase the robustness of our approach, we add a rejection process. We evaluate the quality of each new unwrapped cylindrical 2-D image and discard those which seem to be due to different facial expression or partial occlusions.

The contributions of this paper are as follows.
                        
                           •
                           We infer a very accurate 3-D face model for a freely moving user from a single depth camera. The reconstructed 3-D faces are compared to laser scan ground truth data and show comparable results.

Using unwrapped cylindrical 2-D images enables us to use simple 2-D image processing algorithms to process the 3-D information, making the process computationally efficient.

We use a combination of spatial smoothing and temporal integration for noise removal.

A robust registration and a user-dependent rejection method produce reliable results in the presence of facial expression changes, partial occlusions and wide head pose angle changes.

Our system runs online and in real-time.

In the following sections, we first review the related work, then describe the details of the proposed approach, and discuss our results.

@&#RELATED WORK@&#

Many proposed methods to build a face model start from a generic model which is then deformed to fit the input face. In an early study, Tang and Huang [36] created face models by locating facial features on the input face and deform the generic face model accordingly. The reconstructed models are decent but still not dense enough to have a good accuracy. In [2], a deformable 3-D face model is introduced to build a 3-D face from a stereo system. A nonlinear optimization technique is then used to fit the model to the images. A similar approach is used by Le [19], who uses a linear morphable model on two stereo images to accurately reconstruct the 3D face shape. Some other methods that rely on deformable models have been proposed to infer 3D face models from a single facial image along with extracted landmarks [9,10].

More recently, this type of approach has been used for face modeling using Microsoft's Kinect [18] sensor. Zollhöfer [42] builds a 3-D model by fitting a generic face model to the scans and refining the registration thanks to a set of features detected in the RGB space. In [11], a generic template is matched to the Kinect scans thanks to a graph-based non-rigid ICP approach [21]. This reconstructed model is then used for facial landmark tracking.

All these methods provide results that are good-looking and easy to animate, but the generic model tends to bias the reconstructed model.

Another method for face modeling takes advantage of the SfM (Structure-from-Motion) technique. A data-driven face modeling which takes advantage of SfM with five different views is proposed in [22]. The reconstructed models are very accurate but the process requires high-resolution images and a studio set-up. An interesting technique, proposed in [26], infers a 3-D model from a video sequence by performing stereomotion between pairs of images. The process gives good results but requires very high quality images. In [17], 3-D face models are built from large unstructured photo collections by optimizing over pose, shape and lighting. These results are promising, given the challenge of the process, but not metrically correct.

State of the art methods that provide very high quality face models require special equipment, such as [33], or a studio environment [1,7,22,37]. In [1], the user has to be scanned in a ball-shaped light stage with 156 LED lights that capture the face's geometry and reflectance. [7] uses a setup of 14 high definition video cameras to capture small patches of the face surface, then applies an iterative binocular stereo method to reconstruct the model. The approach in [37] requires the users to wear reflecting markers on their face and a motion capture system is needed to create and animate the models.

Recently, with the release of the affordable PrimeSense technology, some approaches that use low-quality depth sensors for face modeling have been proposed. In [34], a fast approach is presented to reconstruct a face from 4 RGB-D images. To make the process fast, the depth maps are fused instead of the 3D models. This step introduces back-projection error. Also, the approach is limited by the small number of images.

In [28], the KinectFusion system takes streaming depth data from a moving Kinect camera and creates a high-quality 3-D model for a static scene. Dynamic interaction is considered in [16] where camera tracking is performed on a static background scene, and the foreground object is tracked independently. Aligning all depth points with the complete scene model from a large environment (e.g. room) provides very accurate tracking of the camera pose and mapping.

However, getting a face model is not convenient. The user must stay perfectly still while the sensor moves around him. Besides, the resulting face model does not contain enough points to capture accurately the 3-D structure. Note that KinectFusion was designed to model static scenes and not moving objects. It makes extensive use of the static background to register the 3-D scans and therefore is not suitable for modeling moving objects.


                     [24] and [27] present methods that tune the KinectFusion system to model faces instead of room-sized environments. In [24], Macedo et al. detect the face with the classical Viola–Jones face detector [39] in the color image and apply the KinectFusion method inside this region of interest to reconstruct the face. They use this reconstructed model for face tracking later on. In [27], Meyer and Do use a volumetric representation to model the face. The face region is segmented from the depth map, registered and integrated to the model. They report visually good results but the model is not very compact and they did not perform a thorough analysis on their approach.

In [3], a method that uses 2-D canonical images with spherical coordinates is introduced. This method enables the user to reconstruct the entire head from any initial pose but the accuracy of the reconstructed model is very sensitive to the location of the sphere center. In comparison, a cylindrical coordinates approach provides better accuracy because the location of the cylinder axis is not as critical.

In our approach, contrary to KinectFusion, the sensor is fixed and the user can move his head freely in front of it. We propose a method based on a cylindrical representation which is fully data-driven and can produce very accurate results from an affordable noisy low-resolution sensor.

@&#METHOD@&#

For our experiments, we use the PrimeSense [31] camera, which provides both a standard RGB image and a depth image containing the 3-D information at 30 frames per second in VGA format.

Prior to any filtering, we apply a bilateral filter [38] to the raw depth map to reduce the noise and smooth out discontinuities, as in [28]. Appendix B describes how to implement the bilateral filter. The parameters we use in this step are σ
                           
                              s
                           
                           =3.0 and σ
                           
                              r
                           
                           =0.01.

Assuming that only one person is using the system, we use a simple and fast method to detect the user's face from the range data. The main idea is to consider that the chin is a discontinuity in the depth map.

First, the upper body is extracted from the background using a depth threshold. We discard the depth data beyond 0.85m. This limits the intrinsic quantization effect of the depth computation of PrimeSense. Indeed, based on the formula derived in [32], the quantization q depends quadratically on the distance z (in meter) to the sensor: q(z)=2.73z
                           2
                           +0.74z
                           −0.55[mm]. In our case, this effect is reduced to 2.02mm.

Then, the highest point is assumed to be the top of the head. From this point, we look downwards along the Y-axis until we find a discontinuity larger than 15mm in the depth map. That is assumed to be the chin. Finally, we take the right-most and left-most points between the top of the head and the chin to get the sides of the head. We then extract the physical size of the head W
                           ×
                           H.

For the following images, since the typical motion is small, we look for the face in a neighborhood of 5cm around the previous location. False positives are removed by checking that the size of the face remains consistent. The width and height of the detected head should not be bigger than twice the W,
                           H computed in the first frame.

Our system is very simple, hence very fast and can detect the face in every pose regardless of the illumination and can handle some challenging cases such as expression change, cave hair, presence of glasses or some occlusions (Fig. 2
                           ).

This face detection module can be replaced by more advanced methods, such as [29,25]. However, the module must be able to crop the head regardless of its pose. This is why we decided not to use a widely used method such as the OpenCV face detector.

The first frame, containing a near frontal face, is set as a reference and used to generate a 3-D point cloud M
                           0. At each frame, we segment the face region using the method described in the previous part, then we randomly sample 1100 3-D points on the face to get an input point cloud. This number of points was selected as a good trade-off between computation time and accuracy. Note that other heuristics could be used for points selection. For example, we could use uniformly sampled points as a slightly slower option.

In order to estimate the face pose, we use a registration algorithm [41,23] between every input point cloud and the point cloud from the reference frame. This kind of approach is accurate and allows us to recover the pose at any time after an error occurs. Note that we could register consecutive images and incrementally infer the pose. However, such methods suffer from drift propagation [5], meaning that the small pose errors δ between consecutive frames add up and result in an increasingly large error up to nδ after n frames. Using a reference frame enables us to make sure that the pose error is limited to the registration error δ.

Since we have no known correspondences between images, we estimate the pose with a CUDA implementation of EM-ICP [35], which enables to obtain real-time performance [23]. Given the reference and the input 3-D point clouds, EM-ICP [8] iteratively computes the best rigid transformation [R,
                           t] composed of a rotation R and a translation t between the point clouds. The approach is briefly described in Appendix A. Note that this algorithm does not rely on any specific landmarks. A good alternative could be to find and track facial feature points. Unfortunately, landmark tracking is a difficult problem.

A known problem of the EM-ICP algorithm is the choice of the initial transformation matrix T
                           0, which is critical for both accuracy and speed. A wrong initialization would either make the performance drop or converge towards a local minimum and provide inaccurate results. This is handled by initializing the transformation [R
                           0,
                           t
                           0]
                              t
                            at time t by the value [R,
                           t]
                              t
                              −1 it had at time (t
                           −1). This hypothesis holds since the difference of object position between two consecutive frames is typically small.

Another problem is that this method works in a very limited range. To increase the range of reachable yaw angles, we split the reference face, which is assumed frontal, into two halves: the left part and the right part.

Based on the transformation at time (t
                           −1), we decide which strategy should be applied at time t. If the previous yaw angle was inferior to −15°, we use only the left half at time t. If it was superior to +15°, we use the right half only. In other cases, we use the whole face.

Our module provides good pose estimation results for −40° to 70° for pitch angles, −70° to 70° for yaw angles and 360° for roll angle, which is enough for casual behavior in front of a camera. The system can handle some partial occlusions and expression changes, and also recover if the person goes out of the field of view and back in.

Note that this pose estimation module can be replaced by other methods, such as [12,6,25], as long as they provide sufficient accuracy. Even though our approach is sufficient for face reconstruction, we would need an approach that could work with larger pitch and yaw angles if we wanted to reconstruct the entire head.

The key idea for modeling is to accumulate information through time. By processing a video with several face poses, we gather information on the whole face. Moreover, the noise of the depth information provided by the sensor can be significantly removed by filtering in both space and time.

We try to add the information provided by each frame to the model. This is challenging since the accuracy of the point cloud registration is critical to getting consistent results. Besides, we need to introduce an efficient modeling approach to limit the memory space occupied.

We use a cylindrical model to represent our 3-D model, as in [40,22]. Practically, a cylinder is set around the face in the reference frame. The axis of the cylinder is the vertical axis going through the middle of the head. Its location is loosely estimated by taking the middle of the face detected by the method described earlier and setting the z value 10cm deeper than the point of minimum depth. Note that an approximate location of the axis is sufficient.

The geometry of a facial surface can be represented using a single image. This image is an unwrapped cylindrical depth map D, where the value at the pixel D(θ, y) is the horizontal distance ρ to the cylinder axis (Fig. 3
                           ). See Appendix C for details on how to compute the unwrapped cylindrical image.


                           Fig. 3 (right) shows an example of the unwrapped map D generated from one image.

This representation enables us to easily transform the 3-D data into a 2-D image. It has several advantages. First of all, it limits the memory space to a single image, which is suitable for an algorithm where information is incrementally added at each new frame. Second, the 3-D data can be processed as a 2-D image. Processing such as filtering becomes easy to use and can be applied very fast. Third, the mesh can be created easily and quickly by creating triangles among the neighboring pixels on the image.

The main drawback of this model is that it can handle only star-shaped objects. Still, it is suitable for a face and enables fast computation.

To obtain a smooth model, we remove the noise from both the input data and the error in pose estimation by the combination of temporal integration and spatial filtering (Fig. 4
                           ).

For the temporal integration, we apply a running mean on the ρ value of each pixel of the unwrapped cylinder map. This temporal integration reduces the intrinsic noise [4] while aggregating the data. Appendix D describes in detail how the running mean is implemented.

After temporal integration, the obtained model is still not perfect and has to be refined. For each row in the cylindrical map, we apply a simple bilinear interpolation [14] method to fill up the remaining holes. Note that linearly interpolated points in the 2-D map fall on a 3-D circular arc in the original space. Then, we process the unwrapped cylindrical map to remove the remaining noise. We choose to apply a bilateral filter [38], which removes noise while keeping the edges. Given the expression described in Appendix B, the parameters we use are: σ
                           
                              s
                           
                           =30.0 and σ
                           
                              r
                           
                           =30.0. We apply a manually-defined mask to divide the unwrapped cylindrical image into two zones. We use a neighboring window of range 7pixels on the cheeks and a looser filter on the rest of the face by using a window of 5pixels. This enables to smooth out the cheeks area, which does not contain noticeable structural details, more than the nose or the eyes. This mask can be tuned depending on what kind of object is modeled.

As mentioned earlier, a good head pose estimation is needed to produce an accurate model. In order to handle pose estimation failures, we reject images in which the pose could not be properly computed.

Let us note M and I the unwrapped cylindrical images containing respectively the model and the new input image. We reject I if the difference between M and I is too large. We define the difference D(M,
                           I) between two images M and I as the average of the highest 50% of the pixel-wise Euclidean distances.

Thus, a small value of D(M,
                           I) corresponds to a good registration and a high value of D(M,
                           I) is caused by a poor registration or an occlusion of the face. The threshold to discard images is user-dependent and is set to twice the average of the differences of the first 5 frames with the reference frame.

To create the mesh, we use neighboring pixels of the unwrapped cylindrical 2-D image containing the model. A mesh is a triangle whose corners are neighboring pixels. Note that our representation is very flexible in terms of resolution. We can easily predefine areas in the unwrapped image where more resolution is needed (i.e. the nose) and reduce resolution elsewhere (cheeks, forehead, etc.).

Adding the color information is another complex problem. Applying a similar algorithm on the RGB input does not provide good results. Indeed, when the user moves around, the relative illumination condition changes for every 3-D point. The result does not look natural.

In order to get a crisper texture, we use only one RGB image. When the 3-D information of the model is computed, every point of the model is projected onto the reference image in order to fetch the RGB value. This projection can be directly done with OpenNI [30] built-in functions.

@&#RESULTS AND DISCUSSION@&#

In our experiments, we set a resolution of 360×200 for the unwrapped cylindrical map. We work on a single-core Windows 7 system with a 2.79GHz processor. GPU is used for pose estimation and uses a GeForce GTX580. The overall system runs at 10 frames per second.

The results are visually accurate, especially the 3-D shapes (Fig. 5 and 6
                        
                        ).

To quantify the accuracy of our models, we compare them to commercial laser scans from Cyber F/X [13] (Figs. 7–8
                        
                        ). The first thing to notice is that our method can reconstruct the hair while the laser scanning systems cannot. Therefore, we ignore the hair region in our comparative quality analysis. We can see that our model is very close to a laser scan.

We compute the signed and unsigned Euclidean distance from every point in our model to the closest point in the laser scan. For the signed distance, the sign is negative for a 3-D point of our model, falls within the head in the laser-scan and negative otherwise. Fig. 7f shows that the error is evenly distributed. Besides, the average errors are respectively 1.33mm and 1.24mm and the standard deviations are 1.73mm and 1.42mm for the two users in Fig. 7. Note that these errors may be due to the quantization effect during the data acquisition.

To compare our approach to the state-of-the-art, we ran Kinectfusion [28,16] several times on the two users and collected the best results. The reconstructed 3-D faces did not capture many details of the face structure and do not perform as well as our approach (Fig. 7g–8g).

We also quantify the improvement in accuracy with time. Fig. 8 shows the evolution of the average error relatively to the number of frames. We can clearly see that the more images we have, the higher the accuracy. After a certain number of images, the model converges and adding more information does not impact it significantly. Note that the error goes up in several peaks in the process. These increases occur when data that was not previously visible, like the sides of the face, is added to the model. In Fig. 8, the user turns his head towards the right after around 50 frames and left after around 85 frames.

In our approach, the cylinder is set arbitrarily vertical by assuming that the first image is near frontal. As a result, starting from a different pose could affect the subsequent steps. Fig. 9
                            shows the results when starting with a large pitch angle, yaw angle or roll angle. Starting with a large yaw angle is just a translation along the θ axis in cylindrical coordinate and does not impact the final model. However, a large initial pitch or roll angle can impact the overall accuracy. Indeed, starting with a positive pitch angle will result in a reduced resolution in several areas of the face. For example, the nose is not well distributed along the y-axis of the cylinder, as shown in Fig. 10
                           . As a result, the running mean will tend to over-smooth this region. Similarly, starting with a large roll angle impacts the resolution of the resulting model.

This model has several limitations that are imposed by the compactness of the cylindrical representation.

As previously stated, the main drawback of the cylindrical model is that it can handle only star-shaped objects. Indeed, the model considers that there can only be one 3-D point per angle θ in cylindrical coordinate. Therefore, this cannot reconstruct some non-convex parts, like the ears or the nostrils. The constraint should be relaxed in further work.

The location of the cylinder axis can impact the accuracy. This problem could be solved by finding the initial pose of the head in absolute coordinate. Then, the cylinder axis could be set properly regardless of the initial pose.

Also, this system was designed for rigid faces with no expression changes during the acquisition process. Still, it is somewhat robust to expression changes. We can assume that people's faces are neutral the majority of the time. Therefore, the running mean makes the model converge towards the user's neutral face if the acquisition time is long enough. However, if the acquisition time is short and the data contains lots of expression changes, then the model will be largely impacted around the mouth and eyes regions.

@&#CONCLUSION@&#

We have proposed and implemented an efficient, data-driven 3-D face modeling method. The key of our approach is the use of a cylindrical model and the combination of temporal integration and spatial smoothing to reduce the noise. Reducing the noise in each pixel results in reducing the variance of the data, which enables to increase the precision and get a higher resolution.

Experimental results with laser scan data confirm the accuracy of reconstructed models. Our method also performs as well as other state of the art methods while using a low-cost low-resolution noisy sensor. A free-version of the application is available on the OpenNI website [30].

In the future, work must be done on the texture information of the model. Lighting modeling and photometric calibration among RGB images are possible, given the reconstructed 3-D surface. Also, this cylindrical model seems like a good first approximation but this constraint must be released to get even better results in the concave areas of the face such as the nostrils or the ears.

@&#ACKNOWLEDGMENT@&#

This research is funded in part by HP Labs Innovation Research Program (CW 218094) and the IT R&D program of MOTIE/KEIT (10041610).

The point-to-plane Iterative Closest Point [8] is an algorithm that registers two point clouds with unknown correspondences. Given two point clouds P and Q with the point normals n
                     
                        p
                     , n
                     
                        q
                     , ICP is a two step iterative procedure that loops until convergence and works as follows:
                        
                           •
                           For every point p
                              
                                 i
                               in P, find the closest point q
                              
                                 i
                              
                              ⁎ in Q and get the corresponding normal 
                                 
                                    n
                                    
                                       q
                                       i
                                       *
                                    
                                 
                              .

Find the values of R,
                              t that minimize:
                                 
                                    
                                       E
                                       =
                                       
                                          
                                             ∑
                                             i
                                          
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      R
                                                      
                                                         p
                                                         i
                                                      
                                                      +
                                                      t
                                                      −
                                                      
                                                         q
                                                         i
                                                         *
                                                      
                                                   
                                                
                                                
                                                   n
                                                   
                                                      q
                                                      i
                                                      *
                                                   
                                                   T
                                                
                                             
                                          
                                          2
                                       
                                       .
                                    
                                 
                              
                           

There are several ways to solve this equation. One option is to linearize the system and solve via Cholesky decomposition.

A bilateral filter can reduce the noise and keep the edges. It can be used on the raw map to smooth out discontinuities, as in [28]. If we note z
                     
                        u
                      the depth measurement or intensity value at pixel u
                     =[u
                     
                        x
                     ,
                     u
                     
                        y
                     ]⊤ in the image domain 
                        I
                     , then the resulting value D(u) at pixel u is:
                        
                           
                              D
                              
                                 u
                              
                              =
                              
                                 1
                                 W
                              
                              
                                 
                                    ∑
                                    
                                       q
                                       ∈
                                       I
                                    
                                 
                                 
                              
                              
                                 f
                                 
                                    σ
                                    s
                                 
                              
                              
                                 
                                    
                                       u
                                       −
                                       q
                                    
                                 
                              
                              
                                 g
                                 
                                    σ
                                    r
                                 
                              
                              
                                 
                                    
                                       
                                          z
                                          u
                                       
                                       −
                                       
                                          z
                                          q
                                       
                                    
                                 
                              
                              
                                 z
                                 q
                              
                           
                        
                     where q
                     =[q
                     
                        x
                     ,
                     q
                     
                        y
                     ]⊤ is a pixel in the image domain, 
                        
                           f
                           
                              σ
                              s
                           
                        
                      is the spatial kernel for smoothing differences in coordinate and 
                        
                           g
                           
                              σ
                              r
                           
                        
                      is the intensity kernel for smoothing intensities.

We decide to use Gaussian functions for both 
                        
                           f
                           
                              σ
                              s
                           
                        
                      and 
                        
                           g
                           
                              σ
                              r
                           
                        
                     , i.e.
                        
                           
                              
                                 f
                                 
                                    σ
                                    s
                                 
                              
                              
                                 x
                              
                              =
                              exp
                              
                                 
                                    −
                                    
                                       x
                                       2
                                    
                                    
                                       σ
                                       s
                                       
                                          −
                                          2
                                       
                                    
                                 
                              
                           
                        
                     
                     
                        
                           
                              
                                 g
                                 
                                    σ
                                    r
                                 
                              
                              
                                 x
                              
                              =
                              exp
                              
                                 
                                    −
                                    
                                       x
                                       2
                                    
                                    
                                       σ
                                       r
                                       
                                          −
                                          2
                                       
                                    
                                 
                              
                              .
                           
                        
                     
                  


                     
                        W
                        =
                        
                           
                              ∑
                              
                                 q
                                 ∈
                                 I
                              
                           
                           
                        
                        
                           f
                           
                              σ
                              s
                           
                        
                        
                           
                              
                                 u
                                 −
                                 q
                              
                           
                        
                        
                           g
                           
                              σ
                              r
                           
                        
                        
                           
                              
                                 z
                                 
                                    u
                                 
                                 −
                                 z
                                 
                                    q
                                 
                              
                           
                        
                      is a normalizing constant. σ
                     
                        s
                      and σ
                     
                        r
                      are parameters we set depending on how strong we want the smoothing to be.

To be included into the unwrapped cylindrical map, every 3-D point must be converted from Cartesian coordinate to cylindrical coordinate. For each 3-D point of Cartesian coordinates (x, y, z), the cylindrical coordinate (ρ, θ, y) can be computed with the equations:
                        
                           
                              ρ
                              =
                              
                                 
                                    
                                       x
                                       2
                                    
                                    +
                                    
                                       z
                                       2
                                    
                                 
                              
                              ,
                           
                        
                     
                     
                        
                           
                              θ
                              =
                              
                                 
                                    
                                       
                                          0
                                       
                                       
                                          if
                                          
                                             
                                                x
                                                =
                                                0
                                                ∧
                                                y
                                                =
                                                0
                                             
                                          
                                       
                                    
                                    
                                       
                                          arcsin
                                          
                                             
                                                z
                                                /
                                                ρ
                                             
                                          
                                       
                                       
                                          if
                                          x
                                          ≥
                                          0
                                       
                                    
                                    
                                       
                                          π
                                          −
                                          arcsin
                                          
                                             
                                                z
                                                /
                                                ρ
                                             
                                          
                                       
                                       
                                          if
                                          x
                                          <
                                          0
                                       
                                    
                                 
                              
                              .
                           
                        
                     
                  

The resulting point can then be stored in an image in which the pixel (θ,
                     y) has a floating value of ρ.

Note that for a given 3-D point [x,
                     y,
                     z]⊤, the coordinates (ρ,
                     θ,
                     y) computed with the equation above are usually not integers. Therefore, one 3-D point impacts 4pixels in the unwrapped cylindrical map.

Let ⌊y⌋ and ⌊θ⌋ the greatest integers smaller than respectively y and θ, δy
                     =
                     y
                     −⌊y⌋ and δθ
                     =
                     θ
                     −⌊θ⌋. We distribute the computed ρ value among the 4 neighboring pixels 
                        
                           
                              
                                 θ
                              
                              
                                 y
                              
                           
                           
                              
                              ⊤
                           
                        
                        ,
                        
                           
                              
                                 
                                    θ
                                 
                                 +
                                 1
                                 ,
                                 
                                    y
                                 
                              
                           
                           ⊤
                        
                        ,
                        
                           
                              
                                 
                                    θ
                                 
                                 ,
                                 
                                    y
                                 
                                 +
                                 1
                              
                           
                           ⊤
                        
                      and [⌊θ⌋+1,⌊y⌋+1]⊤, and assign a different weight w(θ,
                     y) on each of these pixels, as in Table C.1.
                        
                           Table C.1
                           
                              Weight shares among 4 neighboring pixels.
                           
                           
                              
                              
                              
                              
                                 
                                    
                                       y
                                    
                                    
                                       θ
                                    
                                    
                                       w(θ,
                                       y)
                                 
                                 
                                    ⌊y⌋
                                    ⌊θ⌋
                                    (1−
                                       δy)(1−
                                       δθ)
                                 
                                 
                                    ⌊y⌋+1
                                    ⌊θ⌋
                                    
                                       δy(1−
                                       δθ)
                                 
                                 
                                    ⌊y⌋
                                    ⌊θ⌋+1
                                    (1−
                                       δy)δθ
                                    
                                 
                                 
                                    ⌊y⌋+1
                                    ⌊θ⌋+1
                                    
                                       δyδθ
                                    
                                 
                              
                           
                        
                     
                  

We maintain two images ρ and w and start with a blank cylindrical map in which each pixel [θ,
                     y]⊤ is assigned a value ρ(θ,
                     y)=0 and a weight w(θ,
                     y)=0. When adding a point P
                     =[ρ
                     
                        p
                     ,
                     θ
                     
                        p
                     ,
                     y
                     
                        p
                     ]⊤, we update the 4 neighboring pixels. For example, the pixel (⌊θ
                     
                        p
                     ⌋,⌊y
                     
                        p
                     ⌋)⊤, for which w
                     
                        p
                     
                     =(1−
                     δθ
                     
                        p
                     )(1−
                     δy
                     
                        p
                     ) will be updated by the following equations:
                        
                           
                              
                                 
                                    
                                       ρ
                                       
                                          
                                             
                                                θ
                                                p
                                             
                                          
                                          
                                             
                                                y
                                                p
                                             
                                          
                                       
                                       ←
                                       
                                          
                                             ρ
                                             
                                                
                                                   
                                                      θ
                                                      p
                                                   
                                                
                                                
                                                   
                                                      y
                                                      p
                                                   
                                                
                                             
                                             w
                                             
                                                
                                                   
                                                      θ
                                                      p
                                                   
                                                
                                                
                                                   
                                                      y
                                                      p
                                                   
                                                
                                             
                                             +
                                             
                                                ρ
                                                p
                                             
                                             
                                                w
                                                p
                                             
                                          
                                          
                                             w
                                             
                                                
                                                   
                                                      θ
                                                      p
                                                   
                                                
                                                
                                                   
                                                      y
                                                      p
                                                   
                                                
                                             
                                             +
                                             
                                                w
                                                p
                                             
                                          
                                       
                                       ,
                                    
                                 
                                 
                                    
                                       
                                       w
                                       
                                          
                                             
                                                θ
                                                p
                                             
                                          
                                          
                                             
                                                y
                                                p
                                             
                                          
                                       
                                       ←
                                       w
                                       
                                          
                                             
                                                θ
                                                p
                                             
                                          
                                          
                                             
                                                y
                                                p
                                             
                                          
                                       
                                       +
                                       
                                          w
                                          p
                                       
                                       .
                                    
                                 
                              
                           
                        
                     
                  

The running mean smooths out short-term fluctuations to highlight longer-term trends. In our approach, we use a running mean on ρ(θ,
                     y) values in the unwrapped cylindrical map.

In this mean, the weight w associated with a pixel D(θ,
                     y) at time t is:
                        
                           
                              w
                              
                                 θ
                                 y
                                 t
                              
                              =
                              
                                 
                                    ∑
                                    
                                       T
                                       =
                                       0
                                    
                                    
                                       T
                                       =
                                       t
                                    
                                 
                                 
                              
                              w
                              
                                 θ
                                 y
                                 T
                              
                              .
                           
                        
                     
                  

If we add N point 
                        
                           ρ
                           
                              p
                              i
                           
                        
                      with a weight 
                        
                           w
                           
                              p
                              i
                           
                        
                      at the location (θ,
                     y) and at time t, the update equations are:
                        
                           
                              
                                 
                                    
                                       ρ
                                       
                                          θ
                                          y
                                          t
                                       
                                       =
                                       
                                          
                                             ρ
                                             
                                                
                                                   θ
                                                   ,
                                                   y
                                                   ,
                                                   t
                                                   −
                                                   1
                                                
                                             
                                             w
                                             
                                                
                                                   θ
                                                   ,
                                                   y
                                                   ,
                                                   t
                                                   −
                                                   1
                                                
                                             
                                             +
                                             
                                                
                                                   ∑
                                                   i
                                                
                                                
                                                   
                                                      ρ
                                                      
                                                         p
                                                         i
                                                      
                                                   
                                                   
                                                      w
                                                      
                                                         p
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             w
                                             
                                                
                                                   θ
                                                   ,
                                                   y
                                                   ,
                                                   t
                                                   −
                                                   1
                                                
                                             
                                             +
                                             
                                                
                                                   ∑
                                                   i
                                                
                                                
                                                   
                                                      w
                                                      
                                                         p
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                       w
                                       
                                          θ
                                          y
                                          t
                                       
                                       =
                                       w
                                       
                                          
                                             θ
                                             ,
                                             y
                                             ,
                                             t
                                             −
                                             1
                                          
                                       
                                       +
                                       
                                          
                                             ∑
                                             i
                                          
                                          
                                       
                                       
                                          w
                                          
                                             p
                                             i
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        
                     
                  

As time passes, the model gets increasingly finer and new data has less updating impact.

@&#REFERENCES@&#

