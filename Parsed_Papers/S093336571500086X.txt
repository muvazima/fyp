@&#MAIN-TITLE@&#Robust feature selection to predict tumor treatment outcome

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A novel wrapper method searches forward in a hierarchical feature subset space.


                        
                        
                           
                           Prior domain knowledge is incorporated into the selection procedure.


                        
                        
                           
                           Promising results are obtained on two cancer-patient datasets.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Hierarchical forward feature selection

Prior knowledge

Prediction

Small sample

Positron emission tomography

Support vector machine

@&#ABSTRACT@&#


               
               
                  Objective
                  Recurrence of cancer after treatment increases the risk of death. The ability to predict the treatment outcome can help to design the treatment planning and can thus be beneficial to the patient. We aim to select predictive features from clinical and PET (positron emission tomography) based features, in order to provide doctors with informative factors so as to anticipate the outcome of the patient treatment.
               
               
                  Methods
                  In order to overcome the small sample size problem of datasets usually met in the medical domain, we propose a novel wrapper feature selection algorithm, named HFS (hierarchical forward selection), which searches forward in a hierarchical feature subset space. Feature subsets are iteratively evaluated with the prediction performance using SVM (support vector machine). All feature subsets performing better than those at the preceding iteration are retained. Moreover, as SUV (standardized uptake value) based features have been recognized as significant predictive factors for a patient outcome, we propose to incorporate this prior knowledge into the selection procedure to improve its robustness and reduce its computational cost.
               
               
                  Results
                  Two real-world datasets from cancer patients are included in the evaluation. We extract dozens of clinical and PET-based features to characterize the patient's state, including SUV parameters and texture features. We use leave-one-out cross-validation to evaluate the prediction performance, in terms of prediction accuracy and robustness. Using SVM as the classifier, our HFS method produces accuracy values of 100% and 94% on the two datasets, respectively, and robustness values of 89% and 96%. Without accuracy loss, the prior-based version (pHFS) improves the robustness up to 100% and 98% on the two datasets, respectively.
               
               
                  Conclusions
                  Compared with other feature selection methods, the proposed HFS and pHFS provide the most promising results. For our HFS method, we have empirically shown that the addition of prior knowledge improves the robustness and accelerates the convergence.
               
            

@&#INTRODUCTION@&#

A proportion of cancer patients develop post-treatment recurrence, which increases the risk of death. The ability to predict the treatment outcome prior to or even during the treatment can be of clinical value, as in this case therapy could be individually tailored according to the prediction [1]. Imaging plays a crucial role as it allows for a non-invasive following up of the tumor response to the treatment. Indeed, functional information gathered by positron emission tomography (PET) using the radiotracer FDG has already shown its predictive value for tumor response to treatment in several cancers, including esophageal [2], lung [3] and cervix [4,5]. Well-explored FDG-PET imaging features include, but are not limited to, metabolic tumor volume (MTV) and total lesion glycolysis (TLG), as functional indices describing metabolic tumor burden, and standardized uptake values (SUVs) describing FDG uptake within a region of interest (ROI), e.g. SUVmean, SUVpeak, or single pixel (SUVmax) [2,4]. Characterization of PET images through texture analysis [6,7], tumor shape [6] and intensity volume histogram [6] may also have potential predictive value for treatment outcome, providing additional and complementary indices. The values of these parameters before treatment, as well as during treatment, are claimed as predictors for recurrence [5,8]. The knowledge of the most predictive factors for treatment outcome is valuable, as doctors can then make personalized treatment plan. However, there is no clear consensus regarding the optimum predictive factors. Feature selection techniques facilitate the interpretation of data by identifying meaningful features for patient outcome prediction. However, feature selection methods are hardly robust to small sized datasets of the order of a few dozen samples, as often in the medical domain. In this study, we intend to design a reliable feature selection algorithm to reduce the small size effect, in order to find discriminatory features among a number of patient’ clinical and PET-based features, that allow to accurately predict the treatment outcome.

Feature selection methods can be broadly classified into filter, embedded and wrapper methods [9]. Filter methods consist in applying a statistical measure to assign a score to each feature, independently of a classifier. RELIEF (RELevance In Estimating Features, [10,11]) is considered as one of the most successful filter algorithms, where a margin-based criterion is used to rank the features. Authors in [12] propose the FAST (Feature Assessment by Sliding Thresholds) method, based on the area under the receiver operating characteristic which is generated by sliding threshold values in one dimensional feature space. However, these univariate ranking methods cannot take the interaction between features into account. Studies in [13] have pointed out that features which are irrelevant on their own can be useful in conjunction with other features, and that the combination of two highly correlated features can be more useful than each feature independently. Feature subset selection methods evaluate subsets of features together, as opposed to ranking features according to their individual discrimination power. Feature selection with kernel class separability (KCS) ranks feature subsets according to their class separability. It is claimed robust to small size samples and to the presence of noisy features [14,15]. However, as for other feature ranking methods, a threshold value or a number of features has to be specified by the user to obtain the final subset.

Embedded methods include feature selection as part of the training process, and are usually specific to given learning machines. For example, CART has a built-in mechanism to perform feature selection [16]. To split a node into two, one feature chosen by certain rules is used to differentiate observations, which also raises the univariate problem. Guyon et al. propose a method using the support vector machine (SVM) [17] based on Recursive Feature Elimination, named SVMRFE [18]. Starting with the whole feature space, SVMRFE progressively eliminates the least relevant ones, whose removal minimizes the variations of feature weights, until a user-defined number of features remains. The SVMRFE method yields nested feature subsets and still retains the risk of removing useful features as complementary to others. The SVMRFE method has been used in [19] to select predictive features from a number of 167 features composed of patient's clinical, demographic and imaging data, for the prediction of the treatment response in 20 patients with esophageal cancer. The best 10-fold cross-validation prediction is obtained using SVM with 17 selected features.

The principle of wrapper methods is to compare different feature combinations based on the classification performance. Forward selections perform well when the optimal subset is small. In the literature, sequential forward selection (SFS [20]) and sequential forward floating selection (SFFS [21]) are two representative algorithms. Starting from an empty set, SFS repeatedly selects the best feature among the remaining features, and adds it into the set of already selected ones [22]. The SFS method has been used in [6] to select relevant features among 19 PET imaging features for the outcome prediction, on two datasets composed of 14 cervix cancer patients and 9 head and neck cancer patients, respectively. Logistic regression models of the order of two are then constructed. The SFS method yields a nested feature subset, since once a feature is included, it cannot be reconsidered afterwards. This increases the risk of being trapped in suboptimal solutions. To solve the nesting problem, SFFS performs exclusion steps after each inclusion step [23], improving the chance to find the most relevant features. However, for small sample sized datasets, there is a high probability that some feature subsets will produce similar values or even the same value as the evaluation measure. This fact increases the possibility for SFS and SFFS to be trapped in local minima.

To reduce the problem caused by small sample size in existing wrappers, we propose a novel wrapper forward selection algorithm where SVM is used as the classifier and a feature subset may be retained or discarded based on its prediction accuracy. The novelty of the proposed wrapper is its search strategy. As different feature subsets may yield similar or identical accuracy values on small sample datasets, we propose to keep all candidate feature subsets that increase the accuracy at an iteration, with respect to the previous iteration. Each retained feature subset is then used to generate its successors by adding one feature from the original feature pool which is not yet included in this feature subset. These successors are evaluated at the following iteration. The forward inclusion step does not stop until the accuracy value stops increasing. As a result, the search procedure examines a hierarchical binary search space. Compared with other wrappers where the inclusion or exclusion of a feature is operated on a single feature subset with the highest accuracy at an iteration, our hierarchical search improves the possibility of obtaining the most discriminant feature subset. Another contribution is that we take into account domain knowledge to guide the feature selection. Domain knowledge is often available in the medical field (e.g. organ shape [24], patient characteristics, …). With respect to predictive factors for patient's outcome, many researches have shown that SUV-based features are of great significance [2–5]. We thus propose to incorporate this knowledge into the hierarchical forward selection, by setting the first feature as a SUV-based features. The prior knowledge constrained feature selection obtains a robust feature subset more efficiently.

The rest of this paper is organized as follows. In Section 2, the proposed hierarchical forward selection method and its prior knowledge version are illustrated in detail. In Section 3, experimental results on two real-world datasets are presented and discussed. The last section, Section 4, concludes this study.

@&#METHODS@&#

In this section we describe our wrapper forward selection algorithm. It starts from an empty set and then iteratively searches forward the binary feature subset space. During an iteration, all feature subsets which perform better than their predecessors in the previous iteration are retained, using a pre-defined criterion J as the evaluation measure. Let J
                        0 denote an initial lower-bound on J. The search algorithm relies on the following principle, described thereafter: at the kth (k
                        >0) iteration,
                           
                              •
                              generate the successors of the subsets retained in the previous iteration;

compute the J values of the successors and select the ones with a better J value than the feature subsets in the previous iteration.

Let 
                              
                                 F
                                 k
                              
                            denote the set of candidate subsets of features obtained at the kth iteration, and 
                              
                                 G
                                 
                                    k
                                    +
                                    1
                                 
                              
                            the set of successors to be evaluated at the following iteration. The successors are generated from candidate feature subsets, which are retained in kth iteration, and collected in 
                              
                                 F
                                 k
                              
                           . One candidate feature subset commonly generates several successors, by adding one feature from the original feature vector which is not part of this candidate subset. Define an operator 
                              A
                            as
                              
                                 (1)
                                 
                                    A
                                    (
                                    X
                                    ,
                                    T
                                    )
                                    =
                                    {
                                    X
                                    ∪
                                    {
                                    e
                                    }
                                    |
                                    ∀
                                    e
                                    :
                                    e
                                    ∈
                                    T
                                    }
                                 
                              
                           which returns a set of sets composed by the addition of each element of set T to the set X. For example, let X
                           ={1, 2}, T
                           ={3, 4}, and then 
                              A
                              =
                              {
                              {
                              1
                              ,
                              2
                              ,
                              3
                              }
                              ,
                              {
                              1
                              ,
                              2
                              ,
                              4
                              }
                              }
                           . By using operator 
                              A
                           , the successor generator is formulated as:
                              
                                 (2)
                                 
                                    
                                       G
                                       
                                          k
                                          +
                                          1
                                       
                                    
                                    =
                                    {
                                    A
                                    (
                                    E
                                    ,
                                    
                                    Y
                                    ∖
                                    E
                                    )
                                    |
                                    ∀
                                    E
                                    :
                                    E
                                    ∈
                                    
                                       F
                                       k
                                    
                                    }
                                 
                              
                           where Y is the original feature vector. Therefore 
                              
                                 G
                                 
                                    k
                                    +
                                    1
                                 
                              
                            is the set of sets composed of the union of any feature subset, i.e., any element E of 
                              
                                 F
                                 k
                              
                           , and any feature in Y that is not included in this feature subset. At the next iteration, k
                           =
                           k
                           +1, the elements of 
                              
                                 G
                                 k
                              
                            will be evaluated through a selector.

A feature subset, i.e., an element E in 
                              
                                 G
                                 k
                              
                           , will be retained in the selection process if it improves the J value, where J is an evaluation measure of feature subsets. Let 
                              
                                 J
                                 max
                                 
                                    k
                                    −
                                    1
                                 
                              
                            denote the maximum J value obtained at preceding iteration. When k
                           =0, 
                              
                                 J
                                 max
                                 0
                              
                            is set to the initial lower-bound value J
                           0. The feature subset E is selected as an outperformer if the following condition is verified:
                              
                                 (3)
                                 
                                    J
                                    (
                                    E
                                    )
                                    >
                                    
                                       J
                                       max
                                       
                                          k
                                          −
                                          1
                                       
                                    
                                 
                              
                           As we have stated that all subsets verifying Eq. (3) are retained in the set of candidate solutions 
                              
                                 F
                                 k
                              
                            in our proposed algorithm, 
                              
                                 F
                                 k
                              
                            then has the form:
                              
                                 (4)
                                 
                                    
                                       F
                                       k
                                    
                                    =
                                    {
                                    E
                                    ∈
                                    
                                       G
                                       k
                                    
                                    
                                    |
                                    
                                    ∀
                                    E
                                    :
                                    J
                                    (
                                    E
                                    )
                                    >
                                    
                                       J
                                       max
                                       
                                          k
                                          −
                                          1
                                       
                                    
                                    }
                                 
                              
                           
                        

We use the gmean [25] as evaluation measure J to estimate the prediction performance of a feature subset on test data generated through leave-one-out cross-validation (LOOCV). We use SVM as the classifier based on its good generalization performance. The evaluation measure J is:
                              
                                 (5)
                                 
                                    J
                                    =
                                    
                                       
                                          sensitivity
                                          ×
                                          specificity
                                       
                                    
                                    .
                                 
                              
                           See sensitivity and specificity in [26]. Imbalanced datasets are quite common in medical domain, e.g. a dataset might contain more healthy people than patients. By combining sensitivity and specificity, gmean is an accuracy measure balancing the classification between imbalanced classes.

The forward search stops when the inclusion of one more feature into any current candidate feature subset fails to improve the J value. This stopping criterion does not require prior knowledge about the optimal solution, as for example the size of the optimal feature subset. Since the feature selection process can be computationally intensive, a trade-off between the quality of the selected feature subset, measured by J in our algorithm, and computation time may be required. Thus, in order to make our algorithm more flexible, we define two additional options, based on user-defined values.

The first optional stopping criterion is when J exceeds a prefixed upper-bound value, denoted as J
                           UB (J
                           UB
                           ∈[0, 1], equals to 1 by default). The second optional criterion is the maximum number of features to select, denoted as N
                           max (N
                           max
                           ∈(0, |Y|], equals to |Y| by default, where |·| returns the cardinality).

If a stopping criterion is met, the forward search stops. The candidate solutions are included in 
                              
                                 F
                                 k
                              
                           . As the cardinality of 
                              
                                 F
                                 k
                              
                            may be greater than 1, i.e., 
                              |
                              
                                 F
                                 k
                              
                              |
                              ≥
                              1
                           , and several feature subsets may have the same J value, we thus propose to use a metric called the relative extremal margin to select a unique solution.

In addition to the prediction accuracy, we use the relative extremal margin (rEM) as a second metric to measure the quality of a classifier. The rEM is based on the output values of the SVM classifier of input training data, calculated as follows:
                              
                                 (6)
                                 
                                    rEM
                                    =
                                    
                                       
                                          
                                             min
                                             
                                                
                                                   s
                                                   i
                                                
                                             
                                          
                                          
                                             D
                                             f
                                          
                                          (
                                          
                                             x
                                             
                                                
                                                   s
                                                   i
                                                
                                             
                                          
                                          )
                                          ·
                                          
                                             y
                                             
                                                
                                                   s
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          max
                                          
                                             D
                                             f
                                          
                                          (
                                          x
                                          )
                                          −
                                          min
                                          
                                             D
                                             f
                                          
                                          (
                                          x
                                          )
                                       
                                    
                                 
                              
                           where D
                           
                              f
                            is the decision function, X the input patterns, Y the corresponding labels, and s
                           
                              i
                            the support vector indices. The rEM is then the minimum distance between the outputs of support vectors and decision surface (zero-surface), rescaled by the range of the decision function. As the rEM measures the confidence in classification, a good-quality classifier is supposed to produce a high rEM value. Note that a rEM may be computed on each LOO training set. A mean value may then be used as the final criterion.

The hierarchical forward selection is summarized thereafter. The selection starts from the empty set, such that k
                           =0, 
                              
                                 F
                                 k
                              
                              =
                              {
                              ∅
                              }
                           , and an initial value J
                           0
                           ∈[0, 1]. Afterwards,
                              
                                 (1)
                                 Generate successors 
                                       
                                          G
                                          
                                             k
                                             +
                                             1
                                          
                                       
                                     of 
                                       
                                          F
                                          k
                                       
                                    , where
                                       
                                          
                                             
                                                G
                                                
                                                   k
                                                   +
                                                   1
                                                
                                             
                                             =
                                             {
                                             A
                                             (
                                             E
                                             ,
                                             
                                             Y
                                             ∖
                                             E
                                             )
                                             |
                                             ∀
                                             E
                                             :
                                             E
                                             ∈
                                             
                                                F
                                                k
                                             
                                             }
                                             .
                                          
                                       
                                    
                                 

Compute J for each feature subset in 
                                       
                                          G
                                          
                                             k
                                             +
                                             1
                                          
                                       
                                     where J is the gmean estimated through LOOCV.

Verify if there is a feature subset in 
                                       
                                          G
                                          
                                             k
                                             +
                                             1
                                          
                                       
                                     that outperforms the best feature subset of the preceding kth iteration. If any, save it into 
                                       
                                          F
                                          
                                             k
                                             +
                                             1
                                          
                                       
                                    , i.e.,
                                       
                                          
                                             
                                                F
                                                
                                                   k
                                                   +
                                                   1
                                                
                                             
                                             =
                                             {
                                             E
                                             ∈
                                             
                                                G
                                                
                                                   k
                                                   +
                                                   1
                                                
                                             
                                             |
                                             ∀
                                             E
                                             :
                                             J
                                             (
                                             E
                                             )
                                             >
                                             
                                                J
                                                max
                                                k
                                             
                                             }
                                             ,
                                          
                                       
                                    and go to 4; otherwise, go to 6.

Update:
                                       
                                          (a)
                                          
                                             k
                                             =
                                             k
                                             +1;


                                             
                                                
                                                   J
                                                   max
                                                   k
                                                
                                                =
                                                
                                                   max
                                                   
                                                      E
                                                      ∈
                                                      
                                                         F
                                                         k
                                                      
                                                   
                                                
                                                J
                                                (
                                                E
                                                )
                                             .

Verify the optional stopping criteria, i.e., 
                                       E
                                       ∈
                                       
                                          F
                                          k
                                       
                                    
                                    
                                       
                                          
                                             
                                                (
                                                i
                                                )
                                             
                                             
                                             ∃
                                             E
                                             :
                                             J
                                             (
                                             E
                                             )
                                             ≥
                                             
                                                J
                                                UB
                                             
                                             ∨
                                             
                                                (
                                                ii
                                                )
                                             
                                             
                                             ∀
                                             E
                                             :
                                             |
                                             E
                                             |
                                             ≥
                                             
                                                N
                                                max
                                             
                                             .
                                          
                                       
                                    If neither criterion is met, go to 1; otherwise go to 6.

If a single feature subset is retained in 
                                       
                                          F
                                          k
                                       
                                    , or if there is one subset that has a unique, maximum J value, then it is the solution; otherwise select a unique feature subset among the feature subsets with the best J value using relative extremal margin, referring to Eq. (6).

Stop.

We take an example of a 4-dimensional feature vector to show the principle of the proposed algorithm in Fig. 1
                           . Unlike the univariate feature ranking methods, feature subset selection offers a flexibility to reconsider a feature that was previously discarded. Take x
                           2 for example. It is first discarded, but reconsidered in combination with x
                           1 and x
                           3 at the following iteration.

The structure of iterative candidate solutions is hierarchical. “Hierarchy” refers here to a poset in which any item at the same level (i.e. iteration) is the union of its parent and an element from the complement of its parent in Y. In fact, the SFS method can be considered as a special case of our HFS method, that would represent linear hierarchy. For SFS indeed, the maximum branching degree of any item is 1, while items in our method can have a branching degree of 1, 2 or more during the forward search process, so that the possibility of finding the optimal solution is greater than SFS.

The feature selection process can be guided by the use of prior domain knowledge. The prior knowledge can constrain the search to a confined feature space. For small sized datasets, the constraint can reduce overfitting and accelerates the search process. As the SUV-based features are known for their significancy to predict patient's treatment outcome [2–5], we propose to incorporate this prior knowledge into the feature selection algorithm. Among the SUV-based features, the feature that yields maximum distance between the means of the two classes is fixed as the first feature in the HFS algorithm. Let f
                        (1) denote this first feature. The prior-based version of hierarchical forward selection (pHFS) starts thus from an initial solution 
                           
                              F
                              1
                           
                           =
                           {
                           
                              f
                              
                                 (
                                 1
                                 )
                              
                           
                           }
                         and 
                           
                              J
                              max
                              1
                           
                           =
                           J
                           (
                           
                              f
                              
                                 (
                                 1
                                 )
                              
                           
                           )
                        , instead of 
                           
                              F
                              0
                           
                           =
                           {
                           ∅
                           }
                         and user-defined J
                        0, respectively.

@&#EXPERIMENTAL RESULTS@&#

We first assess the ability of the proposed HFS method to correctly select the optimal features on a synthetic dataset, compared with several state-of-the-art feature selection methods. Comparison is performed with three filter methods: FAST [12], RELIEF [11], KCS [14], and two wrapper forward selection methods: SFS [20,23], and SFFS [21,23], as well as two embedded methods: SVMRFE [18], CART [16]. We then show the experimental results of different feature selection methods on two real-world datasets.

We generate a two-class, synthetic dataset with a small number of imbalanced samples, reflecting the problems of medical data. Three relevant features to the class label are generated from a three-dimensional multivariate normal distribution 
                           N
                           (
                           μ
                           ,
                           
                              
                                 I
                              
                           
                           )
                        . These three features are mutually independent, so that the covariance matrix is an identity matrix I. For the majority class, 20 samples are randomly generated from 
                           N
                           (
                           [
                           3
                           ,
                           −
                           3
                           ,
                           1
                           ]
                           ,
                           
                              
                                 I
                              
                           
                           )
                         or 
                           N
                           (
                           [
                           −
                           3
                           ,
                           3
                           ,
                           1
                           ]
                           ,
                           
                              
                                 I
                              
                           
                           )
                         with equal probability. For the minority class, 10 samples are randomly generated from 
                           N
                           (
                           [
                           −
                           3
                           ,
                           −
                           3
                           ,
                           −
                           1
                           ]
                           ,
                           
                              
                                 I
                              
                           
                           )
                        . The distribution of these three relevant features is illustrated in Fig. 2
                        . We can gather from this figure that the optimal solution is the combination of features f
                        1 and f
                        2. Any other combinations of these three features are suboptimal solutions. To assess the performance of feature selection methods to correctly select the optimal features, we add n
                        
                           i
                        
                        ∈{5, 15, 25, 35, 50} irrelevant features considered as noise, randomly generated from 
                           N
                           (
                           0
                           ,
                           10
                           )
                        .


                        Table 1
                         shows the frequency of the three relevant features f
                        1, f
                        2 and f
                        3 selected by different feature selection methods. Among the state-of-the-art methods, except for the CART algorithm, the number of features to select is fixed beforehand as two. For the proposed HFS method and the other two wrapper methods, i.e., SFS and SFFS, an inner loop of LOOCV inside the training set is performed to calculate the gmean defined in (5), i.e., the evaluation measure of different feature subsets, using SVM as the classifier. For the proposed HFS method, the initial lower-bound value J
                        0 is set to 0.5, and the final upper-bound value J
                        UB for the stopping criterion is set to 1 (default value), the maximum number of features to be selected N
                        max is set to the size of the original feature vector (default value). Experimental results with different n
                        
                           i
                         irrelevant features are all obtained from 30 runs. We can see that the optimal feature combination, i.e., f
                        1 and f
                        2, is correctly selected by the SVMRFE method when the number of irrelevant features is small. However, as the number of irrelevant features increases, the performance of SVMRFE method drops. The same trend can be observed for other methods. Compared to other methods, the proposed HFS method shows more robustness against irrelevant features.

In the following, we perform experiments on two real-world datasets (Table 2
                        ). The first one is obtained from lung tumor patients, and the second one from esophageal tumor patients.

The first dataset is composed of 25 patients with stages II–III non-small cell lung cancer, treated with curative intent chemo-radiotherapy. The patients are treated with chemotherapy, then radiotherapy (of total dose at 60–70Gy delivered in daily fractions of 2Gy, 5 days a week) with possible concomitant chemotherapy. Baseline and following-up FDG-PETs at different time points are performed: PET1 before the treatment, PET2 after chemotherapy, and PET3 during radiotherapy at about 45Gy dose radiation (Fig. 3
                           (a)). The images are 4.06mm×4.06mm pixel size and 2mm slice spacing. Each patient's following-up scans are manually registered to the baseline scan by an expert, with nearest-neighbor interpolation. For longitudinal analysis, all PET scans are converted into SUV which is a standardized decay-corrected value of 18F-FDG activity per unit volume of body weight. Our definition of recurrence at one year after the treatment is primarily clinical, based on the evaluation by the clinician, with biopsy and FDG-PET/CT. Local or distant recurrence (labeled as positive class) is diagnosed on 19 patients, and no recurrence (labeled as negative class) is reported on the remaining 6 patients.

Thirty-six patients with esophageal squamous cell carcinomas are included in the second dataset. They are treated with chemo-radiotherapy, and followed up in a long term up to five years. The included radiotherapy delivered at 2Gy per fraction per day and five sessions per week for a total of 50Gy over five weeks. In this dataset, only FDG-PET before treatment is available for investigation (Fig. 3(c)). PET data are converted into SUV, and the spatial resolution is also 4.06mm×4.06mm×2mm. The disease-free evaluations include a clinical examination, FDG-PET/CT and esophagoscopy with biopsies. The classification of disease-free is based on the evaluations performed one month after the end of treatment, and to the date of tumor recurrence or to the death of a patient who never had a recurrence. The patients are classified as disease-free when neither loco regional nor distant tumor recurrence is detected (labeled as negative class, 13 patients); otherwise classified as with disease (labeled as positive class, 23 patients).


                           Fig. 3 illustrates the great intra- and inter-class variability in terms of PET uptake distributions. These two sources of variability make it necessary and challenging to extract and select discriminant features. We extract SUV-based features, texture features based on gray level size zone matrix (GLSZM) and temporal changes between the baseline and follow-up features to characterize PET images.

The region-of-interest (ROI) around tumor is manually delineated. Five SUV-based features are extracted: SUVmax, SUVmean, SUVpeak (mean value inside 3×3×3 neighborhood of SUVmax), MTV (defined by a thresholding of 40% of SUVmax), TLG (SUVmean×MTV). More details may be found in Table A.7 of the Appendix.

Complementary to these SUV-based features, texture analysis allows us to explore spatial uptake heterogeneity information. We use GLSZM to exact the regional texture patterns in ROI, introduced in [27] for cell nuclei classification. This matrix has also been applied to PET image characterization [7]. A zone is defined as a connected component of the same gray level. An element of GLSZM at row n and column s is the number of zones of gray level n and of size s. In this study, image voxel values inside the ROI are resampled to a range of 23 distinct values. Thus the total row number of GLSZM is 23, while the number of columns of this matrix is dynamic, determined by the size of the largest zones. The more homogeneous the texture, the wider and flatter the matrix. This matrix has the advantage of not requiring calculations in several directions, which are replaced by tagging different areas. From this matrix, 11 regional heterogeneity parameters can then be computed, given in Table A.8 of the Appendix.

Considering that the longitudinal change of image characteristics may provide predictive value, we also propose to calculate temporal changes as the relative difference between baseline features and follow-ups features, obtained from baseline and subsequent PET exams.

Clinical characteristics of the patients are also important features to be taken into account. In summary, for lung tumor data, a total of 79 features is extracted for each patient from their clinical data, i.e., patient gender, tumor histology and primary tumor location, and longitudinal PET images. For esophageal tumor data, 29 features are obtained from each patient clinical data, i.e., patient gender, WHO performance status, tumor stage, dysphagia grade, weight loss from baseline and tumor location, and baseline PET images. The number of feature is smaller than that of the lung tumor dataset, because there are no temporal PET features. The final features are rescaled to the range of [0,1] to make them comparable.

Because of the small sample size, the performance of a feature selection method is evaluated through LOOCV, i.e., each patient is used once as the unseen test sample to assess the performance of the classifier which is learned on the remaining training samples with the selected features by a feature selection method. In our experiments, the cutoff values on subset size to get the final subset for state-of-the-art methods (except for The CART algorithm) are found through a rough grid search. For the wrapper methods, gmean defined in (5) obtained based on SVM is used as the selection criterion. Experiments with all features are also carried out as baseline results.

The predictive value of a feature subset can be evaluated through its classification performance on test data. The accuracy (the ratio of instances that are correctly classified to all instances [26]), is estimated on test samples. Table 3
                               provides the accuracy performance of the selected feature subsets by different methods assessed by using an SVM and a 3-Nearest-Neighbor (3NN) as the classifier, respectively. The SVM classifier performs generally better than the 3NN; we thus focus on the performance obtained by SVM classifier in the following.


                              Lung tumor data. For the HFS algorithm, the initial lower-bound value J
                              0 is set to 0.8, and the final upper-bound value J
                              UB for the stopping criterion is set to 1 (default value), the maximum number of features to select N
                              max is set to the size of the original feature vector (default value). For SVM classifier, a Gaussian kernel with sigma=1 and C=1 (the box constraint value for the soft margin) are chosen empirically for this dataset. As seen in Table 3, the poor accuracy produced with all features makes it very clear that feature selection is necessary. In fact, even though the accuracy value obtained by the SVM classifier is 76%, the corresponding specificity is 0%. Without any selection, a classifier tends to classify all samples into the majority class. The proposed HFS and pHFS methods yield promising results: 100% accuracy.


                              Esophageal tumor data. For the proposed HFS algorithm, the initial lower-bound value J
                              0 is set to 0.5, and J
                              UB and N
                              max are set to their default values. The distribution of these data in the feature space is unclear. Thus we have tried several common kernels for the SVM: linear, Gaussian (sigma=1) and polynomial (order=2), and we have retained in Table 3 the best performing kernel for each method. The box constraint value for the soft margin C is set empirically as 1. The top performance is attained by HFS, reaching 94% accuracy. The pHFS method yields an accuracy of 92%. In fact, compared to HFS, only one additional sample is incorrectly classified by pHFS. However, the robustness of pHFS method is superior to that of HFS, as presented below.

In addition to a good prediction accuracy, robustness of feature selection is a desirable property. Robustness can be viewed as the property of an algorithm to produce a consistent feature subset under varying conditions such as perturbations of training data [28]. Although important, it is often overlooked. In this paper, we examine and report the robustness measured by the relative weighted consistency from [29], which allows the comparison of different sized subsets. Its calculation is based on feature occurrence (frequency) statistics. To calculate this frequency, we record the selected features that are obtained from the different folds of the LOO, and then calculate the frequency for each feature. The values of relative weighted consistency range from 0% to 100%, with 0% indicating empty intersection between all subsets and 100% indicating that all subsets approximate identical. We can gather from Table 3 that the robustness of pHFS is improved compared to HFS.

We illustrate the frequency of selected features obtained via the LOO lung tumor training set in Fig. 4
                              , in order to interpret the meaning of robustness. For sake of clarity, only the frequency plots for the pHFS method with the highest robustness value and SFS method with the lowest value are given. The flatter the plot spreads out, the lower the robustness will be. A high robustness value means that results are reliable. The flat plot of SFS method highlights the fact that a small perturbation of the training set changes the feature subsets obtained by this method. The KCS method also yields high robustness on the two datasets; however, the accuracy performance is not satisfactory.

The average sizes of selected feature subsets on the different folds of LOO are listed in Table 3. The subset sizes are small for most methods.

In the proposed HFS method, only one free parameter, i.e., the initial lower-bound value J
                              0, is mandatory. The impact of different values of J
                              0 is empirically assessed on the lung tumor dataset, with the two optional choices J
                              UB and N
                              max set to their default values (see Table 4
                              . Please refer to Table 3 for the results where J
                              0
                              =0.8. When decreasing the value of J
                              0 to 0.7, 0.6 or 0.5, the prediction accuracy and robustness remain the same. However, computation time has increased to 2, 2.5 or 2.9 times of that when J
                              0
                              =0.8. This conforms to the expectation: decreasing the value of J
                              0 leads to a larger feature subset space to be searched, which increases the searching time.


                              Tables 5 and 6
                              
                               list features selected by both HFS and pHFS methods on the two datasets, respectively, where the number of features is the mean size of feature subsets given in Table 3. The first feature f
                              (1) for pHFS method is SUVmax at PET3, i.e. during the radiotherapy, for lung tumor dataset, and TLG before the treatment for esophageal tumor dataset. Empirically it has been observed that this f
                              (1) was indeed selected by all feature selection algorithms. As mentioned in Section 2.2, SUV-based features are recognized as predictive factors for treatment outcome. However, which SUV parameter and which time point of the PET exam should be used are not known. Our study suggests that SUVmax during the radiotherapy, combined with two other GLSZM-based features, is the main predictive factor for lung tumor recurrence. Combined with two clinical and one GLSZM-based features, TLG before the treatment is predictive for esophageal disease-free survival. Obtaining predictive factors before or during the treatment is valuable in practice, when a potential adaptation of the treatment is still an open choice.

The selected features can be used to classify patients. The features in Table 5 yield 100% accuracy on the whole lung tumor data, and the learned decision surface on the three dimensional feature space is shown in Fig. 5
                              . Recurrent and non-recurrent patients are well separated. The four features in Table 6 yield 97% accuracy on the whole esophageal tumor data.

As esophageal tumor patients are followed up on the long term, the predictive value of the selected features can be assessed by drawing the so-called DFS curves. A DFS curve shows the fraction of people in a given group that survives over time. Let Group 1 represent the group of patients classified as disease-free, and Group 2 as with disease, by the SVM classifier learned on the whole dataset with the four features in Table 4. For each group separately, the KM (Kaplan–Meier) method is used to estimate the survival curve [30], shown in Fig. 6
                              . The principle is to record the time between the beginning of the treatment to (i) the time of tumor recurrence, or (ii) the death of a patient who never had a recurrence, or (iii) the treatment response evaluation at one month after the treatment, for patients whose tumors do not respond to the treatment. We compare KM curves for the two groups, using the most popular test, the log-rank 2-tailed test, which tests the null hypothesis of a common survival curve [31]. The p value is 1e
                              −5, indicating that there is a significant difference in survival between the two groups. We can thus say that patients in Group 2 have significantly worse survival rates than those in Group 1. This information can be helpful for physicians in treatment planning. Adjuvant treatment seems necessary for patients in Group 2 so as to improve their survival rates.

The average computation times of the wrapper methods on one LOO training set are listed in Table 3. Hardware configuration is an Intel i5 P with a 3.3GHz CPU and 8GB RAM. When compared to HFS, pHFS yields a relative gain of time of 37% and 5% on the two datasets respectively. The computational cost for the proposed methods has been compared to that of an exhaustive search. For lung tumor data, three features out of a total of 71 features are selected by our methods (seen in Table 5). While an exhaustive search of 1–3 features among 71 requires to search in a space of 
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    3
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   71
                                                
                                             
                                             
                                                
                                                   k
                                                
                                             
                                          
                                       
                                    
                                 
                                 =
                                 59
                                 ,
                                 641
                               combinations, HFS has been measured to search in a subspace of around 1790, i.e. only 3% of that of the exhaustive search, and pHFS 2%. Similarly, for esophageal tumor data, HFS and pHFS have been found to search in a subspace of 4% and 3%, respectively, of that of the exhaustive search. One can always use feature ranking methods as a preprocessing step for HFS, or make use of parallel computing to further accelerate the convergence of the algorithm, especially for high dimensional feature datasets.

@&#CONCLUSION@&#

In this paper, we aim to find the predictive features for treatment outcome prediction. We propose a hierarchical forward selection based on SVM, which selects a feature subset with promising prediction performance. We also propose to incorporate prior knowledge into feature selection algorithms to improve the performance of the algorithm. We will pursue this work by including more datasets to further investigate our method. PET images obtained from different time points are manually registered in this paper. Automatic registration will be investigated to make the prediction process automatic.

@&#ACKNOWLEDGMENTS@&#

This work is partly supported by China Scholarship Council. We would like to express our gratitude to Mr. Jérémie Calais, Mr. Charles Lemarignier and Mr. Pierrick Gouel (Centre Henri-Becquerel, Rouen, France) for the preparation of the datasets.


                     Tables A.7 and A.8
                     
                     .

@&#REFERENCES@&#

