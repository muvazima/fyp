@&#MAIN-TITLE@&#Analysis of engagement behavior in children during dyadic interactions using prosodic cues

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Engagement level of children reflected in their vocalizations as well as the psychologist's.


                        
                        
                           
                           Engagement level reflected in global prosodic cues as well as local prosodic patterns.


                        
                        
                           
                           Engagement level varies with different interaction settings but universal prosodic patterns also exist.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Engagement

Prosody

Global level cues

Local level cues

Classifier decision fusion

@&#ABSTRACT@&#


               
               
                  Child engagement is defined as the interaction of a child with his/her environment in a contextually appropriate manner. Engagement behavior in children is linked to socio-emotional and cognitive state assessment with enhanced engagement identified with improved skills. A vast majority of studies however rely solely, and often implicitly, on subjective perceptual measures of engagement. Access to automatic quantification could assist researchers/clinicians to objectively interpret engagement with respect to a target behavior or condition, and furthermore inform mechanisms for improving engagement in various settings. In this paper, we present an engagement prediction system based exclusively on vocal cues observed during structured interaction between a child and a psychologist involving several tasks. Specifically, we derive prosodic cues that capture engagement levels across the various tasks. Our experiments suggest that a child's engagement is reflected not only in the vocalizations, but also in the speech of the interacting psychologist. Moreover, we show that prosodic cues are informative of the engagement phenomena not only as characterized over the entire task (i.e., global cues), but also in short term patterns (i.e., local cues). We perform a classification experiment assigning the engagement of a child into three discrete levels achieving an unweighted average recall of 55.8% (chance is 33.3%). While the systems using global cues and local level cues are each statistically significant in predicting engagement, we obtain the best results after fusing these two components. We perform further analysis of the cues at local and global levels to achieve insights linking specific prosodic patterns to the engagement phenomenon. We observe that while the performance of our model varies with task setting and interacting psychologist, there exist universal prosodic patterns reflective of engagement.
               
            

@&#INTRODUCTION@&#

During childhood an individual develops critical social, physical, psychological and cognitive skills and abilities. This development is affected by several factors including society (Walker et al., 2007; Davie et al., 1972), family (Biller, 1993; Egeland and Farber, 1984) and peers (Dodge et al., 2003). Furthermore, developmental changes are reflected in behavioral aspects such as joint attention (Akhtar et al., 1991; Tomasello and Farrar, 1986), and ability to engage, among others (Cloward et al., 1960; Göncü, 1999; Morrissey-Kane and Prinz, 1999). Quantitative assessment of these behavioral aspects, while very challenging, can provide tools for understanding important aspects of child development, both typical and atypical. In turn, this can further inform intervention methods targeted toward assisting healthy child development. Several approaches for quantifying a child's behavioral aspects such as social and language development (Volkmar et al., 1993; Coplan and Gleason, 1990) exist and we note that their specific type and nature is very dependent on, and tailored to the corresponding behavior of interest. Given the vast heterogeneity and variability in developmental trajectories, especially in the presence of neuro-cognitive and behavioral disorders, there is an imminent need for quantitative methods and analysis tools that can help shed further light into developmental behavioral processes and mechanisms.

Understanding and quantitatively characterizing the engagement patterns of a child, a core behavioral construct, can be useful for both diagnostics and intervention design. Child engagement is defined as the child being involved with his/her environment in a contextually appropriate manner (McWilliam and Casey, 2008). Engagement is a complex internal state externalized and reflected in several modalities including face and body language (Xu et al., 2010; Sanghvi et al., 2011), speech (Yu et al., 2004; Manning et al., 1994) and physiology (Nes et al., 2005). Several studies suggest that a greater engagement has a constructive impact on a child's development (McWilliam et al., 2003; Taylor et al., 2003). For instance, investigations by de Kruif and McWilliam (1999) suggest positive multivariate relationships between developmental age and observed child engagement, where the developmental age is determined over personal–social, adaptive, communication, motor and cognitive domains (Newborg et al., 1984). Göncü (1999) has reported on the impact of a child's engagement during social activities to his/her development and underscores the importance of an interdisciplinary approach to such an endeavor. The importance of engagement is also emphasized in the study of children with developmental disorders like autism (Delano and Snell, 2006; Poulsen and Ziviani, 2004) aiming to enhance behavioral intervention methods (Kasari et al., 2010; Rogers, 2000). Furthermore, improved engagement is associated with success of organizations like child care centers (Maher Ridley et al., 2000) and schools (Skinner and Belmont, 1993). Methods of intervention exist to improve child engagement in different settings such as school (Skinner et al., 1990), parent–child interactions (Casey and McWilliam, 2005) and play with peers (Cielinski et al., 1995).

Given that child development and engagement are strongly coupled, several schemes have been proposed to measure engagement. Yatchmenoff (2005) proposes to quantify engagement in child protective services by categorizing it into five dimensions of receptivity, expectancy, investment, mistrust and working relationship. Kishida and Kemp (2006) have proposed measures of engagement specific to practitioners as opposed to researchers motivated by their relation to practicality, sensitivity to the participants and ability to measure across the span of activity types. Libbey (2004) measured engagement of children in schools by defining a few school connectedness measures based on conceptual interrelatedness. Other studies such as in Read et al. (2002) view engagement as a dimension of a higher order construct and propose measures of engagement as a subcomponent to analyze the construct. However, these studies do not extend to the cases involving natural interaction with children and are often limited to artificial settings. Moreover, given that engagement is a latent internal state inferable only using observed cues, a majority of studies rely on a subjective measurement of engagement. Such measures are susceptible to several uncertainties introduced by variability in interaction settings, interpersonal differences, inconsistencies across subjective judgments and even the operational definition of engagement.

We aim to address the need for an objective engagement behavior quantification method that is robust to the variations in environmental parameters. We perform a study in which children interact with a psychologist while performing different socio-cognitive tasks. The child–psychologist dyadic interaction provides an opportunity to investigate interaction engagement under various settings introduced by differences in task conditions. In the study, we develop a computational system based solely on the observed vocal cues, specifically vocal prosody during the dyadic interaction. Furthermore, it is hypothesized that the engagement of the child can be predicted from the acoustic prosodic cues of both the child and the psychologist and data-driven methods can be designed to capture this relationship.

Our approach is inspired by several previous studies that link speech prosody to human behavior based constructs such as emotion (Austermann et al., 2005; Lee et al., 2011), approach-avoidance (Rozgic et al., 2011; Xiao et al., 2012), entrainment (Lee et al., 2014), blame/acceptance (Black et al., 2013), and empathy (Kempe, 2009; Aziz-Zadeh et al., 2010). These studies present techniques that computationally model prosodic patterns which are otherwise difficult to quantify perceptually. We build upon our previous work in Gupta et al. (2013, 2012) and present a data-driven approach to identify global prosodic patterns (task level statistics) as well as those that last over much shorter time spans (local cues). The local level cues also provide a means for capturing the temporal relationship between the local prosodic patterns. We apply this model for engagement level prediction using prosody and our modeling technique can serve as a generic tool extendible to other modalities. We train separate models on the global and local cues and finally fuse their predictions. We observe that individual models using either the global or the local cues carry statistically significant predictive capability. We subsequently fuse the two components to utilize their complementarity. Our model assigns child engagement to one of the three instrument-defined categories, achieving an unweighted average recall of 55.8%. We also investigate the predictive capabilities of each individual cue used in classification.

Besides obtaining an objective decision, we also address the issues of variability introduced by interpersonal differences and dissimilar interaction settings. We show that our methodology captures prosodic patterns that are universally present across various interaction settings in the Rapid ABC protocol. Specifically, we normalize for individual speaker traits and train our model by combining data from all the tasks that comprise an entire dyadic interaction. We present the results categorized per task as well as for each interacting psychologist to estimate the generalizability of our model. We observe that our engagement model performs well over different parameters, but the performance does vary under different settings. This suggests that universal prosodic cues of engagement do exist, but they occur in combination with some setting specific patterns.

This paper is organized as follows: Section 2 describes the database. We explain the global and the local cue extraction scheme and the experimental setup in Section 3 and describe the classification approach and its results in Section 4. We present our analysis on the system in Section 5 and conclude in Section 6.

We use the Rapid ABC database (Ousley et al., 2013; Rehg et al., 2013) collected at the Georgia Institute of Technology as part of an NSF Expeditions project. The Rapid ABC protocol is a 3-5 minutes long semi-structured interaction between a psychologist and a child during a predefined set of tasks. Concurrently, the psychologist assesses the child's social attention, non-verbal communication using gaze, vocalizations and facial expressions, and perceived engagement. The assessments by the psychologist are recorded on a paper form screener. Specifically, the recorded assessments in the screener are designed to identify behavioral markers of atypical social-emotional development, language and motor development. The primary purpose of this dataset is to aid experiments in designing technological solutions that would facilitate the integration of an autism screening tool into a medical office's work-flow. In the following sections, we describe the interaction settings for the Rapid ABC dataset, the screener form based evaluation followed by data statistics.

The Rapid ABC interaction sessions involve semi-structured over-the-table interaction between a child and a psychologist. The session involves five tasks: (i) Smiling and saying hello, (ii) Ball play, (iii) Jointly looking at a book, (iv) Putting a book on the psychologist's head as if it was a hat, and (v) Smiling and tickling. These tasks are designed to capture various aspects related to cognitive, social, language and motor development.
                           1
                        
                        
                           1
                           A description of these tasks can be found at https://www.youtube.com/watch?v=89KnHRLz7EQ.
                         The psychologist is provided a script for each task and concurrently records her assessments in the screener. The dataset is recorded using video, audio and physiology (wrist) sensors. In this work, we use the prosodic measures derived from the audio signal captured by a central farfield microphone on the table that records audio (at 16khz sampling rate) from both the child and the psychologist.

The Rapid ABC screener form was designed to concisely capture various observable/perceivable cues reflective of behavioral aspects like joint attention and social aptitude during each task in a session. The screener had to be manually completed by the psychologist while simultaneously interacting with the child. Part of the screener corresponding to the Ball play task is shown in Fig. 1
                        . For each of the five tasks, the screener consists of two separate fields:
                           
                              •
                              Behavior annotation: The psychologist annotates a set of observed behaviors that are expected from the child during a task. These sets of behaviors of interest typically include actions reflecting joint attention or a social response. In Fig. 1, the instructions to annotate a set of behaviors for the ball task is provided. The psychologist scores a PLUS if behavior mentioned in the instructions is present and a MINUS otherwise.

Ease of engagement: The psychologist annotates the child's engagement state as one of the 3 levels as per the instructions listed in Table 1
                                 . Whereas better metrics to quantify engagement may exist, the three levels used (classes) broadly stratify the engagement phenomenon making it easier for the psychologist to provide an objective judgment. A separate block to annotate the engagement levels (E) is provided in the screener form as shown in Fig. 1.

In our work, we aim to model the above perceived engagement level from observed cues, specifically, of speech prosody. Attributes related to behavioral annotation in the screener can be reliably annotated, but the engagement level annotation is prone to variabilities introduced by interpersonal differences amongst interacting psychologists; including in their perceptions, differences in settings under which the interaction happens, and the developmental stage of the child. We intend to aid the psychologist's judgment by minimizing the effect of the aforementioned factors.

We use 74 sessions containing speech recordings from 63 children in the Rapid ABC data. The Rapid-ABC protocol was repeated for 11 children as a follow up and hence we have two sessions for these children. The children are in the age range of 15–30 months and 39 of them are boys. These children interacted with one of the four psychologists trained for interaction in the Rapid ABC settings. Apart from the psychologist's assessments, these sessions contain manual lexical transcriptions with time alignments for child and psychologist vocalizations. The distribution of the engagement levels for each of the five tasks over the 74 sessions is listed in Table 2
                        .

From the table we observe that we have an uneven distribution of engagement levels. E
                        0 is the majority class in all the tasks. This suggests that the occurrence of other levels of engagement is rather atypical and possibly of greater interest with respect to the goals of the Rapid-ABC protocol in providing quick assessments over several aspects of child development. Also, the distribution of engagement levels varies depending on the task. For instance, the proportion of E
                        0 in the Hat task is significantly higher than any other task. Similarly, the proportion of E
                        1 in the Book task is significantly higher than Ball, Hat and Smiling tasks (We use a conservative difference in proportions test for p-value < 5%. The number of samples for the significance testing is computed such that each class is considered to have the same number of samples as the least represented class.
                           2
                        
                        
                           2
                           The rationale is to give equal importance to all the classes while performing the significance testing. This is particularly important as we use the unweighted average recall per class as our metric later. Since we reduce the number of samples, this test provides a more conservative significance level. However, we do avoid inflated significance which may arise due to different statistics on the majority class. For more details please refer to Bone et al. (2015).
                        ). This indicates that the phenomenon of engagement is contingent on the task at hand and may vary under dissimilar settings.

We focus on using data-driven methods to model the psychologist's engagement level assignment using observed cues, specifically the vocal prosody of the participants. Even though the engagement level is conditioned upon the interaction settings, we hypothesize that there exist common vocal prosodic patterns reflective of engagement across these settings. Furthermore, we hypothesize that the child's engagement will be reflected not only in the child's prosodic cues, but also in those of the interacting psychologist. We aim to objectively capture these cues to infer the engagement levels. We construct multiple models performing the engagement evaluation and combine their decisions to obtain a final prediction. In the remainder of this section we describe our data preparation and prosodic cue extraction framework.

For each of the 74 sessions we have five tasks with engagement evaluation. We segment the audio files by task to allow for task-wise analysis. Hence we have 370 (74 sessions×5 tasks) audio segments, each with corresponding manual diarization and an engagement score assigned by the interacting psychologist.

In order to train our models, we collectively use the 370 files from all the tasks and psychologists. Even though the data statistics suggest that engagement is contingent on the task, we combine the data primarily because of two reasons.
                           
                              (i)
                              First, we aim to capture prosodic patterns that are robust to the variability introduced by task-dependent contexts and different interaction partners.

Second, the small number of instances from E
                                 1 and E
                                 2 classes are not suitable for training specific models for each task and psychologist. As we rely on data-driven techniques, we need sufficient samples to reliably capture patterns.

After obtaining these separate audio chunks with an assigned engagement level E
                        ∈{E
                        0, E
                        1, E
                        2}, we extract the prosodic signals from the speech as discussed in the next section.

We extract a set of prosodic signals that characterize voice source activity: pitch, loudness, jitter and shimmer. All prosodic signals are computed using Praat (Boersma and Weenink, 2001) at a rate of 100frames/s. In our experiments, we consider every 10ms time interval as one analysis frame. Below, we detail prosodic signal computation including utilized signal denoising techniques.
                           
                              •
                              Speaker assignment: We use the manual segmentations to obtain a frame-wise speaker assignment vector S
                                 ={s
                                 1, …, s
                                 
                                    n
                                 , …, s
                                 
                                    N
                                 }, where N represents the total number of analysis frames in the file (assignments are made every 10ms). The element s
                                 
                                    n
                                  assigns the nth frame of the audio file to either psychologist speech (Psy), child vocalization (Child), overlap (Ol) or silence (Sil).

Pitch: We use an autocorrelation based method to perform pitch estimation (F0, fundamental frequency) as described in Boersma (1993). We use an analysis window with a duration of 40ms to estimate pitch at a time step of every 10ms, which synchronizes with our speaker assignments. Since the extracted pitch may have errors due to audio quality, we smooth and cubically interpolate the pitch signal to reduce such errors. P
                                 ={p
                                 1, …, p
                                 
                                    n
                                 , …, p
                                 
                                    N
                                 } represents the vector of processed pitch values, where p
                                 
                                    n
                                  is the pitch value for the nth frame.

Intensity: We obtain the intensity estimates by squaring the audio magnitudes per frame. This is followed by convolution with a Gaussian window to reduce noise effects. The pitch-synchronous intensity ripple is also reduced by this operation to give a smoother intensity contour (Boersma and Weenink, 2001) (as convolution with Gaussian window is also a low-pass filtering operation). We represent the intensity vector as I
                                 ={i
                                 1, …, i
                                 
                                    n
                                 , …, i
                                 
                                    N
                                 }.

Jitter: Jitter serves as a measure of voice quality and is defined as the cycle-to-cycle variation of the fundamental frequency (F0) (Farrús et al., 2007). We estimate the relative jitter using overlapping windows in the audio files. Relative jitter is computed by normalizing the absolute jitter (the average absolute difference between consecutive periods in speech signal) by the average period. Note that a jitter value for a specified window can only be calculated if it contains multiple F0 value estimates. We chose a window length of one second shifted by 10ms. Smaller window lengths lead to several undefined jitter values and larger window lengths lead to imprecise estimates of local values of jitter as voicing from distant intervals is also incorporated.

As jitter measures cycle to cycle variation of periods in the speech signal, its estimation is sensitive to the accurate estimation of F0. We can only estimate jitter when we have several pitch cycles in a window. Since we observed noisy jitter estimates in our data, we choose to smooth the jitter signal using a moving average filter. However, we do not interpolate the values as they are often missing over several windows, leading to poor interpolation. Over such windows with missing values, the jitter is listed to be undefined. We represent the set of jitter values as J
                        ={j
                        1, …, j
                        
                           n
                        , …, j
                        
                           N
                        }, where j
                        
                           n
                         represents the jitter estimate for a window starting at the nth frame, extending for 1s. Note that j
                        
                           n
                         can also be listed as undefined.
                           
                              •
                              Shimmer: Shimmer provides us with another measure of voice quality (Farrús et al., 2007). Shimmer measures cycle-to-cycle variation in intensity (jitter measured variation in periods). We estimate relative shimmer using the same window-wise approach as with jitter, smoothing but not interpolating the signal (again leading to undefined values). We represent the set of shimmer values as H
                                 ={h
                                 1, …, h
                                 
                                    n
                                 , …, h
                                 
                                    N
                                 }, where h
                                 
                                    n
                                  represents the shimmer estimate for a window starting at the nth frame.

The characteristics of the prosodic signal based on engagement level can be captured using either (i) time series modeling tools or (ii) discriminative models on statistical measures computed over the prosodic signals. Time series modeling tools such as Gaussian mixture model-Universal background models (GMM-UBM) (Reynolds, 2002), i-vector systems (Dehak et al., 2009; Shum et al., 2010) train on frame-wise features and attempt to model the probabilistic process governing the time series generation (conditioned on the class label). These methods assume that each sample in the time series is generated from a class dependent probability distribution. Given a set of time series from each class, these time series modeling tools estimate the parameters of the probability distribution for that class. On the other hand discriminative models on statistical estimates rely on capturing the differences between target classes using compact statistical representations. The latter is particularly useful in case of smaller datasets as modeling latent generative processes usually requires a large amount of data. Several other studies have also attempted to model similar time series data using compact statistical representations (Gupta et al., 2012; Bone et al., 2013; Hansen and Arslan, 1995). We use a similar discriminative scheme to capture the statistical properties of the prosodic time series at two levels of granularity: over the entire task duration (global cues) and at smaller time scales (local cues). The global cues help model the characteristics of the prosodic time series over the entire duration of a task, while the local cues quantify the local pattern in prosodic signals. We compare the outputs of the discriminative model based on local and global cues against a standard GMM-UBM model. We expect the discriminative model to perform better for two primary reasons, which are that the dataset is small and unbalanced and that the GMM-UBM model cannot account for temporal prosodic patterns. Over the next two section, we describe the local and global cues in detail.

Global cues are statistical functional estimates calculated per-speaker over the entire interaction segment. Statistical functionals estimates computed over a sample set represent the characteristics of the underlying probability distribution from which the sample is drawn (Fernholz, 1983). These cues capture the overall characteristics but do not model the temporal evolution of the prosodic signals. Fig. 2
                            describes the extraction procedure and below we provide a step-wise description of our methodology involving speaker-specific signal sampling, normalization and statistical computation.
                              
                                 (i)
                                 Speaker-specific prosodic signal sampling: Given the speaker assignment S, we initially selectively sample the prosodic signals (pitch, intensity, jitter and shimmer) to contain frames only belonging to that speaker (overlaps excluded; this is shown in Fig. 2 in the block labeled “Signal sampling”). Note that in some of the tasks, we do not have child speech as the experimental design does not require a child to vocalize during interaction. For such tasks, we sample the prosodic signal corresponding to the psychologist speech only.

Speaker-wise signal normalization: Next, we perform speaker-wise normalization on the sampled signal values to minimize speaker specific traits. We chose z-normalization (similar to cepstral means–variance normalization (Molau et al., 2003)) with means and variances obtained from all available vocalizations for each speaker. This includes all the five tasks as well as audio recordings before and after the Rapid ABC sessions.

Global cue calculation: Finally, we calculate four statistical values over the z-normalized signals for the task at hand. These statistical functionals are mean, median, standard deviation and range. We call these our global cues.

Since the computation of global cues does not capture temporal dynamics associated with the interaction between the two participants, we propose a method to overcome these limitations in the following section, utilizing local prosodic cues.

The global cues computation treats prosodic signals as time series of independently drawn samples and the cue value will be unaffected even if samples in the time series are interchanged. However, the temporal evolution of prosodic signals may impart further information regarding the engagement behavior of children. Therefore, we propose a “prosody word” based scheme to concisely capture the temporal patterns in prosody along with jointly modeling the prosody of the two speakers. The local cues quantify changes in prosodic signals (e.g. increase in intensity, decrease in pitch) which are later associated with the engagement levels. The local cues are inspired from feature quantization methods (Ahalt et al., 1990) and language modeling techniques (Katz, 1987) in automatic speech recognition (Levinson et al., 1983). Recently, feature quantization has been coupled with other modeling techniques to address problems such as topic modeling (Kim et al., 2012; Nakano et al., 2014) and speaker recognition (Shriberg et al., 2005). We evaluate the utility of these cues in engagement prediction over just using the global cues. Fig. 3
                            summarizes the local cue extraction framework, consisting of five steps which are described next in detail.


                           
                              
                                 (i)
                                 Signal quantization: Given a prosodic signal and the speaker assignment, we quantize each frame of the signal based on a threshold (T). If the frame is assigned to a unique speaker (Child or Psy), we set T to be the median of the prosodic signal over the entire available vocal activity for the same speaker. As an example, a graphical illustration for pitch signal quantization is shown in Fig. 4
                                    . Eq. (1) lists the naming convention for quantized values in the provided example. d(p
                                    
                                       n
                                    ) represents the discretized value for p
                                    
                                       n
                                     (the pitch value at the nth frame). Frames assigned to overlap or silence are retained without further quantization.


                                    
                                       
                                          (1)
                                          
                                             d
                                             (
                                             
                                                p
                                                n
                                             
                                             )
                                             =
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  0
                                                                  Psy
                                                                  Pitch
                                                               
                                                            
                                                            
                                                               if
                                                               
                                                               
                                                                  p
                                                                  n
                                                               
                                                               <
                                                               T
                                                               (
                                                               Psy
                                                               )
                                                               ;
                                                               
                                                                  if
                                                                     
                                                                  the
                                                                     
                                                                  frame
                                                                     
                                                                  belongs
                                                                     
                                                                  to
                                                                     
                                                                  the
                                                                     
                                                                  psychologist
                                                                  speech
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  1
                                                                  Psy
                                                                  Pitch
                                                               
                                                            
                                                            
                                                               if
                                                               
                                                               
                                                                  p
                                                                  n
                                                               
                                                               ≥
                                                               T
                                                               (
                                                               Psy
                                                               )
                                                               ;
                                                               
                                                                  if
                                                                     
                                                                  the
                                                                     
                                                                  frame
                                                                     
                                                                  belongs
                                                                     
                                                                  to
                                                                     
                                                                  the
                                                                     
                                                                  psychologist
                                                                     
                                                                  speech
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  0
                                                                  Child
                                                                  Pitch
                                                               
                                                            
                                                            
                                                               if
                                                               
                                                               
                                                                  p
                                                                  n
                                                               
                                                               <
                                                               T
                                                               (
                                                               Child
                                                               )
                                                               ;
                                                               
                                                                  if
                                                                     
                                                                  the
                                                                     
                                                                  frame
                                                                     
                                                                  belongs
                                                                     
                                                                  to
                                                                     
                                                                  the
                                                                     
                                                                  child
                                                                     
                                                                  speech
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  1
                                                                  Child
                                                                  Pitch
                                                               
                                                            
                                                            
                                                               if
                                                               
                                                               
                                                                  p
                                                                  n
                                                               
                                                               ≥
                                                               T
                                                               (
                                                               Child
                                                               )
                                                               ;
                                                               
                                                                  if
                                                                     
                                                                  the
                                                                     
                                                                  frame
                                                                     
                                                                  belongs
                                                                     
                                                                  to
                                                                     
                                                                  the
                                                                     
                                                                  child
                                                                     
                                                                  speech
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               O
                                                            
                                                            
                                                               
                                                                  if
                                                                     
                                                                  the
                                                                     
                                                                  frame
                                                                     
                                                                  contains
                                                                     
                                                                  an
                                                                     
                                                                  overlap
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               
                                                                  if
                                                                     
                                                                  the
                                                                     
                                                                  frame
                                                                     
                                                                  contains
                                                                     
                                                                  silence
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    where T(Psy) is the median pitch value over the entire psychologist vocal activity and T(child) is the median pitch over the entire child vocal activity.

Although discretization reduces information, it allows for more complex modeling and learning with limited data. The feature median gives us a balanced distribution of the two binary categories and is not vulnerable to outliers. We do not perform a finer quantization of the prosodic signals as the number of prosody words (defined later) increases exponentially leading to sparsity issues.


                                    Signal units: Next, we define a “signal unit” over a window consisting of multiple discretized signal frames, aiming to capture the signal dynamics over a shorter time span. We operate sequentially on the discretized prosodic signal given the window length W and window overlap length V. For a window starting at the nth frame, we define the signal unit based on the following W discretized signal values. A signal unit provides a compact statement about the prosody within a window, such as the window containing “high pitch” or “a transition from high to low intensity”. Using the same example of the pitch signal, we list the signal unit assignment strategy in Table 3
                                    . 
                                       
                                          D
                                          n
                                          Pitch
                                       
                                     is a window starting at the nth frame containing the discretized values {d(p
                                    
                                       n
                                    ), …, d(p
                                    
                                       n+W
                                    )}. 
                                       U
                                       (
                                       
                                          D
                                          n
                                          Pitch
                                       
                                       )
                                     is the signal unit assigned to 
                                       
                                          D
                                          n
                                          Pitch
                                       
                                    . An example pitch units assignment with W
                                    =3 and V
                                    =1 is shown in Fig. 5
                                    .


                                    Prosodic words: Next, we concatenate the sequence of signal units from multiple signals to obtain the joint representation over various prosodic signals. In this work, we use the pitch and the intensity signals. Using more signals leads to an exponential increase in the number of prosodic words 
                                    
                                       3
                                    
                                    
                                       3
                                       Just adding one more signal increases the count of potential prosodic words from 6 to 29. This leads to a very large number of n-grams as computed next.
                                     and also jitter and shimmer are poorly estimated 
                                       4
                                    
                                    
                                       4
                                       The jitter and shimmer signals rely upon accurate estimation of F0. In our case, we find that jitter and shimmer estimates for psychologist and child speech are completely absent for about 33% and 50% of the 370 tasks, respectively, as F0 could not be continuously estimated by Praat over long periods of time in these sessions.
                                    . Eq. (2) shows the vector obtained after concatenating signal unit for pitch 
                                       U
                                       (
                                       
                                          D
                                          n
                                          Pitch
                                       
                                       )
                                     and intensity 
                                       U
                                       (
                                       
                                          D
                                          n
                                          Int
                                       
                                       )
                                    , defined as the prosodic word. The prosodic word provides a combined representation of the dynamics captured by the signal units
                                 


                                    
                                       
                                          (2)
                                          
                                             
                                                R
                                                n
                                             
                                             =
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               U
                                                               (
                                                               
                                                                  D
                                                                  n
                                                                  Pitch
                                                               
                                                               )
                                                            
                                                         
                                                         
                                                            
                                                               U
                                                               (
                                                               
                                                                  D
                                                                  n
                                                                  Int
                                                               
                                                               )
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

Each of the prosodic words give us a crude estimation of the prosodic signal dynamics over a shorter time span.

N-grams of prosodic words: Given the sequence of prosodic words for the Rapid ABC tasks, we apply ideas similar to language modeling (Katz, 1987) in automatic speech recognition. We define n-grams on the set of prosodic words. For instance, the bigrams are defined by the pairs of consecutive prosodic words; R
                                    
                                       n
                                     and R
                                    (n+W−V) (note that if a window starts at n, the next window will start at n
                                    +
                                    W
                                    −
                                    V to achieve an overlap of V).

Similarity computation: Next, we compute empirical occurrence probabilities of n-grams on: (a) a given test task and (b) all the tasks in the training set with a specified engagement level. Let n-gram
                                       l
                                     be one of L possible n-grams; then the empirical probability for n-gram
                                       l
                                     on the test task π
                                    test(n-gram
                                       l
                                    ) is as shown in Eq. (3), and the empirical probability on training tasks with engagement level E, π
                                    train:E
                                    (n-gram
                                       l
                                    ) are as shown in Eq. (4).


                                    
                                       
                                          (3)
                                          
                                             
                                                π
                                                test
                                             
                                             (
                                             
                                                n-gram
                                                l
                                             
                                             )
                                             =
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               Total
                                                                  
                                                               count
                                                                  
                                                               of
                                                            
                                                               
                                                            n
                                                            
                                                               −
                                                               gram
                                                            
                                                         
                                                         l
                                                      
                                                   
                                                   
                                                   
                                                      in
                                                         
                                                      the
                                                         
                                                      test
                                                         
                                                      task
                                                   
                                                
                                                
                                                   
                                                      Total
                                                         
                                                      number
                                                         
                                                      of
                                                   
                                                      
                                                   n
                                                   
                                                      −
                                                      grams
                                                         
                                                      in
                                                         
                                                      the
                                                         
                                                      test
                                                         
                                                      task
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          (4)
                                          
                                             
                                                π
                                                
                                                   
                                                      train
                                                      :
                                                   
                                                   E
                                                   
                                                
                                             
                                             (
                                             
                                                n-gram
                                                l
                                             
                                             )
                                             =
                                             
                                                
                                                   
                                                      
                                                         
                                                            Total
                                                               
                                                            count
                                                               
                                                            of
                                                         
                                                            
                                                         n
                                                         
                                                            −
                                                            gram
                                                         
                                                      
                                                      l
                                                   
                                                   
                                                   
                                                      in
                                                         
                                                      training
                                                         
                                                      partition
                                                         
                                                      tasks
                                                         
                                                      w
                                                      /
                                                         
                                                      engagement
                                                         
                                                      level
                                                   
                                                      
                                                   E
                                                
                                                
                                                   
                                                      
                                                         Total
                                                            
                                                         count
                                                            
                                                         of
                                                            
                                                         all
                                                      
                                                         
                                                      n
                                                      
                                                         −
                                                         grams
                                                            
                                                         in
                                                            
                                                         the
                                                            
                                                         training
                                                            
                                                         partition
                                                            
                                                         tasks
                                                            
                                                         w
                                                         /
                                                            
                                                         engagement
                                                            
                                                         level
                                                      
                                                   
                                                      
                                                   E
                                                
                                             
                                          
                                       
                                    
                                 

A final step in local cue computation consists of computing the cosine similarity C
                                    
                                       E
                                     (Eq. (5)) between vectors of empirical probabilities π
                                    test(n-gram
                                       l
                                    ) and π
                                    train:E
                                    (n-gram
                                       l
                                    ). In Eq. (5), 〈π
                                    test(n-gram1), …, π
                                    test(n-gram
                                       L
                                    )〉 represents the vector of π
                                    test(n-gram
                                       l
                                    ) over all the n-grams computed on the test set. Similarly, 〈π
                                    train:E(n-gram1), ldots, π
                                    train:E(n-gram
                                       L
                                    )〉 represents the vector of π
                                    train:E
                                    (n-gram
                                       l
                                    ) computed on the train set. C
                                    
                                       E
                                     is shown as the cosine distance between these two vectors. c
                                    
                                       E
                                    (n-gram
                                       l
                                    ) is simply the product of π
                                    test(n-gram
                                       l
                                    ) and π
                                    train:E(n-gram
                                       l
                                    ). The C
                                    
                                       E
                                     and c
                                    
                                       E
                                    (n-gram
                                       l
                                    ) measures serve as our local cues and concisely capture the dynamics in prosodic signals. This last step in local cue computation is shown in Fig. 6
                                    .


                                    
                                       
                                          (5)
                                          
                                             
                                                C
                                                E
                                             
                                             =
                                             
                                                
                                                   〈
                                                   
                                                      π
                                                      test
                                                   
                                                   (
                                                   
                                                      n-gram
                                                      1
                                                   
                                                   )
                                                   ,
                                                   …
                                                   ,
                                                   
                                                      π
                                                      test
                                                   
                                                   (
                                                   
                                                      n-gram
                                                      L
                                                   
                                                   )
                                                   
                                                      〉
                                                      T
                                                   
                                                
                                                
                                                   |
                                                   〈
                                                   
                                                      π
                                                      test
                                                   
                                                   (
                                                   
                                                      n-gram
                                                      1
                                                   
                                                   )
                                                   ,
                                                   …
                                                   ,
                                                   
                                                      π
                                                      test
                                                   
                                                   (
                                                   
                                                      n-gram
                                                      L
                                                   
                                                   )
                                                   〉
                                                   
                                                      |
                                                      2
                                                   
                                                
                                             
                                                
                                             
                                                
                                                   〈
                                                   
                                                      π
                                                      
                                                         train
                                                         :
                                                         E
                                                      
                                                   
                                                   (
                                                   
                                                      n-gram
                                                      1
                                                   
                                                   )
                                                   ,
                                                   …
                                                   ,
                                                   
                                                      π
                                                      
                                                         train
                                                         :
                                                         E
                                                      
                                                   
                                                   (
                                                   
                                                      n-gram
                                                      L
                                                   
                                                   )
                                                   〉
                                                
                                                
                                                   |
                                                   〈
                                                   
                                                      π
                                                      
                                                         train
                                                         :
                                                         E
                                                      
                                                   
                                                   (
                                                   
                                                      n-gram
                                                      1
                                                   
                                                   )
                                                   ,
                                                   …
                                                   ,
                                                   
                                                      π
                                                      
                                                         train
                                                         :
                                                         E
                                                      
                                                   
                                                   (
                                                   
                                                      n-gram
                                                      L
                                                   
                                                   )
                                                   〉
                                                   
                                                      |
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          (6)
                                          
                                             
                                                c
                                                E
                                             
                                             (
                                             
                                                n-gram
                                                l
                                             
                                             )
                                             =
                                             
                                                π
                                                test
                                             
                                             (
                                             
                                                n-gram
                                                l
                                             
                                             )
                                             ·
                                             
                                                π
                                                
                                                   train
                                                   :
                                                   E
                                                
                                             
                                             (
                                             
                                                n-gram
                                                l
                                             
                                             )
                                          
                                       
                                    
                                 

We train separate classifiers on the set of global and local cues as described in the next section. For the local cues, we tune the window parameters W and V using inner cross-validation on the training set. Given the small amount of data, only unigrams and bigrams are extracted for reliable estimation. As the number of n-grams is still large, we perform feature selection on c
                                    
                                       E
                                    (n-gram
                                       l
                                    ) during classification via the correlation-based feature selection (CFS) filter algorithm proposed in Hall (1998). This feature selection scheme evaluates the worth of a subset of products c
                                    
                                       E
                                    (n-gram
                                       l
                                    ) by considering the predictive ability of each along with the correlation amongst all of them. In the next section, we provide the results.

We initially train a GMM-UBM model on frame-wise features as a baseline. This is followed by the description of the discriminative model trained on the global and local cues. We first describe the GMM-UBM model followed by the discriminative model based on global and local cues.


                     GMM-UBM on frame-wise features: In this modeling scheme, we initially sample the prosodic signals belonging to the psychologist and the child speech frames (for speaker specific sampling please refer to Fig. 2). On the sampled prosodic signals, we train separate psychologist and child speech UBMs. These UBMs are then adapted using the data from tasks with specific engagement levels. For example, frames from psychologist speech belonging to all tasks with engagement level E
                     0 are used to adapt psychologist UBM to obtain a psychologist GMM-model for level E
                     0. Therefore we obtain three different GMM-models each for the two speakers, corresponding to the three engagement levels. Note that these models are trained on a 4-dimensional space defined by the frame-wise values of the four prosodic signals. We evaluate the GMM-UBM by using speaker-independent cross-validation, i.e., leaving sessions from one child for testing and training on the rest. On the test set, we evaluate the prosodic signals belonging to psychologist speech frames using psychologist GMMs (likewise for child frames). The final class likelihoods are obtained by summing up likelihoods from psychologist and child GMMs for that class. Given the unbalanced dataset, we use unweighted average recall (UAR) as our performance metric and the results using the GMM-UBMs are shown in Table 4
                     .

From the results, we observe that though we beat the chance recall (33.33%) the value is relatively low. This is expected as we use the raw feature values without any regards to their characteristics over the entire task duration. Moreover in this simplistic model, the sequence of feature frames is not accounted for and no interaction between child and psychologist prosody dynamics is captured. We expect to represent the global characteristics and local temporal patterns using the global and local cues as presented in the next section.


                     Discriminative model based on global and local cues: We train individual classifiers on both global and local prosodic cues and then fuse the outputs in a stacked generalization framework (Wolpert, 1992). We chose this multi-layered classification approach for two main reasons. First, separate evaluations for global and local cues provide us with an independent measure of their discriminative power. Additionally, their joint performance helps us evaluate the degree of complementarity between the feature sets. Second, an independent training of global and local cue classifiers helps address data sparsity issues. A general schematic of classification experiments and their presentation in this section is given in Fig. 7
                     .

Speaker-independent cross-validation is performed by holding out data segments for each child, leading to 63 splits. Given that the data suffers from class bias, we subsample data points from the majority class (E
                     0, E
                     1) so that each class has the same number of training instances as the least represented class (E
                     2). This effectively optimizes our performance metric unweighted average recall (UAR) as using the same number of samples per class in training assigns equal importance to individual class recalls.

In order to determine class boundaries, we train multiclass support vector machine (SVM) classifiers (Cortes and Vapnik, 1995) with pairwise boundaries. We obtain probabilistic decisions for each engagement level by fitting logistic models to data point distances from SVM hyperplanes. However, the logistic models may not yield class probabilities that sum to one. Thus these probabilities are scaled using the coupling method suggested by Hastie and Tibshirani (1998). The assigned class is the one with the highest probability. The parameters (i. e. kernel, boxconstraint) of the SVM classifier are tuned by internal cross-validation on the training set. We observe that all SVM classifiers perform best using a linear kernel as complex kernels may overfit a small dataset easily.

We train two classifiers with global cues derived from the psychologist (SVMPsy) and the child (SVMChild). Whereas the psychologist speech is present in all the tasks, the child features are available only in 221 of 370 tasks (172 marked as E
                        0, 34 marked as E
                        1 and 15 marked as E
                        2). If the child speech is present, we fuse the outputs from SVMPsy and SVMChild using another classifier: SVMGlo. We train SVMGlo on the probabilities SVMPsy and SVMChild output on the training set itself. In the absence of child speech, we directly use the probabilities output from SVMPsy to determine the overall UAR for the global system.

@&#RESULTS AND DISCUSSION@&#

A schematic of the global cue classification procedure and corresponding results (UAR and class recalls) are displayed in Fig. 8
                           .

Global cues lead to statistically significant classification (53.8%, p
                           <0.05) compared to chance UAR (33.3%), supporting our hypothesis that the vocal prosody of speakers is related to the perceived engagement phenomenon. We observe that the features from psychologist speech are more predictive of the engagement levels than those from the child. This may be due to the fact that we train SVMPsy over more training instances. Another contributing factor may be the fact that our database includes young children in early phases of language development. Hence child prosody may not hold as much information as the child may lack precise control and use of their voice source. However, we do observe some complementarity in predictive power from the child speech and the psychologist speech. Specifically, when child speech is present, fusion of SVMPsy and SVMChild improved UAR by 3.2% (absolute improvement) over SVMPsy alone.

From the class-wise results, we observe that the classification recall for E
                           2 is the poorest. This may suggest that the global cues provide a better indication of the higher level of engagement and that information regarding the most disengaged level is diluted.

The proposed local cues concisely capture the joint evolution of intensity and pitch from both the speakers. We expect the local cues to be complementary in information to the global cues as global cues do not model local events. We train an SVM classifier SVMLoc on the local cues as features, i.e., the cosine similarities C
                        
                           E
                         and individual projections c
                        
                           E
                        (n-gram
                           l
                        ) selected using the CFS filter algorithm.

@&#RESULTS AND DISCUSSION@&#

Classification results with local cues are shown in Fig. 9
                           . After classification using SVMLoc, UAR for instances with and without child speech are separately shown. This helps us interpret the gains made after fusing local and global cues in the absence/presence of child speech.

We observe that characterization of prosodic signal dynamics performs marginally better (p-value<.10) than chance. The UAR is lower compared to the results using global cues, and the reasons may include: (i) use of only two prosodic signals against four used to extract global cues. A separate experiment on using global cues just from intensity and pitch give UAR of 48.3% against 53.8% obtained in Fig. 8. This is indicative of the loss in information after dropping jitter and shimmer. (ii) Extraction of local level cues involves discretization leading to loss of information. (iii) We do not have enough training samples from the minority classes to obtain a reliable empirical estimation of n-gram occurrence probabilities.

The global and local cues characterize prosodic signals at different temporal granularities. In the next section, we fuse the outputs from the two systems and investigate the complementarity between the local cues and global cues.

We fuse the results from the sets of global and local cues using a final level of SVM classifiers. We train separate classifiers to fuse local and global system outputs as per the absence/presence of child speech. The SVMFuse,C0 represents classifier trained on instances with no child speech and SVMFuse,C1 in the presence of child speech. We cannot train a single fusion classifier as the global cue outputs are obtained from either SVMPsy (child speech absent) or SVMGlo (child speech present). Fig. 10
                         summarizes the results.

Our final model significantly beats the baseline GMM-UBM (using difference in proportions test for p-value<5%. The number of samples are determined based on the conservative significance testing stated in Section 2.3) and the results are indicative of the degree of complementarity between local and global cues. An absolute gain of 2% is obtained over the model using global cues only. This indicates that the local cues, despite being weak individually, provide an extra source of information. A higher recall for E
                        1 and E
                        2 during the presence of child speech indicates that the cues on child speech favor classification toward lower engagement levels.

In this section, we further study the individual global and local cues through feature ranking and selection. We present our analysis separately for the two cases below.

We use a one-R classifier (Holte, 1993) to rank the global cues obtained from each speaker over a dataset subsampled for balanced class distribution. A one-R classifier predicts the target label using only one feature at a time and this feature ranks the features based on their performance. We list the top five ranked features from the psychologist and the child in Table 5
                        . We also plot the top two features in Fig. 11
                         to observe the inter-class patterns in a 2-dimensional space. Note that a separate classification experiment using the top few global features based on one-R ranking scheme did not improve the recall.

A different set of features is selected from the psychologist and child global cues to be most predictive of engagement. A good mix of statistical functionals from different prosodic features suggests that the engagement phenomenon affects different aspects of vocal prosody. The plot of the top two features in Fig. 11 shows the class distribution between the two features. Between these features, there is no clear discrimination amongst classes. However we do observe a wider spread for instances from class E
                        0. Even though the class boundary characteristics can be different considering all the features together, a larger training can help better class boundary estimation and application of a more complex modeling technique.

In this section, we analyze the discriminative power of the local cues, the cosine distance and the individual n-gram products. During each cross-validation fold, a subset of the n-gram products (c
                        
                           E
                        (n-gram
                           l
                        )) is selected based on the CFS algorithm. Table 6
                         lists the selection frequency of the top seven n-grams for which a c
                        
                           E
                        (n-gram
                           l
                        ) is selected. We observe that certain n-grams get selected in almost all the cross-validation folds, suggesting some prosodic patterns are more informative than others. Also, as most of the selected n-grams are bi-grams, the count of a sequence of prosodic words is more important than stand alone uni-grams.

We also plot the cosine distances 
                           
                              C
                              
                                 
                                    E
                                    1
                                 
                              
                           
                         and 
                           
                              C
                              
                                 
                                    E
                                    2
                                 
                              
                           
                         over the dataset, sampled with an equal number of instances per class in Fig. 12
                        . Similar to the global cues, in the selected two dimensional space, data points from the three classes overlap in the feature space. A higher value for cosine similarities 
                           
                              C
                              
                                 
                                    E
                                    1
                                 
                              
                           
                         and 
                           
                              C
                              
                                 
                                    E
                                    2
                                 
                              
                           
                         for test tasks assigned to E
                        1 and E
                        2 suggests that similar prosodic patterns exist amongst tasks with a specific engagement level. This encourages us to further investigate and improve our projection scheme and achieve a higher discrimination.

We aim to derive robust universal measures reflective of engagement across variable interaction settings. We present an analysis of the performance of our system in relation to two sources of variability in our dataset: (i) the task at hand and (ii) the interacting psychologist.

The observed distribution of engagement levels across subjects depends on the task, as previously shown in Table 2. We have thus far disregarded the task type, focusing on capturing more universal prosodic patterns. In this section, we investigate the task-wise performance of our approach. We split the results in Fig. 8 based on task type and list them in Table 7
                        .

The recall of our system varies across each task. Performance is higher for Hello, Ball, and Book tasks compared to the lower performance in Hat and Smiling tasks (although all are above chance UAR, 33.3%). In particular, the E
                        1 recall for these three tasks is high, and these tasks account for 76% of the samples from E
                        1. We get a low UAR for the Hat task. However, given the high class imbalance, our systems performs well in correctly classifying 90.1% of the instances belonging to E
                        0. Our system performs the worst in the Smiling task. In this task, the psychologist approaches the child saying “I am gonna tickle you” three times and the child responds both visually and vocally, usually with a non-verbal vocalization like laughter or a grin. Hence the facial expression of the child may be a good indicator of his/her engagement in this case. Given the fact that we have a low UAR, we speculate that engagement may be better captured by a different modality than vocal prosody. Excluding the Smiling task, our prosody-based system achieves an UAR of 60.3% in the other four tasks, which is significantly higher than the performance of the Smiling task (using difference in proportions test for p-value< 10%. The number of samples per class are determined based on the conservative significance testing stated in Section 2.3).

Each child is assessed by one of the four psychologists. Table 8
                         shows the performance of our system per psychologist. A higher UAR is obtained for psychologist #1 as compared to psychologist #2. This may suggest that our model has a slight bias towards the more represented psychologist. In particular, our model did not perform well for the minority classes for psychologist #2. It is hard to interpret the results for psychologist #3 and #4 given the small number of samples, but the model performs well for both these psychologists over the small representative set. Psychologist #3 has most of the samples assigned to E
                        0 and E
                        1 and the model achieves good class recalls for these classes. In the case of psychologist #4 we correctly classify 9 out of 13 instances from E
                        0 achieving good class recall. UAR is imbalanced for psychologist #3 and #4 as class recall for E
                        2 is computed on just one instance.

Overall, our model is able to capture the prosodic patterns that are universally utilized by the psychologists. As of now, the UARs are not significantly different (difference in proportions test) given this small set of samples per psychologist. However, we intend to improve and analyze our framework in the future by including more samples, particularly from scarcely-represented psychologists.

@&#CONCLUSION@&#

Engagement behavior reflects a complex internal state signifying occupation with a person or in a task. Behavioral cues related to engagement may be conveyed visually, vocally or physiologically. In this work, we present a system to use the reflection of engagement in the audio modality, in particular in speech. We present our results over dyadic interaction between a child and a psychologist performing several interactive tasks together. We observe that the perceived engagement level of the child is not only reflected in the child's vocal prosody but also in the psychologist's prosody. This is particularly useful in the case when we predict the child's engagement in the absence of child speech. We develop a generic model to capture the patterns in a time series at two levels of temporal granularities and use it for finding patterns in prosody related to engagement behavior. The first component of our system uses global cues to capture the patterns over the entire duration of a task. In the second component, the local cues capture patterns which last over a shorter time span. The local cues are also capable of capturing the joint evolution of patterns across the two speakers, which is otherwise not accounted for by the global cues. We observe that the systems with global and local cues each provide discriminative power individually, but the best system is obtained after fusing their results. This suggests that the engagement phenomenon is related to the cross-sectional prosodic patterns projected over the entire interaction duration as well as the those over short durations.

We provide analysis on the derived patterns in the global and local cues which provide the discrimination on the engagement level. An analysis of performance per task suggests that our system works well in all tasks except the smiling task. Thus, our system generalizes fairly well under the variations introduced by a different kind of interaction setup. Similar observations were made across ratings from multiple psychologists, where our model was able to capture patterns used across different psychologists.

Our proposed classification schemes perform fairly well on the engagement prediction task from a single modality. We linked speech prosody to engagement phenomenon and the approach may be extended to other properties of speech. As we observe, our system performance may vary under a different interaction setting, other behavioral modalities beyond vocal prosody need to be included to model all observable patterns related to engagement, a goal for future enhanced system development. Our model, however, provides a general automatic behavior analysis tool using observed cues which when combined with the domain knowledge of the psychologist may provide an efficient child behavior assessment scheme.

Hence, our model provides a framework that may be applied to any such time series data but it can be further refined. Currently, the model is susceptible to sparsity problems and can benefit from smoothing techniques such as those common in language modeling (Chen and Goodman, 1999). Also we train a linear classifier to capture the patterns whereas the distribution of the top two features suggests a more clustered appearance. Hence a more efficient classification approach may be developed to take advantage of such a distribution. In addition, since our model captures the common patterns without regards to the factors of variability, an optimal combination with models specific to each setting may further improve the results. Our classifier combines the results from the two components and further investigation needs to be done to understand how the two cues complement each other.

@&#ACKNOWLEDGMENT@&#

This work is supported by National Science Foundation (grant number: NSF IIS-1029373).

@&#REFERENCES@&#

