@&#MAIN-TITLE@&#Enhanced needle localization in ultrasound using beam steering and learning-based segmentation

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Beam steering is used to enhance the appearance of needles in ultrasound images.


                        
                        
                           
                           Needle segmentation is challenging, even when using beam steering-based needle enhancement.


                        
                        
                           
                           Needle segmentation is posed as a machine learning problem. A boosted classifier using a bank of log-Gabor wavelets is proposed for needle pixel classification. The classifier is trained on ex vivo and clinical nerve block datasets.


                        
                        
                           
                           As a result of improved segmentation performance compared to previous methods, needle localization accuracy and robustness are increased using the proposed method.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Ultrasound

Regional anesthesia

Segmentation

Classification

Needle guidance

@&#ABSTRACT@&#


               
               
                  Segmentation of needles in ultrasound images remains a challenging problem. In this paper, we introduce a machine learning-based method for needle segmentation in 2D beam-steered ultrasound images. We used a statistical boosting approach to train a pixel-wise classifier for needle segmentation. The Radon transform was then used to find the needle position and orientation from the segmented image. We validated our method with data from ex vivo specimens and clinical nerve block procedures, and compared the results to those obtained using previously reported needle segmentation methods. Results show improved localization success and accuracy using the proposed method. For the ex vivo datasets, assuming that the needle orientation was known a priori, the needle was successfully localized in 86.2% of the images, with a mean targeting error of 0.48mm. The robustness of the proposed method to a lack of a priori knowledge of needle orientation was also demonstrated. For the clinical datasets, assuming that the needle orientation was closely aligned with the beam steering angle selected by the physician, the needle was successfully localized in 99.8% of the images, with a mean targeting error 0.19mm. These results indicate that the learning-based segmentation method may allow for increased targeting accuracy and enhanced visualization during ultrasound-guided needle procedures.
               
            

@&#INTRODUCTION@&#

Needle-based clinical procedures such as nerve block, fluid aspiration, therapeutic injections, and biopsy can be performed under ultrasound (US) image guidance [2–7]. The use of US imaging allows for real-time visualization of needles within the desired anatomical context. Electronically steering the US beam in a direction perpendicular to the needle orientation produces strong specular reflections that enhance needle visualization [8,9]. However, needle visualization is still challenging in the following scenarios: (1) the needle is not completely contained in the 2D US imaging plane, (2) the background tissue contains long, linear objects such as bone, fascia, or tissue boundaries that resemble the needle, and (3) the needle orientation does not match the US beam steering angle. For example, Fig. 1
                      shows a B-mode US image frame acquired during a nerve block procedure using beam steering hardware and software for improved needle visualization. The needle is only slightly in-plane, and automatic segmentation of the needle is made more challenging by the nearby bright, linear anatomical structures that resemble the needle. In such cases, a robust and reliable segmentation of the needle would allow the ultrasound system to automatically optimize the beam steering angle and image processing settings to increase the contrast of the needle relative to the background structures. In addition, proper identification of the needle position and orientation can help improve targeting accuracy.

Needle localization methods can be divided into two groups: hardware and software methods. Hardware methods seek to detect the needle by augmenting it with tracking sensors or other physical modifications. Hardware approaches to needle tracking include: localization using needle vibration and Doppler US [10], attaching small US receivers to the needle and using time-of-flight principles for spatial localization [11], and using magnetic tracking sensors and camera attachments for localization [12–14].

Software-based methods seek to segment the needle using image processing algorithms. The focus of recent work in this area has largely been applied to 3D US. For example, the pose of long, straight instruments augmented with passive markers was found in 3D US images using a GPU accelerated generalized Radon transform [15]. The algorithm presented in [16] demonstrated localization of long straight tools in near real-time without the use of hardware acceleration by fitting voxels to a maximum likelihood-based tool model using the RANSAC algorithm. A volume rendering-based approach to tool localization was demonstrated in [17]. In [1], line filtering methods were employed to increase the contrast of the needle relative to background structures in 3D US.

Compared to 3D US, 2D US is currently more widely used and has an easier learning curve for needle guided procedures than 3D US due to ease of image interpretation. Nevertheless, automatic needle localization in 2D US still remains challenging. In [18], curved needles in 2D US images were detected using a polynomial Hough transform and a modified coordinate transform. In [19], a real-time Hough transform was developed to increase needle tracking speed, where needle segmentation was accomplished using intensity thresholding. The threshold was found by computing the percentage of pixels that should belong to the needle based on its presumed length, and then finding the corresponding threshold from the intensity histogram. In [20], needle segmentation was performed using a combination of a variance filter, intensity thresholding, and morphological operations. Principle component analysis was used for final pose estimation. Segmentation of prostate biopsy needle path was investigated in [21,22]. In [21], metrics based on pixel temporal difference, temporal variance, and spatial variance, were combined to detect a biopsy core. In [22], a graph-cut-based segmentation was performed on images filtered with a second-order Gaussian derivative.

In real clinical cases, a key challenge is accurate segmentation when the needle is not the most prominent linear structure in the image and the true orientation of needle entry deviates from that expected by the operator. We hypothesized that framing needle segmentation as a classification problem would make the segmentation more robust to inaccurate prior assumptions about needle appearance and orientation. A segmentation method based on statistical boosting of wavelet features was developed and tested on challenging ex vivo and clinical datasets with sub-optimal needle positioning and the presence of tissue artifacts. Results showed enhanced localization accuracy using the proposed method.

@&#METHODS@&#

Image-based needle detection can be thought of as a two step process. First, either a hard or soft assignment to a specific class (“needle” or “background”) is given to each pixel in the image (segmentation step). Second, based on these class assignments, parameters that best describe the needle position and orientation are found (localization step). The key innovation of our approach was the use of a learning-based pixel classifier for the segmentation step. Adaboost [23] was used to train the classifier from manually annotated training images. The classifier was then used to segment the needle in unseen test images. Finally, the Radon transform was applied to the segmented images to determine the needle location and orientation. Throughout the literature, the terms “segmentation”, “classification” and “localization” are often used interchangeably to describe related but different processes. In this paper, the terms “segmentation” and “classification” both refer to the process of assigning a class label to pixels, while “localization” refers to the process of determining the needle position and orientation.

A bank of log-Gabor wavelets was used to generate classification features at each image pixel. Gabor wavelets are complex sinusoids modulated by a Gaussian function, and have been widely used for segmentation and texture classification in medical [24,25], natural [26,27] and remote sensing [28,29] images. Log-Gabor wavelets are Gabor wavelets that are computed on a log rather than linear scale. They were first proposed in [30], where the author suggested that they encode visual information more efficiently than Gabor functions. Features derived from log-Gabor filters were previously used to localize bone surfaces in 2D and 3D US images [31,32].

Log-Gabor wavelets have a spatial wavelength (λ), spatial bandwidth (σ), orientation (θ), and angular bandwidth (α). In our implementation, even spectral coverage is obtained by introducing a parameter β, defined such that σ
                        =
                        βλ. For each value of β, a series of filters with wavelengths λ
                        
                           n
                         are chosen such that 
                           
                              λ
                              n
                           
                           =
                           
                              λ
                              0
                           
                           ·
                           
                              m
                              β
                              n
                           
                        , where m is an empirically determined constant. Details on the choice of filter parameters are specified in [33]. The response to each filter is complex and therefore can be divided into real and imaginary parts, as well as magnitude, each of which is considered a separate feature type ϕ. The filter orientation θ is set equal to an assumed needle orientation. In our implementation, all possible combinations of the parameters listed in Table 1
                         were used to generate K=180 image features:

Adaboost [23,34], a type of statistical boosting algorithm, was used to generate a single strong classifier from a set of weak classifiers. In our application, pixels were classified as either “needle” or “background”, and the weak classifiers, h
                        
                           k
                        
                        ∈{−1, 1}, were the thresholded responses of the image to the log-Gabor filters, f
                        
                           k
                        , such that h
                        
                           k
                        
                        =
                        sgn(f
                        
                           k
                        
                        −
                        τ
                        
                           k
                        ). For each filter response f
                        
                           k
                        , τ
                        
                           k
                         was chosen to be the threshold that minimized the training error for that specific filter. A strong classifier, H
                        
                           T
                        
                        ∈{−1, 1}, was built from a linear combination of T weak classifiers such that 
                           
                              H
                              T
                           
                           =
                           sgn
                           (
                           
                              ∑
                              
                                 t
                                 =
                                 1
                              
                              T
                           
                           
                              w
                              t
                           
                           
                              h
                              t
                           
                           )
                        . Adaboost was used to determine the weights 
                           
                              w
                              t
                           
                         given expertly labeled training data. See [23] for more details on the Adaboost algorithm. For our application, the concept of a “rectified confidence measure”, R
                        
                           T
                        , was introduced, where 
                           
                              R
                              T
                           
                           =
                           max
                           (
                           
                              ∑
                              
                                 t
                                 =
                                 1
                              
                              T
                           
                           
                              w
                              t
                           
                           
                              h
                              t
                           
                           ,
                           0
                           )
                        . Since each pixel is an observation, the classification of each pixel i generates a binary segmentation image, H(x
                        
                           i
                        ) along with a rectified confidence map R(x
                        
                           i
                        ). This segmentation method is referred to as the BCS-T segmentation method, where T refers to the number of features used in the segmentation routine.

Following the segmentation step, the entire needle was localized by applying the Radon transform to the rectified confidence map, R(x
                        
                           i
                        ). Bright lines in R(x
                        
                           i
                        ) appear as peaks in the sinogram output of the Radon transform. The location and orientation corresponding to the highest peak in the sinogram was assumed to represent the line passing through needle. By using the Radon transform in combination with R(x
                        
                           i
                        ) rather than H(x
                        
                           i
                        ), objects with high segmentation confidence were more strongly weighted in the final pose estimate, minimizing the impact of false positive pixel classifications.

Each classifier was trained on data manually annotated by an expert. Because images contain large amounts of observations (pixels), it was not possible to train the data using every pixel. We therefore randomly downsampled the training dataset to 250, 000 observations (based on RAM limitations). We maintained the same ratio of needle to background pixels in the training dataset that existed in the entire dataset. Because the effectiveness of a classifier is measured by its performance on unseen data, a leave-one-out strategy was used with respect to experimental validation: for a given experiment, the classifier used for validation was trained using images from the other experiments.

Validation was performed by comparing the performance of the proposed boosted log-Gabor segmentation to three previously introduced methods: intensity thresholding, filtered intensity thresholding, and a linear needle segmentation method introduced by Uhercik et al. [1].

The intensity thresholding segmentation method (IS) simply labeled a pixel as a needle if its gray-level intensity value was greater than a threshold value τ, otherwise it was assigned as background. Based on previous studies [18,19], τ was set to the mean gray-level value of the needle pixels in the training dataset.

For filtered intensity segmentation (FS), the same methodology as for the IS method was applied, except that the image was first filtered to enhance elongated objects. We used the second-order Gaussian derivative filter, G(x, y), described in [35], similar to that used in [22] and for prostate biopsy needle detection:
                              
                                 (1)
                                 
                                    G
                                    (
                                    x
                                    )
                                    =
                                    
                                       1
                                       
                                          
                                             σ
                                             x
                                          
                                          
                                             
                                                2
                                                π
                                             
                                          
                                       
                                    
                                    
                                    exp
                                    
                                       
                                          
                                             −
                                             
                                                
                                                   
                                                      x
                                                      2
                                                   
                                                
                                                
                                                   2
                                                   
                                                      σ
                                                      x
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (2)
                                 
                                    
                                       G
                                       ″
                                    
                                    (
                                    y
                                    )
                                    =
                                    
                                       
                                          
                                             y
                                             2
                                          
                                          −
                                          
                                             σ
                                             y
                                             2
                                          
                                       
                                       
                                          
                                             σ
                                             y
                                             5
                                          
                                          
                                             
                                                2
                                                π
                                             
                                          
                                       
                                    
                                    
                                    exp
                                    
                                       
                                          
                                             −
                                             
                                                
                                                   
                                                      y
                                                      2
                                                   
                                                
                                                
                                                   2
                                                   
                                                      σ
                                                      y
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (3)
                                 
                                    G
                                    (
                                    x
                                    ,
                                    y
                                    )
                                    =
                                    G
                                    (
                                    x
                                    )
                                    
                                       G
                                       ″
                                    
                                    (
                                    y
                                    )
                                 
                              
                           where y is the direction perpendicular to the needle and x is parallel. An exhaustive search over the range of 0.6–1.5mm for σ
                           
                              y
                            and 1.8–12.0mm for σ
                           
                              x
                            was used to find the optimal filter parameters.

The linear segmentation method (ULS), introduced by Uherick in [1], used a combination of intensity information I and a tubularness measure J (also known as the Frangi vessel enhancement measure [36]). Linear discriminant analysis [37] was used to generate a classification function 
                              L
                              =
                              
                                 
                                    w
                                    →
                                 
                              
                              ·
                              
                                 
                                    [
                                    I
                                    ,
                                    J
                                    ,
                                    1
                                    ]
                                 
                                 ⊤
                              
                           , where 
                              
                                 
                                    w
                                    →
                                 
                              
                              =
                              [
                              
                                 w
                                 I
                              
                              ,
                              
                                 w
                                 J
                              
                              ,
                              
                                 w
                                 0
                              
                              ]
                            projected the input data onto a line L that best separated the needle and background classes.

The Radon transform-based needle localization method was used in conjunction with all segmentation methods, and therefore the experiments tested the ability of the proposed algorithm to localize needles given varying segmentation results. Two validation metrics were considered: targeting error (TE) and needle localization success rate (NLSR).

TE was defined as |d
                           
                              true
                           
                           −
                           d
                           
                              measured
                           |, where d
                           
                              true
                            was the shortest distance between a line representing the true needle and the image central pixel, and d
                           
                              measured
                            was the shortest distance between a line representing the measured needle and the image central pixel (Fig. 2
                           ). TE can be interpreted as a clinically relevant metric associated with the targeting of structures, and is highly dependent on the orientation error. TE was only computed using successful results to prevent gross mis-localizations from misrepresenting the TE statistics.

NLSR measured the percentage of images in which the localization result correctly identified the line passing through the needle. A successful localization should fit the following criteria: (1) the measured segmentation result should closely match the ground truth segmentation and (2) the line found by the localization step should pass through the ground truth. To enforce these criteria, the following conditions had to be met for a localization to be considered successful. First, the set of pixels x labeled as belonging to the needle by the segmentation method, S
                           
                              H
                           
                           ={x|H(x)=1}, and the set of ground truth needle pixels, S
                           
                              G
                           
                           ={x|G(x)=1}, should agree and therefore their intersection should not be empty, S
                           
                              H
                           
                           ∩
                           S
                           
                              G
                           
                           =
                           S
                           
                              HG
                           
                           ≠{}. Second the localization result must pass through at least one pixel in the set S
                           
                              HG
                           .

For image acquisitions, a Philips CX-50 portable US system with the Needle Visualization Mode feature was used. Needle Visualization Mode used beam steering and lower US frequencies to create an image in which metallic objects were enhanced and tissue was suppressed (Fig. 3
                           , left), which was termed a “needle” image (NI). The NI image was combined with a standard B-mode image to create a fused image (FI), which was the standard output image displayed for clinical use by the machine (Fig. 3, middle). The transducer used was a Philips L12-5 linear array probe. Software was written in MATLAB. The implementation of the log-Gabor filtering was performed in the Fourier domain for the purpose of computational efficiency.

Five ex vivo specimens were used, each of which was either a pork shoulder or whole chicken, resulting in a total of 94 images. We attempted to collect what were considered challenging images from the standpoint of needle localization: images with long, straight, bright objects corresponding to bone, fascia or tissue boundaries. Images were collected with 4.5cm depth setting. We used a hollow 18-gauge cannula needle typically used for nerve block, biopsy, or fluid evacuation. Multiple static needle positions were imaged, with varying insertion depths, degrees of alignment with the US imaging plane, and beam steering angles. Both NI and FI images were acquired at each needle position. The experimental set-up is shown in Fig. 4
                           .

The first analysis compared needle localization results when using NI images and when using FI images. Due to the smaller field-of-view in the FI images (see Fig. 3), the needle was not visible in some acquisitions; these images as well as their corresponding NI images were not used in the analysis. NLSR was measured for both image types, using the BCS-50, IS, FS and ULS segmentation methods.

The rest of the analyses for the ex vivo datasets were conducted on the full NI image dataset. These images contained many instances where the needle was inserted to a shallow depth, providing an opportunity to test needle localization in cases where pixel segmentation accuracy was crucial, as the length of the needle could not be relied upon as a distinguishing feature.

For the ex vivo data, NLSR was measured as a function of filter orientation mismatch. For the BCS-T and FS methods, the filter orientation should be applied in the direction of the needle for optimal appearance enhancement. Because the needle orientation is not known a priori, the proper filter orientation is chosen based on an assumption, such as that the needle orientation should be close to the beam steering angle, which is known during the procedure. In order to test the robustness of the localization method when the filter orientation and needle orientation did not match, every image was processed with varying degrees of filter mismatch compared to the needle orientation, which was determined from the ground truth.

Finally, the effect of using different numbers of features for the BCS-T segmentation method was measured by computing the NLSR with an increasing number of weak pixel-classifiers at varying filter orientation mismatches.

Six anonymized clinical nerve block datasets, totaling 577 images, were acquired using the Philips CX-50 Needle Visualization Mode and analyzed. Since this was a retrospective analysis, only the FI images were available for validation as the NI images are not recorded by default. Similarly to the ex vivo setting, each clinical dataset was validated using a leave-one-out strategy.

NLSR and TE were found for the entire dataset using the BCS-50, IS, FS, and ULS segmentation methods. Each frame was processed separately, and the segmentations were processed using the current steering angle set by the operator, meaning that the only information assumed about the needle orientation was that it might be close to the steering angle, a clinically reasonable assumption. Peaks in the Radon transform that corresponded to needle orientations less than 12.5° (nearly horizontal) or greater than 77.5° (nearly vertical) were ignored. This constraint was needed to avoid a nearly 0% NLSR using the IS and ULS segmentation methods.

@&#RESULTS@&#

For a given set of training images, Adaboost was used to select the best 100 features for needle classification from the log-Gabor filter bank. The first 25 features for an example classifier trained on ex vivo images are shown in Fig. 5
                           . A wide variety of filters are present, some of which appear to be suitable for detecting the needle because of their thin, elongated appearance (1, 3, 9), while some have a large spatial support and are likely used for suppressing background speckle noise (10, 11, 16).

Localization results for the NI and FI images are reported in Fig. 6
                           . The dataset consisted of 70 NI images and 70 corresponding FI images. The NLSR for the BCS-50, IS, FS, and ULS methods are reported. For each image, the filter orientation was set to be equal to the orientation of the needle as determined from the ground truth. As expected, localization was more accurate for the NI images than for the FI images for all segmentation methods.

NLSR and TE results for the 94 ex vivo images are shown in Fig. 7
                           . For the BCS-50 and FS segmentation methods, filter orientation mismatches of −25° to 25° at 5° increments were applied. A negative (counter-clockwise) offset shifted the filtering angle toward a more horizontally oriented direction, and vice versa. The BCS-50 method outperformed all other methods, except when the filter orientation mismatch was −25°. The FS method performed better than the IS and ULS methods, but only when filtering was steered to within 5° of the needle orientation.

Generally, the BCS-50 segmentation method also resulted in lower TE errors compared with the other methods, although all methods resulted in relatively low errors (Fig. 7). High TE correlated with low NLSR for all segmentation methods.

With respect to the direction of the filter orientation mismatch, negative (counter-clockwise) offsets tended to cause more localization errors than positive offsets. This is likely because horizontally oriented filters tended to enhance tissue structures, which were mostly horizontal themselves, resulting in more false positive pixel classifications. In fact, the best results in terms of NLSR were obtained when a small positive (clockwise) filter orientation mismatch was applied, rather than no mismatch, as seen in Fig. 7.

An illustration of the performance for each segmentation method is shown in Fig. 8
                           . When the needle was clearly the brightest object in the image, all of the segmentation methods performed well. However, if the needle was relatively faint or obscured, the IS, FS, and ULS methods often failed. In such sub-optimal imaging conditions, the BCS-50 method greatly outperformed the other methods.

For the BCS-T segmentation method, the effect of increasing the number of features used for classification on NLSR is shown in Fig. 9
                           . Feature counts of 1, 5, 10, 25, 50, and 100 features were included in the analysis. Filter orientation mismatches of −20°, −10°, 0°, 10°, 20° were also tested. It can be seen that the NLSR increased as the number of features was added, with the NLSR starting to level off at roughly 50 features. For successful localizations, the TE did not vary greatly as a function of feature count, ranging from 0.168 to 0.186mm. It can also be seen that the best NLSR results were obtained when the filter orientation mismatch was 10°, as was seen in Section 
                           3.1.2. An important result of adding more classification features to the BCS-T method is that the segmentation becomes more robust to filter orientation mismatch, as the relative drop-off in NLSR from −10° to −20° and 10° to 20° is less severe when more features are used.

For the validation on clinical datasets, data were processed such that it would mimic a real clinical application. To do this, the only assumption made about the needle was that its orientation was similar to the beam steering angle, and therefore the filter orientation for each frame was set equal to the beam steering angle. NLSR for the entire dataset of 577 image frames is reported, along with the TE for the successful localizations (Fig. 10
                           ).

The localization NLSR was higher in the clinical datasets than in the ex vivo datasets for the BCS-50 and FS methods, but was lower for the IS and ULS methods. TE errors were lower for all segmentation methods. This is mainly because the ex vivo datasets were designed to have challenging images from the viewpoint of segmentation, while the goal during the clinical procedures was to maintain optimal needle visibility. Still, some of the clinical datasets contained image frames where the needle was barely visible, resulting in localization errors for every method except the BCS-50 method. An example of such an image is shown in Fig. 11
                           , where highly reflective tissue was oriented in the same direction as the needle. This demonstrated the ability of the BCS-50 method to discriminate between the needle and background in highly cluttered images, resulting in better localization performance.

@&#DISCUSSION@&#

The method presented in this paper is not contrary, but rather complimentary to many of the needle localization methods that have been published on this topic. Previous research has focused on detecting the needle using parametric representations of the images (Radon, Hough, [38,15,19]) or by fitting models to localize the needle [16]. The key novelty in this work is the learning-based segmentation framework that can serve as inputs to these localization methods.

A qualitative examination of the results demonstrates the ability of the algorithm to segment and correctly localize the needle when it is not the brightest linear object in the image, which we believe is a key aspect of using a learning-based segmentation method that utilizes a number of features for pixel classification. While the other segmentation methods were successful in many cases, they often failed when the needle was slightly out of the US imaging plane (and therefore contained low intensities), especially images with multiple hyperechoic tissue structures. For example, Figs. 8 and 11 show images where the needle is slightly misaligned with the US imaging plane and tissues appear as long, bright objects. Only the boosted classifier is able to detect the needle, while other segmentation methods result in too many false pixel classifications for accurate needle localization. Another result of this work is that the segmentation performance of the proposed method was robust under weak assumptions about needle orientation. This is very important for applications where little is known about the orientation of the needle prior to the procedure, such as free hand procedures that do not utilize mechanical needle guides.

Aside from increased localization success and accuracy, a potential application of this work is visualization enhancement. In many applications, the operator may not need to know the exact trajectory of the needle, but would benefit from increased contrast of the needle against the background. An example of how our proposed method is capable of accomplishing this is shown in Fig. 12
                     . By combining the B-mode image with the rectified confidence map of the NI image, rather than the NI image itself, an FI image with better contrast is created.

The run-time for the full localization step took roughly 3s for the 50 feature classifier to return a result, and optimization is part of further research on this topic. Parallel CPU processing of each IFFT, or potentially the GPU-based batch cuFFT libraries in NVIDIA CUDA are potential solutions. In addition, more research will need to be conducted to determine if the needle localization is reliable enough for routine clinical use, as well as what types of procedures will benefit. Our initial study was conducted using features from a needle used for nerve block procedures. Thinner needles, however, may not have as distinct of an appearance in terms of texture and shape, and are the focus of further work. Furthermore, as 3D US becomes more common, future work will focus on 3D US implementations.

@&#CONCLUSION@&#

We presented an algorithm for segmentation of needles in 2D US sequences using beam steering and a learning-based framework. We demonstrated a 86.2% and 99.8% needle localization success rate and a targeting error of 0.48mm and 0.19mm in ex vivo and clinical data sets, respectively. We also demonstrated the robustness of the proposed learning-based framework to prior assumptions about needle orientation. Future studies will focus on validation on real-time streaming data in an in vivo setting, and implementation in 3D ultrasound.

@&#REFERENCES@&#

