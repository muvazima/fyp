@&#MAIN-TITLE@&#‘Horses for Courses’ in demand forecasting

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We try to answer the question: “what is the best forecasting method for my data?”.


                        
                        
                           
                           We simulate seven time series features and one strategic decision.


                        
                        
                           
                           
                              Cycle and randomness have the biggest (negative) effect for fast-moving data.


                        
                        
                           
                           
                              Inter-demand interval has the biggest (negative) effect for intermittent data.


                        
                        
                           
                           Increasing length of a series has a small positive effect on forecasting accuracy.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Forecasting methods

Time series methods

Forecasting accuracy

M-Competitions

Simulation

@&#ABSTRACT@&#


               
               
                  Forecasting as a scientific discipline has progressed a lot in the last 40years, with Nobel prizes being awarded for seminal work in the field, most notably to Engle, Granger and Kahneman. Despite these advances, even today we are unable to answer a very simple question, the one that is always the first tabled during discussions with practitioners: “what is the best method for my data?”. In essence, as there are horses for courses, there must also be forecasting methods that are more tailored to some types of data, and, therefore, enable practitioners to make informed method selection when facing new data. The current study attempts to shed light on this direction via identifying the main determinants of forecasting accuracy, through simulations and empirical investigations involving 14 popular forecasting methods (and combinations of them), seven time series features (seasonality, trend, cycle, randomness, number of observations, inter-demand interval and coefficient of variation) and one strategic decision (the forecasting horizon). Our main findings dictate that forecasting accuracy is influenced as follows: (a) for fast-moving data, cycle and randomness have the biggest (negative) effect and the longer the forecasting horizon, the more accuracy decreases; (b) for intermittent data, inter-demand interval has bigger (negative) impact than the coefficient of variation; and (c) for all types of data, increasing the length of a series has a small positive effect.
               
            

@&#INTRODUCTION@&#

Forecasts are important for all decision-making tasks, from inventory management and scheduling to planning and strategic management. Makridakis and Hibon (2000) advocated: “predictions remain the foundation of all science”. To that end, identification of the best forecasting techniques for each data set, or, even, for each series separately, is still the ‘holy grail’ in the forecasting field, and, as a result, empirical comparisons to this direction are considered very important (Fildes & Makridakis, 1995). Advanced, sophisticated and simpler extrapolation methods could be associated with specific features of data. The development of a protocol for automatic selection of the best tools for resolving each problem, a protocol that would guarantee minimum out-of-sample forecasting error and therefore have a substantial impact on decision making, is the ultimate challenge for researchers and practitioners in the field.

As early as the late 1960s and most of the 1970s, several researchers (Cooper, 1972; Groff, 1973; Kirby, 1966; Krampf, 1972; Levine, 1967; Makridakis & Hibon, 1979; Naylor & Seaks, 1972; Newbold & Granger, 1974) sought to determine the accuracy of various forecasting methods in order to select the most appropriate one(s). In addition, psychologists have been concerned with judgmental predictions and their accuracy, as well as the biases that affect such predictions, for more than half a century (Dawes, 1979; Hogarth, 1987; Kahneman & Tversky, 1973; Meehl, 1954, 1986; Slovic, 1972; Tversky & Kahneman, 1982). Amongst these biases, those affecting forecasting include over-optimism and wishful thinking, recency, availability, anchoring, illusory correlations and the underestimation of uncertainty. In a recent book, Kahneman (2011) describes these and other biases whilst also discussing what can be done to avoid, or minimize their negative consequences and emphatically states: “the research suggests a surprising conclusion: to maximize predictive accuracy, final decisions should be left to formulas, especially in low-validity environments” (Kahneman, 2011, p. 225). Moreover, the growing demand for forecasting big data (e.g. more than 200,000 time series for major retailers) renders the use of automatic statistical procedures necessary.

The purpose of this study isto measure the extent to which each of seven time series features (seasonality, trend, cycle, randomness, number of observations, inter-demand interval and coefficient of variation) and one strategic decision (the forecasting horizon) affect forecasting accuracy. In order to do this, we measure the impact of each of these eight factors
                        1
                        We use the term ‘factor’ to refer to both the data features as well as the strategic decision (forecasting horizon) which impact on forecasting accuracy will be examined through this study.
                     
                     
                        1
                      by generating a large number of time series – as well as using real data, and measuring the accuracy of the forecasts derived from 14 methods and five combinations of them. Furthermore, a multiple regression analysis is performed to measure the extent to which each of the factors affects the accuracy of each of the time series methods/combinations. The findings of this research could be very useful for practitioners if used for the appropriate selection of the best statistical forecasting practices based on an ex-ante analysis of their data (and their respective features).

This paper is structured as follows: after the literature review (Section 2), the simulation design for fast-moving and intermittent demand data is discussed in Section 3. In Section 4 the accuracy results are presented. Section 5 discusses the findings and Section 6 presents the practical implications for decision makers. Finally, Section 7 concludes and suggests possible avenues for future research.

Extrapolation models are used very often when facing large amounts of data. Among them, exponential smoothing forecasting approaches were developed in the early 1950s and have become very popular amongst practitioners. Their main advantages are simplicity of implementation, relatively low computational intensiveness and no requirement for lengthy series, whilst being appropriate for short-term forecast horizons over a large number of items. Single Exponential Smoothing (SES – Brown, 1956) uses only one smoothing parameter and is forecasting quite accurately stationary data. Holt’s two parameters approach (1957) expands the Single method with a smoothing parameter for the slope, making the method more appropriate for trended data. The Holt-Winters approach (Winters, 1960) is an expansion upon the Holt trended model, which assumes an additive or multiplicative seasonality in the data. Gardner and McKenzie (1985) added a dampening factor (0<
                     φ
                     <1) applied directly on the trend component, resulting in a very successful approach that is often considered the benchmark in many empirical evaluations. Assimakopoulos and Nikolopoulos (2000) proposed the Theta model – a prima facie variation of SES with drift, with the full theoretical underpinnings presented by Thomakos and Nikolopoulos (2014), a method that topped the M3-Competition, the largest empirical forecasting competition to date (Makridakis & Hibon, 2000, Appendix B).

On the other hand, the more complex but quite popular Box–Jenkins methodology (Box & Jenkins, 1970) uses an iterative three-step approach (model identification, parameter estimation and model checking) in order to find the best-fit ARIMA model. To date ARIMA models are still considered the dominant benchmark in empirical forecasting evaluations, and find great popularity among OR researchers in applications spanning from hospitality and production to healthcare and climate forecasting (for e.g. see Broyles, Cochran, & Montgomery, 2010; Cang & Yu, 2014; Cao, Ewing, & Thompson, 2012).

One result that stands for fast-moving data is that combining improves predictive accuracy (Clemen, 1989; Makridakis & Winkler, 1983; Surowiecki, 2005). In addition to this, combining reduces the variance of forecasting errors and therefore the uncertainty in predictions, rendering the selection of combinations less risky than individual methods (Hibon & Evgeniou, 2005). Many recent studies have verified that the combination of methods leads to more accurate forecasts, whilst, at the same time proposing more sophisticated weightings such as the trimmed and Winsorized means (Jose & Winkler, 2008), and the use of information criteria (Kolassa, 2011; Taylor, 2008).

For count data/intermittent data, Croston (1972) proposed decomposing the data into two subseries (demands and intervals) with Syntetos and Boylan (2005) proposing a bias-correction to the Croston’s method (Syntetos and Boylan Approximation or SBA). More recently, Teunter, Syntetos, and Babai (2011) suggested a decomposition method that relies on the separate extrapolation of the non-zero demands and the probability to have a demand. This method is very useful in cases of obsolescence. Lastly, simpler approaches, such as Naïve, Moving Averages and SES, have also been quite popular for such data especially among practitioners.

An interesting spin-off from the later intermittent demand literature came from Nikolopoulos, Syntetos, Boylan, Petropoulos, and Assimakopoulos (2011) with the ADIDA non-overlapping temporal aggregation forecasting framework, that although designed and successfully evaluated empirically on count data (Babai, Ali, & Nikolopoulos, 2012), the implications pretty fast span out for fast-moving data as well (Kourentzes, Petropoulos, & Trapero, 2014; Spithourakis, Petropoulos, Babai, Nikolopoulos, & Assimakopoulos, 2011). The proposed framework soon was perceived as a forecasting method “self-improving” mechanism that by changing the data series features through frequency transformation, can help extrapolation methods achieve better accuracy performance. The first theoretical results for the ADIDA framework appeared recently in the literature (Rostami-Tabar, Babai, Syntetos, & Ducq, 2013; Spithourakis, Petropoulos, Nikolopoulos, & Assimakopoulos, in press).

Given the plethora of the aforementioned methods, it is now even more unclear: when should each method be used? Many researchers compared the performance of aggregate and individual selection strategies (Fildes, 1989; Shah, 1997; Fildes & Petropoulos, in press). While selecting a single method for an entire data set would make sense for homogeneous data, model selection should be done individually (per series) when we deal with heterogeneous data, as to capture the different features met in each series.


                        Pegels (1969) presented the first graphical classification for exponential smoothing methods, separating trend from cycle patterns, and also as additive from multiplicative forms. In a simulation study, Adam (1973) evaluated several forecasting models across five different demand patterns, including constant, linear trend, seasonal and step function. His findings indicate that no single model is consistently better than the others, and their performance depends primarily on the demand pattern, the forecasting horizon and the randomness, and secondarily on the selected accuracy metric. Gardner and McKenzie (1988) provided a procedure for model identification in the case of large forecasting applications. Their selected course of action involved the calculation of variances at various levels of differences in data, and using those for classifying the underlying pattern of the time series (constant or trended, seasonal or not seasonal, and so on).

A first attempt for a rule-based selection procedure of the best model derived from Collopy and Armstrong (1992). They proposed a framework that combines forecasting expertise with domain knowledge in order to produce forecasts based on the characteristics of the data. Their procedure consisted of 99 rules and four extrapolation techniques, while 18 time series features were used. A simplified domain knowledge-free version of this rule-based procedure was presented by Adya, Armstrong, Collopy, and Kennedy (2000), using just 64 rules, three forecasting methods and six time series features. In order to render the procedure fully automated, Adya, Collopy, Armstrong, and Kennedy (2001) presented an automatic identification of time series features for rule-based forecasting, which reduces significantly the forecasting cost of large data sets without serious losses in accuracy.


                        Shah (1997) proposed a seven-step model selection procedure for univariate series forecasting, using an individual selection rule based on 26 features. In a later study, Meade (2000) used 25 simple statistics as explanatory variables in order to predict the forecasting accuracy performance of nine extrapolation methods and, thus, select the most promising one. His results were evaluated on two empirical data sets.

A different path for model selection focuses on the application of families of methods (for example, Exponential Smoothing or ARIMA) and subsequently the selection of the single method that has the best trade-off between the goodness-of-fit and the complexity of modeling (number of parameters). To this end, the use of information criteria (Hyndman, Koehler, Snyder, & Grose, 2002) has been very popular. At the same time, there is little to distinguish from the application of different information criteria (Billah, King, Snyder, & Koehler, 2006). A disadvantage of model selection with information criteria is the inability to compare across different families of methods. An alternative to the use of information criteria is the performance evaluation of methods in a hold-out sample where forecasts are calculated for single or multiple origins and lead times (Fildes & Petropoulos, in press). Depending on the specific experimental design, this strategy is known as “validation” or “cross-validation”.

For count data, Bacchetti and Saccani (2012) provided a comprehensive literature review of the classification methods, whilst a demand-based classification for intermittent demand was proposed by Syntetos, Boylan, and Croston (2005), later revised by Kostenko and Hyndman (2006).

Lastly, it is worth emphasizing that various similar attempts to identify suitable methods for forecasting cross-sectional data had been presented over the years; as for example in Nikolopoulos, Goodwin, Patelis, and Assimakopoulos (2007) in a marketing application and Bozos and Nikolopoulos (2011) in a strategic financial decision-making application, where a series of economics, econometrics, time series, artificial intelligence and computational intensive approaches as well as human judgment were compared within the context of the respective investigations.

Forecasting competitions have evaluated the performance of time series methods (Makridakis & Hibon, 2000; Makridakis et al., 1982, 1993) in order to better understand their relative accuracy and improve their usefulness. Since then, a large number of studies have compared the accuracy of various methods in different forecasting settings. For example, Franses and van Dijk (2005) concluded that simpler (linear) models for seasonality perform better for short horizons. At the same time, more complex (non-linear) models should be preferred for longer forecasting horizons.

Furthermore, many researchers focused on the performance of exponential smoothing methods. Gardner (2006) compared the performance of damped trend to the class of state-space models. Using the data sets from the M and M3 forecasting competitions, he concluded that the damped approach is more robust and accurate than the individual selection of models through information criteria in almost every case, except for the short horizons of the M3-Competition monthly data. Gardner and Diaz-Saiz (2008) explored the performance of exponential smoothing methods using telecommunications data. An analysis of the results suggested that SES with drift, a simplification of the Theta method (Assimakopoulos & Nikolopoulos, 2000) as proposed by Hyndman and Billah (2003), provided the most accurate forecasts compared to any other smoothing method for every horizon.


                        Crone, Hibon, and Nikolopoulos (2011) conducted a forecasting competition for Computationally Intensive approaches, most notably Artificial Neural Networks (ANN). One of their main findings was that only one ANN method outperformed the damped trend. Lastly, Athanasopoulos, Hyndman, Song, and Wu (2011) found that tourism data are best extrapolated using time series approaches rather than causal models, whilst the forecasting performance of Naive for annual data was “hard to beat”. A similar insight was also offered by another study using tourism data (Gil-Alana, Cunado, & De Garcia, 2008), in which a simple seasonal model outperformed more complex ones for short horizons.

Having revisited all this literature, we believe there is still scope for studies investigating what makes some methods more (or less) accurate, and under what conditions; having that said, the main Research Question (RQ) of this study is as follows:
                           RQ: How various factors affect, if at all, the forecasting accuracy of time series extrapolation methods?
                           
                        
                     

To address this question, we design two extensive simulations for fast-moving and intermittent data respectively, as well as empirical evaluations in real data, involving 14 univariate forecasting methods and five combinations of them. Consequently, the extent of the influence of each factor is calculated through regression analysis.

Before we start elaborating on the empirical investigations, we need to formally introduce the data features that we will simulate in this study.

To that end we were inspired originally by the work of Adam (1973) that he identified the importance of trend, seasonality, randomness and the forecasting horizon. We were further influenced by the work of Collopy and Armstrong (1992) where they proposed their Rule-Based Forecasting (RBF) framework, the very essence of which is dominated by the identification of data features. Among the selected time series features, trend, cycle, seasonality and length of the series were of key importance. Finally, the work of Nikolopoulos and Assimakopoulos (2003) where many of these data features were used as key elements in the object-oriented architecture of a prototype Forecasting Support System – TIFIS, gave us more firm evidence on the importance of the aforementioned data features.

The level of temporal aggregation (frequency) and the level of cross-sectional aggregation (level in hierarchy) of the data were not considered in this study. We focus on the forecasting performance of a specific level of temporal/cross-sectional aggregation. Most of the patterns described above may be observed at any level of aggregation and, thus, this latter feature was not simulated. The only exception is the seasonality. The effects of temporal and cross-sectional aggregation on seasonality have been addressed elsewhere. Nikolopoulos et al. (2011) and the ADIDA framework indicate that forecasters may, via temporal aggregation, switch to other different frequencies (than the ones the data are observed), while Kourentzes et al. (2014) present a framework to efficiently combine forecasts derived from multiple frequencies. Lastly, Athanasopoulos, Ahmed, and Hyndman (2009) explore different approaches to hierarchical forecasting, proposing an “optimal” approach, which provides optimally reconciled forecasts at every level.

So the RQ can now be narrowed for fast-moving data as follows:
                           RQ1: How six factors (seasonality, trend, cycle, randomness, the number of observations and the forecasting horizon) affect, if at all, the forecasting accuracy of time series extrapolation methods on fast-moving data?
                           
                        
                     

To generate simulated series for testing the above-mentioned research question, each of the first five factors (seasonality, trend, cycle, randomness and the number of observations) was varied around six levels (see Table 1
                        ) while 10,000 series were randomly generated at each level (ceteris paribus). Since there are six levels and five factors to vary, there is a total of 7776 (65) combinations, resulting in 77,760,000 generated time series covering every possible combination. For each of the generated time series, 18 forecasts were produced. The values and variation of each of the six levels was selected based on the respective ranges of the 1428 real monthly series of the M3-Competition.

The generation procedure assumes a deterministic, multiplicative model where each component is applied individually as suggested by Miller and Williams (2003), but in addition we introduce a cycle component as well. Thus:
                           
                              (1)
                              
                                 
                                    
                                       X
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 
                                    
                                       S
                                    
                                    
                                       t
                                    
                                 
                                 ·
                                 
                                    
                                       T
                                    
                                    
                                       t
                                    
                                 
                                 ·
                                 
                                    
                                       C
                                    
                                    
                                       t
                                    
                                 
                                 ·
                                 
                                    
                                       R
                                    
                                    
                                       t
                                    
                                 
                              
                           
                        where Xt
                         is the series, St
                         is the seasonal component, Tt
                         is the trend component, Ct
                         is the cycle component and Rt
                         is the random component.

The procedure of simulating fast-moving series is described below. First, a vector of length equal to a selected number of observations plus 18 (out-of-sample) is defined, with all values being set equal to a randomly selected initial level (L). This vector is multiplied by the respective seasonal indices, which, given a seasonality level (SL), are defined as:
                           
                              (2)
                              
                                 
                                    
                                       S
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 
                                    
                                       SI
                                    
                                    
                                       k
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                SL
                                             
                                             
                                                MAP
                                                (
                                                FD
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where SI is a single dimensional array of 12 values, containing the mean seasonality curve of the monthly seasonal time series of M3-Competition and MAP(FD) represents a normalization factor calculated as the mean absolute percentage of first differences of the SI values. Then, the trend component is applied to each observation. This component is equal to:
                           
                              (3)
                              
                                 
                                    
                                       T
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             (
                                             1
                                             +
                                             TL
                                             )
                                          
                                          
                                             t
                                          
                                       
                                    
                                    
                                       p
                                    
                                 
                              
                           
                        where TL is the selected trend level and p represents the number of periods within a year. The cyclical component, Ct
                        , is introduced as:
                           
                              (4)
                              
                                 
                                    
                                       C
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 
                                    
                                       C
                                    
                                    
                                       t
                                       -
                                       1
                                    
                                 
                                 +
                                 
                                    
                                       N
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 CL
                                 ,
                                 CV
                                 =
                                 1
                                 /
                                 3
                                 )
                              
                           
                        where C
                        0
                        =0 and Nt
                         is a normally distributed random variable with mean value CL (the selected level of the cycle component) and a standard deviation so that CV
                        =1/3. A new value of Nt
                         is generated for each data point. Lastly, for the randomness component, a normally distributed and randomly selected variable with mean value RL (level of randomness) and CV
                        =1/3 is generated, or:
                           
                              (5)
                              
                                 
                                    
                                       R
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 
                                    
                                       N
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 RL
                                 ,
                                 CV
                                 =
                                 1
                                 /
                                 3
                                 )
                              
                           
                        
                     

So, Xt
                         is calculated as follows:
                           
                              (6)
                              
                                 
                                    
                                       X
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 L
                                 ·
                                 
                                    
                                       S
                                    
                                    
                                       t
                                    
                                 
                                 ·
                                 
                                    
                                       T
                                    
                                    
                                       t
                                    
                                 
                                 ·
                                 
                                    
                                       C
                                    
                                    
                                       t
                                    
                                 
                                 ·
                                 
                                    
                                       R
                                    
                                    
                                       t
                                    
                                 
                              
                           
                        
                     

The forecasting methods used in the study are (for more details on these methods, see Appendix A):


                        Naïve 1, Naïve 2, four exponential smoothing methods (Single, Holt, Damped, Holt-Winters), Theta (Assimakopoulos & Nikolopoulos, 2000), Linear Trend and two commercial packages (Autobox and Forecast Pro).

Moreover, the following five combinations of the above methods were constructed:
                           
                              •
                              Single-Damped (SD).

Single-Holt-Damped (SHD).

Single-Theta (ST).

Single-Damped-Theta (SDT).

Single-Holt-Damped-Theta (SHDT).

With the six methods (Naïve 2, Single, Holt, Damped, Linear Trend and Theta) not suitable to handle seasonality, the forecasts were produced following a three-step procedure: firstly the data was deseasonalized via a Classical Decomposition approach. Secondly, 18 forecasts were computed using the deseasonalized data. Finally, these 18 forecasts were reseasonalized using the same seasonal indices as the Classical Decomposition. The remaining methods (Naïve 1 and Holt-Winters) and the methods implemented in the commercial packages (Autobox and Forecast Pro) were applied directly to the original data. The extrapolation of each series was always carried out without using the last 18 out-of-sample observations which were being kept for evaluating the forecasting accuracy of each method. For all forecasting methods, except the two commercial packages, 10,000 series were generated for each of the 7776 permutations; in contrast, for the two commercial packages only 300 series were generated for each of the 7776 permutations (i.e. 2,332,800 series in total) because of the time required in order to run them.

The forecasting accuracy was measured by comparing the 18 hold out data points with the 18 point forecasts. Three accuracy metrics were calculated:
                           
                              •
                              The Symmetric Mean Absolute Percentage Error (sMAPE), the main metric of the M3-Competition (Makridakis & Hibon, 2000).

The Percentage Better, where the accuracy of each method was benchmarked against Naïve 2.

The Mean Absolute Scaled Error (MASE), introduced by Hyndman and Koehler (2006).

The overall evaluation involves the calculation and comparison of over 55billion forecast errors.

We also considered the case of intermittent demand data, where two main factors were considered, namely average inter-demand interval (IDI) and squared coefficient of variation (CV
                        2) of the non-zero demands as it is dictated by the work of Syntetos et al. (2005). On top of that, we examined also the effect of the length of the series (number of available observations) that is quite ignored in the respective literature as usually in practice these series are short. Lastly, in line with the investigation on fast-moving data, we study the effects of forecasting horizon. So the basic research question may now be narrowed to:
                           RQ2: How four factors (inter-demand interval, coefficient of variation, the number of observations and the forecasting horizon) affect, if at all, the forecasting accuracy of time series extrapolation methods on intermittent data?
                           
                        
                     

To generate simulated series for testing the above-mentioned research question, each of the first three factors (IDI, CV
                        2 and the number of observations) was varied around six levels (see Table 2
                        ) while 10,000 series were generated at each level (ceteris paribus). Given the number of different combinations (216=63) considered, we examine in total 2,160,000 simulated time series. For each series, we produce forecasts for the next 12 periods.

The procedure followed to generate the intermittent demand data is given below. For a selected level of number of observations (l) and level of IDI, we generate a vector which specifies the occurrence of non-zero demands as a Bernoulli distribution (Croston, 1972; Syntetos & Boylan, 2001), where p
                        =1/IDI. The output of this step is a binary vector I of length l. For the demand sizes we use randomly generated numbers following a negative binomial distribution (see Syntetos, Babai, Lengu, & Altay, 2011) and increase them by one (1) as not to generate zero demands. The number of successful trials (n) and the success probability (p) can be easily derived as:
                           
                              (7)
                              
                                 n
                                 =
                                 
                                    
                                       μ
                                       p
                                    
                                    
                                       (
                                       1
                                       -
                                       p
                                       )
                                    
                                 
                                 
                                 p
                                 =
                                 
                                    
                                       μ
                                    
                                    
                                       
                                          
                                             c
                                          
                                          
                                             v
                                          
                                          
                                             2
                                          
                                       
                                       
                                          
                                             (
                                             μ
                                             +
                                             1
                                             )
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        where cv
                           >
                        0 is the required coefficient of variation (square root of CV
                        2) and μ is the mean, selected randomly in [10,50]. Letting the l generated values of the negative binomial distribution be the vector d, the required non-zero demand sizes can be derived as D
                        =
                        d
                        +1. Finally Xt
                         is calculated as Xt
                           =It
                         · Dt
                         for t
                        =1,…,
                        l. If cv
                           
                        =0, then d
                        =
                        round(μ) for every t, where round denotes the rounding function.

The forecasting methods used in this simulation are (for more details on these methods, see Appendix A):


                        Naïve, Simple Moving Averages (SMA) of length 4, 8 and 12, SES with a prefixed smoothing parameter equal to 0.1 (SES(0.1)), SES with optimized smoothing parameter (SES(auto)), Croston’s method, Syntetos and Boylan Approximation (SBA) and Teunter, Syntetos and Babai method (TSB).

For the methods designed specifically for intermittent demand (Croston, SBA, TSB), small values for the smoothing parameters were applied, as suggested by the literature (Syntetos & Boylan, 2005). In more detail, the smoothing parameter for estimating both demands and intervals in Croston’s method and SBA and demands in TSB method is set equal to 0.1. Moreover, the smoothing parameter for estimating the probability of the occurrence of a non-zero demand in TSB method is set equal to 0.02.

The forecasting accuracy was measured by comparing the 12 hold out data points with the 12 point forecasts of each method. Due to the presence of zero demands, the use of percentage errors is not appropriate. We, therefore, use a scaled version of the Mean Absolute Error (sMAE), where the scaling is performed through dividing with the in-sample mean demand. The point forecast error (scaled absolute errors or sAE) for the h-step-ahead forecast can be calculated as follows:
                           
                              (8)
                              
                                 
                                    
                                       sAE
                                    
                                    
                                       h
                                    
                                 
                                 =
                                 
                                    
                                       |
                                       
                                          
                                             X
                                          
                                          
                                             N
                                             +
                                             h
                                          
                                       
                                       -
                                       
                                          
                                             F
                                          
                                          
                                             h
                                          
                                       
                                       |
                                    
                                    
                                       
                                          
                                             1
                                          
                                          
                                             N
                                          
                                       
                                       
                                          
                                             ∑
                                          
                                          
                                             t
                                             =
                                             1
                                          
                                          
                                             N
                                          
                                       
                                       
                                          
                                             X
                                          
                                          
                                             t
                                          
                                       
                                    
                                 
                              
                           
                        where X is the vector of observations generated previously, F is the vector of point forecasts and N is the length of the in-sample. The sMAE is simply derived by averaging across horizons and series.

In this section we present results from simulated data for fast-moving and intermittent series, as well as for real fast-moving data from the M3-Competition.

The accuracy results for the entire dataset are summarized in Table 3
                        .

We note:
                           
                              •
                              The very good performance of the five combinations, a finding consistent with the conclusions from the M-Competitions.

The strong similarities in the performance of methods (in terms of sMAPEs) with comparison to the ones in the M3-Competition, as depicted by the close values of the respective ratios.

There are three single methods that consistently perform better: Single Exponential Smoothing,
                                    2
                                    Also referred in the literate as ‘Simple’ Exponential Smoothing, or just abbreviated as SES and is equivalent to an ARIMA(0,1,1) without constant model.
                                 
                                 
                                    2
                                  Damped Exponential Smoothing and the Theta method.

The quite similar average results for MASE and Percentage Better as presented in the last two columns of Table 3.


                        Table 4
                         shows the results of the regression analysis. It lists the standardized beta coefficients for each factor, the corresponding t-tests as well as the overall R
                        2 and standard errors. Table 4 suggests that for all variables and methods the regression coefficients are statistically significant (at 0.01) with Naïve 1 being the only exception. Moreover the R
                        2 values range from 0.850 to 0.932, indicating a very good fits as expected from such a rich dataset.

A positive beta coefficient means less accuracy whilst a negative one means improvement. Furthermore, the bigger the absolute value of the coefficient the greater the deterioration or improvement in forecasting performance. The signs of most regression coefficients are positive (seasonality, cycle, randomness and the forecasting horizon) and this means that when these following factors increase, the forecasting accuracy for all methods and combinations decreases.

Notable exceptions are the negative betas for: (a) the number of observations factor for all methods with the exception Naïve 1, thus when the length of the series increases the accuracy increases as well even if it is a marginal improvement and (b) the trend factor for Holt, Holt-Winters and Linear Trend methods as these methods capture the trend in the data (see Table 4, column 4), and therefore marginally improve accuracy.


                        Randomness is the variable that most affects forecasting accuracy. Moreover, the values of these coefficients for the majority of methods are similar (ranging from 0.823 to 0.878); this means that the accuracy of all methods rapidly decreases as the randomness in the data increases, but also that practically all methods are equally capable of dealing with increasing levels randomness. The variable with the least influence is trend, in particular for the Holt, Holt-Winters and Linear Trend methods as the values of the corresponding regression coefficients are small. Cycle and the forecasting horizon variables appear to have less influence compared to randomness in terms of the extent to which they affect forecasting accuracy. Furthermore, the seasonal fluctuations in the data are captured in a similar way by practically all methods, as their regression coefficients are small, ranging from 0.026 to 0.063 (the exceptions are Naïve 1 and one of the commercial packages).

One way to apply the findings of the previous section in real data is as follows: when decomposing a time series we can estimate the levels of seasonality, cycle, trend, and randomness while we also do know exactly the number of available observations of the series and we decide on the forecasting horizon. This information allows us to estimate the percentage error for each forecasting horizon by utilizing the corresponding regression equation for each single method shown in Table 4. Consequently we can identify the method with the smallest error as well rank all methods according to their respective errors. We can, therefore, select for each series and forecasting horizon the method with the minimum expected error.

Often, the difference between the best method identified and the second, third and fourth are small and to account for this we consider the simple combination of these methods for which a specific criterion – hereafter called Threshold Ratio – is more than a pre-defined value. The Threshold Ratio may be calculated by dividing the smallest expected sAPE of the ‘optimal’ method for a given forecasting horizon with the expected sAPE of the second, third and fourth method respectively. The maximum number of methods to be combined could be limited and in this example we use the ad hoc value of 6, as combinations of large pools of methods is not regarded as beneficial (Fildes & Petropoulos, in press).

We apply this method selection and combination approach to the monthly series of the M3-Competition (1428 time series, Makridakis & Hibon, 2000). The results are presented in Table 5
                         illustrating that the proposed method selection protocol, based on the regression estimation in Table 4, results in improved forecasting performance of 9.9% and 5.4% compared to SES and Damped respectively. For low values of the threshold so as to allow more methods to be included in the simple combination, the performance gets even better than that of the Theta model (Fig. 1
                        ).

The sMAE results for the entire dataset are shown in Table 6
                        .

There are a few interesting findings in Table 6:
                           
                              •
                              TSB performs slightly better than the other methods for all horizons.

The improvement in forecasting accuracy as the length of the Simple Moving Average increases.


                                 Forecasting horizon does not affect accuracy.

In Table 7
                         we present the results of the multiple regression analysis where the standardized beta coefficients and their corresponding values of t-test are listed for each factor. Moreover, the overall goodness of fit (R
                        2) and the standard error estimates are provided. The reported R
                        2 values indicate very good fit for all equations as in the case of fast-moving data.

Forecasting performance for all methods is heavily affected by the increase of intermittence of the data (IDI) as well as the coefficient of variation. As the values of these two factor increase, the respective accuracy of all methods decreases. Even if the differences are very small, Croston’s method and SBA are the two methods most affected by the increase of the average IDI, despite the fact that both methods are specifically designed for intermittent demand data. On the other hand, Naive is the method which is least affected by the IDI, as increase of intermittence is likely to result in spot-on forecasts (for zero demand). The exact opposite is true for the coefficient of variation. SBA and Croston are the two methods with the lowest effect of on their forecasting accuracy. On the contrary, Naïve, SMA(4) and TSB are the three methods affected the most by variability in demand, according to the standardized beta coefficients.

The number of available observations has a very small positive impact on the forecasting accuracy of SES(auto), Croston and SBA. Finally, forecasting horizon has practically no impact on the predictive power of the alternative methods (non-statistically significant beta coefficients).

@&#DISCUSSION@&#

For regular/fast-moving data, three relatively simple methods, Single and Damped exponential smoothing as well as the Theta method, demonstrated the best performance for all the three accuracy measures used. In between them, Theta performed better for longer forecasting horizons, the reason being its ability to more accurately predict the trend in the data, whilst Single and Damped exponential smoothing were more accurate for shorter forecasting horizons. Moreover, the five combinations of forecasting methods performed well, often exceeding the accuracy of the individual methods being combined, whilst also reducing the variance of forecasting errors. The above three findings are consistent and corroborate to the findings of the literature on empirical forecasting competitions.

The regression coefficients as presented in Table 4 allow us to identify which factors affect methods’ accuracy as well as measure the extent of such influences. Cycle and randomness have the biggest effect on the forecasting accuracy of all methods. It is interesting that Naïve 1 exhibits the best performance in respect to these two factors (it has the smallest regression coefficients of all methods), which could be attributed to the random walk nature of many of the simulated data series. Seasonality is well captured by all methods with the expected exception of Naïve 1. The similarity of the regression coefficients for this specific factor is due to the fact that the seasonal indexes, with the exceptions of Naive 1, Holt-Winters and the commercial software packages, were estimated using the classical decomposition approach. As expected, trend is best captured by Holt, Holt-Winters and Linear Regression, whilst Single, Naïve and Naïve 2 are unable to do so.

The regression coefficients of the number of observations factor, although negative, have a minimal effect on improving forecasting accuracy. The forecasting horizon factor coefficient indicates that accuracy decreases for longer forecasting horizons, providing a considerable advantage to Theta whose beta coefficient is smaller than that of other well performing methods.

The poor performance, in contrast to other empirical studies in the literature, of the two commercial packages may be the result of three factors. Firstly, Autobox and Forecast Pro were tested only in 300 time series for each of the 7776 cases (i.e. for a total of 2,332,800 time series versus 77,760,000 for all the other methods) due to time and computational constraints. Secondly, Tom Reilly (personal communications) pointed out that the poor performance of Autobox could be explained by the deterministic character of the models used to generate the data however the authors believe that this could only be partially true as the simulated data were derived from ranges and values from the real M3-Competition monthly data, where there as well Autobox was not one of the top-performing methods. Lastly, the commercial packages were used in fully automatic mode, with no supervision at all in this study and thus their parameters were not manually adjusted for the features of the generated data. We believe that this latter argument provides the best explanation for the software packages not being en par with the benchmarks.

We believe that through the regression analysis we can isolate the influence of each factor on forecasting accuracy, in a ceteris paribus fashion. This means that we can determine the most/least important factors influencing forecasting accuracy. Seasonality, for instance, seems to be able to be captured through the classical decomposition process. Trend, on the other hand, seems to be a much bigger challenge, as random and cyclical changes make the identification of a robust trend very difficult and, for this reason, Single and even Naïve 2 perform so well. Furthermore, additional historical information in the form of more observations and lengthier series improves accuracy but to a small extent, whilst cyclical fluctuations are found to play the most important role in forecasting accuracy. Furthermore, randomness also plays a significant role, but this is to be expected to a large extent and hard to deal with. Finally, combining forecasts appears to improve forecasting accuracy in the majority of cases whilst also reducing the variance of forecasting errors, highlighting that it is almost impossible to identify one single optimal method based on the data features.

For intermittent data there were no big surprises given the lot of attention in the recent years in the respective literature and the plethora of empirical investigations. The negative effects on forecasting accuracy from marginal increases of IDI or CV
                     2 were more or less expected. However, the good performance of TSB is an interesting result, given the extended simulation exercise. Also, a few more remarkable findings were surfaced: first and foremost further research should shed light why the length of the series has so small impact and even more why the forecasting horizon is not influencing at all the forecasting accuracy.

One of the most important challenges of any academic article is how the results of the proposed research can be translated into practical recommendations for decision makers. To that end, we are in the comfortable position to claim to be informing real-life professionals on the appropriate uses of ‘Horses for Courses’ in demand forecasting. While the fundamental question still remains: “what is the best method for my data?”, we believe now, and through the illustration of the use of the protocol in real data with improved forecasting performance (see section 4.2), that we are in a position to partially answer this question.

In essence, through this study we are providing guidance to practitioners on which are the most appropriate forecasting methods, given the specific data features they are facing. In order to accommodate this, we provide a graphical representation of the main results of our study (Fig. 2
                     ). The numerical standardized beta coefficients of Table 4 have been converted into an 11-scale (−5 to +5), representing the effect of each factor (seasonality, trend, cycle, randomness, number of observations and forecasting horizon) upon forecasting accuracy for the most popular forecasting methods among practitioners.

So, how this Fig. 2 should be used from practitioners? First they need to decompose their time series so as to find out its seasonality, cycle, trend and randomness (whilst also number of observations and the forecasting horizon are known in advance). This information will allow them through Fig. 2 to select the most appropriate methods amongst the 10 presented in this study, the methods that are expected to end up with the lowest out-of-sample (sMAPE) and thus the best forecasting accuracy.

For example, following the Method Selection Protocol for regular/fast-moving data in Fig. 2, if seasonality and trend are the only features present in the data, then Holt, Holt-Winters and Linear Trend should be selected, with Linear Trend being the best approach. When trended data with many observations are to be extrapolated, Holt seems the most obvious choice. In the case of a strong presence of cycle, randomness or both, Naïve is by far the best option. If cycle is present as well as seasonality, then Naïve 2 is the method to select. On the other hand, if cycle is the dominant feature, while we are dealing with time series with many data points, then Linear Trend should be avoided. In the case of large forecasting horizons, Holt-Winters, Holt and Forecast Pro are not considered as good options. On the contrary, Naïve and Theta should be selected. Concluding, this specific protocol could be used from practitioners as a broad ‘rule of thumb’ for method selection given their specific data.

When it seems from the protocol that many methods could be used for a specific data series then a simple combination of those methods could well be used instead. The benefit from the aforementioned combination is twofold as the variance of the forecasting errors is also reduced.

The respective Method Selection Protocol for Intermittent data is illustrated in Fig. 3
                     , which is practically the graphical equivalent of Table 7.

By and large for high values of IDI forecasters are prompted to use either TSB or SMA (Naïve is not suggested, as this method will result in under-stocks), while for high values of CV
                     2 SES, Croston’s method or SBA should be used. In the unlikely event of having many observations, then yet again SES, Croston’s method or SBA should be used.

It needs to be emphasized that the Method Selection Protocol can be applied to any level of temporal (frequency) and hierarchical/family cross-sectional aggregation. Forecasters might select a specific frequency for either reasons of plain convenience or scientifically driven from the need to improve forecasting accuracy (see the ADIDA framework, Nikolopoulos et al., 2011), and in a similar fashion a specific level of cross-sectional aggregation. As a result, a specific time series will be constructed; this time series will have specific features and, based on these features, the Method Selection Protocol could well be applied as illustrated in Fig. 4
                     .

One of the biggest challenges that forecasters and practitioners are constantly facing is the selection of the most appropriate method for a specific data set or even for a single time series. The quest for forecasting approaches tailored to specific types of data, led researchers to the development of method selection protocols, usually based on a large number of rules necessitating advanced data analysis.

In this study we assume that seven time series features (seasonality, trend, cycle, randomness, number of observations/length, IDI, CV
                     2) and one strategic decision (forecasting horizon) are the dominant determinants of forecasting accuracy. Through two extensive simulations with almost 80million regular/fast-moving and intermittent time series, we study the accuracy of the most popular forecasting methods and measure the factors that affect their respective performance. Fourteen forecasting methods plus five combinations of them are employed in order to predict 12–18 periods ahead and respectively measure the out-of-sample forecasting accuracy using four fit-for-purpose metrics (sMAPE, Percentage Better, MASE and sMAE). Consequently, regressions analysis was performed so as to determine the sign and the amplitude of the effect of each of the factors on the performance of the evaluated methods. Our main findings conclude that in terms of the achieved forecasting accuracy:

For regular/fast-moving data, where demand occurs each and every period:
                        
                           •
                           
                              Cycle and, most predominantly, randomness have the biggest (negative) effect on forecasting accuracy.

The longer the forecasting horizon, the more the accuracy decreases.

For intermittent data:
                        
                           •
                           Increasing IDI has the biggest (negative) effect in accuracy.

Increasing CV
                              2 has also a negative effect.

For all types of data
                        
                           •
                           Increasing the length of the series has a small positive effect in accuracy.

One of the main practical contributions of the current work is that it translates the statistical findings into a graphical Method Selection Protocol that enables decision makers and practitioners to select the most appropriate forecasting method for their own data.

As far as the future of similar investigations is concerned, it is important to expand the pool of both methods and factors/data features being considered. Paths for future research could also include:
                        
                           •
                           Evaluating density forecasts rather than point forecasts in similar simulation setups.

Introducing temporary and permanent structural changes in the level and trend of the data and determine their impact on accuracy.

Performing sliding simulations in the forecast evaluations (Tashman, 2000).

Epilogue: as the quest for ‘Horses for Courses’ in demand forecasting is a long standing issue in the operational research literature, we hope that we have shed some light towards that direction…

@&#ACKNOWLEDGEMENTS@&#

We would like the thank: the handling editor Professor Immanuel Bomze for his insightful comments, three anonymous referees for their constructive comments, Professor Aris Syntetos and Professor Dimitrios Thomakos for their comments on earlier versions of this paper, Tom Reilly and Eric Stellwagen from Autobox and Forecast Pro respectively for providing us with evaluation versions of their commercial forecasting software.


                     Naïve. This is the simplest forecasting approach. Point forecasts for all lead times are equal to the latest available actual observation (random walk).


                     Naïve 2. The point forecasts are derived as in the Naïve method, using the seasonally adjusted data. A deterministic multiplicative seasonality is assumed. Finally, point forecasts are reseasonalized using the appropriate seasonal indices.


                     Exponential Smoothing. The exponential smoothing methods are based on averaging (smoothing) past values of a time series in a decreasing (exponential) manner. Single Exponential Smoothing (Single) assumes no trend or seasonal patterns (Brown, 1956). Forecasts for Single can be produced via the following formula:
                        
                           (A.1)
                           
                              
                                 
                                    F
                                 
                                 
                                    t
                                    +
                                    1
                                 
                              
                              =
                              
                                 
                                    aX
                                 
                                 
                                    t
                                 
                              
                              +
                              (
                              1
                              -
                              a
                              )
                              
                                 
                                    F
                                 
                                 
                                    t
                                 
                              
                           
                        
                     where α refer to the exponential smoothing parameter for the level.

Holt Exponential Smoothing (Holt) expands Single adding one additional parameter for smoothing the short-term trend (Holt, 1957). The point forecasts for Holt can be calculated via:
                        
                           (A.2)
                           
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                              
                              =
                              
                                 
                                    aX
                                 
                                 
                                    t
                                 
                              
                              +
                              (
                              1
                              -
                              a
                              )
                              (
                              
                                 
                                    L
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              +
                              
                                 
                                    T
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              )
                           
                        
                     
                     
                        
                           (A.3)
                           
                              
                                 
                                    T
                                 
                                 
                                    t
                                 
                              
                              =
                              β
                              (
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                              
                              -
                              
                                 
                                    L
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              )
                              +
                              (
                              1
                              -
                              β
                              )
                              
                                 
                                    T
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                           
                        
                     
                     
                        
                           (A.4)
                           
                              
                                 
                                    F
                                 
                                 
                                    t
                                    +
                                    m
                                 
                              
                              =
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                              
                              +
                              
                                 
                                    mT
                                 
                                 
                                    t
                                 
                              
                           
                        
                     where β is the smoothing parameter for the trend, Lt
                      refers to the forecast of the level for period t and Tt
                      is the forecast for the trend for period t.

Damped Exponential Smoothing (Damped) introduces a dampening factor (φ) that is multiplied on the trend component of Holt’s method in order to give more control over the long-term extrapolation of the trend (Gardner & McKenzie, 1985). Forecasts for Damped can be calculated as:
                        
                           (A.5)
                           
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                              
                              =
                              
                                 
                                    aX
                                 
                                 
                                    t
                                 
                              
                              +
                              (
                              1
                              -
                              a
                              )
                              (
                              
                                 
                                    L
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              +
                              ϕ
                              
                                 
                                    T
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              )
                           
                        
                     
                     
                        
                           (A.6)
                           
                              
                                 
                                    T
                                 
                                 
                                    t
                                 
                              
                              =
                              β
                              (
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                              
                              -
                              
                                 
                                    L
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              )
                              +
                              (
                              1
                              -
                              β
                              )
                              ϕ
                              
                                 
                                    T
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                           
                        
                     
                     
                        
                           (A.7)
                           
                              
                                 
                                    F
                                 
                                 
                                    t
                                    +
                                    m
                                 
                              
                              =
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                              
                              +
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       m
                                    
                                 
                              
                              
                                 
                                    ϕ
                                 
                                 
                                    i
                                 
                              
                              
                                 
                                    T
                                 
                                 
                                    t
                                 
                              
                           
                        
                     
                  

Single, Holt and Damped are applied on non-seasonal or seasonally adjusted data, where the final forecasts are reseasonalized. However, their performance is contrasted with Holt-Winters (Winters, 1960) a method which includes stochastic multiplicative modeling of seasonality. Forecasts for Holt-Winters are derived as follows:
                        
                           (A.8)
                           
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                              
                              =
                              a
                              
                                 
                                    
                                       
                                          X
                                       
                                       
                                          t
                                       
                                    
                                 
                                 
                                    
                                       
                                          S
                                       
                                       
                                          t
                                          -
                                          s
                                       
                                    
                                 
                              
                              +
                              (
                              1
                              -
                              a
                              )
                              (
                              
                                 
                                    L
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              +
                              
                                 
                                    T
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              )
                           
                        
                     
                     
                        
                           (A.9)
                           
                              
                                 
                                    T
                                 
                                 
                                    t
                                 
                              
                              =
                              β
                              (
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                              
                              -
                              
                                 
                                    L
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                              )
                              +
                              (
                              1
                              -
                              β
                              )
                              
                                 
                                    T
                                 
                                 
                                    t
                                    -
                                    1
                                 
                              
                           
                        
                     
                     
                        
                           (A.10)
                           
                              
                                 
                                    S
                                 
                                 
                                    t
                                 
                              
                              =
                              γ
                              
                                 
                                    
                                       
                                          X
                                       
                                       
                                          t
                                       
                                    
                                 
                                 
                                    
                                       
                                          L
                                       
                                       
                                          t
                                       
                                    
                                 
                              
                              +
                              (
                              1
                              -
                              γ
                              )
                              
                                 
                                    S
                                 
                                 
                                    t
                                    -
                                    s
                                 
                              
                           
                        
                     
                     
                        
                           (A.11)
                           
                              
                                 
                                    F
                                 
                                 
                                    t
                                    +
                                    m
                                 
                              
                              =
                              (
                              
                                 
                                    L
                                 
                                 
                                    t
                                 
                              
                              +
                              
                                 
                                    mT
                                 
                                 
                                    t
                                 
                              
                              )
                              
                                 
                                    S
                                 
                                 
                                    t
                                    -
                                    s
                                    +
                                    m
                                 
                              
                           
                        
                     where St
                      is the estimate of the seasonal index for period t and γ is the seasonal smoothing factor.


                     Theta model. The Theta model (Assimakopoulos & Nikolopoulos, 2000; Thomakos and Nikolopoulos, 2014) decomposes the seasonally adjusted series into two so-called ‘Theta lines’. The first Theta-line is calculated as the time regression of the data, thus corresponds to the long-term trend of the data. This Theta-line is extrapolated as usual (linear regression line). The second Theta-line has double the curvatures of the seasonally adjusted data and is simply calculated as 2X
                     −
                     LRL, where X is the vector of the seasonally adjusted data and LRL are the values of the linear regression line. The second Theta-line is extrapolated using Single Exponential Smoothing. The point forecasts of the two Theta-lines are combined using equal weights. The final forecasts are reseasonalized.


                     Linear Regression. This method assumes a relationship between the values (dependent variable) and the timestamps of the respective periods (independent variable). The mathematical model of this relationship is the linear regression equation. We calculate this equation using ordinary least squares. In this research, Linear Regression is applied on the seasonally adjusted data, while final forecasts are reseasonalized.


                     Forecast Pro. Forecast Pro’s Expert Selection is a routine implemented in the Forecast Pro commercial package (www.forecastpro.com). This method analyses each time series and performs individual model selection.


                     Autobox. A fully automated Box–Jenkins model building process is implemented in the commercial package Autobox (www.autobox.com). This includes model identification, estimation and diagnostic feedback loop.


                     Simple Moving Average (SMA). The point forecasts are calculated as an unweighted average of the last k observations, as follows:
                        
                           (A.12)
                           
                              
                                 
                                    F
                                 
                                 
                                    t
                                    +
                                    1
                                 
                              
                              =
                              
                                 
                                    1
                                 
                                 
                                    k
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       t
                                       -
                                       k
                                       +
                                       1
                                    
                                    
                                       t
                                    
                                 
                              
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                              
                           
                        
                     
                  


                     Croston’s method. Croston (1972), suggested the decomposition of an intermittent demand series in the non-zero observed demands and the intervals (in time periods) between successive non-zero demands. The demands and the intervals are extrapolated separately, using Single Exponential Smoothing with relatively low smoothing parameters (α). Updating for both demands and intervals occurs only after non-zero demands. The ratio of the two forecasts will constitute the final forecast:
                        
                           (A.13)
                           
                              
                                 
                                    F
                                 
                                 
                                    t
                                    +
                                    1
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             
                                                z
                                             
                                             
                                                ˆ
                                             
                                          
                                       
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                p
                                             
                                             
                                                ˆ
                                             
                                          
                                       
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           
                              
                                 
                                    
                                       z
                                    
                                    
                                       ˆ
                                    
                                 
                              
                              
                                 t
                                 +
                                 1
                              
                           
                        
                      and 
                        
                           
                              
                                 
                                    
                                       p
                                    
                                    
                                       ˆ
                                    
                                 
                              
                              
                                 t
                                 +
                                 1
                              
                           
                        
                      are the forecasts for the demands and the intervals respectively for period t
                     +1.


                     Syntetos and Boylan Approximation (SBA). Syntetos and Boylan (2001) proved that Croston’s method is positively biased. Subsequently, they proposed (Syntetos & Boylan, 2005) that final forecasts should be multiplied by a debiasing factor, which depends on the value of the smoothing parameter of the intervals. The forecasts for their proposed approximation (SBA) can be derived as follows:
                        
                           (A.14)
                           
                              
                                 
                                    F
                                 
                                 
                                    t
                                    +
                                    1
                                 
                              
                              =
                              
                                 
                                    
                                       1
                                       -
                                       
                                          
                                             a
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             
                                                z
                                             
                                             
                                                ˆ
                                             
                                          
                                       
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                p
                                             
                                             
                                                ˆ
                                             
                                          
                                       
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                 
                              
                           
                        
                     
                  


                     Teunter, Syntetos and Babai (TSB) Method. Teunter, Syntetos, and Babai (2011) considered an alternative decomposition approach. They proposed that the forecasts of the non-zero demands should be multiplied by the forecast of the probability that a non-zero demand will occur, thus:
                        
                           (A.15)
                           
                              
                                 
                                    F
                                 
                                 
                                    t
                                    +
                                    1
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          z
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    t
                                    +
                                    1
                                 
                              
                              ·
                              
                                 
                                    
                                       
                                          ρ
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    t
                                    +
                                    1
                                 
                              
                           
                        
                     where 
                        
                           
                              
                                 
                                    
                                       ρ
                                    
                                    
                                       ˆ
                                    
                                 
                              
                              
                                 t
                                 +
                                 1
                              
                           
                        
                      is the forecast of the probability observing a non-zero demand. While z is updated only after a non-zero demand occurs, ρ will be updated every period.

The M3-Competition (Makridakis & Hibon, 2000) is the largest empirical forecasting competition up-to-date. The study compared the forecasting performance of 24 different approaches (17 research teams/benchmarks and 7 forecasting packages) on 3003 time series. The data covered a range of frequencies (yearly, quarterly, monthly, other) and various types of time series (micro, macro, industry, etc.). The results of the M3-Competition have referred to numerous publications, with its data being used very often for empirical studies. Theta model (Assimakopoulos & Nikolopoulos, 2000) achieved the best performance, while Forecast Pro (http://www.forecastpro.com/) topped the table of commercial forecasting packages.

With regards to the replicability of this study, readers can download a special built software from http://www.forlab.eu/forecasting-software entitled ‘Horses for Courses Simulator’. Simulated time series as well as forecasts for all methods considered in this research can be reproduced using the provided software, with the only exception being for forecasts produced from Forecast Pro and Autobox as these are copyrighted commercial software packages.

@&#REFERENCES@&#

