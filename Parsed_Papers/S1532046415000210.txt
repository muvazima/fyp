@&#MAIN-TITLE@&#Efficient and sparse feature selection for biomedical text classification via the elastic net: Application to ICU risk stratification from nursing notes

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           To date, ICU mortality prediction has relied on structured clinical features.


                        
                        
                           
                           However, clinical free text features seem to perform just as well, or better.


                        
                        
                           
                           Existing clinical NLP methods largely do not rely on feature selection techniques.


                        
                        
                           
                           Applying regularization can aid discovery of important features in these problems.


                        
                        
                           
                           Indeed, only a small fraction of all features are needed to predict mortality well.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Text mining

Feature selection

Elastic net

ICU

Risk stratification

Machine learning

@&#ABSTRACT@&#


               
               
                  Background and significance
                  Sparsity is often a desirable property of statistical models, and various feature selection methods exist so as to yield sparser and interpretable models. However, their application to biomedical text classification, particularly to mortality risk stratification among intensive care unit (ICU) patients, has not been thoroughly studied.
               
               
                  Objective
                  To develop and characterize sparse classifiers based on the free text of nursing notes in order to predict ICU mortality risk and to discover text features most strongly associated with mortality.
               
               
                  Methods
                  We selected nursing notes from the first 24h of ICU admission for 25,826 adult ICU patients from the MIMIC-II database. We then developed a pair of stochastic gradient descent-based classifiers with elastic-net regularization. We also studied the performance-sparsity tradeoffs of both classifiers as their regularization parameters were varied.
               
               
                  Results
                  The best-performing classifier achieved a 10-fold cross-validated AUC of 0.897 under the log loss function and full L
                     2 regularization, while full L
                     1 regularization used just 0.00025% of candidate input features and resulted in an AUC of 0.889. Using the log loss (range of AUCs 0.889–0.897) yielded better performance compared to the hinge loss (0.850–0.876), but the latter yielded even sparser models.
               
               
                  Discussion
                  Most features selected by both classifiers appear clinically relevant and correspond to predictors already present in existing ICU mortality models. The sparser classifiers were also able to discover a number of informative – albeit nonclinical – features.
               
               
                  Conclusion
                  The elastic-net-regularized classifiers perform reasonably well and are capable of reducing the number of features required by over a thousandfold, with only a modest impact on performance.
               
            

Feature selection methods have recently been growing in importance within the fields of genomics, bioinformatics, and computational biology, where they have found wide utility in problems ranging from microarray DNA analysis to genome-wide association studies, among others [1–3]. During the course of microarray DNA analysis, for example, one common objective is to classify tumor samples from patients with cancer, based on the gene expression profiles of those samples. However, the number of genes under consideration is almost always much larger than the number of tumor samples, with only a small subset of these genes being putatively associated with the tumor classes. This problem hence exemplifies the so-called “p
                     ≫
                     n” setting [4,5], where one is faced with many more candidate features p than examples n. Thus, an ideal classifier for this problem setting should not only be accurate and exhibit other good performance characteristics – it should be able to select only those genes that make up the pathways present in the cancer of interest, easing interpretation of the resulting predictions, while also neglecting genes that are not discriminative of the outcome.

With this in mind, many parallels can be drawn between the example of microarray DNA analysis given above and the usual setting of biomedical text classification, where the goal may be to predict outcomes (i.e., mortality) from text or to perform information extraction [6], such as ongoing smoking status [7], or receiving a procedure such as mechanical ventilation in the ICU [8]. In these setting, nearly all input features derived from the underlying text are noisy in the sense that they carry little information about the outcome or clinical entity of interest, and therefore are not discriminative. The problem is also further complicated by the fact that the dimensionality of the input feature space often proves large – and can be increased even further by extracting bigram or higher-order n-gram features, or via other, more sophisticated methods of feature extraction.

Very generally, feature selection algorithms for linear models, including logistic regression and support vector machines (SVM) can be classified as follows: a method may carry out explicit feature selection by setting some feature weights or parameter estimates zero based on a set of criteria (which vary with the algorithm used). On the other hand, an algorithm may instead perform shrinkage, where the feature weights are smoothly shrunk toward zero while never being made exactly zero (ridge penalty), or implicit variable selection via a shrinkage process that allows for making at least some weights exactly zero (lasso), or while also performing ridge-like shrinkage (elastic net). In the cases of the ridge, lasso, and elastic net penalties, the shrinkage and selection effects are enforced by a constraint on the feature weights, or a regularization penalty that constrains how large the weights can be while they are being estimated. The lasso penalizes the sum of the absolute values of the weights (L
                     1 norm), while the ridge penalizes the square root of the sum of the squared weights (L
                     2 norm), and the elastic net combines these penalties into a linear combination of the L
                     1 and L
                     2 norms.

Two examples of explicit feature selection are stepwise regression, and exhaustive best-subset methods, which have been widely applied within the biostatistical and epidemiological literature [9], but have enjoyed somewhat less application elsewhere, particularly to high-dimensional learning and other, related contexts [10]. One reason for this is that exhaustive best-subset methods suffer from the limitation that with p candidate input features, the algorithm must train 2
                        p
                     
                     −1 classifiers (less the null classifier). While forward and backward stagewise methods have worst-case O(p
                     2) complexity, they are not guaranteed to select the best possible permutation of input features [11,12]. In either case, when p is in the hundreds of thousands to millions of features, as is usually the case when dealing with text-based problems, these approaches quickly become computationally infeasible. In addition, stepwise regression performs poorly when features are correlated; in practice, it does not exhibit a grouping effect, a desirable property of a feature selection method, where correlated features tend to be included together in a final model [13].

Examples of shrinkage-based methods for linear models include ridge regression [14] and the lasso [15]. Prior to the development of the lasso, ridge regression enjoyed preeminence among shrinkage methods, in part because the ridge penalty, represented as the L
                     2 norm of the vector parameter estimates, resembles the usual ordinary least squares objective and thus is easier to optimize [16]. In contrast, optimizing those non-smooth objective functions that include the lasso penalty requires specialized algorithms, e.g. least-angle regression (LARS) [17]. Moreover, ridge regression has generally been found to outperform lasso and exhibit a grouping effect when p<n; it is only when p grows larger than n that the performance of ridge regression begins to degrade and it no longer handles correlated features well [4]. However, the lasso penalty enforces automatic feature selection by forcing at least some features to be zero, as opposed to ridge regression, where only shrinkage is performed. Nevertheless, the use of the lasso proves problematic when at least some features are highly correlated. In this case, the lasso will select from among these features at random. Moreover, given n training examples, the lasso is capable of selecting only at most n features [13].

The elastic net, in contrast, represents a compromise between the ridge and lasso penalties [13]. Indeed, the elastic net penalty is simply written as a linear combination of these two penalties: the lasso penalty term acts to encourage sparsity in the parameter estimates of the resulting model, while the ridge term acts to “average out” the parameter estimates of correlated features, which imposes a grouping effect [4,18]. Hence, the elastic net performs both shrinkage (although milder than that obtained via ridge regression) and automatic feature selection. Depending on the preferences of the user and the properties of the underlying problem, the elastic net penalty can be smoothly adjusted so as to give more weight to either the lasso or ridge penalties. Compared to the lasso, the elastic net is able to yield a model including more features p than training examples n, but with possibly far fewer than would be selected via the ridge penalty alone (depending on the parameter settings chosen), which, taken with the fact that the elastic net exhibits a grouping effect [13], represents a clear advantage over either method.

Despite holding promise for feature selection and model development within the usual problem setting of biomedical text classification, the elastic net has yet to be applied toward these problems in clinical NLP. In particular, the elastic net allows those relevant word or n-gram features associated with the outcome to be discovered far more easily, and hence has the potential to supersede existing “black-box” approaches [8,19,20], e.g., unregularized SVMs, which are widely used in clinical NLP Furthermore, it is also possible that regularization techniques applied to classifiers could be used to validate and improve on what we term the “expert input” approach [7,21], where potentially relevant word or n-gram features are manually selected and extracted before a classifier is trained. In particular, Walsh and Hripcsak’s recent work [21] constitutes an example of the use of “expert input”: while they developed a series of classifiers with the aid of the lasso, using a combination of free text and a series of clinical features, these text features were manually chosen in advance based on their potential association with readmission, before combining them with other features with which to train the classifier.

Here, we describe the application of elastic net regularization to a pair of classifiers developed to predict mortality risk among adult ICU patients based on the free text of their first 24h of nursing notes, and we report a sample of the relevant features discovered by these classifiers. (A full list of the features of one such classifier, along with their coefficients, is included in the Supplementary Information.) Nursing notes constitute a good candidate source of information for mortality risk prediction, as they contain a detailed and regularly-updated record of the interventions performed, medications administered, vital signs, and physical examination findings, all of which carry highly specific information about the patient’s dynamic physiological state and eventual outcome. We then characterize what we term the sparsity-performance tradeoff of both classifiers as the elastic net regularization parameter is varied. We also report examples of informative features found by the classifier, and compare them to what is currently known of predictors of ICU mortality. Finally, we also compare the performance of our models to an existing method of ICU risk stratification based on the Simplified Acute Physiology Score (SAPS) and validated on the same dataset.

@&#METHODS@&#

The nursing notes were derived from the Multiparameter Intelligent Monitoring in Intensive Care-II (MIMIC-II) database, version 2.6. The MIMIC-II database contains complete sets of clinical free text notes from roughly 40,000 ICU stays for nearly 33,000 patients at Beth Israel Deaconess Medical Center (BIDMC) in Boston, Massachusetts, dating from between 2001 and 2008 [22,23]. For each adult ICU patient, we selected and combined all nursing notes dated within 24h of the first recorded ICU admission time. On this basis, 25,826 adult patients and their notes were selected, of which 2099 died in the hospital prior to discharge at any point following their ICU admission; mortality was determined via the ICUSTAY_EXPIRE_FLG variable.

Next, each set of notes for a single patient (i.e., each document) was processed in order to extract unique unigram and bigram features. We have previously reported [8] that extracting 3-grams or higher-order n-grams (i.e. setting n greater than 2) does not improve classifier performance. Moreover, extracting these higher-order n-grams drastically increases the time needed for feature extraction and classifier training. We removed numbers, punctuation, and neutral stop-words (e.g. ‘and’, ‘the’) and performed stemming on each word. The feature counts – i.e., the counts of each unigram and bigram – for each document were then extracted and mapped to their term frequency-inverse document frequency (tf-idf) values, i.e., the count of a feature in that document, divided by the number of total notesets in which it appears. Essentially, each document was transformed into a vector of tf-idf values of features, which were then used for classification.

We then trained, by way of stochastic gradient descent (SGD) [24], two different classifiers – essentially logistic regression and a linear support vector machine – which are distinguished only by their loss functions. The loss function defining logistic regression, with respect to a single training example yi
                      is given by 
                        
                           L
                           (
                           β
                           ,
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           )
                           =
                           log
                           (
                           1
                           +
                           exp
                           (
                           -
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           
                              
                                 β
                              
                              
                                 T
                              
                           
                           x
                           )
                           )
                        
                     , or the log loss, while the loss function giving the linear SVM is 
                        
                           L
                           (
                           β
                           ,
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           )
                           =
                           max
                           (
                           0
                           ,
                           1
                           -
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           
                              
                                 β
                              
                              
                                 T
                              
                           
                           x
                           )
                        
                     , which is also commonly known as the hinge loss. In both cases, the objective function J to be minimized took the form
                        
                           
                              J
                              (
                              β
                              )
                              =
                              α
                              (
                              
                                 
                                    λ
                                 
                                 
                                    1
                                 
                              
                              |
                              β
                              
                                 
                                    |
                                 
                                 
                                    1
                                 
                              
                              +
                              
                                 
                                    λ
                                 
                                 
                                    2
                                 
                              
                              |
                              β
                              
                                 
                                    |
                                 
                                 
                                    2
                                 
                                 
                                    2
                                 
                              
                              )
                              +
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       all
                                       
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                              
                              L
                              (
                              β
                              ,
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                              )
                           
                        
                     where α gives the overall regularization strength, the regularization terms |β|1 and 
                        
                           |
                           β
                           
                              
                                 |
                              
                              
                                 2
                              
                              
                                 2
                              
                           
                        
                      correspond to the L
                     1 and L
                     2 norms of the vector of weights or parameter estimates β, respectively, and the rightmost cost term gives the average loss by summing over all training examples yi
                     . For brevity, we express both elastic net penalty hyperparameters λ
                     1 and λ
                     2 in terms of one hyperparameter, 
                        
                           λ
                           =
                           
                              
                                 λ
                              
                              
                                 1
                              
                           
                           /
                           (
                           
                              
                                 λ
                              
                              
                                 1
                              
                           
                           +
                           
                              
                                 λ
                              
                              
                                 2
                              
                           
                           )
                        
                     , which represents the ratio of L
                     1 to L
                     2 regularization strength imposed on the classifier, and ranges from 0 to 1. Setting λ
                     =1 results in only L
                     1 regularization, while λ
                     =0 puts full weight on the L
                     2 penalty term. However, values of λ between 0 and 1 correspond to a linear combination of the two penalty terms. We interpret relative feature influence as proportional to the absolute values of the parameter estimates 
                        
                           
                              
                                 
                                    
                                       β
                                    
                                    
                                       ˆ
                                    
                                 
                              
                              
                                 i
                              
                           
                        
                     . We also considered a feature selected if its parameter estimate or weight learnt by a classifier was nonzero (no matter how small).

We trained all classifiers via SGD with constant learning rate η
                     =1.4. The performance metric used was the area under the ROC curve (AUC), and we performed 10-fold nested cross-validation to estimate AUC for different values of λ 
                     [25]. The optimal values of λ and η for each classifier were also determined by 10-fold cross-validation over a reasonable parameter grid of values for λ and η. We used a constant value of α
                     =10−5 in all of our experiments.

In order to better characterize each classifier’s ability to induce sparsity, we also made use of an additional sparsity measure, defined as follows. Denote the sets of all unigram and bigram features having nonzero coefficients as U and B, respectively. Then the quantity M(λ) defined as
                        
                           
                              M
                              (
                              λ
                              )
                              =
                              
                                 
                                    |
                                    elements of
                                    
                                    B
                                    
                                    
                                    containing an unigram from
                                    
                                    U
                                    |
                                 
                                 
                                    |
                                    B
                                    |
                                 
                              
                           
                        
                     where the operation |A| denotes the cardinality – i.e., the number of elements – of a set A. As the input feature space contains both unigrams and bigrams, it is possible – and often the case – that bigram features selected by a sparse classifier contain unigrams already included in the model, which would, in a sense, render those features less informative. For example, a classifier could preferentially select the bigram “metabolic acidosis” over just “acidosis”, which could also refer to respiratory acidosis, an unrelated condition, and “metabolic”, a unigram that would likely not be selected, as the term is not discriminative of the outcome by itself.

On the other hand, it is possible that a differently trained classifier would select both “metabolic acidosis” and “acidosis”, which could complicate interpretation of the resulting predictions. Ideally, as metabolic and respiratory acidosis denote the only types of acidosis, the classifier should select only “metabolic acidosis” and “respiratory acidosis”, and not just “acidosis” or “metabolic” or “respiratory”. Therefore, the ideal sparse classifier would select only those bigrams that carry maximal marginal information content, i.e., those that carry information not contained in the unigrams already selected; thus, we hypothesize that M(λ) denotes, though somewhat crudely, how sparse the feature space of selected bigrams is. If M(λ) is zero, then all the bigrams selected by the classifier very likely do not contain information already present in the unigrams already selected; conversely, if M(λ)=1, then all bigrams are redundant, as they repeat one or at most two unigram feature(s).

We used McNemar’s test for paired nominal data to compare classifiers. Furthermore, in order to compare our model to existing methodology, we also made use of a logistic regression model utilizing the Simplified Acute Physiology Score, version 1 (SAPS-I) [26]. The SAPS model includes as its components laboratory values, ventilator settings, age, Glasgow Coma Score (GCS), among others. The SAPS model used the patient’s highest recorded value of SAPS within the first 24h of their ICU stay. Modified Hosmer–Lemeshow goodness-of-fit tests [27] were used to assess calibration for all classifiers, i.e., how well a classifier’s predicted probabilities of mortality agreed with the actual probabilities by decile of risk.

The Committee on Human Research of the University of California, San Francisco deemed this study exempt from review.

@&#RESULTS@&#

A total of 101,806 nursing notes were selected for 25,826 patients within the first 24h following ICU admission. Following processing, the mean length of the combined 24h of nursing notes for each patient was 468 words (median 334, interquartile range 249–466). Extracted were 1,842,522 candidate input features, of which 91,317 were unigrams and 1,751,205 were bigrams. On this dataset, the SAPS-based logistic regression classifier achieved an AUC of 0.791.

The sparsity-performance tradeoff curves of both text-based classifiers are presented in Fig. 1
                     . As expected, the SGD classifier with log loss selected all features at λ
                     =0, resulting in an AUC of 0.897. At λ
                     =1, just 465 features were selected, which represents just 0.00025% of the number of candidate input features, or almost a ten-thousandfold reduction, and the AUC for this classifier was 0.889. Under the log loss, intermediate values of lambda resulted in a smooth and robust tradeoff between sparsity and performance.

Interestingly, the same classifier equipped with the hinge loss selected just over half (970,319 out of 1,842,522, or 53%) of the candidate feature space at λ
                     =0; the resultant AUC with these features was 0.850. At λ
                     =1.0, 345 features were selected, which proved fewer than the log-loss classifier at the same value of λ, and resulted in an AUC of 0.876. Compared to the log loss, the hinge loss also appeared less robust to changes in the size of the selected feature space. The log loss also displayed a more optimal (i.e., shallower) sparsity-performance tradeoff compared to the hinge loss (Fig. 1.) The differences in AUC between the two classifiers were significant at each value of λ by McNemar’s test. A nonsignificant Hosmer–Lemeshow statistic was obtained for all classifiers for each value of λ studied at the 0.05 confidence level, indicating adequate calibration.

The empirical cumulative distribution functions of nonzero features of the classifier under log loss are presented in Fig. 2
                      for some choices of λ, as well as for the unregularized classifier. The shrinkage effect of the ridge regularizer clearly dominates at smaller values of λ; at λ
                     =0, compared to the unregularized classifier, the effect is fairly strong, with roughly 95% of nonzero features having parameter estimates lying in the interval [−0.1, 0.1]. In contrast, the lasso penalty obtained at λ
                     =1 tends to result in a wider range of parameter estimates for the features selected, which eases their interpretation. The effects of the lasso regularizer also clearly dominate those of the ridge at λ
                     =0.85; while some shrinkage is observed, the classifier selects roughly the same number of features (in terms of orders of magnitude) as it does at λ
                     =1.

A plot of M(λ) versus λ is depicted in Fig. 3
                     . The classifier under hinge loss displays markedly lower values of M(λ) for all values of λ, compared to the log loss. This effect was most marked for λ
                     =1.0. However, the increased diversity of bigram features selected by the hinge classifier did not lead to any improvements in performance. We also present a selection of the features selected by a log-loss classifier with λ
                     =0.85 in Table 1
                     .

@&#DISCUSSION@&#

All classifiers under both loss functions yielded reasonable results, with AUCs ranging from 0.85 to 0.90, and compared well to the performance of a logistic model based on SAPS, which achieved an AUC of 0.791. The best-performing classifier yielded an AUC of 0.897 when furnished with the log loss. These results compare well with some past studies; in [29], physicians were able to achieve roughly similar prediction performance only via manual chart review, and in [30], where Lehman et al. utilized topic models of text [31] (a dimensionality reduction technique) to stratify risk among patients from the same dataset, the resulting AUC was 0.78. Equipping the classifier with a hinge loss did result in somewhat more sparse models (Fig. 1) compared to the log loss. For both classifiers, the tradeoffs inherent in adjusting λ appear favorable and suggest that a substantially more sparse and interpretable model can be achieved with only a modest performance cost.

The classifier equipped with the hinge loss function exhibited two interesting behaviors related to the sparsity of the resulting models. First, with full L
                     2 regularization, only 970,319 features are selected out of a possible 1,842,522 (53%), so at λ
                     =0, the feature space is already more sparse compared to the log-loss classifier, which selects all features. Second, as measured by M(λ), a larger proportion of the bigram features it selects appear to be “informative”, compared to the classifier with log loss, and this effect holds for all values of λ (Fig. 3), although this did not translate into improved predictive performance (Fig. 1.) These results are consistent with other studies into the hinge loss, e.g. in [32]. The precise reasons for these behaviors are unclear, but seem to be related to the behavior of the hinge loss function and its derivative, which both act to encourage sparsity.

Among the more influential features uncovered by the sparser classifiers (with larger values of λ, Table) include those physical examination signs associated with neurological status, specifically level of consciousness and motor function, and which overlap with certain components of the Glasgow Coma Score (Table). Furthermore, the classifiers also selected features that serve as indicators of poor prognosis, such as those related to hemodynamic instability and the use of vasopressors, which also form a major component of the SOFA risk scoring system [33] commonly used in ICUs.

The management of mechanical ventilation is crucial to ICU outcomes, and this was also reflected in our models (Table). Even the sparsest models retained those features serving as indicators of ventilation status, as well as those marking the progression of weaning a patient from ventilation, to the point where they are extubated. While the precise protocols differ among ICUs [34], the process of ventilator weaning usually progresses in stages. When patients are first ventilated, they are often so extremely ill that they cannot cooperate and cannot be slowly put to sleep because they could not survive the slow respiratory rate created. Therefore, they need to be paralyzed briefly and put on a form of ventilation called controlled mechanical ventilation (CMV), in which all the work of breathing is done by the ventilator. As their respiratory function improves, they transition to synchronized intermittent mandatory ventilation (SIMV) and often then on to continuous positive airway pressure (CPAP) ventilation, following which the patient may then finally be capable of breathing on their own and is extubated [35]. Furthermore, since CPAP can also be applied without intubation, its presence in a note could also denote a trial of externally applied CPAP to prevent ever having to go to CMV, SIMV, or other forms of intubated ventilation, and this indicates that the patient is not as ill as one on CMV or SIMV. Accordingly, we observed the selection of features corresponding to these modes, and their weights—CMV higher than SIMV higher than CPAP—were commensurate with the mortality risk implied by each mode (Table).

Our classifiers also were able to glean a set of interventions that appear to serve as proxies for comorbidities not explicitly documented in the nursing note that indicate poor prognoses. For example, in order to treat a patient exhibiting increased intracranial pressure (ICP), a physician would likely order the administration of mannitol, an osmotic diuretic that is widely used to lower ICP, and a nurse would carry out this order. However, the nurse’s note will usually not document the increased ICP – at least not explicitly, in that an ICP value might be given in their note although no judgment is made as to whether it is abnormal or not – whereas their note will almost always document the administration of mannitol. Other examples of selected features themselves also serve as outright indicators of poor prognosis, such as metabolic acidosis, midline shift, and cardiac arrest.

An interesting set of features that we did not expect to discover, but nevertheless were selected by the sparser classifiers, were related to a patient’s family visitors and relationship status (Table). It is common for a patient’s family to visit them in the hospital, and a nurse usually documents these visits in their notes. We observed that terms such as “wife”, “son”, and “daughter” were associated with mortality, while “father”, “mother”, and “parent” carried negative associations. One explanation for this observation is that those patients having parents present at their bedside are more likely to be younger, and consequently have better prognoses compared to patients whose spouse or children are visiting, the presence of whom implies increased age, on average. This is consistent with the findings of other studies that have developed clinical models to predict mortality risk among ICU patients [36–39], which have found a positive association between age and mortality.

We also remark that these classifiers are performing a form of information extraction; given a set of notes, not only can the extracted features that overlap with those learnt for the outcome be used to generate predictions – those features associated with the predicted outcome and in the text of a given note can be extracted and presented to users This capability could prove useful in other contexts with more complex outcomes having longer time horizons for intervention, such as that of readmission. In that case, the information – in the form of n-gram features and potentially also phrases – extracted would be more likely to be actionable by, and thus be utilized by, providers and care transition teams. For example, a regularized classifier developed to predict readmission risk based on text would be able to quickly find and “tag” charts that predicted positive also with those features contributing to the increased risk. In such a problem setting, we envision such a classifier learning groups of features corresponding to the presence of complex histories involving substance abuse, medication non-compliance, mental illness, or similar, enabling these patients to receive more targeted transitional care.

A limitation of our study is that our classifiers used only the text of nursing notes as input features, and did not include other types of notes or features that could be built from structured data elements present in the EMR, such as physiological vital signs or laboratory values. It is possible that including these additional data elements could improve classifier performance, although it is not yet clear how to best combine these structured data elements with text under the influence of regularization, nor how the resulting models would be interpreted. In addition, beyond the grouping effect, the classifier did not explicitly account for correlations between features or groups of features; such correlations, if found and determined to be clinically relevant, could uncover novel interactions between risk factors not yet recognized by clinicians. Furthermore, the interpretability of our models was somewhat complicated by the presence of nonclinical predictors (such as those relating to the presence of family at the bedside) selected by both classifiers; a filter list of terms and an associated methodology to generate such a list (potentially via comparison to nonclinical corpora serving as references) could be developed to restrict terms to only those that are clinically relevant, and to minimize the impact of variation in documentation styles between different care units and caregivers.

Finally, while feature selection methods largely do not figure into existing clinical NLP approaches making use of free text, we believe they have the potential to substantially improve models based on NLP. First, feature selection approaches such as ours could be used to validate the features utilized by classifiers previously considered as “black boxes” and to evaluate their clinical relevance, ultimately improving their usability and generalizability (or portability). As clinical decision support and other systems come to rely on classifiers derived from clinical text, it is imperative that the output of such systems be clinically relevant and interpretable, and thus actionable by providers. Second, while classifiers based on “expert input” may perform adequately when the features associated with the outcome are well characterized, and in conjunction with other non-text clinical features, such methods run the risk of missing out on potentially interesting predictors latent in the text and hence yielding suboptimal classifier performance. These approaches also hamper portability of the resulting classifier, as a classifier relying on features resulting from “expert input” must necessarily be re-validated on new corpora with different sets of manually derived text features in order to ensure optimal performance.

Lastly, several recent studies and reviews [7,40–42] have investigated and emphasized the portability of classifiers based on clinical free text and other NLP systems. Indeed, the need for clinical NLP systems to be portable as they mature out of the lab and begin to integrate and make use of data from multiple institutions will only grow. Even though we were not able to obtain data from different institutions in order to validate our hypothesis, regularization methods have the potential to restrict the feature space so as to avoid overfitting and thus improve classifier generalizability, and in turn, its portability. However, regularization by itself likely will not serve as a panacea to the question of classifier portability; other, synergistic approaches, such as model blending or ensembling [43–45], could be used to improve the portability of a clinical free-text-based classifier that must necessarily generalize well between multiple institutions.

@&#CONCLUSION@&#

Applying elastic-net regularization to classifiers based on clinical free text reduced the number of features selected by more than a thousandfold, thereby making those classifiers more easily interpretable, while sparing performance. The features selected were also clinically relevant, and correlated well with what is currently known about ICU outcomes. In addition, by avoiding overfitting, regularized text classifiers have the potential to improve on the usability and portability of existing methods within the field of clinical NLP.

All authors meet the ICMJE criteria for authorship. Their contributions were: BJM: conception and design, acquisition of data, analysis and interpretation of data, drafting the article, and final approval of the version to be published. WJB: analysis and interpretation of data, critical revision of the manuscript for important intellectual content, and final approval of the version to be published.

RAD: conception and design, analysis and interpretation of data, critical revision of the manuscript for important intellectual content, and final approval of the version to be published.

This work was supported by Innovations Fund of the Philip R. Lee Institute for Health Policy Studies, the Center for Healthcare Value, both at the University of California, San Francisco, and the Andrew Grove Family Foundation. The funders were not involved in the design, conduct, or evaluation of the research.

The study was deemed to be exempt from review by the Committee on Human Research of the University of California, San Francisco.

@&#ACKNOWLEDGMENTS@&#

The authors are especially grateful to Bethany Percha of Stanford University and the anonymous reviewers for their helpful comments on earlier drafts of this paper.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2015.02.003.


                     
                        
                           Supplementary data 1
                           
                        
                     
                  

@&#REFERENCES@&#

