@&#MAIN-TITLE@&#The effects of prototype medium on usability testing

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Interface medium has no effect on users' perception of usability.


                        
                        
                           
                           The impact of medium on usability issues detected varies by application tested.


                        
                        
                           
                           Usability testing can begin with low fidelity models.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Usability

Perception

Human computer interaction

Prototype

Fidelity

@&#ABSTRACT@&#


               
               
                  Inconsistencies among testing methods and results in previous research prompted this study that builds upon a systematic usability testing research framework to better understand how interface medium influences users' abilities to detect usability flaws in applications. Interface medium was tested to identify its effects on users' perceptions of usability and abilities to detect usability problems and severe usability problems. Results indicated that medium has no effect on users' abilities to detect usability problems or perceptions of usability. However, results did indicate an interaction between the medium and the tested application in which users were able to identify significantly more usability problems on a higher fidelity medium using a particular application. Results also indicated that as users' perceptions of an application's usability increases, the users are less able to detect usability problems in that application. Usability testing should begin early in the design process, even if low fidelity mediums will be used.
               
            

@&#INTRODUCTION@&#

The general idea of fidelity in user interfaces is agreed upon in literature as describing how different a prototype is from a finished product (Walker et al., 2002). Many researchers simply take this general definition and perform studies using fidelity as an independent variable (Catani and Biers, 1998; Sefelin et al., 2003). Even with a common general definition of fidelity, these studies have demonstrated vastly different interpretations of the definition and, therefore, have resulted in different outcomes. Sauer et al. (2010) acknowledge these inconsistencies and propose a four-factor framework of contextual fidelity which asserts that the definition of fidelity is too narrow. They expand the definition to include the fidelity of the testing environment, user characteristics, and task scenarios along with the currently accepted system prototype. Their assumption is that research results are varied because these results are not considering every important factor. Virzi et al. (1996) also address the poor definition of fidelity but believe fidelity can be broken into separate parts, and explain that fidelity is composed of multiple components or dimensions. These dimensions include breadth of features, degree of functionality, similarity of interaction, and aesthetic refinement. Breadth of features describes the number of final system functions a prototype has. Degree of functionality describes how functional each final system function is in a prototype. Similarity of interaction describes how similar a user's interaction with the prototype is to his or her interaction with the final system. Aesthetic refinement describes how similar a prototype looks compared to the final design. Table 1
                         shows how a number of studies that have been designed to test the effects of variations in fidelity have often varied fidelity by changing different, and sometimes multiple, components within a single study.

The simultaneous variation of multiple components of fidelity shown in Table 1 make it clear why the results of these studies have often been split, with some saying fidelity has no impact on usability testing and others saying just the opposite. Therefore, the appropriate method for testing the effects of fidelity is testing each of its components as separate variables rather than grouping them together under the umbrella of general fidelity. Walker et al. (2002) support this conclusion, agreeing that fidelity should be separated into components for experimentation, but they divide fidelity differently than Virzi et al. (1996) do by identifying interface medium (e.g. computer, paper, etc.) as one of the separate components. While a change in interface medium could simultaneously change multiple components of fidelity as defined by Virzi et al. (1996), Walker et al. (2002), either intentionally or by chance, avoided these impacts in their study and only affected the degree of functionality of the interface.

Increasing or decreasing the fidelity of an interface medium (e.g. advancing from a paper prototype to a computer prototype) has a direct impact on an interface's overall fidelity. However, the opposite is not necessarily true. An interface's overall fidelity may be increased or decreased without actually changing the interface medium. For example, an interface can lose overall fidelity by no longer having interactive buttons but still be presented using the same medium (e.g. a computer).

The research authored by Walker et al. (2002) most inspired this study. The idea behind their research is that in order to evaluate fidelity, it must first be broken down into components. In their case, the component that was studied was interface medium or degree of functionality. The challenge in studying the effects of interface mediums is keeping all fidelity components consistent (except the one being tested) across each medium. For the Walker et al. (2002) experiment, this involved keeping the breadth of features, similarity of interaction, and aesthetic refinement stable while testing the degree of functionality as an independent variable.

While Walker et al. (2002) found no significant impact in the number, type, or severity of usability issues discovered by the users due to interface medium, there is a shadow cast on the results because of the small number of participants tested. And while Virzi et al. (1996) found no effect from variation in interface medium, the systems in their experimentations are somewhat outdated today, and a similar methodology applied to more modern technology could prove valuable. Furthermore, more research is needed before the usefulness of low-fidelity prototypes can be fully evaluated (Sauer et al., 2010).

A review of current literature led to the following question: how do reduced fidelity interface mediums affect a user's ability to detect usability problems? With this question, four hypotheses were formed:
                           
                              1.
                              Users are equally likely to detect usability problems using either computer or paper mediums to test a prototype.

Users are equally likely to detect severe usability problems using either computer or paper mediums to test a prototype.

Users' perceptions of applications' usability is unaffected by using either computer or paper mediums to test a prototype.

The fewer usability problems a user detects, the higher the user's perception of the application's usability will be. As users' perceptions of applications' usability increases, they are less likely to detect usability problems.

@&#METHODS@&#

The independent variables in this experiment were interface medium, interface application, and testing order. Interface medium, the primary focus of the study, was measured at two levels: computer monitor and paper. Both levels were controlled so that the breadth of features, similarity of interaction, and aesthetic refinement were kept consistent between each level (Virzi et al., 1996).

The primary dependent variable measured in this experiment was the identification of pre-defined usability problems. This variable was measured as a binary for each usability problem (i.e., 1 if discovered, 0 if not) and was measured with a checklist of pre-defined usability problems that the researcher checked off as participants actively identified the problems while using the applications. Furthermore, perceived usability was measured using the Post-Study System Usability Questionnaire (PSSUQ) (Lewis, 1992). Perceived usability is a separate construct from the identification of usability problems, and is used in an attempt to capture a user's overall assessment of usability for an interface, even if they are unable to clearly identify the aspects of the interface that led to that assessment.

Thirty-two participants were recruited using email with extra credit offered as an incentive and campus flyers with a $10.00 cash incentive offered to each potential participate. Participants were required to be able to read at the sixth-grade level, possess no form of blindness or colorblindness, and have the ability to accurately touch objects with their fingers.

There were two interface mediums in this experiment: a computer monitor and a paper interface. During the course of the study, each participant was exposed to each of the two interface mediums. The computer monitor interface had reduced screen dimensions to mimic a standard 8.5 × 11 inch letter sheet of paper. All remaining area on the monitor was blacked out. Instead of interacting with the controls presented on the monitor with a keyboard or mouse, as would be typical while operating a desktop computer, the participant touched the monitor as if it were a touch-screen interface. A researcher adjusted the screen image manually, based on participant input. The aesthetics of the applications for this interface were as fully developed as a finished product would be. The paper interface was a series of photo-quality, full-color printouts of each of the screens within each application. Each printout was printed on a standard 8.5 × 11 inch sheet of paper. The users interacted with the printouts by touching button images printed on the sheets of paper. A researcher switched the printout manually based on participant input. The use of the photo-quality, full-color printouts was meant to preserve the same aesthetic qualities present on the computer monitor interface. Pages were even placed inside page protectors to prevent them from becoming distorted from a hole-puncher or dirt and grime.

Two applications were used in this study: a grocery store self-checkout kiosk and a movie rental kiosk. Both applications were built by linking several Microsoft PowerPoint slides together using hyperlinks attached to buttons. This technique, combined with an extensive pool of available slides for the participant to access, created the illusion of an appropriately complex application. Both applications were strictly prototypes, designed only with the functionality participants need to accomplish the tasks they were given. Fig. 1
                         shows an image of the home screen of the grocery application and Fig. 2
                         shows an image of the home screen of the movie rental application.

In an effort to define what the actual usability problems were in each prototype, three user interface analysts were asked to evaluate both applications for usability problems. After compiling their results into two lists – one for each prototype – the lists were analyzed by a fourth usability analyst to determine which were severe and which were not. Severe usability problems were defined as those that a reasonable user would find so disconcerting that he or she would choose a different system if available based solely on that one usability problem. A total of nine usability problems (four severe, five not severe) were identified in the grocery store application. A total of eleven usability problems (four severe, seven not severe) were identified in the movie rental application.

Each participant was asked to complete two evaluations. Each evaluation required the use of a different interface medium as well as a different application for that medium. The experimental procedure lasted less than one hour for each participant. Participants began by completing a demographics questionnaire. After completing the questionnaire, they were asked to complete four tasks per medium/application combination. The participants were told that the purpose of the test was to not only complete the tasks but also to identify any problems that might make it hard to use the application. While participants were performing tasks, the researcher used a participant evaluation form to check off the pre-defined usability problems as the participants identified them. Audio/video data was collected as a backup in the event the researcher did not record a participant response. Finally, the participants were given Post-Study System Usability Questionnaires (PSSUQ) to evaluate their perceived usability of the application (Lewis, 1992).

There were four sources of data for this experiment. The first was the pre-test questionnaire, the second was the participant evaluation form, the third was the PSSUQ, and the fourth was the audio/video data collected during testing.

To effectively analyze the raw data collected during the study, many steps had to be taken. A PSSUQ score had to be calculated to evaluate participants' perceived usability. The PSSUQ score was calculated for each participant for each session by averaging the scores across all the questions in the PSSUQ. This method of scoring the PSSUQ is consistent with previous research (Lewis, 2002). Furthermore, experience scores for overall kiosk experience, movie rental kiosk specific experience, and grocery store self-checkout kiosk specific experience were necessarily categorized into high and low experience levels in order to perform some of the chi-squared analyses. A participant who used the kiosk once a month, or more frequently, was categorized as having a high level of experience. A participant who used the kiosk less than once per month was categorized as having low experience. Finally, after data was collected, a normality test performed in Minitab indicated that the PSSUQ scores did not meet the normality requirement necessary to perform an ANOVA. A natural log transformation was used to normalize the data.

Hypotheses one and two were analyzed using chi-squared analyses and percentages were used during post-hoc analyses to allow for comparison between different variable states. Hypothesis three was analyzed using the general linear model ANOVA approach because it involved repeated measures, and hypothesis four was analyzed using a correlation analysis. The Pearson correlation analysis was chosen over the Spearman correlation analysis because of the interval nature of the variables tested.

Analyses were performed to investigate the effects of gender, order, application, and experience on the likelihood of a participant identifying a usability problem, likelihood of a participant identifying a severe usability problem, and a participant's PSSUQ score. Each potential effect was analyzed in the same way. Chi-squared analyses were used to analyze the impact of gender, order, application, and experience on the likelihood of a participant identifying a usability problem and the likelihood of a participant identifying a severe usability problem. Again, percentages were used for post-hoc analyses to allow for comparison between the variable states. In the cases where categories had fewer than five discovered usability problems, Fisher's exact tests were used instead of chi-squared analyses. ANOVAs were performed to analyze the impact of gender, order, application, and experience on a participant's PSSUQ score. Additional chi-squared analyses were performed on subsets of data to analyze potential interaction effects involving likelihoods of participants finding overall or severe usability problems, and an ANOVA was used to analyze potential interaction effects involving PSSUQ scores. The general linear model ANOVA approach was used to test for order effect, application effect, and the interaction effects because each involved repeated measures.

@&#RESULTS@&#

Participants included 22 males and 10 females, ranging in age from 18 to 44 years old with an average age of 22.72 years (SD = 4.43). Furthermore, all participants were college students with 6 classified as juniors, 21 classified as seniors, and 5 classified as graduate students. The average overall experience score was 3.88 (SD = 1.81) with 21 participants falling into the high experience level and 11 falling into the low experience level. The average movie rental experience score was 4.78 (SD = 1.52) with 16 participants falling into the high experience level and 16 falling into the low experience level. The average grocery store self-checkout kiosk experience score was 5.00 (SD = 1.70) with 13 participants falling in the high experience level and 19 falling into the low experience level.

A chi-squared test was performed using Minitab to test the hypothesis that users are equally likely to detect usability problems using either computer or paper mediums to test a prototype. The results indicated that participants were able to find the same number of usability problems regardless of the interface medium they used (χ2 (1, N = 640) = 0.22, p = 0.640).

A chi-squared test was also performed using Minitab to test the hypothesis that users are equally likely to detect severe usability problems using either computer or paper mediums to test a prototype. However, the results indicated that participants were not able to find the same number of usability problems regardless of the interface medium they used (χ2 (1,N = 256) = 3.87, p = 0.049), with the computer medium having a mean of 12.50% (SD = 20.08%) and paper medium having a mean of 5.47% (SD = 13.82%) of usability problems identified respectively.

Further analysis was conducted to test for an interaction between application and interface medium. This analysis was conducted by first sorting data into subsets based on the application used to collect it. Then, each subset of data was tested to see if medium had an effect on the number of usability problems participants found. The results indicated that medium had no effect when participants were using the self-checkout kiosk application (p = 1.000), but medium did have an effect when participants were using the movie rental kiosk application (p = 0.002). Fig. 3
                            depicts the mean percent of severe usability problems participants identified, categorized by medium-application combinations. Standard error bars are also included in the figure. The computer/movie category shows a significantly higher percent of severe usability problems discovered as compared to the other categories.

An ANOVA was performed using Minitab to test the hypothesis that users' perceptions of applications' usability is unaffected by using either computer or paper mediums to test a prototype. The results indicated that a user's PSSUQ score was not significantly different based on the medium used to test the applications (F (1,63) = 0.44, p = 0.514).

A correlation analysis was performed using Minitab to test the hypothesis that a lower number of detected usability problems is related to a higher perception of usability. The results indicated a significant negative correlation between a reversed PSSUQ score and the corresponding chance of a participant identifying a usability problem (r (62) = −0.32, p = 0.009). In general, more usability problems were found by users with lower perceived usability (as measured by the PSSUQ score). A separate correlation analysis indicated that there was also a significant negative correlation between a reversed PSSUQ score and the corresponding chance of a participant identifying a severe usability problem(r (62) = −0.43, p < 0.001). In general, more severe usability problems were found by users with lower perceived usability (as measured by the PSSUQ score). Fig. 4
                            is a plot of PSSUQ scores versus the percent of usability problems and severe usability problems found. This figure supports the correlation analyses' results by depicting a trend of the percent of usability problems discovered rising as the PSSUQ scores rises.

Gender did not have a significant effect on the likelihood of a participant identifying a usability problem (χ2 (1, N = 640) = 0.01, p = 0.910) or the likelihood of a participant identifying a severe usability problem (χ2 (1, N = 256) = 0.01, p = 0.930). Furthermore, gender did not have a significant effect on a participant's PSSUQ score (F (1,62) = 1.77, p = 0.188).

The order in which participants tested each interface medium and application did not have a significant effect on the likelihood of a participant identifying a usability problem (χ2 (1, N = 640) = 0.01, p = 0.925) or the likelihood of a participant identifying a severe usability problem (χ2 (1, N = 256) = 0.05, p = 0.827). However, order did seem to have a slight, though not statistically significant, effect on participants' PSSUQ scores (F(1,63) = 3.37, p = 0.076), with the second testing session having an average PSSUQ score of 2.20 (SD = 0.83) and the first testing session having an average PSSUQ score of 2.37 (SD = 0.92).

Application did not have a significant effect on the likelihood of a participant identifying a usability problem (χ2(1, N = 640) = 1.71, p = 0.191). Furthermore, application had no effect on participants' PSSUQ scores (F (1, 63) = 1.76, p = 0.195). However, an application effect was found on the likelihood of a participant identifying a severe usability problem (χ2 (1, N = 256) = 8.07, p = 0.004), with the movie rental application having a mean of 14.06% (SD = 21.00%) and grocery self-checkout application having a mean of 3.91% (SD = 11.20%).

Further analysis was conducted to test for an interaction between application and interface medium. This analysis was conducted by first sorting data into subsets based on the medium used to collect it. Then, each subset of data was tested to see if application had an effect on the percent of usability problems participants found. The results indicated that application had no effect when participants were using the paper medium (p = 1.000), but application did have an effect when participants were using the computer medium (p = 0.002) (see Fig. 3).

Participants' overall experience levels seemed to have a slight, though not statistically significant, effect on the likelihood of participants identifying usability problems (χ2 (1, N = 640) = 2.99, p = 0.084), with low experience level participants identifying 40.15% (SD = 13.85%) of usability problems and high experience level participants identifying 28.97% (SD = 10.74%) of usability problems. However, overall experience did not seem to have an effect on participants' likelihood of identifying severe usability problems (χ2 (1, N = 256) = 0.17, p = 0.667). Furthermore, participants' overall experience levels did not affect participants' PSSUQ scores (F (1,62) = 0.31, p = 0.581).

@&#DISCUSSION@&#

Most of the initial hypotheses in this study were supported by the results of the data analysis; however, the data suggest a more complex theory may be required to explain the effect of medium in usability studies.

Hypothesis one stated that users are equally likely to detect usability problems using either computer or paper mediums to test a prototype, and this was supported by the results of this study. These results were also found to be consistent with the Walker et al. (2002) and Virzi et al. (1996) studies which both had similar experimental designs to the one in this paper. These results occurred because, out of all the usability problems participants were able to find, only a few of those were significantly easier to find using a particular medium; most could be found no matter which interface medium participants used.

Hypothesis two stated that users are equally likely to detect severe usability problems using either computer or paper mediums to test a prototype, but was not supported by the results of this study. These findings were also contradictory to those of Walker et al. (2002) which indicated that deviations in medium have no effect on the subgroups of usability problems based on severity or type, but the results were in line with those of other studies (e.g., Nielsen, 1990; Sauer et al., 2010). There was a significant main effect showing that when participants used the computer medium they were able to find more severe usability problems. For both applications, the computer medium was the highest fidelity interface medium the participants used, meaning the computer was the closest medium to that of the final design.

Evidently, some aspects of the paper medium did not deliver the same experience to the participants as the computer medium did, and the participants were unable to accurately imagine what problems might be present as the fidelity increased. An interaction effect between interface medium and application indicated that this result was only true when participants were evaluating the movie rental kiosk application. This result can be explained by different applications having different sets of typical severe usability problems. Furthermore, some applications' typical severe usability problems are more easily identified using a particular interface medium while other applications' severe problems are not. For example, two severe usability problems of an online survey may be the use of confusing question wording and text coloring that that does not have appropriate contrast. For this example, these problems could be easily identified using a simple paper printout of the survey questions or a fully functioning online survey evaluated using a computer. However, a severe usability problem of a database may be its access time. This problem would be nearly impossible to detect using a series of paper printouts, and would most likely be found during evaluations using a higher fidelity prototype.

Hypothesis three stated that users' perceptions of applications' usability is unaffected by using either computer or paper mediums to test a prototype, and it was supported by the results. This result was found because participants were able to look past the medium they were using and evaluate the underlying application they were asked to test.

Hypothesis four stated that the fewer usability problems a user detects, the higher the user's perception of the application's usability will be, and this was supported by the results. This result was expected because participants' perceptions of usability are shaped, in part, by the problems they discover. If they discover fewer problems, then they will perceive an application as more usable.

While the results in this paper are not contradictory, the results do lead in two different directions. Users are able to find usability problems regardless of the interface they use, and there are only an insignificant number of usability problems that are easier to find using a particular medium. However, when looking at the particular subset of severe usability problems, users are sometimes, depending on the application, able to find more problems using a particular medium. One proposed solution is to test early using a low fidelity, cost effective interface medium to catch a significant portion of the usability problems before redesign costs increase, and subsequently test with a higher fidelity medium as the design naturally advances to a stage where a higher fidelity medium is called for.

Furthermore, users' perceptions are linked to the number of usability problems they discover, without influence from the interface medium they use. This means that if users are asked how usable they think a system is, their answers will generally be associated with the number of usability problems they would identify in a usability study. Although identifying users' perceptions of usability of an application alone will most likely not help in identifying particular usability problems to be corrected, it will provide insight into the usability status of an application that could prompt a subsequent usability study if necessary. Another issue surrounding both informal surveys, questionnaires and formal usability studies is the low percentage of usability problems users are likely able to find. This study revealed that participants were only able to identify a small portion of the total usability problems in a system in even the best cases. This can likely be attributed to the general user's lack of training and ability to consciously detect usability problems in a system during the relatively short time he or she has during a usability evaluation. This finding should encourage developers to use multiple usability testing methods while developing new interfaces in order to identify the most usability problems possible.

@&#CONCLUSIONS@&#

This study sought to examine the effects of a prototype's medium on the user's perception and ability to detect overall usability problems and severe usability problems. The study also sought to explore the relationship between the user's perception of usability and ability to detect usability problems. The results indicated that although interface medium does not affect the user's ability to detect usability problems, interface medium does sometimes affect the user's ability to detect severe usability problems, depending on the application the user is evaluating. Results also indicated that the user's perceived usability of an application is unaffected by interface medium and that as the user's perception of an application increases, his or her ability to detect usability problems decreases. A possible way to implement these results would be to evaluate applications early with lower fidelity interface mediums and evaluate later with higher fidelity interface mediums, and to utilize user surveys and questionnaires to easily measure applications' usability.

This study can naturally lead in two directions for future research. While this study did uncover an issue with an application effect, the focus was not on applications. Therefore, the first direction is one in which a large variety of applications are tested to determine which application types are sensitive to interface mediums. This route would focus on deciphering the possible cause or causes for the severe usability discrepancies between applications. The second direction would be testing one of the other Virzi et al. (1996) components of fidelity which include: breadth of features, degree of functionality, similarity of interaction, and aesthetic refinement. Of course, there are also a number of other issues surrounding public kiosk systems (e.g. glare, finger size, etc.) that were outside the scope of this study but could provide valuable results.

@&#REFERENCES@&#

