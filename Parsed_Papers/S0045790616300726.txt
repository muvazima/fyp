@&#MAIN-TITLE@&#Image retrieval by addition of spatial information based on histograms of triangular regions

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The addition of spatial information to the inverted index of the BoF representation.


                        
                        
                           
                           Image representation in the form of triangular histograms.


                        
                        
                           
                           Three different classifiers are evaluated in order to determine the best performance of the proposed work.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Content-based image retrieval

Histogram of triangles

Dense features

Image classification

Deep belief networks

Support vector machines

@&#ABSTRACT@&#


               
               
                  The compositional and content attributes of images carry information that enhances the performance of image retrieval. Standard images are constructed by following the rule of thirds that divides an image into nine equal parts by placing objects or regions of interest at the intersecting lines of the grid. An image represents regions and objects that are in a spatial semantic relationship with respect to each other. While the Bag of Features (BoF) representation is commonly used for image retrieval, it lacks spatial information. In this paper, we present two novel image representation methods based on the histograms of triangles, which add spatial information to the inverted index of BoF representation. Histograms of triangles are computed at two levels, by dividing an image into two and four triangles that are evaluated separately. Extensive experiments and comparisons conducted on two datasets demonstrate that the proposed image representations enhance the performance of image retrieval.
               
            

@&#INTRODUCTION@&#

Content-Based Image Retrieval (CBIR) provides a sustainable way to search for similar images in image archives by using the visual features of a query image [1,2]. Occlusion, overlapping objects, image resolution, variations in illumination, semantic gaps, spatial layout as well as exponential growth in multimedia archives all contribute to make CBIR a challenging problem [1,2]. In CBIR, feature vectors are used to represent images in the form of low-level visual features [1,2]. The feature vector of a query image is calculated and compared with the feature vectors of the images placed in an archive [3]. The closeness of the feature vector values determines the output [3]. The appearance of a similar view in two different images results in the closeness of feature vectors and it impairs the performance of image retrieval [3]. In an image, there is a spatial relationship between different regions of the image, yet due to the order-less representation of the image using BoF [4], this spatial information is lost. Approaches based on query expansion [5], soft quantization [6] and large vocabulary size [7] are used to enhance the performance of image retrieval. The main limitation of all these approaches is the lack of spatial information that provides discriminating details [7]. Two approaches are commonly used to add spatial information to the BoF representation [8]. The first approach uses geometric relationships [9] or the co-occurrence of visual words [10]. However, modeling these approaches in the case of large vocabulary size is computationally expensive [10]. The second approach [11] splits an image into sub-regions and constructs the histograms from each sub-region; it is considered robust for content-based image matching [11]. In view of the robust performance of the second approach, our proposed work involves extracting the histograms from triangular regions of the image.

Images are constructed by following the rule of thirds, which divides an image into nine equal grids; objects or regions of interest are placed at intersecting lines of the grid (either on the left or on the right). The rule of thirds represents compositional and content attributes of the image, and incorporating these attributes into image retrieval enhances its performance [12]. Fig. 1
                     (a)
                        1
                     
                     
                        1
                        
                           www.tympanus.net/codrops/2012/05/23/understanding-the-rule-of-thirds-in-web-design/
                        
                      represents a standard image constructed by following the rule of thirds, while Fig. 1(b) represents a possible spatial semantic solution for efficient image retrieval. In a standard image, the sky, the sun or clouds are located at the top, objects of interest are at the right or left, and the ground, grass or water are found at the bottom of the image. This sequence represents a triangular relationship between objects and regions of interest in a scene. Fig. 2
                      represents this content-based triangular relationship between different images of the Corel dataset. Discriminating objects (such as horses or elephants) and regions of interest (such as the sky, the ground or grass) are usually located in different sub-regions of the image. The construction of histograms from triangular regions of the image adds discriminating information to image retrieval in the form of objects and regions of interest that are located at the top, left, right and bottom of the image.

Inspired by this, we propose two novel methods of image representation in the form of histograms computed from the triangular regions of an image. Dense features are extracted from an image; then, the feature space is quantized and visual vocabulary is constructed in order to achieve a compact representation. An image is divided into two and four triangular regions, while histograms are computed from the triangular regions. This information is added to the inverted index of the BoF representation. Images are divided into two and four triangular regions, which are referred to as Level 1 and Level 2, respectively. The main contributions of this paper are

                        
                           (1)
                           The addition of spatial information to the inverted index of the BoF representation.

Image representation in the form of triangular histograms.

The rest of the paper is organized as follows: Section 2 reviews the related work. Section 3 provides an overview of the BoF representation and presents our proposed methodology. While Section 4 presents evaluation measures and results obtained from two challenging image datasets, Section 5 concludes our work and points towards future directions in research.

@&#RELATED WORK@&#

Query By Image Content (QBIC) is the first system launched by IBM for image search [1,2]. After that, a variety of features extraction techniques have been proposed, that are based on color, texture and shape [1,2,13,14]. Interest point detectors like Scale Invariant Feature Transform (SIFT) [15], Histogram of Oriented Gradients (HOG) [16], Speeded-Up Robust Features (SURF) [17] and Maximally Stable Extremal Regions (MSER) [18] are used in robust content-based image matching [1]. Lin et al. [19] proposed CBIR by applying a combination of color and texture. Color Histogram for K-Mean (CHKM) is applied to extract color while texture is extracted by the use of the Color Co-occurrence Matrix (CCM). The probability of the occurrence of the same pixel color and its adjacent one is calculated by the use of conventional CCM and is considered an attribute for that image. When color histograms are used, two different images with a similar color distribution result in a degradation of the image retrieval performance [2]. Lai et al. [20] proposed user-oriented image retrieval based on Interactive Genetic Algorithms (IGA). Color is represented by mean value, standard deviation and an image bitmap, while texture is extracted by applying the Gray Level Co-Occurrence Matrix (GLCM) and an edge histogram.

Spectral texture techniques like curvelet, wavelet packets and Gabor features are used in CBIR to reduce the semantic gap [3,21,22]. Youssef et al. [21] proposed an Integrated Curvelet-Based Image Retrieval Scheme (ICTEDCT-CBIR). A combination of curvelet transform at different orientations is applied with Region-Based vector codebook Subband Clustering (RBSC) for the extraction of dominant colors and texture. The curvelet transform is applied as it restores sparsity by reducing the repeatability across scales. The Most Similar Highest Priority (MSHP) principle is used to search images from the database. Irtaza et al. [3] proposed semantic image retrieval by using wavelet packets with Eigen values of Gabor features. To enhance semantic image retrieval, back propagation neural network architecture is trained on the output of Relevance Feedback (RF). A combination of wavelet packets, Gabor features and curvelet transform is used [22] for image representation. Yuan et al. [23] proposed image retrieval by applying a combination of Local Binary Pattern (LBP) and SIFT. The approach of applying a combination of SIFT and LBP aims to achieve high performance in the case of noisy background. Visual features are separately extracted from an image by using SIFT and LBP [23]. Wang et al. [24] proposed Spatial Weighting BoF (SWBoF), which uses local entropy, local variance and adjacent block distance. Tian et al. [25] proposed the rotation and scale-invariant Edge Oriented Difference Histogram (EODH). A weighted word distribution is applied by using a combination of Color-SIFT and EODH. Ashraf et al. [26] combined bandlet transform with color features and extracted the objects from the images. An Artificial Neural Network (ANN) is used as a classifier and the inverted index is applied to retrieve similar images.

In the BoF representation [4], local features are extracted on a dense scale. The extracted local features contain visual information about an image. For a compact representation of an image, the feature space is reduced to clusters by applying a quantization algorithm like k-means [4]. The cluster centers are called visual words and the combination of visual words represents the visual vocabulary. Local features are extracted from a given image, and then quantized; visual words are assigned to the image by using the Euclidean distance between the visual words and the quantized descriptor. An image is represented as a histogram of words and each bin of the histogram corresponds to the respective visual word of that image. The size of the histogram represents the size of the visual vocabulary. BoF-based order-less image representation provides flexibility to pose and viewpoint changes [4].

The basic problem of BoF is the lack of spatial information [7]. Visual words are represented in a histogram without considering their locations in the 2D image space. However, spatial information provides discriminating details in recognition problems [7]. The block diagram of the proposed work is represented in Fig. 3
                        .

                           
                              (1)
                              The BoF representation starts from a raw image I:

                                    
                                       (1)
                                       
                                          
                                             I
                                             =
                                             (
                                             
                                                a
                                                
                                                   
                                                   m
                                                   ,
                                                   n
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              

where a
                                 
                                    m, n
                                  is the pixel at the position (m,n).

Dense features are extracted from an image I and are represented as

                                    
                                       (2)
                                       
                                          
                                             I
                                             =
                                             {
                                             
                                                d
                                                1
                                             
                                             ,
                                             
                                                d
                                                2
                                             
                                             ,
                                             .
                                             .
                                             .
                                             .
                                             ,
                                             
                                                d
                                                z
                                             
                                             }
                                          
                                       
                                    
                                 
                              

Where d
                                 1 to dz
                                  are image descriptors.


                                 K-means unsupervised clustering is applied to construct a visual vocabulary voc consisting of t words, which are represented as

                                    
                                       (3)
                                       
                                          
                                             v
                                             o
                                             c
                                             =
                                             {
                                             
                                                w
                                                1
                                             
                                             ,
                                             
                                                w
                                                2
                                             
                                             ,
                                             .
                                             .
                                             .
                                             .
                                             ,
                                             
                                                w
                                                t
                                             
                                             }
                                          
                                       
                                    
                                 
                              

where w
                                 1 to wt
                                  are visual words.

Dense features are extracted from an image and quantized in the feature space. The mapping of each visual word is done over the triangular regions of an image by dividing an image into two (Level 1) and four triangles (Level 2). The nearest words are assigned to the quantized descriptors by using the following equation:

                                    
                                       (4)
                                       
                                          
                                             w
                                             
                                                (
                                                
                                                   d
                                                   k
                                                
                                                )
                                             
                                             =
                                             
                                                argmin
                                                
                                                   w
                                                   
                                                   
                                                      ɛ
                                                   
                                                   
                                                   v
                                                   o
                                                   c
                                                
                                             
                                             
                                             D
                                             i
                                             s
                                             t
                                             
                                                (
                                                w
                                                ,
                                                
                                                   d
                                                   k
                                                
                                                )
                                             
                                          
                                       
                                    
                                 where w(dk
                                 ) represents the visual word assigned to the kth
                                  descriptor dk
                                  while Dist(w, dk
                                 ) is the distance between the descriptor dk
                                  and visual word w. Each image is represented as a collection of patches and each patch is represented by a visual word.

The histograms of N visual words are extracted from each of the triangular regions of an image (Level 1 and Level 2). Two histograms of N visual words are extracted from Level 1 and four histograms of N visual words are extracted from Level 2. The histograms of triangles (Level 1 and Level 2) add spatial information to the inverted index of BoF representation. The representations of the image as histograms of the Level 1 and Level 2 triangles are shown in Fig. 4
                                 .

The histograms constructed from the triangular regions of the images are normalized, and classification is performed by using Radial Basis Function Neural Networks (RBF-NN) [27], Support Vector Machines (SVM) [28] (with Hellinger kernel [29]) and Deep Belief Networks (DBN), which consist of autoencoders [30]. Three different classifiers are selected in order to evaluate the best performance of the proposed image representations.

@&#EXPERIMENTS AND RESULTS@&#

This section provides the details of experiments conducted for the evaluation of the proposed image representations. Dense SIFT with a bin size of 8 and a step size of 10 is used for the features extraction and all the processing is performed using gray scale images. The proposed image representations are evaluated on two image datasets (Corel A and Fifteen Scene). Due to the unsupervised nature of clustering using k-means, all experiments are repeated 10 times and the average retrieval values are reported. The size of the visual vocabulary is a major parameter affecting the performance of content-based image retrieval. Increasing the size of the vocabulary increases the performance and a larger vocabulary tends to overfit [31]. Visual vocabularies of different sizes are evaluated in order to determine the best performance of the proposed image representations. For every experiment, images are randomly divided into a training dataset and a test dataset. A visual vocabulary is constructed from the training dataset and evaluated by using the test dataset. Lazebnik et al. [11] proposed Spatial Pyramid Matching (SPM), which divides an image into several rectangular grids and constructs histograms from each region of the grid. The proposed image representations based on triangular histograms are compared with rectangular histograms that are constructed by dividing an image into 2 × 2 rectangular grids. The representation of an image as rectangular histograms (2 × 2) is represented in Fig. 5
                     . The 2 × 2 rectangular grid (with four rectangular regions) is selected to perform a comparison with the histograms of Level 2 triangles (with four triangular regions).

To evaluate the performance of the proposed image representations, we determined the relevant images retrieved in response to a query image. A computer simulation is used to select images randomly from the test dataset to be used as query images. The response of the query image is evaluated on the basis of relevant images retrieved. Precision is used to determine the number of relevant images retrieved in response to a query image, and it shows the specificity of the image retrieval.

                           
                              (5)
                              
                                 
                                    P
                                    r
                                    e
                                    c
                                    i
                                    s
                                    i
                                    o
                                    n
                                    =
                                    
                                       
                                          N
                                          u
                                          m
                                          b
                                          e
                                          r
                                          
                                          o
                                          f
                                          
                                          r
                                          e
                                          l
                                          e
                                          v
                                          a
                                          n
                                          t
                                          
                                          i
                                          m
                                          a
                                          g
                                          e
                                          s
                                          
                                          r
                                          e
                                          t
                                          r
                                          i
                                          e
                                          v
                                          e
                                          d
                                       
                                       
                                          Total
                                          
                                          n
                                          u
                                          m
                                          b
                                          e
                                          r
                                          
                                          o
                                          f
                                          
                                          i
                                          m
                                          a
                                          g
                                          e
                                          s
                                          
                                          r
                                          e
                                          t
                                          r
                                          i
                                          e
                                          v
                                          e
                                          d
                                       
                                    
                                 
                              
                           
                        Recall measures the sensitivity of the image retrieval, and is calculated by the ratio of correct images retrieved to the total number of images of that class in the dataset.

                           
                              (6)
                              
                                 
                                    R
                                    e
                                    c
                                    a
                                    l
                                    l
                                    =
                                    
                                       
                                          N
                                          u
                                          m
                                          b
                                          e
                                          r
                                          
                                          o
                                          f
                                          
                                          r
                                          e
                                          l
                                          e
                                          v
                                          a
                                          n
                                          t
                                          
                                          i
                                          m
                                          a
                                          g
                                          e
                                          s
                                          
                                          r
                                          e
                                          t
                                          r
                                          i
                                          e
                                          v
                                          e
                                          d
                                       
                                       
                                          Total
                                          
                                          n
                                          u
                                          m
                                          b
                                          e
                                          r
                                          
                                          o
                                          f
                                          
                                          r
                                          e
                                          l
                                          e
                                          v
                                          a
                                          n
                                          t
                                          
                                          i
                                          m
                                          a
                                          g
                                          e
                                          s
                                       
                                    
                                 
                              
                           
                        
                     

The Corel A image dataset [32] is used for the evaluation of the proposed image representations. The results are compared with state-of-the-art research [3,24,26]. The Corel A image dataset contains 1000 images that are divided into ten semantic categories, namely: Africa, Buildings, Beach, Dinosaurs, Buses, Elephants, Horses, Flowers, Mountains and Food. Fig. 6
                         shows the images from all of the categories from the Corel A image dataset. Each semantic category consists of 100 images with a resolution of 256 × 384 pixels or 384 × 256 pixels. In order to obtain a sustainable performance, we test various sizes of vocabulary [ 50, 100, 150, 200, 300, 400, 500 ]. The best mean average precision is obtained from the proposed image representations on a vocabulary with a size of 200. The results are reported for a random selection of 300 images from the test dataset. The average retrieval precision for the top 20 retrievals is calculated by using DBN, SVM and RBF-NN and is shown in Table 1
                        . The mean average precision obtained from each of the classifiers is graphically represented in Fig. 7
                        .

According to the experimental results, the best mean average precision is obtained from the proposed image representation (Level 2) by using DBN with a value of 87.65%. The best mean average precision obtained by using SVM and RBF-NN (Level 2) is 86.27% and 84.87% respectively. The precision and recall values of the proposed image representation (Level 2) for the top 20 retrievals calculated by using DNN and SVM are compared with the results of state-of-the-art research [3,24,26]. The average precision and recall values obtained from the proposed image representations are shown in Table 2
                         and Table 3
                        , respectively.

The experimental results obtained from the Corel A image dataset and the comparisons made with them prove the robustness of the proposed image representations based on spatial triangular histograms. The comparison of mean average precision is graphically represented in Fig. 8
                        . The image retrieval results obtained from the triangular histograms (Level 2) for the semantic classes of “Buses” and “Buildings” are represented in Figs. 9
                         and 10
                        , respectively.

There are fifteen semantic classes in the Fifteen Scene image dataset and each class contains 200–400 images. There are a total of 4485 images with an average resolution of 300 × 250 pixels. The images are divided into the semantic categories of Office, Kitchen, Living Room, Bedroom, Store, Industrial, Tall Building, Inside City, Street, Highway, Coast, Open Country, Mountain, Forest and Suburb (shown in Fig. 11
                        ). The mean average precision values obtained from the proposed image representations are compared with rectangular histograms (2 × 2) and the standard BoF representation.

The results are reported for a random selection of 1352 images from different classes of test dataset. Various sizes of visual vocabularies are constructed [ 100, 200, 300, 400, 800, 1000, 1500 ] and the mean average precision on vocabularies of different sizes is calculated. The best mean average precision is obtained from the proposed image representations on a vocabulary with a size of 800, which is graphically represented in Fig. 12
                        . The experimental results obtained from the Fifteen Scene category image dataset and the comparisons made with them prove the robustness of the proposed image representations based on triangular histograms. The histograms of triangles (Level 2) outperform the rectangular histograms (2 × 2) while the histograms of triangles (Level 1) outperform the standard BoF representation. The best mean average precision obtained from the proposed image representation (Level 2) using DBN and SVM is 79.7% and 77%, respectively.

The algorithms are computed using Intel(R) Core i7 (third generation) 2.4 GHz CPU, 8 GB RAM, 2 GB dedicated Graphic Processing Unit (GPU) and the Windows-7 operating system. The proposed algorithms are implemented in MATLAB and the visual vocabulary (codebook) is constructed offline using a training dataset and tested at run-time using a test dataset. The average CPU time (in seconds) required from features extraction to image retrieval is presented in Table 4
                        .

In this paper, we proposed two novel image representations based on spatial triangular histograms (Level 1 and Level 2) that add spatial information to the inverted index of BoF representation. Semantic information is available at the top, right, left and bottom of the image. Constructing histograms from triangular image areas is a possible solution for reducing the semantic gap and adding the spatial attributes of images to image retrieval. The proposed image representations are evaluated by using two challenging image datasets. Three different classifiers are selected in order to evaluate the best performance of the proposed image representations. The performance of the proposed image representations is compared with existing state-of-the-art research, including rectangular histograms (2 × 2). The use of triangular histograms (Level 2) with DBN is found to be robust and to outperform the state-of-the-art techniques, including rectangular histograms (2 × 2) and the standard BoF representation. In the future, we will extend our work to large-scale image retrieval by using the adopted triangular approach for features extraction.

@&#ACKNOWLEDGMENTS@&#

We are thankful to the Higher Education Commission (HEC) of Pakistan for a fellowship grant (PIN No. IRSIP 28 ENGG 03 sanctioned in favor of Nouman Ali) which enabled the research to be carried out at the Institute of Computer Aided Automation, Computer Vision Lab, Vienna University of Technology, Austria. We are also thankful to Amy Bruno-Lindner, University of Vienna, Austria for her valuable suggestions during the writing of the paper.

@&#REFERENCES@&#

