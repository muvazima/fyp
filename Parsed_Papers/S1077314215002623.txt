@&#MAIN-TITLE@&#Learning object-specific DAGs for multi-label material recognition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present a multi-label material recognition framework.


                        
                        
                           
                           Object-specific DAGs are better to encode the correlations of material labels.


                        
                        
                           
                           Object recognition can provide semantic cue to enhance the material recognition.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Material recognition

Object recognition

Multi-label learning

Graph model

@&#ABSTRACT@&#


               
               
                  A real-world object surface often consists of multiple materials. Recognizing surface materials is important because it significantly benefits understanding the quality and functionality of the object. However, identifying multiple materials on a surface from a single photograph is very challenging because different materials are often interweaved together and hard to be segmented for separate identification. To address this problem, we present a multi-label learning framework for identifying multiple materials of a real-world object surface without a segmentation for each of them. We find that there are potential correlations between materials and that correlations are relevant to object category. For example, a surface of monitor likely consists of plastic and glasses rather than wood or stone. It motivates us to learn the correlations of material labels locally on each semantic object cluster. To this end, samples are semantically grouped according to their object categories. For each group of samples, we employ a Directed Acyclic Graph (DAG) to encode the conditional dependencies of material labels. These object-specific DAGs are then used for assisting the inference of surface materials. The key enabler of the proposed method is that the object recognition provides a semantic cue for material recognition by formulating an object-specific DAG learning. We test our method on the ALOT database and show consistent improvements over the state-of-the-arts.
               
            

@&#INTRODUCTION@&#

Recognizing materials (plastic, wood, metal, etc.) enables us to understand and introduce novel objects or scenes more conveniently. For example, identifying the materials which an object is make of helps to reason about the physical properties of that object [1]. Automatic material recognition is also potentially helpful in a wide range of computer vision applications, such as image recoloring, image-based object modeling and scene reconstruction. Recently, materials have attracted increasing attention gradually in the visual recognition literatures. To identify materials from photographs, existing researches mainly focus on extracting distinguishable features for describing materials [2–7]. However, unlike other visual recognition tasks such as object recognition and texture recognition, it is difficult to find good and reliable features that can tell material categories apart [8]. The experimental study also reveals that material categorization is less accurate than object categorization [9].

So far, existing literatures only take a single material per photograph into account [2–8,10,11]. However, the surface of a real world object (especially the man-made object) often consists of multiple materials. For example, a drum is always made up of leather, wood, and metal, and a stapler’s surface contains plastic and metal, which are illustrated in Fig. 1. We are therefore interested in considering the surface which contains multiple materials. Specifically, we aim to identify multiple materials on an object surface from a single photograph where the photograph covers a whole object. We treat this problem as a multi-label material recognition problem. Due to the diverse shapes as well as the complex surface textures of real-world objects, it is hard to segment different materials on the object surface and then identify them separately. Therefore, we introduce a multi-label learning framework to deal with this issue, where each sample (photograph) is associated with a set of labels (multiple materials).

Surface materials are always related to the quality and functionality of an object, so there are strong relationships between material categories and object categories. For example, a surface of monitor likely consists of plastic and glasses rather than wood or stone. This observation motivate us to use object identity as semantic cues for material recognition. In view of the above, we wish to exploit the connections between material categories and object categories, and give insight of the relationship between object recognition and material recognition. In this paper, we want to address the following questions: (1) Is object recognition helpful for enhancing material identification in the multi-label case? (2) How to effectively utilize object recognition for assisting multi-label material recognition? (3) How well object recognition performance is required in our approach?

For the above purpose, we present a multi-label learning framework for recognizing multiple materials of an object surface by exploiting label correlations locally. The localization is specified by grouping samples semantically according to their object labels. For each group of samples, we employ a Bayesian network structure (or Directed Acyclic Graph, DAG) to encode the conditional dependencies of the labels. Relying on the object-specific DAGs, a scheme of decomposing the multi-label classification problem into a set of binary classification problems (one for each possible label) meanwhile utilizing the label correlations for multi-label material recognition is developed. Experimental results will reveal that using the object-specific DAGs outperforms using a global DAG for identifying materials in multi-label case.

@&#RELATED WORK@&#


                     Material recognition from photographs. Many techniques have been developed to search for specific materials in real world photographs such as transparent glasses [12,13] and metal [14]. Also, some methods have been proposed to identify general multi-class materials [2–7,15]. Existing approaches mainly focus on extracting distinguishable features for representing materials. Specially, through characterizing materials by bidirectional reflectance distribution functions (BRDF), many methods address the problem of recognizing materials from photographs mainly in the context of surface reflectance estimation or classification [16–19]. Another feasible approach for material recognition is to recognize textures [4,20]. Both surface reflectance properties and textures are often correlated with material categories. However, surfaces made of different materials would lead to similar reflectance properties as well as similar textures. Therefore, the systems designed for surface reflectance recognition or texture recognition may not be adequate for material category recognition [11]. Accordingly, Liu et al. [8,11] proposed using a rich set of low and mid-level image features for material recognition. Gu and Liu proposed using coded illumination to directly measure discriminative features for material classification [10]. Weinmann et al. [15] proposed synthesizing training material samples under different lighting conditions to cope with the richness in appearance variation found in real-world material classification. In previous literatures, each concerned photograph contains only a single type of material. In this paper, we consider the case that each photograph contains a real-word object composed of multiple materials.


                     Connections between object recognition and material recognition. In spite of the difference between material recognition and object recognition, their correlations have drawn some researchers’ attention. For example, Mannan et al. [21] proposed a material information acquisition method for interactive 3D object recognition, where the reflection pattern of infrared light is used to estimate the object material by utilizing a ToF range sensor. Hu et al.  [22] further utilized the outputs from an object recognizer as input to the material recognizer so as to improve material recognition accuracy. They also pointed out that material information does not help object recognition much. Recently, Sharan et al. [11] has suggested that the future progress in material recognition will come from modeling the non-local aspects of surface appearance (e.g., object identity). Motivated by previous work, we focus on investigating the help of object recognition on material recognition in the multi-label case. Recently, we present another novel model for addressing this problem [23].


                     Multi-label learning. In multi-label learning, each sample is associated with a set of labels and the task is to find a mapping from the feature space to the space of label sets. The main challenge of multi-label learning is caused by the tremendous (exponential) number of possible label sets. One ordinary approach is to decompose the multi-label learning into a set of independent binary classification problems, one for each possible label [24,25]. Since there are often strong co-occurrence patterns and dependencies among the class labels, the correlations between different labels are always exploited to facilitate multi-label learning, such as considering the pairwise relations between labels [26,27], recovering the full-order style of imposing the relationships between each label and all other labels [28,29], or addressing correlations among a random subset of labels [30]. For exploiting the correlations between labels, graphical models such as the conditional random field (CRF) [31] and the directed acyclic graph (DAG) [32] often play an important role.

Under the observation that label correlations are usually shared by subsets of instances in real world multi-label learning tasks, Huang and Zhou proposed a multi-label learning by exploiting label correlations locally (ML-LOC) [33]. Specifically, to group samples, they proposed measuring similarity between instances in the label space.

Existing approaches are addressing general multi-label learning problem, but are not able to utilize the special properties of material distribution on surfaces as well as the connection between material categories and object categories. In comparison, our methods exploit these cues in formulating learning schemes.

Let Xo
                         and Xm
                         be the feature spaces associated with the prediction of objects and materials, respectively. Denote 
                           
                              
                                 Y
                                 o
                              
                              =
                              
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 
                                    L
                                    o
                                 
                                 }
                              
                           
                         and 
                           
                              
                                 Y
                                 m
                              
                              ⊂
                              
                                 
                                    {
                                    −
                                    1
                                    ,
                                    +
                                    1
                                    }
                                 
                                 
                                    L
                                    m
                                 
                              
                           
                         as the label spaces of object and material, respectively. In our framework, each instance is characterized by (xo, xm, yo, ym
                        ) with xo
                         ∈ Xo, xm
                         ∈ Xm, yo
                         ∈ Yo
                        , and 
                           
                              
                                 y
                                 m
                              
                              =
                              
                                 [
                                 
                                    y
                                    1
                                    m
                                 
                                 ,
                                 …
                                 ,
                                 
                                    y
                                    
                                       L
                                       m
                                    
                                    m
                                 
                                 ]
                              
                              ∈
                              
                                 Y
                                 m
                              
                              ,
                           
                         where ym
                         indicates whether this instance is associated with the kth material label (
                           
                              
                                 y
                                 k
                                 m
                              
                              =
                              1
                           
                        ) or not (
                           
                              
                                 y
                                 k
                                 m
                              
                              =
                              0
                           
                        ). We call xo, xm, yo
                        , and ym
                         as object feature, material feature, object label, and material label vector, respectively.
                     

Our goal is to learn a function fm
                        : Xm
                         → Ym
                         which predicts the material label vector for unseen instances. When considering the sample clustering based on the object label, fm
                         may be composed of a group of object-specific material prediction functions 
                           
                              
                                 f
                                 l
                                 m
                              
                              :
                              
                                 X
                                 m
                              
                              →
                              
                                 Y
                                 m
                              
                              ,
                              l
                              =
                              1
                              ,
                              …
                              ,
                              
                                 N
                                 G
                              
                           
                         with each one associated to one sample cluster, where NG
                         is the number of sample clusters. To support the above approach, an automatic object recognition is required, and therefore the formulated scheme should be tolerant to the error of object identification in some degree.

As aforementioned in the introduction, we come up with an approach to constructing 
                           
                              f
                              l
                              m
                           
                         by exploiting the correlations of labels by Bayesian network. Fig. 2 illustrates the proposed method. Relying on an object identification, the material feature together with the object-specific DAG is used to infer the surface materials of an object. In the following, we first describe how to group objects for multi-label learning and detail our approaches later.

The original sample labeling manner in the ALOT database gives a very detailed categorization of objects. However, such a fine object categorization is unnecessary for our issue, because (1) many object categories hold identical material distribution; (2) too detailed categorization may cause an over learning problem. To overcome these problems, we propose to reorganize a label space by grouping some original categories in the ALOT database, yielding a hierarchical (two-layer) structure for object label space as illustrated in Fig. 3
                        . The first-layer label space is generated by the basic categories used in the ALOT database (e.g., the labels “spoon”, “pan”, “monitor”, and “cellphone” ). The second-layer label space is generated by a semantical manual clustering from the first-layer, e.g., {“spoon”, ⋅⋅⋅, “pan”} → “kitchenware” and {“monitor”, ⋅⋅⋅, “cellphone”} → “digital production”.

It is more beneficial to learn the material correlations locally on each object category in the second-layer object label space than that in the first-layer label space. However, it is more difficult to learn a predictor directly from feature space to the second-layer label space than that to the first-layer one, even though the number of categories in the second-layer space is smaller. For example, it is more difficult to identify a kitchenware than to identify a spoon by using a single-layer classifier. The main reason is that as compared to the first-layer label space, the second-layer label space actually enlarges samples’ intra-class variations, since it holds multi-mode distribution for a category of samples and thus it makes the object categorization more difficult.

We have conducted an experiment to verify this statement. We trained two classifiers associated with the first-layer and the second-layer label spaces from the feature space Xo
                        , respectively, by using the linear SVM [34] along with using bag of word (BOW) [35] on SIFT features [36]. In our experiment, the numbers of labels in the first-layer and the second-layer spaces are 221 and 21, respectively. Totally 949 object instances (two images per object instance) were used in the experiment, half for training and the rest for testing, where an object instance occurs in either training set or testing set. The rank-1 recognition rates are 76.6% and 72.1%, respectively, for the first-layer and the second-layer spaces.

To address this problem, we construct a two-layer classifier corresponding to the hierarchical structure of object labels. For each category in the first-layer label space, we define a mapping to map a particular category to a label in the second-layer label space, which is an off-line operation. To be specific, we define a single-valued mapping (denoted as A) from the first-layer label space to the second-layer one by a manual clustering. We also learn a classifier fo
                        : Xo
                         → Yo
                         from the feature space Xo
                         to the first-layer object label space. When the object classification result for an unseen sample in the first-layer label space is obtained, the corresponding category in the second-layer label space, which is the final classification result, can be simply obtained by looking up the pre-defined mapping. The Fig. 3 illustrates the composite classifier A○f o
                        . The classifier f o
                         is built by the SVM under BOW framework on SIFT features. In our experiment, the rank-1 recognition rate of A○f o
                         is 79.3%, which is significantly higher than that of using direct mapping from feature space to the second-layer space (i.e., 72.1%). When using the non-linear SVM based on BRF kernel instead of the linear SVM, the corresponding recognition rates are 83.6% and 80.8%, respectively.

Note that the manual clustering used in our approach is an off-line process and our method is still completely automatic for inferring materials on an unseen sample.

For facilitating the multi-label learning, one practicable approach is to decompose the multi-label classification problem into a set of binary classification problems with each for a possible label. Furthermore, exploiting correlations between labels is often helpful for enhancing the classification performance. Huang and Zhou claim that label correlations are usually shared by subsets of instances [33], which means that the label correlations should be learned locally. In our method, the localization is specified by grouping samples semantically according to their object labels (see Fig. 2) 
                           1
                        
                        
                           1
                           When using Huang and Zhou’s method[33] directly in our problem, the localization should be specified based on the material labels of samples.
                        . This approach takes the advantage that real-world objects from a semantic category often consist of a subset of materials. For example, a “digital production” is likely made of plastic, metal, and glass. Our experiment will also verify that object label is the optimal localization reference compared with object feature and material label. We use the method introduced in Section 3.2 to build the semantic object group and to assign each unseen sample.

Mathematically, building the classifier fm
                        : Xm
                         → Ym
                         for multi-label material recognition is to model and predict p(ym
                        |xm
                        ), where 
                           
                              
                                 y
                                 m
                              
                              =
                              
                                 
                                    {
                                    
                                       y
                                       k
                                       m
                                    
                                    }
                                 
                                 
                                    k
                                    =
                                    1
                                 
                                 
                                    L
                                    m
                                 
                              
                           
                         denote a set of material labels. We use the Bayesian network (or directed acyclic graph, DAG) to encode the conditional dependencies among material labels 
                           
                              
                                 y
                                 k
                                 m
                              
                              ,
                              k
                              =
                              1
                              ,
                              …
                              ,
                              
                                 L
                                 m
                              
                           
                        . These dependencies may make the prediction of label combination more stable. Denote pak
                         as the parents of the label 
                           
                              
                                 y
                                 k
                                 m
                              
                              ,
                           
                         we have

                           
                              (1)
                              
                                 
                                    p
                                    
                                       (
                                       
                                          y
                                          m
                                       
                                       |
                                       
                                          x
                                          m
                                       
                                       )
                                    
                                    =
                                    
                                       ∏
                                       
                                          k
                                          =
                                          1
                                       
                                       
                                          L
                                          m
                                       
                                    
                                    p
                                    
                                       (
                                       
                                          y
                                          k
                                          m
                                       
                                       |
                                       p
                                       
                                          a
                                          k
                                       
                                       ,
                                       
                                          x
                                          m
                                       
                                       )
                                    
                                    .
                                 
                              
                           
                        
                     

In this way, the multi-label classification problem is decomposed into a set of single-label binary classification problems, where a classifier is constructed for each label by incorporating its parent’s labels as additional features. Note that since the the parents pak
                         denotes the relationship between 
                           
                              y
                              k
                              m
                           
                         and other labels, the function implies the conditional dependency between labels. In practice, we use the BDAGL (Bayesian DAG learning) package
                           2
                        
                        
                           2
                           
                              http://www.cs.ubc.ca/~murphyk/Software/BDAGL/index.html
                           
                         for learning the Bayesian network structures among labels in each cluster. We denote the learned Bayesian network structures of the lth cluster as Gl
                        , 
                           
                              l
                              =
                              1
                              ,
                              ⋯
                              ,
                              
                                 N
                                 G
                              
                           
                        .

Examples of the learned DAGs from different object clusters are illustrated in Fig. 4
                        . For a comparison, the DAG learned from the whole training samples (i.e., over all object clusters) is also shown in Fig. 4(a). As shown, different DAGs hold distinctive structures. This further supports exploiting the label correlations locally relevant to object clusters.

When using the BDAGL package to learn a DAG based on a specified label space and a given training data set, where each node in the network corresponds to a label, then the learned DAG will not contain any isolated node, even though there exists a label that is not associated to any sample in the training data set. In our implementation, we use the whole material labels in our database for learning the DAG on each object cluster, and therefore a label that does not appear in the samples of a particular object cluster may be also connected to other labels in the learned DAG for that object cluster. For example, the material “candle” does not appear in the object cluster of “box”, but there is still a link between “candle” and “wire” in the DAG of “box” cluster. This observation indicates that a link between two labels in the DAG does not necessarily stand for the co-occurrence between them, but it implies an inherently conditional dependence between them.

Relying on the learned DAGs, a scheme by utilizing the label correlations for multi-label material recognition can be developed. For each label 
                           
                              
                                 y
                                 k
                                 m
                              
                              ,
                              k
                              =
                              1
                              ,
                              …
                              ,
                              
                                 L
                                 m
                              
                              ,
                           
                         the binary classifiers 
                           
                              
                                 f
                                 
                                    l
                                    ,
                                    k
                                 
                              
                              ,
                              l
                              =
                              1
                              ,
                              ⋯
                              ,
                              
                                 N
                                 G
                              
                           
                         are constructed by utilising the feature set 
                           
                              
                                 x
                                 m
                              
                              ⋃
                              p
                              
                                 a
                                 
                                    l
                                    ,
                                    k
                                 
                              
                              ,
                           
                         where pa
                        
                           l, k
                         denotes the parents of the label 
                           
                              y
                              k
                              m
                           
                         in the lth network Gl
                        . In practice, the SVM is used to learn the classifiers.

For an unseen sample, if it is assigned to the l
                        *th sample cluster, i.e., 
                           
                              
                                 l
                                 *
                              
                              =
                              A
                              ∘
                              
                                 f
                                 o
                              
                              
                                 (
                                 
                                    x
                                    o
                                 
                                 )
                              
                              ,
                           
                         we will employ the classifier set 
                           
                              
                                 f
                                 
                                    l
                                    *
                                 
                              
                              =
                              
                                 
                                    {
                                    
                                       f
                                       
                                          
                                             l
                                             *
                                          
                                          ,
                                          k
                                       
                                    
                                    }
                                 
                                 
                                    k
                                    =
                                    1
                                 
                                 
                                    L
                                    m
                                 
                              
                           
                         associated to 
                           
                              G
                              
                                 l
                                 *
                              
                           
                         for material identification. Specifically, the label combination of material, ym
                        , can be recognized by recursively predicting 
                           
                              y
                              k
                              m
                           
                         using the classifier 
                           
                              f
                              
                                 
                                    l
                                    *
                                 
                                 ,
                                 k
                              
                           
                         based on feature set 
                           
                              
                                 x
                                 m
                              
                              ⋃
                              p
                              
                                 a
                                 
                                    
                                       l
                                       *
                                    
                                    ,
                                    k
                                 
                              
                           
                         according to the ordering of the labels implied in 
                           
                              G
                              
                                 l
                                 *
                              
                           
                        .

The presented material recognition scheme directly learns the correlations among material labels but ignores the inherent dependence of all labels on the feature set xm
                        . In other words, the xm
                         should have been the common parent of all labels in the network structure. However, we have to discard such a parent node, and otherwise the building of DAG is an ill-posed problem. Fortunately, such an operation is often acceptable in practice. Actually, eliminating the influences of feature set on all labels is still an unsolved problem. To address this problem, Zhang et al. [32] suggested to approximately model the label correlations by learning a Bayesian network on the prediction errors of a group of binary classifications. However, even prediction errors are not completely independent of the feature set. Furthermore, the estimation of prediction errors may be sensitive to training samples, which may incur an over-fitting problem when the training samples in a certain class are inadequate. For the above reasons, we still learn the DAGs directly on labels. In the experiment section, we will show that these two manners, i.e., learning DAG on labels and learning on errors, respectively, indeed reach comparable performances in our issue.

The proposed approach relies on an automatic object recognition. The accuracy of object identification will certainly affect the performance of material recognition. However, it should be aware of that an incorrect object identification does not necessarily lead to a fully wrong prediction of materials because: (i) the DAGs built on different sample clusters may still share common subgraphs, and (ii) besides the label correlations, the input sample feature xm
                         also contributes to the prediction result.

We have conducted experiment to evaluate the proposed method for material recognition in the multi-label case. The experiment is designed to answer the following questions: (1) Is object category information helpful for material recognition? (2) Does the proposed approach effectively exploit the cues of object category for material recognition? (3) How is the influence of object recognition accuracy on the proposed method for material recognition?

All experiments were done on the Amsterdam Library of Textures (ALOT) [5], which is a color image collection of real world objects with variations in viewing angle, illumination angle, and illumination color. By excluding the images containing abnormal objects without providing object label or material label, we selected an subset of images from ALOT to use in our experiment. The image set contains 1,898 images from 949 object instances, with two images (under different illumination conditions) for each instance. All images were resized to 500 × 500 pixels. All objects are from 221 categories (in the first-layer label space), and were further grouped into 21 clusters (for the second-layer label space in Fig. 3).

Each image contains one object that holds one or more than one type of materials. Totally, there are 15 types of natural materials in the image set. The example images are shown in Fig. 5
                        , and the detailed information about image set are listed in the Table 1. For each object category, we randomly selected half of object instances for training and the rest for testing (both two images of one object instance occur in couples either for training or for testing). Note that the training set was shared by the learning for object classifier as well as the learning for material classifiers.

For both object identification and material recognition, we used the bag of word (BOW) [35] framework coding on SIFT features [36] for feature representation. In fact, SIFT has been verified a very effective feature among usual features that can be potentially useful for material recognition (e.g., color, SIFT, jet, micro-SIFT, micro-jet, curvature, edge-slice, and edgeribbon) [8]. The number of visual words in the BOW framework was set to 300. The Chi2 distance was used for matching the histograms that were produced by the BOW coding. The LibSVM [34] was used to learn the SVM classifiers. Specifically, we adopted the linear kernel for object recognition in order to obtain a fast computation, and employed the RBF kernel for material recognition so as to take the same experimental settings as in the compared methods.

To evaluate the performance of the related approaches, we adopted five popular multi-label evaluation criteria: hamming loss (hloss), 1-error, coverage, ranking loss (rloss), and average precision (aveprec). These criteria measure the performance from different aspects (detailed definitions can be found in [37,38]). For the average precision, the larger value the better performance is; but for the other four criteria, the smaller the better is.

Some state-of-the-art multi-label learning methods, including the ML-kNN[25], the ML-LOC [33], B-SVM[24], LEAD[32], and the LIFT [39] were carried out for comparisons. Specially, the B-SVM [25] is a baseline method to decompose a multi-label classification problem into a set of single-label binary classification problems without considering label dependencies. We implemented the LEAD algorithm in three manners, (i) using a random DAG, (ii) learning the DAG on material labels, and (iii) learning the DAG on the prediction errors of different material categories. Each algorithm was run 10 times to compute the means and standard deviations of recognition accuracies. The recognition results of different schemes are shown in Table 2
                        . As shown, our methods get relatively better performance than other compared methods. It is surprising that both ML-LOC and ML-kNN, which are classical multi-label learning methods to exploit the label correlations, attain worse performance than the B-SVM. This suggests that how to exploit valid label correlations for a particular task is very important. The experimental results show that the LEAD using DAG learnt on prediction errors attain comparable results with that using DAG learnt on labels. This indeed supports our learning DAGs directly on material labels. However, the experiments also show that the LEAD using the globally learned DAG attains very limited improvement compared with the one using a random DAG, which indicates that the global DAG makes little contribution to the multi-label material recognition problem.

Overall, the experimental results have revealed that the object recognition is very helpful for material recognition. Furthermore, to take full advantage of the information of object categories, a semantic analysis on objects should be introduced for exploiting stable correlations between material labels. In this experiment, the automatic object recognition was used in our approach. Later experimental results will show that better performance can be attained by our methods when the ground-truth object category of testing samples have been known (i.e., the object recognition accuracy is 100%).

The proposed method depends on the localization of samples (i.e., dividing samples into groups). In this experiment, we investigate the influence of grouping reference on our material-recognition approach. Besides using object labels as grouping reference, we alternately implemented our framework by using other grouping references, i.e., using the material label and the object feature, respectively. The material-label space based clustering approach is like the grouping method used in [33]. Specifically, clusters were generated by performing k-means algorithm on training samples, using the similarity between label sets of training samples as metric. Each testing sample was assigned to a cluster according to the similarities between cluster centers and testing sample based on their material features since the material labels of testing samples are still unseen. The object-feature space based clustering and assigning were achieved by k-means algorithm using the similarity based on object features. When using material labels or object features as the grouping reference, we have tried various numbers of clusters in order to obtain the best performance of material recognition. As a result, the optimal number of clusters for using material labels is 5 and 30 for using object features. We also wish to see whether local learning in our multi-label material-object recognition approach is better than the global one, and hence we simplify the proposed methods to make the classifiers learned over all samples (i.e., learning multi-label classifiers globally) for a more comprehensive investigation.


                        Table 3
                         shows the comparison result. As shown, by introducing the semantic grouping on samples based on object labels, the performances of material recognition can be improved obviously as compared to that without grouping. However, when specifying the localization of samples according to material labels or object features, the performances of the proposed method descend, in most cases even become worse than the global approaches.

In fact, when using the object feature for grouping reference, the method is completely based on the image features with no extra information. This method actually assumes that the samples with similar object features hold similar material correlations. However, this assumption is not always correct. For example, a box and a bus have similar shape and may have similar object features as well in corresponding images, but these two kinds of objects obviously hold different material distributions. For the method using the material label to specify the localization, many clusters may hold multi-mode distributions in the feature space. In this case, automatically assigning a test sample to a cluster is indeed a very challenging task, which may cause inaccurate cluster assignments so that an incorrect material recognition is conducted. Our method lies in the fact that object recognition is more accurate than material recognition [9], and hence utilizes the object recognition to enhance material recognition. When using the object label to specify the localization for multi-label learning on materials, our method actually brings in the object-material semantic relationships which are extra prior information beyond the image features. This is the main reason why the object label based grouping manner works better than other ones.

The last experiment has revealed that our methods using semantic object label for grouping can attain better performance for material recognition than other compared methods. In this experiment, we study the impact of object recognition accuracy on our method for material recognition. To this end, we simulate the object classifier A○fo
                         by randomly assigning the testing samples to object clusters under different accuracy rates, and then obtained the corresponding results of material recognition. All random tests were repeated 10 times to produce the mean and standard deviation of recognition results. Note that the situation of using the maximal information of object category in our methods can be investigated by assuming that all object instances are recognized correctly.

The recognition results are shown in Table 4
                        . As shown, when the object recognition error rate is zero, which corresponds to the situation of using maximal information of object category in our methods, the best recognition results for material recognition can be obtained. The performances of our method drop when the error rate of object recognition increases. In general, the results of the proposed method are comparable to other state-of-the-art methods (see Table 2) when the error rate of object recognition is smaller than 40%.


                        Table 2 also shows the results of our method using an actual classifier for object recognition, where the error rate of object assignment is 20.7%. As shown, the material recognition results under this actual object classifier are close to that under a simulated object classifier with a recognition error rate of 20%. Such a result supports the reasonability of our simulation for object classification. As conclusions, the automatic object recognition is indeed able to exploit useful object information for enhancing material recognition, and a more accurate object classifier can more significantly improve the performance of the proposed method.

@&#CONCLUSION@&#

We have introduced a novel approach of applying the graphical model to multi-label material recognition. Specifically, a semantic analysis on object categories provides a logical partition on the samples, and then correlations between material labels are learned locally by object-specific DAGs. To our best knowledge, this is the first work towards addressing the problem of multi-label material identification from a single image. Rather than simply applying existing multi-label learning methods to material recognition, the main objective of our research work here is to utilize the connection between material categories and object categories. In such a scheme, more stable correlations between materials can be obtained to enhance the multi-label material recognition performance.

Our experimental results show that object recognition plays as a contextual cue for promoting the performance of material recognition. In the future, it is necessary to develop a framework that integrates features learning simultaneously for both material recognition and object recognition. Also, a more advanced graphical model may be used to integrate “material-material”, “object-material”, and “object-object” correlations in complex scenarios.

@&#ACKNOWLEDGMENTS@&#

This work was supported partially by the Natural Science Foundation of China (Grant no. 61202223, 61522115), the Natural Science Foundation of Guangdong Province, China (Grant no. 2015A030311047), and Guangzhou Pearl River Science and Technology Rising Star Project (No. 2013J2200068). This research was partly supported by Guangdong Provincial Government of China through the Computational Science Innovative Research Team Program.

@&#REFERENCES@&#

