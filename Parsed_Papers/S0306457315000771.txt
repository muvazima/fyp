@&#MAIN-TITLE@&#Summarization of changes in dynamic text collections using Latent Dirichlet Allocation model

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The summarization of changes addresses a new challenge – the automatic summarization of changes in dynamic text collections.


                        
                        
                           
                           Four different approaches are proposed for the summarization of changes.


                        
                        
                           
                           A system based on Latent Dirichlet Allocation model is used to find the hidden topic structures of changes.


                        
                        
                           
                           The approach based on LDA model outperforms all the other approaches.


                        
                        
                           
                           The differences in ROUGE scores for LDA based approach is statistically significant at 99% over baseline.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Changes summarization

Temporal term weighting

Sentence ranking

Latent Dirichlet Allocation

Wikipedia

@&#ABSTRACT@&#


               
               
                  In the area of Information Retrieval, the task of automatic text summarization usually assumes a static underlying collection of documents, disregarding the temporal dimension of each document. However, in real world settings, collections and individual documents rarely stay unchanged over time. The World Wide Web is a prime example of a collection where information changes both frequently and significantly over time, with documents being added, modified or just deleted at different times. In this context, previous work addressing the summarization of web documents has simply discarded the dynamic nature of the web, considering only the latest published version of each individual document. This paper proposes and addresses a new challenge - the automatic summarization of changes in dynamic text collections. In standard text summarization, retrieval techniques present a summary to the user by capturing the major points expressed in the most recent version of an entire document in a condensed form. In this new task, the goal is to obtain a summary that describes the most significant changes made to a document during a given period. In other words, the idea is to have a summary of the revisions made to a document over a specific period of time. This paper proposes different approaches to generate summaries using extractive summarization techniques. First, individual terms are scored and then this information is used to rank and select sentences to produce the final summary. A system based on Latent Dirichlet Allocation model (LDA) is used to find the hidden topic structures of changes. The purpose of using the LDA model is to identify separate topics where the changed terms from each topic are likely to carry at least one significant change. The different approaches are then compared with the previous work in this area. A collection of articles from Wikipedia, including their revision history, is used to evaluate the proposed system. For each article, a temporal interval and a reference summary from the article’s content are selected manually. The articles and intervals in which a significant event occurred are carefully selected. The summaries produced by each of the approaches are evaluated comparatively to the manual summaries using ROUGE metrics. It is observed that the approach using the LDA model outperforms all the other approaches. Statistical tests reveal that the differences in ROUGE scores for the LDA-based approach is statistically significant at 99% over baseline.
               
            

@&#INTRODUCTION@&#

In the area of Information Retrieval, it is recognized that retrieval from dynamic text collections on the web brings several new research challenges (Allan, Croft, Moffat, & Sanderson, 2012). Web pages are continually added, removed, or edited, resulting in active collections of documents that are always being modified. It is common to observe a high rate of changes as a consequence of the occurrence of real-world events. However, there are also modifications to documents which are generic, namely those resulting from minor revisions or additions/modifications of outdated information. This paper addresses a problem that gains relevance in this context—the summarization of changes.

The summarization of changes can be described as follows: given an evolving document collection and a temporal period, generate a summary of significant alterations made to the collection of documents during that period. Wikipedia is a prime example of a dynamic collection, with clearly identified documents—the articles, whose evolution in time can be seen in the revision history. When searching “Pete Seeger” on Wikipedia, for example, it is possible to access the history of collaborative editing for the corresponding article, as all revisions of this article are stored. In order to obtain the summary of changes to “Pete Seeger” within the time range “January 2014”, we would expect to obtain “Pete died in New York City on January 27, 2014”. Even though it is true that “he was an American folk singer and activist”, this summary expresses a more general and more static information which, in a context of summarization of changes, does not pertain to the period “January, 2014”.

This paper considers the task of generating a summary of the changes in a set of documents. The set of documents may include different documents on the same topic originated in a collection, or a series of versions of the same document. The following three properties are expected from a summary of the changes:
                        
                           •
                           Time-dependency. The summary is expected to highlight the information that has been changed on the set of documents between two points in time. However, it should also exclude the static information existing in the documents.

Significance. During a given time period, changes to the text take place for different reasons. Often, changes are not very significant, such as the correction of syntax or grammatical errors, the modification of links or changes regarding a past time period. This outdated information seems to be updated simultaneously when an event draws attention to a Wikipedia article. However, these updates do not focus on the reason for those changes within that particular time period. Hence, the challenge is to identify the meaningful information which has the potential to be a significant part of the summary, besides all other unnecessary changed texts for the given period. Irrelevant details do not belong in the summary.

Non-redundancy. The summary is expected to be synthetic and therefore avoid redundant information. Two similar sentences carrying the same information should not be selected simultaneously.

The summarization of changes is a task that can have any kind of time-dependent text collections as input. Research has been conducted in the past on related tasks. The concept of monitoring changes (Allan, Gupta, & Khandelwal, 2001) has been proposed in 2001. This concept focuses on unstructured dynamic text collections such as news articles. The goal of monitoring changes is to keep users informed by extracting the main events from a stream of news stories on the same topic. Similarly, the task of summarizing changes can be used in news articles to generate a summary of changes within a given time period. Given a collection of document groups, Comparative Extractive Document Summarization (CDS) Wang et al. was proposed to generate a summary of the differences among these document groups sharing a similar topic. If these document groups have evolved over time, the goal is to generalize the CDS problem, extending it to the evolution of the differences over time. The result will then be a summary of the differences among time-dependent comparable document groups for a selected time period. Moreover, the summarization of changes can easily be adapted to other frameworks.

Along with news articles and document groups, Wikipedia is another example of a collection of documents where the data is dynamic by nature. Wikipedia is being continuously updated and maintained by a community of editors. Often, new contents originate from recent events such as sports competitions, political controversies, new research outcomes, awards, resignations, births, deaths or natural calamities. Modifications are also made when existing contents are revised. Other large dynamic collections are generated by social media, such as Facebook, Twitter or blogs. Social networks are very popular and people use them to provide status updates, share opinions or broadcast news. These media are characterized by a strong temporal dynamics of the content and a high posting volume.

The summarization of changes can be applied in several scenarios, one of them being search. When queries include temporal information, summarization can provide more focused snippets for search results. Query-oriented update summarization is another use for the summarization task in dynamic text collections. It poses new challenges to the sentence-ranking algorithms as it requires not only important and query-relevant information to be identified, but also novelty (Wenjie, Furu, Qin, & Yanxiang, 2009) to be captured. ChangeDetect has been proposed to detect a list of changes made to a user-given web page, and to notify the users via email. This allows web users to track the important changes made to their favorite web pages. Because it is impossible to see every possible change, with this service the users can pay attention to the details only when a summary of significant changes triggers enough interest. A similar situation occurs in enterprise and public environments, where information is always being updated into the existing shared repositories. This summary will make people aware of the changes in a concise form, either on a daily or weekly basis. A summary of changes can also be very useful for a journalist or a student exploring historical information that is no longer available in the current version of the documents on a specific topic. The summarization of changes can play an important role in online social networks. On Twitter or on Facebook, people often comment on events in real time by posting messages (tweets) or updating status publicly and instantly. Similarly, in blogs people express their views or opinions on a particular topic. If we collect the tweets/status updates/blogs posts on a specific topic within a time frame, then a summary of changes can provide an overview of the significant alterations made to the topic during that period. Overall, the aim of summarization of changes is to reveal details of the significant changes made to a topic over a given time period with a reasonable amount of effort.

The summarization of changes is defined as a task in the context of dynamic text collections. This study uses a collection of articles from Wikipedia where data is dynamic by nature. One of the most important challenges in this context is the diversity of intentions of the users while updating an article. When an event draws attention to a given Wikipedia article, it is possible to verify two types of positive revisions: revisions related to the specific event and revisions to generally update the whole article (Nunes, Ribeiro, & David, 2008). The system proposed here should present the significant changes as a final summary by filtering the general updates for a given time period. In general, there are two approaches to automatic summarization: extractive summarization and abstractive summarization (Jing & McKeown, 2000; Knight & Marcu, 2002). Extractive summarization selects a set of important sentences from the original documents. In contrast, abstractive summarization builds an internal semantic representation, and then uses natural language processing techniques to create a summary. This paper focuses on extractive summarization, proposing different approaches for executing this task. To the best of our knowledge, there is only one explicit reference to this in the literature. Hence, this previous research has been adapted and used as a baseline approach. The second approach studies the temporal aspects of each change made to the article. In particular, this approach examines a trade-off between the inserted (or modified) and deleted texts which have been revised frequently in a set of document revisions. The third approach is based on Latent Dirichlet Allocation (LDA) model (Blei, Ng, & Jordan, 2003), which is used in order to find hidden/latent topic structures of changes. The fourth approach is a combination of the two previous approaches. The summaries produced by all of the approaches are evaluated by comparison with the manual summaries using ROUGE metrics. The strengths and weaknesses of each are discussed in detail.

The rest of the paper is organized as follows: Section 2 presents a survey of related areas and research. In general terms, three major related areas were found and they are discussed separately and compared to the task proposed. Section 3 describes the basic architecture of the proposed system. Section 4 defines the sentence-scoring measurements using different approaches and presents a simple similarity measurement to identify the unique sentences, discarding the redundant ones. Section 5 describes the experimental details and Section 6 presents the analysis of experimental results. Finally, Section 7 provides conclusions and future research topics.

@&#RELATED WORK@&#

As mentioned previously, most research in the field of automatic summarization focuses on static collections. Previous research addressing the problem of summarizing changes, as opposed to summarizing the current version of a document, is relatively scarce. This section describes previous research focusing on the three main areas this paper builds upon, in particular update summarization, new event detection and temporal summarization.

The Document Understanding Conference (DUC) launched update summarization as a pilot task in 2007 (
                           DUC, 2007). This task focuses on generating an update summary for multiple documents based on a common topic under the assumption that the user is familiar with a set of past documents. The purpose of each update summary is to inform the reader with novel information on that specific topic. The task of summarizing changes is closely related to update summarization, but it is more general. There are two main differences between update summarization and summarization of changes. First, the information needs addressed in update summarization are restricted to current and novel updates whereas the summarization of changes is not limited to current changes and should address any user-defined period. Second, the assumption in update summarization is that the user is already familiarized with the past information related to the topics. In contrast, in summarization of changes there is no explicit prior assumption, and the user may or may not be familiar with the topic.

There are previous Bayesian works based on topic models to address the problem of automatic summarization. Haghighi and Vanderwende (2009), Chemudugunta, Smyth, and Steyvers (2006) used generative models which learn to discriminate between the collection and the document-specific distributions in order to capture the major pieces of information in a dataset. DualSum Delort and Alfonseca (2012), a topic-model based approach used explicitly in the update summarization task, is basically a variation of Latent Dirichlet Allocation (LDA) model (Blei et al., 2003). The goal of the DualSum model is to learn to distinguish the collection of earlier documents (the base) and the collection of recent documents (the update). In DualSum, documents are modeled as a bag of words, which are assumed to be sampled from a mixture of four latent topics. The four latent topics are: (i) the background topic, where the distribution is based on the general background words; (ii) the document specific topic, where the distribution is based on each document in the base and update collection pair; (iii) the joint topic, where the distribution is based on the common words between the base and the update collection; and (iv) the update topic, where the distribution is based on the specific words from the update collection. All four distributions in the DualSum model are learnt from a set of news collections. Once this is done, any of the four distributions or a combination of them can be used to provide the summary that best approximates the collection. In summarization of changes, LDA model is also used, but in a completely different way as described in the following sections.

Clustering based methods are also popular in the context of summarization. These methods usually apply different clustering techniques on the term-sentence matrices formed from the documents. After grouping the sentences into clusters, a centroid score is assigned to each sentence based on the average cosine similarity between one sentence and the rest of the sentences in the same cluster. Finally, the sentences with the highest score from each cluster are selected to form the summary. Wang and Li (2010) proposed an incremental hierarchical sentence clustering (IHSC) framework combined with document summarization techniques to update document summaries in real time. When new documents or sentences are added, the IHSC framework re-organizes the sentence clusters so that the corresponding summaries can be updated quickly. The COBWEB algorithm, which was originally built by Fisher (1987, 1989), is applied to build a sentence hierarchy of the document collection. When a new element is added, the COBWEB algorithm traverses the tree in a top-down fashion starting from the root. During the traversal, the COBWEB algorithm executes one of the four possible operations (insert, create, merge or split) in order to maximize the criterion function. When all the documents or sentences arrive, the users can cut the hierarchy tree at any level based on the length of the summary.

Graph-based ranking algorithms are also used in the update summarization task. Ranking Sentences with Positive and Negative Reinforcement (PNR2) (Wenjie et al., 2009) and Manifold ranking with sink points (MRSP) (Du, Guo, Zhang, & Cheng, 2010) are two such methods. PNR2 models both the positive and negative reinforcements between sentences to decide their scores and, as a result, it extracts the sentences that are not only salient but also novel comparatively to the base collection. Positive reinforcement captures the idea that a sentence (from either the base or update collection) is more important if it correlates to the other important sentences in the same collection whereas negative reinforcement reflects the notion that a sentence from the base collection is less important if it correlates to the important sentences in the update collection and vice versa. In MRSP, the sink points are the sentences whose ranking scores are fixed to the minimum ranking score (i.e. zero in this case) on the manifold during the ranking process. The sentences sharing similar information with the sink points are penalized during the ranking process based on the intrinsic sentence manifold. Recently, QCQPSum (Li, Du, & Shen, 2012) was developed to avoid the problem identified in PNR2 and MRSP, namely that the salience determination of the sentences in the update collection is disturbed by the base collection. QCQPSum is a quadratically constrained quadratic programming (QCQP) problem which is NP-hard. To overcome this problem, the authors proposed an approximate method (QPSum) that can solve it in polynomial time.

When capturing the information changed in current documents in comparison with previous documents, the first challenge is filtering the redundant information. Zhang, Du, Xu, and Cheng (2009) proposed three filtering approaches to measure the similarity of sentences between earlier and current information: document filtering, summary filtering and union filtering based on the degree of membership from the fuzzy set theory. After that, the filtered sentences are ranked using two approaches. The first is a signature based approach in which temporal topic signatures are extracted from the filtered sentences. The second is a manifold ranking based approach in which the macro-structure of the filtered sentences can be reserved. Support Vector Regression is used as a filter (Schilder, Kondadadi, Leidner, & Conrad, 2008) to extract sentences that resemble first sentences in the entire news articles. The purpose behind this idea is that first sentences are very focused and contain less anaphoric expressions. After extracting the sentences, a modified version of FastSum (Schilder & Kondadadi, 2008) is applied.

In new event detection (Allan, 2002), a task within the area of Topic Detection and Tracking (TDT), the goal is to classify a discussed event as new or old given a sequence of news. In contrast, in the problem addressed in this paper, the aim is not only to categorize an event as being new or old, but also to extract the significant changed information within a time period, either from events or non-events and finally present them as a meaningful summary. New event detection traditionally focuses on news articles to discover and cluster events. However, other studies have been conducted with other dynamic text collections. Subašić and Berendt (2010) considered the problem of tracking and representing the evolution of stories using co-occurrence analysis. The problem of on-line new event detection from a stream of Twitter posts has been addressed using locality-sensitive hashing (LSH) (Petrović, Osborne, & Lavrenko, 2010), an approach that can overcome the limitations of traditional approaches used in data streams. The WikiPop (Ciglan & Nørvåg, 2010) service can detect and provide popular topics related to the users’ interests using Wikipedia page view statistics. Another interesting task (Tsagkias, Rijke, & Weerkamp, 2011) was studied for discovering implicitly linked social media utterances (such as blog posts, tweets, diggs etc.) with news events. Most technical works on this task are based on news articles modeling; a three-step approach is followed via query modeling, retrieval and fusion. In the query modeling step, multiple query models are derived from a given news article source to generate multiple queries. In the retrieval step, these queries are submitted to retrieve utterances from an index of social media utterances. After retrieval, in the final step, the fusion, multiple ranked lists are merged into a single result list using data fusion techniques. Another work is Wikipedia Live Monitor (Steiner, Hooland, & Summers, 2013), which tracks article edits on different language versions of Wikipedia as signals for breaking news events.


                        Temporal summarization basically means summarizing the changes observed in dynamic text collections over specific periods of time. The idea about monitoring changes over time in news coverage has started with Allan’s et al. work Allan et al. (2001). The authors stated that “simple approaches are effective, but that the problem is far from solved”. ChangeSummarizer (Jatowt, Bun, & Ishizuka, 2004) periodically monitors a web collection to look for new textual changes and present them in a condensed form. However, contrarily to the problem presented here, the information needs are specified as “recent, important changes”. Filatova and Hatzivassiloglou (Filatova & Hatzivassiloglou, 2004) proposed a new set of low-level features called atomic events that can describe the relationships between important actors in a document or set of documents. The authors studied the effect of using atomic events as a feature set in extractive summarization and made comparisons with state-of-the-art summarization systems. The authors also stated that event-based features not only improves the summary, but also avoids redundancy in the output. WikiChanges (Nunes et al., 2008) is a web-based application designed to plot the revision history of Wikipedia articles and to produce a temporal summary. The authors have suggested that the task of summarizing changes should adopt a broader view that should not be limited to a “recent” time frame. Blogs are also an example of a medium where information changes frequently and substantially over time. A framework (Lin & Sundaram, 2007) has been presented to analyze and summarize the temporal dynamics in the activity of blogs. More recently, the Wikipedia Event Reporter (Georgescu et al., 2013) was proposed. This system combines two tasks, event detection and temporal summarization using Wikipedia revision history as a source of data. The system extracts the event-related updates automatically by analyzing the entire history of updates for a given entity. However, in general, a specific time frame cannot be defined in the system in order to obtain a temporal summary regardless of the events detected.

This section describes the overall architecture and methodologies for the task of summarizing changes. Various notations which are used in the following sections are also introduced. When a user gives a time interval to an entity of interest, the time range actually determines how many versions of the document are used for detecting the changes. To incorporate the temporal dimension of documents, it is considered that for any article 
                        A
                      there are T document versions represented as 
                        
                           A
                           =
                           {
                           rev
                           
                              
                              1
                           
                           ,
                           rev
                           
                              
                              2
                           
                           ,
                           ⋯
                           ,
                           rev
                           
                              
                              T
                           
                           }
                        
                      in the given time range. 
                        
                           rev
                           
                              
                              1
                           
                        
                      is the most recent document version and 
                        
                           rev
                           
                              
                              T
                           
                        
                      is the oldest document version in the given time frame for the article 
                        A
                     .

The conceptual architecture of our system is displayed in Fig. 1
                     . It shows the information flow at each step. The input of the diff process is a set of document versions for the article 
                        A
                      and the outputs are a set of word-diff’s and a set of block-diff’s. The set of block-diff’s is the input for the sentence extraction process, which is shown in the left hand side of the figure and the set of word-diff’s is the input for the feature extraction process, shown in the right hand side. The feature extraction process generates the feature files for each approach separately and provides them for the purpose of generating the term scores. The set of sentences extracted with the sentence extraction process is used by each approach to generate the summary.

The system extracts the differences (diff) from the collection of T document versions by comparing the consecutive versions starting from 
                        
                           rev
                           
                              
                              1
                           
                        
                     . The last extracted difference is between 
                        
                           rev
                           
                              
                              
                                 T
                                 −
                                 1
                              
                           
                        
                      and 
                        
                           rev
                           
                              
                              T
                           
                        
                      document versions. Therefore, at the end of the diff process, the total number of extracted differences is 
                        
                           T
                           −
                           1
                        
                      for T document versions. The set of 
                        
                           T
                           −
                           1
                        
                      differences is defined as 
                        
                           D
                           =
                           {
                           diff
                           
                              
                              1
                           
                           ,
                           diff
                           
                              
                              2
                           
                           ,
                           ⋯
                           ,
                           diff
                           
                              
                              
                                 T
                                 −
                                 1
                              
                           
                           }
                        
                     .

In practice, the changes in document versions can be made in three ways: insertion, modification or deletion. Based on this, the set D is divided into two categories: the changes made by insertions and modifications are put into one category, and the changes made by deletions are put into another. In order to differentiate the two categories of changes, D
                     (ins) is the subset of D containing the differences which occurred due to insertions and modifications whereas D
                     (del) is the subset of D containing the differences which occurred due to deletions. Therefore, the set D can be written as 
                        
                           D
                           =
                           
                              
                                 D
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                           ∪
                           
                              
                                 D
                              
                              
                                 (
                                 del
                                 )
                              
                           
                        
                     .

The set of differences caused by insertions and modifications (D
                     (ins)) and the set of differences caused by deletions (D
                     (del)) are processed further in two modes: word mode which is denoted as word-diff and block mode which is denoted as block-diff. In word-diff, the diff process extracts the changed words by comparing the consecutive document versions on the word basis (Diff, match and patch libraries for plain text) while in block-diff, the diff process extracts the changed paragraphs (The diffutils library for computing diffs, applying patches, generationg side-by-side view). D
                     (ins) in word mode is represented as 
                        
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                           =
                           
                              {
                              word
                              -
                              diff
                              
                                 
                                 
                                    1
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                              ,
                              word
                              -
                              diff
                              
                                 
                                 
                                    2
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                              ,
                              ⋯
                              ,
                              word
                              -
                              diff
                              
                                 
                                 
                                    T
                                    −
                                    1
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                              }
                           
                        
                      and D
                     (ins) in block mode is represented as 
                        
                           
                              D
                              
                                 block
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                           =
                           
                              {
                              block
                              -
                              diff
                              
                                 
                                 
                                    1
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                              ,
                              block
                              -
                              diff
                              
                                 
                                 
                                    2
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                              ,
                              ⋯
                              ,
                              block
                              -
                              diff
                              
                                 
                                 
                                    T
                                    −
                                    1
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                              }
                           
                        
                     . Similarly, D
                     (del) in word mode and D
                     (del) in block mode are denoted as 
                        
                           D
                           
                              word
                           
                           
                              (
                              del
                              )
                           
                        
                      and 
                        
                           D
                           
                              block
                           
                           
                              (
                              del
                              )
                           
                        
                      respectively.

After obtaining the sets of all extracted differences, namely, 
                        
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                           ,
                           
                              D
                              
                                 block
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                           ,
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 del
                                 )
                              
                           
                        
                      and 
                        
                           D
                           
                              block
                           
                           
                              (
                              del
                              )
                           
                        
                     , four main approaches are proposed for the task. The basic framework for all approaches starts by scoring the words/terms and then ranking the sentences on the basis of those words scores. In the sentence ranking process, a set of sentences is provided to each of the approaches as input. This set of sentences S is built through a sentence extraction process from all the 
                        
                           block
                           -
                           diff
                           
                              
                              i
                           
                           ∈
                           
                              D
                              
                                 block
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                        
                     . Then, the sentence ranking process assigns a score to each of the sentences in S. The sentence score is basically calculated by the sum of the scores of all its terms, divided by the total number of terms. Each of the approaches calculates the term’s score differently. After the sentence ranking process, a set of sentences is obtained of the form 
                        
                           {
                           
                              (
                              
                                 sentence
                                 i
                              
                              ,
                              
                                 sentence
                                 
                                    i
                                 
                                 
                                    (
                                    s
                                    )
                                 
                              
                              )
                           
                           ,
                           
                              sentence
                              i
                           
                           ∈
                           S
                           }
                        
                     , where sentencei
                      refers to the i-th sentence itself from the set S, whereas 
                        
                           sentence
                           
                              i
                           
                           
                              (
                              s
                              )
                           
                        
                      is the corresponding score of sentencei
                     . Then, all the sentences are ranked in descending order according to their scores. Finally, from each approach, the top ranked sentences are presented as a summary.

In the first approach, term scores are computed using the scoring function which is adapted from an existing work (Jatowt et al., 2004), and then the score of each sentence is computed using those term scores. The sets 
                        
                           A
                           ,
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                        
                      and 
                        
                           D
                           
                              word
                           
                           
                              (
                              del
                              )
                           
                        
                      are used to generate scores for terms, whereas 
                        
                           D
                           
                              block
                           
                           
                              (
                              ins
                              )
                           
                        
                      is used to build a set of sentences. This method is considered the baseline approach in the task proposed here. The score of a term generated by this approach is called baseline temporal term score (BTTS) and the score of a sentence using BTTS is called baseline temporal sentence score (BTSS).

A different scoring function is proposed in the second approach. Here, the focus is on the temporal aspects to be incorporated into the scoring function. The two sets 
                        
                           D
                           
                              word
                           
                           
                              (
                              ins
                              )
                           
                        
                      and 
                        
                           D
                           
                              word
                           
                           
                              (
                              del
                              )
                           
                        
                      are used to rank words while 
                        
                           D
                           
                              block
                           
                           
                              (
                              ins
                              )
                           
                        
                      is used to rank sentences. The basic idea behind this scoring function is that the word which occurs frequently in 
                        
                           D
                           
                              word
                           
                           
                              (
                              ins
                              )
                           
                        
                      will obtain a higher score but if the word occurs in 
                        
                           D
                           
                              word
                           
                           
                              (
                              del
                              )
                           
                        
                     , it will get a lower score. This means that the score of a word is higher as it is inserted in the document versions more frequently; simultaneously the score decreases as the word is deleted from the document versions. The score of a term generated by this approach is called temporal term score (TTS) and the score of a sentence using TTS is called temporal sentence score (TSS).

In the third approach, words scores are generated via Latent Dirichlet Allocation (LDA) model. The set, 
                        
                           D
                           
                              word
                           
                           
                              (
                              ins
                              )
                           
                        
                      is used to generate a feature file for the LDA model. The feature file has a total of 
                        
                           M
                           =
                           T
                           −
                           1
                        
                      feature vectors where each feature vector w
                     
                        i
                      consists of a sequence of words 
                        
                           (
                           
                              w
                              1
                           
                           ,
                           
                              w
                              2
                           
                           ,
                           ⋯
                           
                              w
                              
                                 j
                                 −
                                 1
                              
                           
                           ,
                           
                              w
                              j
                           
                           ,
                           
                              w
                              
                                 j
                                 +
                                 1
                              
                           
                           ,
                           ⋯
                           )
                        
                      generated from 
                        
                           word
                           -
                           diff
                           
                              
                              i
                           
                           ∈
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                        
                     . Here, each word wj
                      in the sequence belongs to any of the words from a set of V distinct words 
                        
                           BOW
                           =
                           {
                           
                              
                                 w
                              
                              
                                 (
                                 1
                                 )
                              
                           
                           ,
                           
                              
                                 w
                              
                              
                                 (
                                 2
                                 )
                              
                           
                           ,
                           ⋯
                           
                              
                                 w
                              
                              
                                 (
                                 V
                                 )
                              
                           
                           }
                        
                     , which is directly created from 
                        
                           D
                           
                              word
                           
                           
                              (
                              ins
                              )
                           
                        
                      as well. This feature file is given as an input file in Latent Dirichlet Allocation (LDA) model. LDA model generally tries to backtrack from the documents to find out a set of latent topics that consist of terms with certain probabilities. Here, the LDA model is used to figure out the important changed terms assigned to different latent topics. It is assumed that each latent topic corresponds to at least one significant change, and different changes can be interpreted by different latent topics. The words with the corresponding scores for each latent topic are generated as an output via the LDA model. Let there be K number of latent topics 
                        
                           {
                           
                              z
                              i
                           
                           :
                           i
                           =
                           1
                           ,
                           2
                           ,
                           ⋯
                           ,
                           K
                           }
                        
                      and each topic is described by a set of terms, each word is associated with a score. Consider the set of terms of the form 
                        
                           {
                           
                              (
                              
                                 term
                                 ij
                              
                              ,
                              
                                 term
                                 
                                    ij
                                 
                                 
                                    (
                                    s
                                    )
                                 
                              
                              )
                           
                           ,
                           
                              term
                              ij
                           
                           ∈
                           BOW
                           }
                        
                     , where termij
                      is the j-th word of topic zi
                      and 
                        
                           term
                           
                              ij
                           
                           
                              (
                              s
                              )
                           
                        
                      is the corresponding score of termij
                     , produced by the LDA model as an output. In general, any term is denoted as wj
                      but while describing the scoring function term
                     .j
                      is introduced instead of wj
                      to make it conventional. Apart from calculating the scores of sentences, the label is also assigned to each sentence based on the terms which belong to a particular topic. One sentence belongs to zi
                      be decided, if zi
                      gives the highest score to that sentence. Therefore, in this approach, the ranking process generates a set of sentences of the form 
                        
                           {
                           
                              (
                              
                                 sentence
                                 i
                              
                              ,
                              
                                 sentence
                                 
                                    i
                                 
                                 
                                    (
                                    s
                                    )
                                 
                              
                              ,
                              z
                              )
                           
                           ,
                           
                              sentence
                              i
                           
                           ∈
                           S
                           }
                        
                     , where sentencei
                      refers to the i-th sentence itself from the set 
                        
                           S
                           ,
                           
                              sentence
                              
                                 i
                              
                              
                                 (
                                 s
                                 )
                              
                           
                        
                      is the corresponding score of sentencei
                      and z is the label to identify to which topic the sentence belongs. The score of a term generated by LDA is called latent topic term score (LTTS) and the score of a sentence using LTTS is called laten topic sentence score (LTSS). The system ranks the sentences in descending order based on the sentences scores. The top ranked sentences are presented to convey the main changes in the defined period.

The goal is to develop a system that can provide a summary with two main characteristics: (i) the summary should only describe the information that has been changed, and (ii) at the same time, the summary should contain only the significant changes among all other general changes made within the given time period. Since, we consider 
                        
                           D
                           
                              word
                           
                           
                              (
                              ins
                              )
                           
                        
                      for computing the words/terms scores, by default, the first characteristic is incorporated into all the approaches. However, to find out the most significant changed words and to assign higher scores to them, different scoring functions are used in different approaches. The main difference of the third approach comparatively to the other two is that it can separate groups of related changed words, in which each group is likely to carry at least one significant change. In the third approach, however, only the changes caused by insertions and modifications (
                        
                           D
                           
                              word
                           
                           
                              (
                              ins
                              )
                           
                        
                     ) are considered while scoring the terms. Nevertheless, the changes caused by deletions (
                        
                           D
                           
                              word
                           
                           
                              (
                              del
                              )
                           
                        
                     ) can also play an important role in temporal aspects. In order to indirectly incorporate the changes caused by deletions in LDA, another approach is introduced which is a combination of the second and third approaches. Here, the top ranked sentences generated by LDA are re-ranked with a combination of LTSS and TSS. Fig. 1 describes the outline of the four sentence scoring measurements in different approaches, where the first three sentence scoring measurements, namely BTSS, TSS and LTSS are independent whereas the fourth is a combination of the second and third (TSS and LTSS). Each sentence scoring measurement is described in detail in the following sections.

To evaluate the system, a collection of articles from Wikipedia, including their revision history, is used. For each article, a temporal interval and a reference summary are manually selected from the article’s contents. The articles and intervals in which significant events occurred are carefully selected. To construct a reference summary for a given time range, a set of sentences are previously selected and extracted such that these sentences can describe the exact significant change. The problem of having multiple reference summaries for that time period does not arise for two reasons. First, in general, for a reference summary the sentences are extracted from the latest version of that specified Wikipedia article in the given time period instead of writing a reference summary manually. Second, the significant change is so prominent, it is easy to select those sentences as a reference summary. The summaries produced by each of the approaches are evaluated comparatively to the reference summaries using ROUGE metrics. The most important sentences are selected manually for a reference summary and since this reference summary is provided for comparison against the system generated summaries, ROUGE scores can express whether the best sentences are peaked or not by different approaches. Intuitively, the higher the ROUGE scores, the better sentences are selected using that approach.

Previous summarization tasks usually focused either on a single document or on a set of documents from a static collection on a given topic. However, document collections change dynamically when the topic evolves over time, as new documents are continuously added, modified or deleted. These changes usually bring the new information to the topic, which poses new challenges to the sentence ranking process when summarizing a dynamic collection of documents (Allan et al., 2012).

The objective of the sentence ranking process is to calculate scores for all sentences so that they can be arranged in descending order of their scores. Usually, the scores for all sentences are calculated in such a way that the most significant sentences are likely to obtain higher scores. The two main steps associated with the sentence ranking process are: (i) calculating each term’s score in a sentence and (ii) calculating each sentence’s score using the scores of these terms. This section describes different sentence scoring measurements obtained with the proposed approaches.

In the first approach, each term’s score is generated using the following scoring function:
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             BTTS
                                             
                                                (
                                                
                                                   term
                                                   j
                                                
                                                )
                                             
                                             =
                                             
                                                (
                                                1
                                                +
                                                
                                                   
                                                      
                                                         ∑
                                                         
                                                            r
                                                            =
                                                            1
                                                         
                                                         
                                                            N
                                                            doc
                                                         
                                                      
                                                      
                                                         [
                                                         
                                                            
                                                               n
                                                               rc
                                                            
                                                            
                                                               
                                                                  N
                                                                  rc
                                                               
                                                               +
                                                               1
                                                            
                                                         
                                                         −
                                                         α
                                                         ×
                                                         
                                                            
                                                               n
                                                               rs
                                                            
                                                            
                                                               
                                                                  N
                                                                  rs
                                                               
                                                               +
                                                               1
                                                            
                                                         
                                                         ]
                                                      
                                                   
                                                   
                                                      N
                                                      doc
                                                   
                                                
                                                )
                                             
                                             ×
                                             exp
                                             
                                                (
                                                
                                                   
                                                      n
                                                      jcp
                                                   
                                                   
                                                      
                                                         N
                                                         cdoc
                                                      
                                                      +
                                                      1
                                                   
                                                
                                                −
                                                α
                                                ×
                                                
                                                   
                                                      n
                                                      jsp
                                                   
                                                   
                                                      
                                                         N
                                                         sdoc
                                                      
                                                      +
                                                      1
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where BTTS(termj
                        ) is the baseline temporal term score (BTTS) for the j-th term in our system and the meanings of individual symbols are described in Table 1
                        . This scoring function was introduced in ChangeSummarizer system (Jatowt et al., 2004), which periodically monitors a web collection in search for new changes and generates their summary related to a specific topic. To the best of our knowledge, the only explicit reference to the idea put forward here is the ChangeSummarizer system (Jatowt et al., 2004) (discussed in Section 2.3). However, contrarily to the theory presented here, the information addressed in the ChangeSummarizer system (Jatowt et al., 2004) is limited to “recent, important changes”. Hence, the scoring function used in the ChangeSummarizer system (Jatowt et al., 2004) is adapted here as the baseline, but the meanings of the symbols used in Eq. (1) are slightly changed. Table 1 describes the basic differences in the meaning of the symbols between the ChangeSummarizer system and the proposed system. The motivation of adapting Eq. (1) as the baseline is to compare against a system which is already in the literature for a similar task.

The main difference between both systems is that we consider an article’s different versions instead of using individual web pages devoted to a common topic. In the summarization of changes system, it is assumed that the number of static versions of an article within a time period, Nsdoc
                         is equal to zero and the number of changed versions of an article within a time period, Ncdoc
                         is equal to the total number of document versions made to an article within that time period. If there is a total T number of document versions of an article in the given time period, then the values for the variables are 
                           
                              
                                 N
                                 sdoc
                              
                              =
                              0
                           
                         and 
                           
                              
                                 N
                                 cdoc
                              
                              =
                              T
                           
                        . The changed and the static terms are figured out by comparing between consecutive versions of an article. For example, the changed and the static terms for 
                           
                              rev
                              
                                 
                                 
                                    T
                                    −
                                    1
                                 
                              
                           
                         version are obtained by comparing 
                           
                              rev
                              
                                 
                                 
                                    T
                                    −
                                    1
                                 
                              
                           
                         and 
                           
                              rev
                              
                                 
                                 T
                              
                           
                         consecutive versions for that article. After removing stop words obtained from that comparison, the fixed terms are considered the static terms for 
                           
                              rev
                              
                                 
                                 
                                    T
                                    −
                                    1
                                 
                              
                           
                         version whereas the terms which are either added, modified or deleted in 
                           
                              rev
                              
                                 
                                 
                                    T
                                    −
                                    1
                                 
                              
                           
                         version are the changed terms. The sets, 
                           
                              A
                              ,
                              
                                 D
                                 
                                    word
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                           
                         and 
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 del
                                 )
                              
                           
                         are used to obtain the changed and static terms separately for all the versions of an article in the given time period.

The basic idea of this term scoring function is to give higher scores to the terms that appear more often in the changed parts of an article’s different versions but occur rarely in the static parts of those versions. In Eq. (1), the first part gives higher scores to the popular terms that occurred in the changed parts of the article’s different versions, but not in the static parts. The second part of Eq. (1) has another motivation. The terms appearing frequently in the changed parts (i.e. popular terms in changed parts) may have low semantic values for a particular topic. Therefore, the second part of Eq. (1) tries to assign higher scores to those changed terms which are less common or typical to a specific domain. The parameter α is used to control the scoring of these changed uncommon terms. α ranges between 0 and 1.

It is stated earlier in Section 3 that the basic framework for all approaches starts by scoring the words/terms and then ranks the sentences on the basis of those words scores. In the sentence ranking process, a set of sentences is provided to each of the approaches as input. This set of sentences S is built through a sentence extraction process from all the 
                           
                              block
                              -
                              diff
                              
                                 
                                 i
                              
                              ∈
                              
                                 D
                                 
                                    block
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                           
                        . Then, the sentence ranking process assigns a score to each of the sentences in S. The sentence score is basically calculated by the sum of the scores of all its terms, divided by the total number of terms after excluding the stop words. Thus, the sentence ranking process generates a set of sentences of the form 
                           
                              {
                              
                                 (
                                 
                                    sentence
                                    i
                                 
                                 ,
                                 
                                    sentence
                                    
                                       i
                                    
                                    
                                       (
                                       s
                                       )
                                    
                                 
                                 )
                              
                              ,
                              
                                 sentence
                                 i
                              
                              ∈
                              S
                              }
                           
                        , where sentencei
                         refers to the i-th sentence from the set S and 
                           
                              sentence
                              
                                 i
                              
                              
                                 (
                                 s
                                 )
                              
                           
                         is the corresponding score of sentencei
                        . Here, 
                           
                              sentence
                              
                                 i
                              
                              
                                 (
                                 s
                                 )
                              
                           
                         is computed with the BTSS(sentencei
                        ) function, the baseline temporal sentence score (BTSS) for sentencei. BTSS(sentencei
                        ) is defined as
                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             BTSS
                                             
                                                (
                                                
                                                   sentence
                                                   i
                                                
                                                )
                                             
                                             =
                                             
                                                
                                                   
                                                      ∑
                                                      j
                                                   
                                                   BTTS
                                                   
                                                      (
                                                      
                                                         term
                                                         j
                                                      
                                                      )
                                                   
                                                
                                                
                                                   N
                                                   (
                                                   
                                                      sentence
                                                      i
                                                   
                                                   )
                                                
                                             
                                             ,
                                             
                                                
                                                   0.35
                                                   e
                                                   m
                                                
                                                
                                                   0
                                                   e
                                                   x
                                                
                                             
                                             
                                                term
                                                j
                                             
                                             ∈
                                             
                                                sentence
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where BTTS(termj
                        ) for the j-th term is calculated using Eq. (1) and N(sentencei
                        ) denotes the total number of terms in sentencei
                         after excluding the stop words. Once the sentence ranking process assigns a score to each of the sentences, the sentences are ranked in a descending order of their scores. Finally the top ranked sentences are selected to produce as a summary.

In the second approach, a different scoring function is introduced. Here, the emphasis is on the temporal aspects to be incorporated into the scoring function. The aim is to extract the significant terms from a set of document versions which bring changed and novel information into an article. Research has been done on term weighting using the time impact on changes occurring either in the collection (Efron, 2010) or in an individual document (Nunes, Ribeiro, & David, 2011). Instead of directly taking into account the periods of time in which a term occurs throughout an article’s revision history, the score of a term is calculated here by considering the joint probabilities of both the insertion and deletion events occurring in a set of document versions.

The basic idea behind the second approach is that a term which has been revised frequently in a set of document versions should be more important because it could reflect a significant change. However, simultaneously, if the term is deleted frequently in the set of document versions, the term might not be the important term to present any significant change and as a result that term should obtain less importance. In particular, a term which occurs frequently in 
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                         will obtain a higher score; consequently, if it occurs less frequently in 
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 del
                                 )
                              
                           
                        , its score will be lower. The probability of termj
                         occurring as a result of insertions is calculated as
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   P
                                                
                                                
                                                   (
                                                   ins
                                                   )
                                                
                                             
                                             
                                                (
                                                
                                                   term
                                                   j
                                                
                                                )
                                             
                                             =
                                             
                                                
                                                   t
                                                   
                                                   f
                                                   (
                                                   
                                                      term
                                                      j
                                                   
                                                   ,
                                                   
                                                      D
                                                      
                                                         word
                                                      
                                                      
                                                         (
                                                         ins
                                                         )
                                                      
                                                   
                                                   )
                                                
                                                
                                                   
                                                      ∑
                                                      j
                                                   
                                                   t
                                                   
                                                   f
                                                   
                                                      (
                                                      
                                                         term
                                                         j
                                                      
                                                      ,
                                                      
                                                         D
                                                         
                                                            word
                                                         
                                                         
                                                            (
                                                            ins
                                                            )
                                                         
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where, 
                           
                              tf
                              (
                              
                                 term
                                 j
                              
                              ,
                              
                                 D
                                 
                                    word
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                              )
                           
                         is the frequency of j-th term (termj
                        ) in the set 
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                        . Similarly, the probability of termj
                         occurring as a result of deletions is calculated as
                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   P
                                                
                                                
                                                   (
                                                   del
                                                   )
                                                
                                             
                                             
                                                (
                                                
                                                   term
                                                   j
                                                
                                                )
                                             
                                             =
                                             
                                                
                                                   tf
                                                   (
                                                   
                                                      term
                                                      j
                                                   
                                                   ,
                                                   
                                                      D
                                                      
                                                         word
                                                      
                                                      
                                                         (
                                                         del
                                                         )
                                                      
                                                   
                                                   )
                                                
                                                
                                                   
                                                      ∑
                                                      j
                                                   
                                                   tf
                                                   
                                                      (
                                                      
                                                         term
                                                         j
                                                      
                                                      ,
                                                      
                                                         D
                                                         
                                                            word
                                                         
                                                         
                                                            (
                                                            del
                                                            )
                                                         
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where, 
                           
                              tf
                              (
                              
                                 term
                                 j
                              
                              ,
                              
                                 D
                                 
                                    word
                                 
                                 
                                    (
                                    del
                                    )
                                 
                              
                              )
                           
                         is the frequency of j-th term (termj
                        ) in the set 
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 del
                                 )
                              
                           
                        . Finally, the temporal term score (TTS) for termj
                         is calculated as
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             TTS
                                             
                                                (
                                                
                                                   term
                                                   j
                                                
                                                )
                                             
                                             =
                                             
                                                
                                                   P
                                                
                                                
                                                   (
                                                   ins
                                                   )
                                                
                                             
                                             
                                                (
                                                
                                                   term
                                                   j
                                                
                                                )
                                             
                                             ×
                                             
                                                (
                                                1
                                                −
                                                
                                                   
                                                      P
                                                   
                                                   
                                                      (
                                                      del
                                                      )
                                                   
                                                
                                                
                                                   (
                                                   
                                                      term
                                                      j
                                                   
                                                   )
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        The sentence extraction process builds a set of sentences S, extracted from all the 
                           
                              block
                              -
                              diff
                              
                                 
                                 i
                              
                              ∈
                              
                                 D
                                 
                                    block
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                           
                        . The sentence ranking process assigns a score to each sentence, which is calculated with the sum of the scores of all terms, divided by the number of terms. Thus, the sentence ranking process generates a set of sentences of the form 
                           
                              {
                              
                                 (
                                 
                                    sentence
                                    i
                                 
                                 ,
                                 
                                    sentence
                                    
                                       i
                                    
                                    
                                       (
                                       s
                                       )
                                    
                                 
                                 )
                              
                              ,
                              
                                 sentence
                                 i
                              
                              ∈
                              S
                              }
                           
                        , where 
                           
                              sentence
                              
                                 i
                              
                              
                                 (
                                 s
                                 )
                              
                           
                         is the score of sentencei
                        . Here, 
                           
                              sentence
                              
                                 i
                              
                              
                                 (
                                 s
                                 )
                              
                           
                         is computed with the TSS(sentencei
                        ) function, the temporal sentence score (TSS) for sentencei. TSS(sentencei
                        ) is defined as
                           
                              (6)
                              
                                 
                                    
                                       
                                          
                                             TSS
                                             
                                                (
                                                
                                                   sentence
                                                   i
                                                
                                                )
                                             
                                             =
                                             
                                                
                                                   
                                                      ∑
                                                      j
                                                   
                                                   TTS
                                                   
                                                      (
                                                      
                                                         term
                                                         j
                                                      
                                                      )
                                                   
                                                
                                                
                                                   N
                                                   (
                                                   
                                                      sentence
                                                      i
                                                   
                                                   )
                                                
                                             
                                             ,
                                             
                                                
                                                   0.35
                                                   e
                                                   m
                                                
                                                
                                                   0
                                                   e
                                                   x
                                                
                                             
                                             
                                                term
                                                j
                                             
                                             ∈
                                             
                                                sentence
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where TTS(termj
                        ) is calculated for the j-th term using Eq. (5) and N(sentencei
                        ) is the total number of terms in sentencei
                         after excluding the stop words. Like in the previous approach, the top ranked sentences are selected at the final step to produce a summary.

In this approach, the term score is generated using Latent Dirichlet Allocation (LDA) model. Before defining the latent topic sentence score (LTSS), it is necessary to describe the theoretical background of the LDA model. Although the basic concepts of the LDA model and its learning paradigm are already available in the literature, introducing briefly the theoretical part of the LDA model makes it easier to explain how LTSS can be computed using the LDA model.

Latent Dirichlet Allocation (LDA) is a statistical model that tries to capture the latent topics in a collection of documents. LDA was first introduced by David Blei Blei et al. (2003). The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. One important assumption about the LDA generative model is that the number of topics is known in advance. Before describing the LDA model, formally, the following definitions are required:
                              
                                 1.
                                 A word is the basic unit of discrete data, defined to be an item from a vocabulary of size V denoted by 
                                       
                                          BOW
                                          =
                                          {
                                          
                                             
                                                w
                                             
                                             
                                                (
                                                1
                                                )
                                             
                                          
                                          ,
                                          
                                             
                                                w
                                             
                                             
                                                (
                                                2
                                                )
                                             
                                          
                                          ,
                                          ⋯
                                          ,
                                          
                                             
                                                w
                                             
                                             
                                                (
                                                V
                                                )
                                             
                                          
                                          }
                                       
                                    .

A document (di
                                    ) is a sequence of N words denoted by 
                                       
                                          w
                                          =
                                          (
                                          
                                             w
                                             1
                                          
                                          ,
                                          
                                             w
                                             2
                                          
                                          ,
                                          ⋯
                                          ,
                                          
                                             w
                                             N
                                          
                                          )
                                       
                                    , where each 
                                       
                                          
                                             w
                                             j
                                          
                                          ,
                                          j
                                          =
                                          1
                                          ,
                                          2
                                          ,
                                          ⋯
                                          N
                                       
                                     belongs to any of the V vocabulary words from the set BOW.

A corpus is a collection of M documents denoted by 
                                       
                                          A
                                          =
                                          
                                             {
                                             
                                                d
                                                1
                                             
                                             ,
                                             
                                                d
                                                2
                                             
                                             ,
                                             ⋯
                                             ⋯
                                             ,
                                             
                                                d
                                                M
                                             
                                             }
                                          
                                          =
                                          
                                             {
                                             
                                                w
                                                1
                                             
                                             ,
                                             
                                                w
                                                2
                                             
                                             ,
                                             ⋯
                                             ⋯
                                             ,
                                             
                                                w
                                                M
                                             
                                             }
                                          
                                       
                                    
                                 

In LDA model, for each document (di
                           ), there is a multinomial distribution over K topics 
                              
                                 {
                                 
                                    z
                                    i
                                 
                                 :
                                 i
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 ⋯
                                 ,
                                 K
                                 }
                              
                           , with parameters 
                              
                                 
                                    θ
                                 
                                 
                                    (
                                    
                                       d
                                       i
                                    
                                    )
                                 
                              
                           , so for a word in document 
                              
                                 
                                    d
                                    i
                                 
                                 ,
                                 P
                                 
                                    (
                                    
                                       z
                                       i
                                    
                                    =
                                    j
                                    )
                                 
                                 =
                                 Multinomial
                                 
                                    (
                                    
                                       θ
                                       
                                          j
                                       
                                       
                                          (
                                          
                                             d
                                             i
                                          
                                          )
                                       
                                    
                                    )
                                 
                              
                           . The jth
                            topic 
                              
                                 (
                                 
                                    z
                                    i
                                 
                                 =
                                 j
                                 )
                              
                            is represented further by a multinomial distribution over the set of vocabulary words (BOW), with the parameters ϕ
                           (j), so 
                              
                                 P
                                 
                                    (
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       z
                                       i
                                    
                                    =
                                    j
                                    )
                                 
                                 =
                                 Multinomial
                                 
                                    (
                                    
                                       ϕ
                                       
                                          
                                             w
                                             i
                                          
                                       
                                       
                                          (
                                          j
                                          )
                                       
                                    
                                    )
                                 
                              
                           . To make the predictions about new documents, it is assumed a prior distribution on the parameters 
                              
                                 
                                    θ
                                 
                                 
                                    (
                                    
                                       d
                                       i
                                    
                                    )
                                 
                              
                           . It is well known that the Beta distribution is the conjugate prior of the Bernoulli distribution and the Dirichlet distribution is the conjugate prior of the multinomial distribution. Therefore, for 
                              
                                 
                                    θ
                                 
                                 
                                    (
                                    
                                       d
                                       i
                                    
                                    )
                                 
                              
                            a Dirichlet prior with parameters α, i.e., 
                              
                                 
                                    
                                       θ
                                    
                                    
                                       (
                                       
                                          d
                                          i
                                       
                                       )
                                    
                                 
                                 ∼
                                 Dir
                                 
                                    (
                                    α
                                    )
                                 
                              
                            is chosen. Similarly for ϕ
                           (j), a Dirichlet with parameters β is chosen as prior i.e., ϕ
                           (j) ∼ Dir(β). For K-dimensional Dirichlet random variable θ (
                              
                                 
                                    θ
                                    i
                                 
                                 ≥
                                 0
                                 ,
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    K
                                 
                                 
                                    θ
                                    i
                                 
                                 =
                                 1
                              
                           ), the probability density function is defined as
                              
                                 (7)
                                 
                                    
                                       
                                          
                                             
                                                p
                                                
                                                   (
                                                   θ
                                                   |
                                                   α
                                                   )
                                                
                                                =
                                                
                                                   
                                                      Γ
                                                      (
                                                      
                                                         ∑
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         K
                                                      
                                                      
                                                         α
                                                         i
                                                      
                                                      )
                                                   
                                                   
                                                      
                                                         ∏
                                                         
                                                            i
                                                            =
                                                            1
                                                         
                                                         k
                                                      
                                                      Γ
                                                      
                                                         (
                                                         
                                                            α
                                                            i
                                                         
                                                         )
                                                      
                                                   
                                                
                                                
                                                   θ
                                                   
                                                      1
                                                   
                                                   
                                                      
                                                         α
                                                         1
                                                      
                                                      −
                                                      1
                                                   
                                                
                                                ⋯
                                                
                                                   θ
                                                   
                                                      K
                                                   
                                                   
                                                      
                                                         α
                                                         K
                                                      
                                                      −
                                                      1
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where α is a K-vector with components αi
                            > 0, K is the number of hidden topics and Γ(x) is the Gamma function. Then, the distribution over words for any document is modeled as the mixture
                              
                                 (8)
                                 
                                    
                                       
                                          
                                             
                                                P
                                                
                                                   (
                                                   
                                                      w
                                                      i
                                                   
                                                   )
                                                
                                                =
                                                
                                                   ∑
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   k
                                                
                                                P
                                                
                                                   (
                                                   
                                                      w
                                                      i
                                                   
                                                   |
                                                   
                                                      z
                                                      i
                                                   
                                                   =
                                                   j
                                                   )
                                                
                                                P
                                                
                                                   (
                                                   
                                                      z
                                                      i
                                                   
                                                   =
                                                   j
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           In the learning stage, the probability 
                              
                                 P
                                 
                                    (
                                    
                                       z
                                       i
                                    
                                    =
                                    j
                                    )
                                 
                                 =
                                 Multinomial
                                 
                                    (
                                    
                                       θ
                                       
                                          j
                                       
                                       
                                          (
                                          
                                             d
                                             i
                                          
                                          )
                                       
                                    
                                    )
                                 
                              
                            is computed in terms of θ
                           
                              M × K
                            matrix and 
                              
                                 P
                                 
                                    (
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       z
                                       i
                                    
                                    =
                                    j
                                    )
                                 
                                 =
                                 Multinomial
                                 
                                    (
                                    
                                       ϕ
                                       
                                          
                                             w
                                             i
                                          
                                       
                                       
                                          (
                                          j
                                          )
                                       
                                    
                                    )
                                 
                              
                            in terms of ϕ
                           
                              K × V
                            matrix. In order to generate topic wise word score, ϕ
                           
                              K × V
                            matrix is used and then the words are sorted in descending order of their scores for each topic.

So far motivations and intuitions for LDA have been described. One of the key challenges associated with LDA is the inference problem, in particular computing the posterior probabilities for the hidden variables given a document. Variational EM algorithm (Blei et al., 2003) is introduced for obtaining approximate maximum-likelihood estimates for ϕ
                           (j) and the hyper-parameters of the prior on 
                              
                                 
                                    θ
                                 
                                 
                                    (
                                    
                                       d
                                       i
                                    
                                    )
                                 
                              
                           . Gibbs sampling (Griffiths, 2002) is another method where a symmetric Dir(α) prior on 
                              
                                 
                                    θ
                                 
                                 
                                    (
                                    
                                       d
                                       i
                                    
                                    )
                                 
                              
                            for all documents, and a symmetric Dir(β) prior on ϕ
                           (j) for all topics are considered in the model and Markov Chain Monte Carlo technique is used for the inference. In this paper, Gibbs sampling technique, which is comparatively faster than other existing algorithms is used to infer the model parameters for the given dataset. The plate diagram of the LDA model is shown in Fig. 2
                           .

In Gibbs sampling, the next state is reached by sequentially sampling all variables from their distribution depending on the current values of all other variables and the data. One advantage of Gibbs sampler is that it deals with the subset of the words seen so far rather than the whole data. So, the conditional posterior distribution for j-th topic, 
                              
                                 
                                    z
                                    i
                                 
                                 =
                                 j
                              
                            is given by
                              
                                 (9)
                                 
                                    
                                       
                                          
                                             
                                                P
                                                
                                                   (
                                                   
                                                      z
                                                      i
                                                   
                                                   =
                                                   j
                                                   |
                                                   
                                                      z
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   ,
                                                   w
                                                   )
                                                
                                                ∝
                                                P
                                                
                                                   (
                                                   
                                                      w
                                                      i
                                                   
                                                   |
                                                   
                                                      z
                                                      i
                                                   
                                                   =
                                                   j
                                                   ,
                                                   
                                                      z
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   ,
                                                   
                                                      w
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   )
                                                
                                                P
                                                
                                                   (
                                                   
                                                      z
                                                      i
                                                   
                                                   =
                                                   j
                                                   |
                                                   
                                                      z
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where, 
                              
                                 z
                                 
                                    −
                                    i
                                 
                              
                            is the assignment of all other topics except topic j. From Eq. (9), the first term on the right hand side can be written as
                              
                                 (10)
                                 
                                    
                                       
                                          
                                             
                                                P
                                                
                                                   (
                                                   
                                                      w
                                                      i
                                                   
                                                   |
                                                   
                                                      z
                                                      i
                                                   
                                                   =
                                                   j
                                                   ,
                                                   
                                                      z
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   ,
                                                   
                                                      w
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   )
                                                
                                                =
                                                ∫
                                                P
                                                
                                                   (
                                                   
                                                      w
                                                      i
                                                   
                                                   |
                                                   
                                                      z
                                                      i
                                                   
                                                   =
                                                   j
                                                   ,
                                                   
                                                      
                                                         ϕ
                                                      
                                                      
                                                         (
                                                         j
                                                         )
                                                      
                                                   
                                                   )
                                                
                                                P
                                                
                                                   (
                                                   
                                                      
                                                         ϕ
                                                      
                                                      
                                                         (
                                                         j
                                                         )
                                                      
                                                   
                                                   |
                                                   
                                                      z
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   ,
                                                   
                                                      w
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   )
                                                
                                                d
                                                
                                                   
                                                      ϕ
                                                   
                                                   
                                                      (
                                                      j
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           For a multinomial-Dirichlet model, Eq. (10) gives the predictive distribution when a new word appears. The first term 
                              
                                 P
                                 (
                                 
                                    w
                                    i
                                 
                                 |
                                 
                                    z
                                    i
                                 
                                 =
                                 j
                                 ,
                                 
                                    
                                       ϕ
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                 
                                 )
                              
                            of the integral is equal to 
                              
                                 ϕ
                                 
                                    
                                       w
                                       i
                                    
                                 
                                 
                                    (
                                    j
                                    )
                                 
                              
                           , the multinomial distribution over words associated with topic j. The second term of the integral can be written from Bayes’ rule
                              
                                 (11)
                                 
                                    
                                       
                                          
                                             
                                                P
                                                
                                                   (
                                                   
                                                      
                                                         ϕ
                                                      
                                                      
                                                         (
                                                         j
                                                         )
                                                      
                                                   
                                                   |
                                                   
                                                      z
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   ,
                                                   
                                                      w
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   )
                                                
                                                ∝
                                                P
                                                
                                                   (
                                                   
                                                      w
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   |
                                                   
                                                      
                                                         ϕ
                                                      
                                                      
                                                         (
                                                         j
                                                         )
                                                      
                                                   
                                                   ,
                                                   
                                                      z
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   )
                                                
                                                P
                                                
                                                   (
                                                   
                                                      
                                                         ϕ
                                                      
                                                      
                                                         (
                                                         j
                                                         )
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           Since, P(ϕ
                           (j)) follows Dir(β) as prior and conjugate to 
                              
                                 P
                                 (
                                 
                                    w
                                    
                                       −
                                       i
                                    
                                 
                                 |
                                 
                                    
                                       ϕ
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                 
                                 ,
                                 
                                    z
                                    
                                       −
                                       i
                                    
                                 
                                 )
                              
                            multinomial, then according to the definition of conjugate, the posterior distribution 
                              
                                 P
                                 (
                                 
                                    
                                       ϕ
                                    
                                    
                                       (
                                       j
                                       )
                                    
                                 
                                 |
                                 
                                    z
                                    
                                       −
                                       i
                                    
                                 
                                 ,
                                 
                                    w
                                    
                                       −
                                       i
                                    
                                 
                                 )
                              
                            will also follow 
                              
                                 Dir
                                 (
                                 β
                                 +
                                 
                                    n
                                    
                                       −
                                       i
                                       ,
                                       j
                                    
                                    
                                       (
                                       
                                          w
                                          i
                                       
                                       )
                                    
                                 
                                 )
                              
                           , where 
                              
                                 n
                                 
                                    −
                                    i
                                    ,
                                    j
                                 
                                 
                                    (
                                    
                                       w
                                       i
                                    
                                    )
                                 
                              
                            is the number of occurrences of word wi
                            assigned to topic j, not including the current observing word. So, the integral of Eq. (10) comes as:
                              
                                 (12)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   P
                                                   (
                                                   
                                                      w
                                                      i
                                                   
                                                   |
                                                   
                                                      z
                                                      i
                                                   
                                                   =
                                                   j
                                                   ,
                                                   
                                                      z
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   ,
                                                   
                                                      w
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   )
                                                
                                                
                                                   
                                                      =
                                                      ∫
                                                      
                                                         ϕ
                                                         
                                                            
                                                               w
                                                               i
                                                            
                                                         
                                                         
                                                            (
                                                            j
                                                            )
                                                         
                                                      
                                                      P
                                                      
                                                         (
                                                         
                                                            
                                                               ϕ
                                                            
                                                            
                                                               (
                                                               j
                                                               )
                                                            
                                                         
                                                         |
                                                         
                                                            z
                                                            
                                                               −
                                                               i
                                                            
                                                         
                                                         ,
                                                         
                                                            w
                                                            
                                                               −
                                                               i
                                                            
                                                         
                                                         )
                                                      
                                                      d
                                                      
                                                         
                                                            ϕ
                                                         
                                                         
                                                            (
                                                            j
                                                            )
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      =
                                                      E
                                                      (
                                                      
                                                         ϕ
                                                         
                                                            
                                                               w
                                                               i
                                                            
                                                         
                                                         
                                                            (
                                                            j
                                                            )
                                                         
                                                      
                                                      |
                                                      
                                                         z
                                                         i
                                                      
                                                      =
                                                      j
                                                      ,
                                                      
                                                         z
                                                         
                                                            −
                                                            i
                                                         
                                                      
                                                      ,
                                                      
                                                         w
                                                         
                                                            −
                                                            i
                                                         
                                                      
                                                      )
                                                   
                                                
                                                
                                                   
                                                      =
                                                      
                                                         
                                                            
                                                               n
                                                               
                                                                  −
                                                                  i
                                                                  ,
                                                                  j
                                                               
                                                               
                                                                  (
                                                                  
                                                                     w
                                                                     i
                                                                  
                                                                  )
                                                               
                                                            
                                                            +
                                                            β
                                                         
                                                         
                                                            
                                                               n
                                                               
                                                                  −
                                                                  i
                                                                  ,
                                                                  j
                                                               
                                                               
                                                                  (
                                                                  .
                                                                  )
                                                               
                                                            
                                                            +
                                                            V
                                                            β
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where, 
                              
                                 n
                                 
                                    −
                                    i
                                    ,
                                    j
                                 
                                 
                                    (
                                    .
                                    )
                                 
                              
                            is the total number of words assigned to topic j, not including the current observing word. Similarly, from Eq. (9), the second term on the right hand side can be written as
                              
                                 (13)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      P
                                                      (
                                                      
                                                         z
                                                         i
                                                      
                                                      =
                                                      j
                                                      |
                                                      
                                                         z
                                                         
                                                            −
                                                            i
                                                         
                                                      
                                                      )
                                                   
                                                
                                                
                                                   
                                                      =
                                                      ∫
                                                      P
                                                      
                                                         (
                                                         
                                                            z
                                                            i
                                                         
                                                         =
                                                         j
                                                         |
                                                         
                                                            
                                                               θ
                                                            
                                                            
                                                               (
                                                               
                                                                  d
                                                                  i
                                                               
                                                               )
                                                            
                                                         
                                                         )
                                                      
                                                      P
                                                      
                                                         (
                                                         
                                                            
                                                               θ
                                                            
                                                            
                                                               (
                                                               
                                                                  d
                                                                  i
                                                               
                                                               )
                                                            
                                                         
                                                         |
                                                         
                                                            z
                                                            
                                                               −
                                                               i
                                                            
                                                         
                                                         )
                                                      
                                                      d
                                                      
                                                         
                                                            θ
                                                         
                                                         
                                                            (
                                                            
                                                               d
                                                               i
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      =
                                                      
                                                         
                                                            
                                                               n
                                                               
                                                                  −
                                                                  i
                                                                  ,
                                                                  j
                                                               
                                                               
                                                                  (
                                                                  
                                                                     d
                                                                     i
                                                                  
                                                                  )
                                                               
                                                            
                                                            +
                                                            α
                                                         
                                                         
                                                            
                                                               n
                                                               
                                                                  −
                                                                  i
                                                                  ,
                                                                  .
                                                               
                                                               
                                                                  (
                                                                  
                                                                     d
                                                                     i
                                                                  
                                                                  )
                                                               
                                                            
                                                            +
                                                            K
                                                            α
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where, 
                              
                                 n
                                 
                                    −
                                    i
                                    ,
                                    j
                                 
                                 
                                    (
                                    
                                       d
                                       i
                                    
                                    )
                                 
                              
                            is the number of words from document di
                            assigned to topic j, not including the current observing word and 
                              
                                 n
                                 
                                    −
                                    i
                                    ,
                                    .
                                 
                                 
                                    (
                                    
                                       d
                                       i
                                    
                                    )
                                 
                              
                            is the total number of words in document di
                           , not including the current observing word. Combining the results of Eqs. (12) and (13) in Eq. (9) leads to
                              
                                 (14)
                                 
                                    
                                       
                                          
                                             
                                                P
                                                
                                                   (
                                                   
                                                      z
                                                      i
                                                   
                                                   =
                                                   j
                                                   |
                                                   
                                                      z
                                                      
                                                         −
                                                         i
                                                      
                                                   
                                                   ,
                                                   w
                                                   )
                                                
                                                ∝
                                                
                                                   
                                                      
                                                         n
                                                         
                                                            −
                                                            i
                                                            ,
                                                            j
                                                         
                                                         
                                                            (
                                                            
                                                               w
                                                               i
                                                            
                                                            )
                                                         
                                                      
                                                      +
                                                      β
                                                   
                                                   
                                                      
                                                         n
                                                         
                                                            −
                                                            i
                                                            ,
                                                            j
                                                         
                                                         
                                                            (
                                                            .
                                                            )
                                                         
                                                      
                                                      +
                                                      V
                                                      β
                                                   
                                                
                                                
                                                   
                                                      
                                                         n
                                                         
                                                            −
                                                            i
                                                            ,
                                                            j
                                                         
                                                         
                                                            (
                                                            
                                                               d
                                                               i
                                                            
                                                            )
                                                         
                                                      
                                                      +
                                                      α
                                                   
                                                   
                                                      
                                                         n
                                                         
                                                            −
                                                            i
                                                            ,
                                                            .
                                                         
                                                         
                                                            (
                                                            
                                                               d
                                                               i
                                                            
                                                            )
                                                         
                                                      
                                                      +
                                                      K
                                                      α
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

It has been previously mentioned that the LDA model returns a ϕ
                           
                              K × V
                            matrix which describes the probability of terms (wi
                           ) assuming that they belong to a specific topic (
                              
                                 
                                    z
                                    i
                                 
                                 =
                                 j
                              
                           ) i.e., 
                              
                                 P
                                 (
                                 
                                    w
                                    i
                                 
                                 |
                                 
                                    z
                                    i
                                 
                                 =
                                 j
                                 )
                              
                           . The goal is to discover different significant changes in terms of different latent topics within a set of revisions on an article. This will make a cluster of related terms which reflect the same kind of change. The number of latent topics, which corresponds to the number of different changes is selected beforehand depending on the choice of K. Even though the conventional notations 
                              
                                 
                                    z
                                    i
                                 
                                 =
                                 j
                              
                            and wi
                            are used in the theoretical descriptions, in order to make them consistent throughout the paper, 
                              
                                 
                                    z
                                    i
                                 
                                 =
                                 j
                              
                            to 
                              
                                 
                                    z
                                    i
                                 
                                 =
                                 i
                              
                            and wi
                            to wj
                            are flipped. Moreover, wj
                            is replaced with termj
                            while describing the scoring function. More specifically, when termj
                            belongs to a particular topic, say 
                              
                                 
                                    z
                                    i
                                 
                                 =
                                 i
                              
                           , it is used as termij
                           . Let be the set of terms of the form 
                              
                                 {
                                 
                                    (
                                    
                                       term
                                       ij
                                    
                                    ,
                                    
                                       term
                                       
                                          ij
                                       
                                       
                                          (
                                          s
                                          )
                                       
                                    
                                    )
                                 
                                 ,
                                 
                                    term
                                    ij
                                 
                                 ∈
                                 BOW
                                 }
                              
                           , where termij
                            ∈ BOW is the j-th word of topic zi
                            and 
                              
                                 term
                                 
                                    ij
                                 
                                 
                                    (
                                    s
                                    )
                                 
                              
                            is the corresponding score of termij
                            which is generated via LDA i.e., 
                              
                                 
                                    term
                                    
                                       ij
                                    
                                    
                                       (
                                       s
                                       )
                                    
                                 
                                 ≃
                                 
                                    ϕ
                                    ij
                                 
                              
                           . The latent topic term score (LTTS) for termij
                            ∈ zi
                            is computed as
                              
                                 (15)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            LTTS
                                                            
                                                               (
                                                               
                                                                  term
                                                                  ij
                                                               
                                                               )
                                                            
                                                            =
                                                            
                                                               term
                                                               
                                                                  ij
                                                               
                                                               
                                                                  (
                                                                  s
                                                                  )
                                                               
                                                            
                                                            ×
                                                            
                                                               it
                                                               
                                                               f
                                                            
                                                            
                                                               (
                                                               
                                                                  term
                                                                  ij
                                                               
                                                               )
                                                            
                                                            ,
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               it
                                                               
                                                               f
                                                            
                                                            
                                                               (
                                                               
                                                                  term
                                                                  ij
                                                               
                                                               )
                                                            
                                                            =
                                                            log
                                                            
                                                               
                                                                  (
                                                                  K
                                                                  +
                                                                  1
                                                                  )
                                                               
                                                               
                                                                  tf
                                                                  (
                                                                  
                                                                     term
                                                                     ij
                                                                  
                                                                  )
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where topic frequency (tf) returns the count of a term’s presence in different topics and inverse topic frequency (itf) accounts for a higher weight to the terms which are not common in different topics. Inverse topic frequency (itf) follows a concept similar to the inverse document frequency (idf) which is well known in the literature. This happens because the terms appearing in almost all the topics may have low semantic values for an article. In Eq. (15), the numerator of log  function is increased by 1 i.e., from K to 
                              
                                 (
                                 K
                                 +
                                 1
                                 )
                              
                           , such that when the term appears in all K topics, at least a minimum weight is assigned instead of zero.

Similarly, the latent topic sentence score (LTSS) for sentenceik
                            ∈ zi
                            is calculated as
                              
                                 (16)
                                 
                                    
                                       
                                          
                                             
                                                LTSS
                                                
                                                   (
                                                   
                                                      sentence
                                                      ik
                                                   
                                                   )
                                                
                                                =
                                                
                                                   
                                                      
                                                         ∑
                                                         j
                                                      
                                                      LTTS
                                                      
                                                         (
                                                         
                                                            term
                                                            ij
                                                         
                                                         )
                                                      
                                                   
                                                   
                                                      N
                                                      (
                                                      
                                                         sentence
                                                         ik
                                                      
                                                      )
                                                   
                                                
                                                ,
                                                
                                                   
                                                      0.35
                                                      e
                                                      m
                                                   
                                                   
                                                      0
                                                      e
                                                      x
                                                   
                                                
                                                
                                                   term
                                                   ij
                                                
                                                ∈
                                                
                                                   sentence
                                                   ik
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           Finally, regardless of latent topic zi
                           , the latent topic sentence score (LTSS) for sentencek
                            is calculated
                              
                                 (17)
                                 
                                    
                                       
                                          
                                             
                                                LTSS
                                                
                                                   (
                                                   
                                                      sentence
                                                      k
                                                   
                                                   )
                                                
                                                =
                                                
                                                   max
                                                   i
                                                
                                                LTSS
                                                
                                                   (
                                                   
                                                      sentence
                                                      ik
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           and the sentence is assigned with a topic label which will provide the maximum score to that sentence i.e., 
                              
                                 
                                    z
                                    i
                                 
                                 =
                                 
                                    arg
                                 
                                 
                                    max
                                    i
                                 
                                 LTSS
                                 
                                    (
                                    
                                       sentence
                                       ik
                                    
                                    )
                                 
                              
                           . Therefore, in this approach, the ranking process generates a set of sentences of the form 
                              
                                 {
                                 
                                    (
                                    
                                       sentence
                                       i
                                    
                                    ,
                                    
                                       sentence
                                       
                                          i
                                       
                                       
                                          (
                                          s
                                          )
                                       
                                    
                                    ,
                                    z
                                    )
                                 
                                 ,
                                 
                                    sentence
                                    i
                                 
                                 ∈
                                 S
                                 }
                              
                           , where 
                              
                                 sentence
                                 
                                    i
                                 
                                 
                                    (
                                    s
                                    )
                                 
                              
                            is the corresponding score of sentencei
                            and z is the label to identify which topic the sentence belongs to. Once, the sentence ranking process assigns a score to each sentence, then all the sentences are ranked in a descending order of their scores. Finally the top ranked sentences are chosen to produce as a summary.

When generating score of a term using the third approach, only the changes caused by insertions and modifications (
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                        ) are considered. However, the changes caused by deletions (
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 del
                                 )
                              
                           
                        ) can also play an important role in temporal aspects. In order to incorporate all kinds of changes, another approach is introduced which is a combination of the second and third approaches, where the top ranked sentences of the form 
                           
                              {
                              
                                 (
                                 
                                    sentence
                                    i
                                 
                                 ,
                                 
                                    sentence
                                    
                                       i
                                    
                                    
                                       (
                                       s
                                       )
                                    
                                 
                                 ,
                                 z
                                 )
                              
                              ,
                              
                                 sentence
                                 i
                              
                              ∈
                              S
                              }
                           
                         generated by LDA are re-ranked with a combination of the LTSS and TSS scores. In some situations, there are changes which are inserted and deleted multiple times in the document versions for different reasons. However, LTTS can capture those multiple insertions from the set, 
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                         but is unable to account for the deletions. Unlike LTTS, TSS uses both sets, 
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                         and 
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 del
                                 )
                              
                           
                        , and therefore those multiple insertions and deletions influence TSS. For that reason, using TSS, the highest ranked sentences selected by LTSS will receive lower scores if they are deleted frequently in the revisions. The motivation is that the changes which are frequently deleted in document versions are implied as non significant/general changes. Therefore, the sentence re-ranking process reconsiders the selected sentences so that the effects of non-significant changes are likely to be reduced in the final summary. The combined score is defined as
                           
                              (18)
                              
                                 
                                    
                                       
                                          
                                             
                                                sentence
                                                
                                                   i
                                                
                                                
                                                   (
                                                   s
                                                   )
                                                
                                             
                                             =
                                             λ
                                             ×
                                             LTSS
                                             
                                                (
                                                
                                                   sentence
                                                   i
                                                
                                                )
                                             
                                             +
                                             
                                                (
                                                1
                                                −
                                                λ
                                                )
                                             
                                             ×
                                             TSS
                                             
                                                (
                                                
                                                   sentence
                                                   i
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where the constant term λ ∈ [0, 1] is a regulator parameter which provides flexibility to decide on the proportion of the LTSS and TSS scores to be considered in the final sentence score.

In the sentence ranking process, we need to identify unique sentences by discarding the redundant ones for all proposed approaches. One of the properties expected from a summary of changes is that it should contain no redundant information. In other words, two similar sentences carrying the same information should not be chosen. In practice, it is observed that many sentences have a similar meaning although they contain different terms. These different terms can occur because they are replaced with their synonyms or they appear in various changed forms (e.g., as a result of stemming). However, it can also happen that the words of the two sentences are the same and yet they can carry different meaning. Therefore, it is always difficult to say whether two sentences are equal or not without analyzing their semantics. There have been works on natural language and semantics-based metrics for the Sematic Textual Similarity (STS) task (Agirre, Diab, Cer, & Gonzalez-Agirre, 2012) in the literature. Here, we will use a simple similarity measurement to address the non-redundancy requirement. The similarity measurement between two sentences, sentencei
                         and sentencej
                         is defined as:
                           
                              (19)
                              
                                 
                                    
                                       
                                          
                                             ρ
                                             =
                                             
                                                
                                                   2.0
                                                   ×
                                                   Count
                                                   (
                                                   term
                                                   :
                                                   term
                                                   ∈
                                                   
                                                      sentence
                                                      i
                                                   
                                                   ∧
                                                   term
                                                   ∈
                                                   
                                                      sentence
                                                      j
                                                   
                                                   )
                                                
                                                
                                                   Count
                                                   
                                                      (
                                                      term
                                                      :
                                                      term
                                                      ∈
                                                      
                                                         sentence
                                                         i
                                                      
                                                      )
                                                   
                                                   +
                                                   Count
                                                   
                                                      (
                                                      term
                                                      :
                                                      term
                                                      ∈
                                                      
                                                         sentence
                                                         j
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where Count(.) returns the number of terms. In Eq. (19), the numerator gives the number of common terms between two sentences and the denominator gives the sum of the number of terms in each sentence. Two sentences sentencei
                         and sentencej
                         are said to be equal if ρ ≥ ξ, where ξ ∈ [0, 1] is used as a threshold.

@&#EXPERIMENTS@&#

The experiments to validate the proposed system were organized as follows. First the dataset was prepared with 54 distinct Wikipedia articles. Next, in the data pre-processing stage, an algorithm is proposed for filtering the articles versions with positive contributions. Then, a framework to automatically evaluate the system was used. In the sequel we describe these steps in detail and present a case study to show how the system is built using different approaches.

Wikipedia, the collaboratively edited encyclopedia available on the web, is a major example of a dynamic text collection. Wikipedia is constantly updated by the supporting community to maintain the article’s quality. Currently, Wikipedia has more than 30 million articles written in 287 languages and the English Wikipedia alone has more than 4 million articles (Meta-wiki: List of wikipedias). Wikipedia is a pertinent resource in the context of the summarization of changes task for two main reasons; first, the entire revision history of every web page is kept and these revisions can be accessed publicly through an API; second, because this is a publicly available resource, other people can easily reproduce someone’s findings. To the best of our knowledge, there are three possible ways of accessing Wikipedia’s revision history.
                           
                              •
                              Programmatically parsing the XML revision dumps (Wikimedia database dumps) published by the Wikimedia Foundation on a regular basis. The English Wikipedia is dumped monthly and smaller projects are often dumped twice a month. Nevertheless, the huge size of the English Wikipedia dumps (terabytes in size) makes it impractical to work with this approach.

RevisionMachine (Wikipedia revision toolkit), a part of the Wikipedia Revision Toolkit (Ferschke, Zesch, & Gurevych, 2011) provides an API for retrieving the data from the XML revision dumps and stores them into offline databases (MySQL) in a compressed format. According to the author’s description (Ferschke et al., 2011), “we achieve to reduce the demand for disk space for a recent English Wikipedia dump containing all article revisions from 5470GB to only 96GB, i.e. by 98%”. Although the required storage space is much lower than its original size, initially it is still necessary to have a large amount of space. Since, in our context, it is not necessary to store all of the article’s full revision dumps, it would be preferable to choose a sample set of articles for testing the proposed algorithms.

Another possibility is using the MediaWiki API (Mediawiki api), a web service which directly downloads live data from Wikipedia. This option has another advantage, it is flexible to download an up-to-date article of our own choice and if required, the results can be easily compared to other systems.

In our case, the download process is executed as follows. After selecting the sample of Wikipedia articles, the full revision history of each article is downloaded in XML format using the MediaWiki API. The content for each article version is parsed from the downloaded XML and stored as an individual flat file. A folder is created for each article gathering all the versions with the same article-ID. For ease of use, the filename for a document version follows a specific naming convention. The format is shown below.
                           
                              
                                 
                                    sequence
                                    -
                                    ID
                                    _
                                    time
                                    -
                                    stamp
                                    _
                                    annonymous
                                    -
                                    flag
                                    _
                                    minor
                                    -
                                    flag
                                    .
                                    dmp
                                 
                              
                           
                        The first field is sequence-ID, which assigns 1 to the most recent revision of an article and is sequentially incremented as older revision files are added to the article-ID’s directory. Therefore, the sequence-ID of the last revision file of a particular article-ID represents the oldest revision of that article and at the same time it identifies the total number of revisions made to that article. The second field is the time-stamp of the revision. The third field is the anonymous flag which indicates whether the revision was created by an anonymous user (an unregistered user) or not. The anonymous flag is set to true if the revision is made by an anonymous user, otherwise it is set to false. Similarly, the last field denotes whether it is a minor change or not.

There is plenty of meta information available for any revision of an article. Three meta fields (times-stamp, anonymous flag and minor flag) were used here while creating the name for a revision file. It is found that the other “metadata”, such as user (who made the revision), user-id (id of the revision creator), size (the size of the revision texts in bytes) and comment (why the revision was made) might be useful for further processing. It is worth noting that for an anonymous user the meta field ‘user’ holds the IP address and the meta field ‘user-id’ holds zero. On the other hand, with a registered user the meta field ‘user’ holds the user name, and the meta field ‘user-id’ holds an id. All the four metadata items are saved in a different file with the same naming convention but in .inf format. The template is shown below.
                           
                              
                                 
                                    sequence
                                    -
                                    ID
                                    _
                                    time
                                    -
                                    stamp
                                    _
                                    annonymous
                                    -
                                    flag
                                    _
                                    minor
                                    -
                                    flag
                                    .
                                    inf
                                 
                              
                           
                        The next step is converting each revision file from wiki markup to plain text format. Again each file is saved with the same naming convention but in .txt format. The template is shown below.
                           
                              
                                 
                                    sequence
                                    -
                                    ID
                                    _
                                    time
                                    -
                                    stamp
                                    _
                                    annonymous
                                    -
                                    flag
                                    _
                                    minor
                                    -
                                    flag
                                    .
                                    txt
                                 
                              
                           
                        These plain text files are used directly for pre-processing.

There is a lot of revisions where the changes are reverted back mainly due to vandalism issues for the article. Therefore, it is necessary to filter out the article’s revisions during the pre-processing stage. The authors in Wikipedia Event Reporter (Georgescu et al., 2013) simply discarded the updates made by anonymous users to avoid most suspicious edits. However, this assumption does not seem reasonable since an anonymous user can also make positive contributions to an article.

In a preliminary step during pre-processing, the reverted or undid revisions which are simply identified from the corresponding metadata are simply discarded. However, later it is found that discarding only the reverted or undid revisions is not enough because, at the same time it is necessary to discard the revisions where the vandalism texts actually have been inserted. For this purpose, an algorithm (see Algorithm 1
                        ) is proposed to filter out the bad revisions before the proposed methodology starts working. This is one of the possible ways of handling vandalized revisions, so that it is possible to focus on the valuable changes that were made to the revisions.


                        Table 2
                         presents the current overall statistics for the complete revision history for 49 selected Wikipedia articles, using each articles lifespan up to January 2015. The columns in Table 2 show the total number of versions made, the number of versions where the minor changes were made, the number of versions where the changes were made by the unregistered (anonymous) users and the number of identified bad revisions using the following proposed algorithm respectively.

The experiments are performed on 54 case studies for 49 distinct Wikipedia articles within different given time periods. These 54 case studies are selected based on two criteria: (i) there can be exactly one significant change made to an article within the chosen time period; (ii) the change should be known a priori. The second criterion makes it possible to build a framework for evaluating the proposed approaches. If the significant change to any article within a time period is known beforehand, it is possible to make a corresponding reference summary. To construct a reference summary for a given time range, a set of sentences are previously selected and extracted to describe the exact significant change. As a result, a set of reference summaries were prepared for all the articles selected corresponding to their given time periods. It is worth noting that the first criterion helps to prepare the reference summaries without any ambiguity. In the summarization of changes system, when a user selects an article of interest for a given time period, the total number of revisions within that time period are counted in order to detect the changes. For a very long time period, in general, the article may have a large number of revisions, which can either be significant or general changes. Now the question is how to prepare a reference summary for that time period. If a very long time period is chosen for an article, the reference summaries of the article may vary from person to person because they can give different priorities to different significant changes. Suppose that there are four significant changes made in a very long time frame. While building a reference summary, one person can choose the first and second as being significant, whereas other may consider the third and fourth as the most significant. Therefore, it would be difficult to prepare a proper reference summary without any ambiguity. In order to avoid these complications initially, the focus is on those time intervals, which have one strong significant change besides the general changes and that change should be known a priori. This way, a non-ambiguous human-created summary can be attributed for every input period. The significant change is so prominent, there does not arise any doubt of having multiple reference summaries within the given time period. Moreover, for every reference summary, the sentences are extracted from the latest version of the Wikipedia article in the given time period instead of writing a reference summary manually. This also implies that multiple reference summaries are not required for an article within a time period. In the evaluation framework proposed here, these reference summaries are provided in comparison with the corresponding system-generated summaries.

In the evaluation, the results obtained by using different approaches (system-generated summaries) are compared with summaries created by humans (reference summaries) using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics (Lin & Hovy, 2003) as ROUGE metrics are widely used by the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) for update summarization task (Delort & Alfonseca, 2012; Li et al., 2012; Schilder et al., 2008; Schilder & Kondadadi, 2008; Wang & Li, 2010; Wenjie et al., 2009; Zhang et al., 2009). These metrics automatically measure the quality of a summary by counting the number of overlapping words between the system-generated summary and a reference summary. When constructing a reference summary for a given time range, the best sentences are selected and extracted from the latest version of that specified Wikipedia article in the given time period. These manually created reference summaries are provided to compare against the system-generated summaries. Therefore, ROUGE scores can express whether the best sentences are picked or not by the proposed approaches. Intuitively, a higher ROUGE score means the system-generated summary using one of the proposed approaches and the human-created summary are more similar. Moreover, according to the authors of the ROUGE toolkit (Lin & Hovy, 2003), ROUGE-1 and ROUGE- 2 have high correlation with the human judgments.

There are different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S and ROUGE-SU. ROUGE-N is an n-gram recall between a candidate summary and a set of reference summaries. ROUGE-N is computed as follows:
                           
                              (20)
                              
                                 
                                    
                                       
                                          
                                             ROUGE
                                             -
                                             N
                                             =
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         S
                                                         ∈
                                                         {
                                                         ReferenceSummaries
                                                         }
                                                      
                                                   
                                                   
                                                      
                                                         0.35
                                                         e
                                                         m
                                                      
                                                      
                                                         0
                                                         e
                                                         x
                                                      
                                                   
                                                   
                                                      ∑
                                                      
                                                         
                                                            gram
                                                            n
                                                         
                                                         ∈
                                                         S
                                                      
                                                   
                                                   
                                                      Count
                                                      match
                                                   
                                                   
                                                      (
                                                      
                                                         gram
                                                         n
                                                      
                                                      )
                                                   
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         S
                                                         ∈
                                                         {
                                                         ReferenceSummaries
                                                         }
                                                      
                                                   
                                                   
                                                      
                                                         0.35
                                                         e
                                                         m
                                                      
                                                      
                                                         0
                                                         e
                                                         x
                                                      
                                                   
                                                   
                                                      ∑
                                                      
                                                         
                                                            gram
                                                            n
                                                         
                                                         ∈
                                                         S
                                                      
                                                   
                                                   Count
                                                   
                                                      (
                                                      
                                                         gram
                                                         n
                                                      
                                                      )
                                                   
                                                
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where n is the length of the n-gram, gramn, Countmatch
                        (gramn
                        ) is the maximum number of n-grams co-occurring in a system-generated summary and a set of reference summaries, and Count(gramn
                        ) is the number of n-grams in the reference summaries. ROUGE-1 and ROUGE-2 metrics of ROUGE-N are used here with the length of the n-gram as 
                           
                              n
                              =
                              1
                           
                         and 
                           
                              n
                              =
                              2
                           
                        , respectively. The other ROUGE metrics used is ROUGE-L, which measures the longest common subsequence (LCS) between a system-generated summary and a reference summary. ROUGE-W is similar to ROUGE-L except it is based on weighted LCS where the weighting function is 
                           
                              f
                              
                                 (
                                 L
                                 )
                              
                              =
                              
                                 
                                    L
                                 
                                 weight
                              
                              ,
                              L
                           
                         indicates the length of LCS. Here, the input of the weight is given as 
                           
                              weight
                              =
                              1.2
                           
                         i.e., the metric ROUGE-W-1.2 is calculated. ROUGE-S measures the overlapping of skip-bigrams where the maximum gap length between two words is given as 4 i.e., ROUGE-S4 is calculated. ROUGE-SU4 is calculated here to perform an evaluation similar to ROUGE-S, where the maximum gap length between two words is given as 4 with the addition of unigram as a counting unit.

Although each of these ROUGE metrics has three scores (recall, precision and F-measure), there is similar conclusion in terms of any of them. For simplicity, in this paper, the average F-measure (the harmonic mean of precision and recall) scores are reported as generated by ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-W-1.2, ROUGE-S4 and ROUGE-SU4 to compare the proposed approaches.

The experiments are performed on 54 different case studies for a set of Wikipedia articles. However, in order to easily understand the overall framework, a case study is conducted on a Wikipedia article where the flow is described from beginning to end and the intermediate results obtained with the different proposed approaches in the summarization of changes system are demonstrated.

For this case study, the Wikipedia article on Narendra Modi with article id 444222 is chosen. The number of revisions made to this article over time is plotted in Fig. 3
                         using the WikiChanges system (Nunes et al., 2008). As discussed in Section 2.3, the WikiChanges system is a web-based application designed to plot the distribution of the revisions made to an article on a monthly/daily basis. Fig. 3 shows that there are 515 revisions made in May, 2014 alone. In these 515 revisions, the significant change is “Narendra Modi was elected as Prime Minister during the month of May, 2014”. However, most of these edits were not related to this main reason, but instead were general edits. Basically, the number of revisions in an article grows significantly while some important events are taking place, because of the upcoming new information and also due to the update of general information with the growing popularity of the article. The challenge in our system proposed here is picking solely the change that reflects the main reason for edits.

Because the entire month of May, 2014 is selected for the article on Narendra Modi, 515 revisions are considered initially. In the pre-processing step, the vandalized revisions are filtered out from the 515 revisions using Algorithm 1 discussed in Section 5.2. After filtering, the number of revisions is reduced to 441. These revisions are supposed to contain the changes with positive contributions made to the article. These 441 revisions are defined as 
                           
                              A
                              =
                              {
                              rev
                              
                                 
                                 1
                              
                              ,
                              rev
                              
                                 
                                 2
                              
                              ,
                              ⋯
                              ,
                              rev
                              
                                 
                                 441
                              
                              }
                           
                         (discussed in Section 3). It should also be noted here that 
                           
                              rev
                              
                                 
                                 1
                              
                           
                         is the latest revision and 
                           
                              rev
                              
                                 
                                 441
                              
                           
                         is the oldest revision in that given time period.

In the following step, the system extracts the two sets of changes by comparing consecutive revisions. In practice, the changes in revisions can be made in three ways: insertion, modification and deletion. The changes which are caused by insertions or modifications in revisions form the first set, D
                        (ins) whereas the second set, D
                        (del) consists of the changes caused by deletions. Therefore, the two sets of changes can be written together as 
                           
                              D
                              =
                              
                                 
                                    D
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                              ∪
                              
                                 
                                    D
                                 
                                 
                                    (
                                    del
                                    )
                                 
                              
                           
                        . Both sets of changes D
                        (ins) and D
                        (del) are processed further in two modes: word mode and block mode. In the word mode, the changed words are taken from the consecutive revisions by comparing on a word basis while in block mode, the changed information are excerpted in paragraph basis. After this process, the four sets of changes 
                           
                              
                                 D
                                 
                                    word
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                              ,
                              
                                 D
                                 
                                    word
                                 
                                 
                                    (
                                    del
                                    )
                                 
                              
                              ,
                              
                                 D
                                 
                                    block
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                           
                         and 
                           
                              D
                              
                                 block
                              
                              
                                 (
                                 del
                                 )
                              
                           
                         are generated. The maximum cardinality for each of these four sets is 440.

In the following step for sentence ranking, different term scoring measurements are used in different approaches. In the first approach, baseline temporal term score (BTTS) for each term is generated using Eq. (1) with the sets 
                           
                              A
                              ,
                              
                                 D
                                 
                                    word
                                 
                                 
                                    (
                                    ins
                                    )
                                 
                              
                           
                         and 
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 del
                                 )
                              
                           
                        . In Eq. (1), the range of parameter α varies between 0 and 1. According to the author’s (Jatowt et al., 2004) explanation, increasing the value of parameter α allows higher relative scores to be given to the rare or specific terms rather than terms with a general meaning. Because in this step the goal is to preferably find the significant terms, the value of parameter α is set to 1.

In the second approach, the temporal term score (TTS) is calculated using Eq. (5) with the two sets of changes, 
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                         and 
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 del
                                 )
                              
                           
                        . In the third approach, a feature file is created using the set 
                           
                              D
                              
                                 word
                              
                              
                                 (
                                 ins
                                 )
                              
                           
                        . The feature file consists of 440 feature vectors. This feature file is given as an input file in the LDA model and the output from the LDA model using Eq. (15) represents a set of words/terms in which each word is associated with a score for each of the K latent topics. To facilitate the system’s evaluation, the time period with one significant change is chosen. For this reason, the number of latent topics is always given as 
                           
                              K
                              =
                              2
                           
                        , where one topic is supposed to correspond to the significant change and the other one reflects general changes. Section 4.3.2 mentions that Gibbs sampling (Griffiths, 2002) and Markov Chain Monte Carlo technique are used to infer the LDA model parameters. Here, Gibbs sampling is run for 1000 iterations and for all runs, a symmetric Dirichlet prior on θ with 
                           
                              α
                              =
                              0.5
                           
                         and a symmetric Dirichlet prior on ϕ with 
                           
                              β
                              =
                              0.1
                           
                         are used. The terms with corresponding scores are generated using LDA model for 
                           
                              K
                              =
                              2
                           
                         latent topics.

A first exploratory examination of the proposed approaches is performed and the approaches are compared by looking at the illustrative case study presented in Table 3
                        . This table lists the 30 best scoring terms obtained with the three main approaches, baseline temporal term score (BTTS), temporal term score (TTS) and latent topic term score (LTTS) for the Wikipedia article on Narendra Modi for the month of May, 2014. LTTS is shown separately in the last two columns for topic 1 (LTTS: Topic 1) and for topic 2 (LTTS: Topic 2). It is possible to confirm that there are clear differences between each pair of columns, even when the top 30 terms are considered. The terms which are directly related to the significant change are marked in bold in each column of Table 3. In the first column, it is obvious that the top 30 terms selected by BTTS do not clearly indicate the significant change. Among the 30 terms, only the terms elections2014 and 26th are relevant when using BTTS. TTS selects a higher number of relevant terms than BTTS. Basically, TTS assigns higher scores to the terms which have been inserted more times and deleted less times in the revisions for a given time period. If, within a period, there have not been many general changes besides the significant change, then TTS can pick the relevant terms to describe the significant change. However, if there are general changes within that period, individually TTS is not sufficient to pick and assign the relevant terms with higher scores due to conflict with the terms in the general changes. This problem is handled using LTTS. The third column (LTTS: Topic 1) provides the relevant terms, which are more specifically related to the significant change. Relevant terms such as prime, minister, 2014, india, bjp, election, indian, born and general are selected in both TTS and LTTS: Topic 1. Besides these relevant terms, terms such as victory, lok, sabha, chief, president are more specific to describe the significant change are further captured in LTTS: Topic 1. Moreover, there are no terms related to the significant change in LTTS: Topic 2, whereas the terms that describe other common changes are reflected in LTTS: Topic 2.

An observable point to be noted in Table 3 that there are the terms such as gujarat, narendra, modi, which are common in both columns, LTTS: Topic 1 and LTTS: Topic 2. This is the reason why the concept of inverse topic frequency (itf) is used in Eq. (15) to calculate the latent topic term score (LTTS). In Eq. (15), a weight which is assigned by LDA model to each term in a latent topic is further multiplied by the inverse topic frequency (itf) of that term. Therefore, the common terms in both topics obtain lower scores with the motivation that they may convey less information to the significant change.

The following step in sentence ranking is calculating a score for each sentence so that the sentences with the higher scores are presented as a summary. The sentence score in both the first and second approaches, i.e. baseline temporal sentence score (BTSS) using Eq. (2) and temporal sentence score (TSS) using Eq. (6) are simply calculated based on the sum of the scores of all terms divided by the total number of terms (excluding stop words) the sentence contains. In the third approach, latent topic sentence score (LTSS) using Eq. (17) is calculated in a similar way, specified in the first and second approaches. However, the only difference in LTSS is that each sentence’s score needs to be calculated for each latent topic. That score is finally decided when a topic gives the maximum score to that sentence. LTSS generates a set of sentences of the form 
                           
                              {
                              
                                 (
                                 
                                    sentence
                                    i
                                 
                                 ,
                                 
                                    sentence
                                    
                                       i
                                    
                                    
                                       (
                                       s
                                       )
                                    
                                 
                                 ,
                                 z
                                 )
                              
                              ,
                              
                                 sentence
                                 i
                              
                              ∈
                              S
                              }
                           
                        , where 
                           
                              sentence
                              
                                 i
                              
                              
                                 (
                                 s
                                 )
                              
                           
                         is the corresponding score of sentencei
                         and z is the label to identify which topic the sentence belongs to. The last three approaches are a combination of LTSS and TSS approaches. In these approaches, basically the top ranked sentences of the form 
                           
                              {
                              
                                 (
                                 
                                    sentence
                                    i
                                 
                                 ,
                                 
                                    sentence
                                    
                                       i
                                    
                                    
                                       (
                                       s
                                       )
                                    
                                 
                                 ,
                                 z
                                 )
                              
                              ,
                              
                                 sentence
                                 i
                              
                              ∈
                              S
                              }
                           
                         generated by LTSS are re-ranked with a combined score of LTSS and TSS. The regulator parameter λ ∈ [0, 1] with values 
                           
                              λ
                              =
                              0.75
                              ,
                              λ
                              =
                              0.50
                           
                         and 
                           
                              λ
                              =
                              0.25
                           
                         are given in Eq. (18). The top 5 scoring sentences with different approaches, regarding the Wikipedia article on Narendra Modi for the month of May, 2014 are shown in Table 4
                        . The table shows that no redundant sentence is chosen in 6 different summaries generated by different approaches. Non-redundancy is one of the objectives of the summarization of changes task. The threshold, 
                           
                              ξ
                              =
                              0.70
                           
                         is used here to measure the similarity between two sentences using Eq. (19).

The final step is automatically evaluating the summaries generated by the system using different approaches. Different ROUGE metrics are used to measure the quality of a summary comparing a system-generated summary using one of the proposed approaches, and a summary created by a human. The higher the ROUGE scores, the more similar are the summary created with the proposed approach and the summary created by a human. Table 5
                         represents an example of a human-generated summary used as reference summary for the Wikipedia article on Narendra Modi. Table 6
                         shows the performances of the different approaches after evaluating Wikipedia article on Narendra Modi for the month of May, 2014. Table 6 shows that the approach using the combination of LTSS and TSS (
                           
                              λ
                              =
                              0.75
                           
                        ) provides the best summary for this case study.

The different approaches proposed are compared in 54 case studies on 49 distinct Wikipedia articles. Table 7
                      demonstrates each article’s given time period, the number of revisions made during that period and the actual number of revisions considered within that period after filtering with Algorithm 1. Some articles are chosen more than once to make different case studies by selecting several time periods. For example, the fifth, sixth and seventh rows in Table 7 are selected for the same article, Steve Fossett with article ID 186642, but with different time periods. The overall performances of all the approaches based on 6 ROUGE metrics are shown in Table 8
                     . From the comparison results, the following observations are noticed:
                        
                           •
                           
                              BTSS chooses mainly the sentences which have uncommon/typical words on a specific domain. This may be advantageous in order to detect the significant changes that reflect new information. However, in practice the changes are not always made using typical words. That is why the overall ROUGE scores for BTSS are the lowest.


                              TSS chooses the sentences-whose terms are inserted more frequently and simultaneously deleted less times in the revisions. A term which has been revised frequently in a set of document revisions should be more important because it could reflect a significant change. Therefore, TSS performs better than the implemented baseline approach, BTSS. However, again it is not always possible to capture the main change because the terms are revised both by adding new information and by updating the general information of the article.


                              LTSS selects the sentences from different topics where each topic is a cluster of terms which reflects the same kinds of changes. This way, all the words related to any change are likely to be grouped. The sentences which contain more words related to a particular topic have higher weights. Moreover, it is important to notice that if a topic brings a significant change, the terms associated to this topic have higher weights than the terms associated to another topic. This is why LTSS outperforms BTSS and TSS.

In the last approach, the sentences produced by LTSS are re-ranked by combining LTSS and TSS. The goal is to incorporate the deletion effects into LTSS. However, the overall performance of the last approach is not improved comparatively to LTSS, which is shown in Table 8. This happens because, as TSS itself does not outperform LTSS, the re-ranking approach using a linear combination of TSS does not perform better or the selected articles do not have the effects of deletions in LTSS.

The statistical distributions of ROUGE scores for all approaches are shown using boxplots in Fig. 6. It is observed that the performances of different approaches are arranged from lower to higher order as BTSS, TSS, the proportions of LTSS and individual LTSS. The statistical tests are further performed to see if the differences in ROUGE scores for different proposed approaches are significant or not. A Friedman test (Friedman, 1940) is used because the samples (ROUGE scores for all case studies) are not normally distributed. This test reveals for all ROUGE metrics, p-value is lower than 0.001. This indicates that for each ROUGE metric, there is at least one statistically significant difference between two of the approaches. To identify these cases, a post hoc analysis using Nemenyi tests (Nemenyi, 1963) is conducted. Tables 9 and 10
                     
                      show that there are significant differences between BTSS and TSS (p < 0.005), between BTSS and LTSS (p < 0.001), between BTSS and LTSS + TSS (
                        
                           λ
                           =
                           0.75
                        
                     ) (p < 0.001), between BTSS and LTSS + TSS (
                        
                           λ
                           =
                           0.50
                        
                     ) (p < 0.001), and between BTSS and LTSS + TSS (
                        
                           λ
                           =
                           0.25
                        
                     ) (p < 0.001) for ROUGE-1 and ROUGE-L scores. Similar results are found for other ROUGE metrics as well.

We have also studied the same article in different time periods. As an example, three different time periods are chosen for the Wikipedia article on late USA adventurer Steve Fossett with article ID 186642. Table 7 shows the details of these three records. The first time period is the month of September, 2007 where the main changes are related to the fact that Fossett was reported missing; the second is the month of February, 2008, where the main changes are related to the fact that Fossett was declared dead and the third one is the month of October, 2008, where the main changes are related to the identification of Fossett’s airplane wreckage and other personal items, which were found near Mammoth Lakes, California. The purpose is to test whether the system is able to capture those significant changes for the same article but for different time periods. Table 11
                      shows the best summaries given by either LTSS or with the combination approach. Table 11 shows that the three different significant changes do not overlap, and the summarization of changes system is able to detect the changes for the different time periods.

This paper also studies the effects on the ROUGE scores of the increasing number of revisions made to the different articles for the given time periods. Generally, when the number of revisions increases, more conflicts are likely to occur while choosing the sentences with the significant change vs other changes. Because a similar conclusion is obtained for different ROUGE scores, ROUGE-1 and ROUGE-2 results are shown in Figs. 4 and 5
                     
                     
                     . It is observed from the figures that the system performs well even when the number of revisions is within the higher range of [1500, 5750].

@&#CONCLUSIONS@&#

The summarization of changes focuses on the generation of abridged and non-redundant accounts of document modifications in dynamic text collections. This research introduces a new framework for summarizing changes from a set of revisions made to a Wikipedia article during a given time period. Four different approaches are proposed for the summarization of changes. The first approach provides a baseline that is adapted from an existing related work (Jatowt et al., 2004), which periodically monitors a web collection in search for recent changes and generates their summary with respect to a specific topic. The summarization of changes differs from this task, as it addresses the changes and generates their summary in dynamic text collections within any user-defined period. In the second approach, each term’s temporal aspect is investigated by considering the joint probabilities of both the insertion and deletion events over a set of document versions within the given period. The third approach is based on Latent Dirichlet Allocation (LDA) model for finding hidden/latent topic structures of changes. The fourth approach is a combination of the previous two approaches in which the top ranked sentences generated from the third approach are re-ranked with a combined score from the second and third approaches.

The four approaches are used initially to estimate the term scores, and then to rank the sentences based on those scores. Finally, for generating a summary, a few top ranked sentences are chosen independently for each of the approaches. All of them are evaluated using ROUGE metrics by comparing the system-generated summaries and the human-created reference summaries. It is observed that the third approach based on LDA model outperforms the others.

Although a set of articles from Wikipedia have been used with their full revision histories as a document collection, these approaches can be used in other time-dependent collections. For example, any of them can be used to generate a change summary on the history of a single web page from a web archive.

In this study, a simple metric is used for equality measurement between two sentences. Future work is expected to investigate richer equality measurement metrics, such as the ones used in Natural Language Processing for the Semantic Textual Similarity task. This will improve the identification of redundant sentences. In this work, a framework was set up and used for automatic evaluation. The summaries produced by each of the approaches are evaluated comparatively to the manual summaries using ROUGE metrics. An extrinsic evaluation, considering human feedback in this task is expected as future work.

To evaluate the proposed system, we took into consideration the time periods where exactly one significant change has occurred in any article. For this reason, the number of latent topics is considered as 
                        
                           K
                           =
                           2
                        
                     . However, when there is more than one significant change within a given time range, then it is necessary to increase the value of K. In those cases, finding an optimum K is an interesting research challenge to address in the future.

@&#ACKNOWLEDGMENTS@&#

This work is financially supported by Erasmus Mundus Mover and “NORTE-07-0124-FEDER-000059” project by the North Portugal Regional Operational Programme (ON.2—O Novo Norte), under the National Strategic Reference Framework (NSRF), through the European Regional Development Fund (ERDF), and by national funds, through the Portuguese funding agency, Fundação para a Ciência e a Tecnologia (FCT). We thank the anonymous reviewers, whose comments have contributed to important improvements to the final version of the article.

@&#REFERENCES@&#

