@&#MAIN-TITLE@&#ValWorkBench: An open source Java library for cluster validation, with applications to microarray data analysis

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Cluster validation for microarray data analysis is an essential task in bioinformatics and biomedicine that is not receiving enough attention.


                        
                        
                           
                           The state of the art does not offer software systems able to help the development of much needed new measures.


                        
                        
                           
                           We propose ValWorkBench and describe its internal modules and classes, in order to provide the much needed software platform for the development and testing of new validation measures as well as clustering algorithms.


                        
                        
                           
                           From the programmer point of view, no other platform in the same area offers what ValWorkBench provides.


                        
                        
                           
                           From the user point of view, the measures contained in it have been the object of, rigorous scientific studies, i.e., in BMC Bioinformatics 9 (2008) 462; Algorithms for Molecular Biology 6 (2011) 1; Natural Computing, pages 2014 (in press).


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Microarray cluster analysis

Bioinformatics software

Pattern discovery in bioinformatics and biomedicine

@&#ABSTRACT@&#


               
               
                  The prediction of the number of clusters in a dataset, in particular microarrays, is a fundamental task in biological data analysis, usually performed via validation measures. Unfortunately, it has received very little attention and in fact there is a growing need for software tools/libraries dedicated to it. Here we present ValWorkBench, a software library consisting of eleven well known validation measures, together with novel heuristic approximations for some of them. The main objective of this paper is to provide the interested researcher with the full software documentation of an open source cluster validation platform having the main features of being easily extendible in a homogeneous way and of offering software components that can be readily re-used. Consequently, the focus of the presentation is on the architecture of the library, since it provides an essential map that can be used to access the full software documentation, which is available at the supplementary material website [1]. The mentioned main features of ValWorkBench are also discussed and exemplified, with emphasis on software abstraction design and re-usability. A comparison with existing cluster validation software libraries, mainly in terms of the mentioned features, is also offered. It suggests that ValWorkBench is a much needed contribution to the microarray software development/algorithm engineering community. For completeness, it is important to mention that previous accurate algorithmic experimental analysis of the relative merits of each of the implemented measures [19,23,25], carried out specifically on microarray data, gives useful insights on the effectiveness of ValWorkBench for cluster validation to researchers in the microarray community interested in its use for the mentioned task.
               
            

@&#INTRODUCTION@&#

The advent of high throughput technologies for biological and biomedical research, in particular microarrays, has demanded fast progress in many areas of the information sciences, including the development of mathematical and statistical software environments able to “standardize” many of the data analysis pipelines for biological investigation. Well known examples are BioJava [44], BioPerl [50], SeqAn [13], MatLab [32] and R [46]. The first three are software libraries that privilege the “programmer and algorithm engineer point of view”, in the sense that modules and procedures within the entire libraries can be used to develop new tools. The second two privileges the “user point of view”, since the available tools are offered either via a GUI or as a set of implemented functions, easy to use as a black box. However, from the “programmer and algorithm engineer point of view”, there is no direct access to the functions in order to use them as building blocks to implement new methods. That is due to copyright restrictions and/or to the complex structure and constrains of the existing packages.

In terms of methodologies, cluster analysis, with its long record of deep mathematical and statistical studies on which it is based [16,33,29,34,30] and well documented success in many applied sciences, e.g., biomedicine [4], is a natural candidate for biological data analysis. As for microarrays, its analysis potential was shown almost immediately after their introduction in a pioneeristic paper by Eisen et al. [15]. Shortly thereafter additional results, mostly related to cancer classification [2,3,14,27,41,42,48], helped in establishing cluster analysis as one of the essential techniques to discover “biologically significant groups” in microarray data [10]. Since then, most of the attention has been devoted to the development of new clustering algorithms, although cluster analysis is a process that goes beyond the mere production of a partition of the data. Indeed, essential to the process is the assessment of the quality of the partition obtained by the algorithm, with the use of specific indices, referred to as validation measures, as well explained in [33]. Such a practice was mostly ignored in microarray data analysis, as well argued by Handl et al. [28] in a study that certainly contributed to the adoption of cluster validation techniques as a common practice in microarray cluster analysis. Indeed, some of the validation methods that have been specifically designed for microarrays have become very popular and seem to be used in many studies, e.g., the methods in [14,40]. Unfortunately, novel and effective validation measures are not easy to come by, in particular for very challenging high dimensional data such as microrrays, and in fact the entire area of cluster analysis for post-genomic studies is still the object of intense research [22,35].

Relevant for this contribution is the state of the art regarding cluster validation software libraries In particular, there is no software library that offers a wide range of measures and that can be useful both for data analysis and for research in the development, prototyping and benchmarking of new validation measures. Indeed, with reference to the software environments mentioned above, clustering software is absent in BioJava, BioPerl and SeqAn. MatLab and R, being designed to cater to an audience wider than bioinformatics, computational biology and biomedicine, offer cluster analysis tools. However, as discussed in detail in Section 3, they privilege mostly the “user point of view”, the only notable exception being mosclust 
                     [52] that offers full documented access to its modules for program development. Unfortunately, the library consists of two internal validation measures only. Moreover, most of the validation measures present in the mentioned software environments do not allow to be used in conjunction with a clustering algorithm that has been developed outside of their specific programming environment. That is, an algorithm that is external to the system, available as an executable and compatible with the input/output conventions of the validation measures. Such a level of “algorithm independence” would allow the fast validation of novel external clustering algorithms, as discussed in Section 3.

Therefore, given the state of the art depicted above, the main contribution of this paper is to fill an important gap in the literature by carrying out the non-trivial task of providing the full software documentation of ValWorkBench, an open source and portable Java library for cluster validation specifically tested on microarray data [23,25,19]. The objective is to grant full access to the wealth of software modules and classes present in the library, which can be used for the fast development, prototyping and testing of new internal validation measures as well as clustering algorithms, therefore privileging here the “programmer and algorithm engineer point of view”. For completeness, we mention ValWorkBench is the result of the accurate and robust comparative experimental analysis mentioned earlier (see [23,25,19] again). However, the primary intent of that line of research is to provide useful information for the choice of a measure in microarray applications, i.e., its precision in identifying the correct number of clusters and its time performance, in order to give the potential user useful insights on the effectiveness of the proposed library. However, as of now, its internal structure is not readily accessible and its wealth of modules and classes cannot be used for algorithm development and experimentation.


                     ValWorkBench is freely and anonymously available at [1]. It is open source and distributed under the GNU licence. Javadocs and instructions on how to install it are available at the website, as well as the clustering algorithms binary executables (e.g. K-means [33], Non Negative Matrix Factorization [38] and Hierarchical [33]). ValWorkBench is tested and runs on any platform that supports Java version 1.6 or higher, in particular for Windows, Linux and Mac OS X.

The remainder of this paper is organized as follows. Section 2 offers an overview of the library software structure as well as some background material on clustering, essential for the presentation of ValWorkBench. In particular, Section 2.1 provides the mentioned prerequisites. Section 2.2 presents, at a high level, the main software components of ValWorkBench. The next four sub-sections offer some details of particularly relevant classes. A fully commented account of code details is provided in the Additional Files available at the supplementary material website [1]. For completeness, JavDocs and user manuals are also provided there. Section 3 discusses the main progress, in terms of software design and algorithmic benchmarking, granted by the full documentation reported here of the internals of ValWorkBench. Moreover, such a contribution is also highlighted via a comparison of ValWorkBench with other libraries offering implementations of internal validation measures. The last section contains the conclusions and future steps.

Following Handl et al. [28], cluster analysis is seen here as a three step process. The first, usually referred to as preprocessing, consists of data normalization, feature selection and of the choice of a distance function. The state of the art is given in [45] for normalization, in [39] for feature selection and in [21,20,22,43] for the choice of similarity/distance functions. Regarding the other two steps, which consist of the choice of a clustering algorithm and of a validation technique, in what follows we highlight the essential aspects of them, with some emphasis on cluster validation since it is central for this paper. To this end, we need to introduce some notation.

Consider a set of n items Σ={σ
                        1, …, σ
                        
                           n
                        }, where σ
                        
                           i
                        , 1≤
                        i
                        ≤
                        n, is defined by m numeric values, referred to as features or conditions, and let C
                        
                           k
                        
                        ={c
                        1, c
                        2, …, c
                        
                           k
                        } be a partition of Σ. Each subset c
                        
                           i
                        , 1≤
                        i
                        ≤
                        k, is referred to as a cluster, and C
                        
                           k
                         is referred to as a clustering solution.

Usually, the partition of the items in Σ is accomplished by means of a clustering algorithm A. A recent survey of classic as well as more innovative methods, specifically designed for microarray data, is given in [4,49] and a more in depth treatment can be found for instance in [16,28,29,33,34]. For the convenience of the reader, we recall that clustering algorithms are classified into: partitional and hierarchical.

The first type of clustering algorithms take as input Σ and an integer k and give as output a partition C
                           
                              k
                            of Σ, with |C
                           
                              k
                           |=
                           k. It is worth pointing out that a partitional clustering algorithm can take as input a partition of the data and use it as an initial clustering solution that the algorithm refines, hopefully improving its quality. In this paper, we refer to this input option as external initialization. The second type of clustering algorithms produce a nested sequence of partitions, i.e. a tree. However, they can be easily adapted to generate a partition of a dataset into k clusters. The details are left to the reader.

Let 
                              
                                 
                                    
                                       C
                                       ¯
                                    
                                 
                                 j
                              
                            be a reference classification for Σ, consisting of j classes. That is, 
                              
                                 
                                    
                                       C
                                       ¯
                                    
                                 
                                 j
                              
                            may either be a partition of Σ into j groups, usually referred to as the gold solution, or a division of the universe generating Σ into j categories, usually referred to as class labels.

An an external measure E is a function that takes as input two partitions C
                           
                              j
                            and C
                           
                              k
                            and returns a value assessing how close C
                           
                              k
                            is to C
                           
                              j
                           . It is external because the quality assessment of the partition is established via criteria external to the data. Notice that it is not required that j
                           =
                           k. ValWorkBench provides the three most prominent external measures known in the literature: the Adjusted Rand Index [31], the F-Index [47] and the Fowlkes and Mallows Index (FM-Index for short) [18] (see Section 2.4 and Additional File 3 at the supplementary material website [1]).

An important aspect of cluster analysis, referred to as model selection, is the determination of the number of clusters in a dataset. Technically, one is interested in the following:
                              
                                 •
                                 Given: (a) A sequence of clustering solutions C
                                    1, …, C
                                    
                                       s
                                    , obtained for instance via the repeated application of a clustering algorithm A; (b) a function R, usually referred to as a relative measure, that estimates the relative merits of a set of clustering solutions. One is interested in identifying the partition 
                                       
                                          C
                                          
                                             
                                                k
                                                *
                                             
                                          
                                       
                                    , among the ones given in (a), providing the best value of R. In what follows, the optimal number of clusters according to R is referred to as k
                                    *.

It is worth pointing out that, in the specialistic literature, it is usual to refer to relative measures with the term internal. We follow that convention here. For the state of the art on internal measures, the reader is referred to [23,24,28,26]. Some of the most prominent internal measures are based on: (a) compactness; (b) hypothesis testing in statistics; (c) stability-based techniques and (d) jackknife techniques. This also gives a natural division of the main measures that are provided by ValWorkBench:
                              
                                 (a)
                                 Within Clusters Sum of Square (WCSS for short) [30] and Krzanowski and Lai Index (KL for short) [37].

Gap Statistics (Gap for short) [51].

CLEST [14], Model Explorer (ME for short) [6], Consensus Clustering (Consensus for short) [40] and Fast Consensus (FC for short) [25].

Figure of Merit (FOM for short) [56].

Conceptually, ValWorkBench can be thought of as having two layers, referred to as task and service, respectively. A high level diagram of the library architecture is given in Fig. 1
                        . The first layer consists of the measure package, further subdivided into two subpackages, offering all the methods to carry out various validation tasks. The second one consists of five additional packages that offer “basic service” to methods within all packages, and in particular to the ones contained in the task layer. They range from ensuring a uniform handling of data input to graphic routines. A synoptic description of each of them is provided next, grouped by layer.


                        
                           
                              •
                              The service layer packages:
                                    
                                       –
                                       
                                          datatypes: it contains classes encapsulating methods and state information for storing data related to the application of validation measures. It is presented in Section 2.3 and detailed in Additional File 1 at the supplementary material website [1].


                                          algorithms: it contains classes encapsulating methods and state information for computing clustering partitions of a specific dataset. In this library, only the class of Hierarchical Clustering algorithms is provided. Indeed, it consists of the class HLink.java only, supporting Average, Complete and Single Link cluster merging [33]. This choice is dictated by efficiency. Indeed, the availability of Hierarchical algorithms within the library allows to design measures that interleave the execution of a Hierarchical Clustering algorithm with the computation of the measure itself. That is, the computation proceeds by level of the hierarchical tree being built rather than by starting the entire clustering process again at each iteration of the measure being computed. The algorithms package is presented in Additional File 2 at the supplementary material website [1].


                                          graphics: it contains classes encapsulating methods and state information for visualizing results of clustering algorithms and validation measures. It is presented in Additional File 4 at the supplementary material website [1].


                                          nullmodels: it contains classes encapsulating methods to generate datasets from null models, those latter being a formalization of the intuition of “no structure” or “randomness” in a dataset. Those “data generation” procedures are central for the computation of many internal validation measures (see for instance [23,25,26]), although their range of application goes beyond those measures [33]. Consequently, internal to ValWorkBench, the ones most essential for data analysis are implemented [33]: Poisson, Principal Components and Permutational. The package is presented in Additional File 5 at the supplementary material website [1].


                                          exceptions: it contains classes encapsulating methods and state information for handling exceptions related to the application of validation measures. It is presented in Additional File 6 at the supplementary material website [1].

The task layer package(s):
                                    
                                       –
                                       
                                          measures: it contains the Measure class that defines abstract and public methods common to all measures. Moreover, it contains two subpackages encapsulating methods and state information that implement external and internal validation measures. In particular, the ones that have been listed in Section 2.1. The two subpackages are defined as follows.

  
                                          external-it contains the following main subpackages:
                                             
                                                ⁎
                                                
                                                   adjustedRand
                                                


                                                   findex
                                                


                                                   fmindex
                                                


                                                   nullmeasures
                                                

The first three packages naturally correspond to the external measures mentioned in Section 2.1.2. Moreover, it results convenient, for uniformity of notation, to consider the partition of a dataset D into clusters as a task performed by a null measure, whose software is contained in the forth package. Some level of detail about some of those packages is provided in Section 2.4. Full details are given in Additional File 3 at the supplementary material website [1].

  
                                          internal-it contains the following subpackages:
                                             
                                                ⁎
                                                
                                                   WCSS
                                                


                                                   KL
                                                


                                                   Gap
                                                


                                                   FOM
                                                


                                                   diffFOM
                                                


                                                   CLEST
                                                


                                                   ConsensusC
                                                


                                                   modelExplorer
                                                

Each of those packages naturally corresponds to each of the internal measures mentioned in Section 2.1.2. Again, some level of detail about some of them is provided in Section 2.5. Full details are given in Additional File 3 at the supplementary material website [1].

It contains the classes that define the basic data types of the library.
                           
                              •
                              
                                 Data Input Matrix. Given a dataset Σ, consisting of n elements, each being an m-dimensional vector, it can be represented in two different ways: as (1) a data matrix D, of size n
                                 ×
                                 m, in which the rows represent the items and the columns represent the condition values; (2) a similarity/dissimilarity matrix S, of size n
                                 ×
                                 n, in which each entry S
                                 
                                    i,j
                                 , 1≤
                                 i
                                 ≠
                                 j
                                 ≤
                                 n, quantifies the similarity/dissimilarity of the pair of items (i, j). Specifically, the value of S
                                 
                                    i,j
                                  can be computed using rows i and j of D. The DataMatrix.java and SimilarityMatrix.java classes allow to store and handle the data matrix D and the similarity matrix S.


                                 Gold/Clustering Solution. Given a clustering solution C, the ClusterMatrix.java class allows to store and handle a clustering solution as well as a gold solution, as a matrix, while a linked list representation is managed by the ClusterList.java class. Moreover, C can also be represented by an n
                                 ×
                                 n connectivity matrix M
                                 
                                    C
                                 , in which each entry M
                                 
                                    C
                                 (i, j) is 1 if the items i and j are in the same cluster, and 0 otherwise. The ConnetivityMatrix.java class allows to store and handle M
                                 
                                    C
                                 .


                                 Indicator Matrix. Given a dataset represented as a data matrix D, one can define a sampling dataset D′ as a dataset obtained taking n′ rows from D, with 0<
                                 n′<
                                 n. Let I
                                 
                                    D
                                  be an n
                                 ×
                                 n indicator matrix in which each entry I
                                 
                                    D
                                 (i, j) is 1 if the items i and j belong to D′, and 0 otherwise. The IndicatorMatrix.java class allows to store and handle I
                                 
                                    D
                                 .


                                 Consensus Matrix. Let 
                                    
                                       D
                                       1
                                       ′
                                    
                                    ,
                                    
                                       D
                                       2
                                       ′
                                    
                                    ,
                                    …
                                    ,
                                    
                                       D
                                       h
                                       ′
                                    
                                  be h sampling datasets of D and let C
                                 1, C
                                 2, …, C
                                 
                                    h
                                  be the corresponding partitions into k clusters.

A consensus matrix is defined as follows:


                                 
                                    
                                       (1)
                                       
                                          
                                             M
                                             
                                                (
                                                k
                                                )
                                             
                                          
                                          =
                                          
                                             
                                                
                                                   ∑
                                                   h
                                                
                                                
                                                   M
                                                   
                                                      (
                                                      h
                                                      )
                                                   
                                                
                                             
                                             
                                                
                                                   ∑
                                                   h
                                                
                                                
                                                   I
                                                   
                                                      (
                                                      h
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

where M
                                 (i) is the connectivity matrix of C
                                 
                                    i
                                  and I
                                 (i) is the indicator matrix of 
                                    
                                       D
                                       i
                                       ′
                                    
                                 , 1≤
                                 i
                                 ≤
                                 h. The ConsensusMatrix.java class allows to store and handle 
                                    
                                       M
                                       
                                          (
                                          k
                                          )
                                       
                                    
                                 .


                                 Header Data. The HeaderData.java class allows to store and handle book-keeping information of a computational experiment. Indeed, it maintains all the information about the experimental set-up, e.g., the list of the command-line arguments used for the analysis as well as the running time of the experiment.


                                 Measure Vector. The MeasureVector.java class allows to store and handle the results of both external and internal measures.


                                 Input Measure. The InputMeasure.java class is an abstract class that encapsulates different state information fields, as well as some of the input data needed to compute an internal/external validation measure. Each measure has, as a parameter, a corresponding input class that is extension of the InputMeasure.java class. Such an organization allows to group together the input that is common to all measures, while delegating specialization to a lower level of implementation. This point is exemplified in the next subsection.

The structure and content of this package is depicted in Fig. 2
                        . For brevity, we limit ourselves to discuss with some level of detail only the classes contained in the packages corresponding to the Adjusted Rand Index R
                        
                           A
                         and to the null measure. As anticipated earlier, a full description of the entire package is given in Additional File 3 at the supplementary material website [1].

In what follows, for the definition of R
                           
                              A
                           , we assume that one of the two partitions is the gold solution 
                              
                                 
                                    
                                       C
                                       ¯
                                    
                                 
                                 r
                              
                            while the other partition C
                           
                              t
                            is provided as output by a clustering algorithm, since a generalization of the definition to the case of two arbitrary partition is straightforward. Let n
                           
                              i,j
                            be the number of items common to both 
                              
                                 
                                    
                                       c
                                       ¯
                                    
                                 
                                 i
                              
                            and c
                           
                              j
                           , 1≤
                           i
                           ≤
                           r and 1≤
                           j
                           ≤
                           t. Moreover, let 
                              |
                              
                                 
                                    
                                       c
                                       ¯
                                    
                                 
                                 i
                              
                              |
                              =
                              
                                 n
                                 
                                    i
                                    .
                                 
                              
                            and |c
                           
                              j
                           |=
                           n
                           .j
                           . We have:
                              
                                 
                                    
                                       R
                                       A
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                ,
                                                j
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               n
                                                               
                                                                  i
                                                                  ,
                                                                  j
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   
                                                      
                                                         ∑
                                                         i
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           n
                                                                           
                                                                              i
                                                                              .
                                                                           
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        2
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         ∑
                                                         j
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           n
                                                                           
                                                                              .
                                                                              j
                                                                           
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        2
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               n
                                                            
                                                         
                                                         
                                                            
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             1
                                             2
                                          
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      i
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        n
                                                                        
                                                                           i
                                                                           .
                                                                        
                                                                     
                                                                  
                                                               
                                                               
                                                                  
                                                                     2
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   +
                                                   
                                                      ∑
                                                      j
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        n
                                                                        
                                                                           .
                                                                           j
                                                                        
                                                                     
                                                                  
                                                               
                                                               
                                                                  
                                                                     2
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          −
                                          
                                             
                                                
                                                   
                                                      
                                                         ∑
                                                         i
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           n
                                                                           
                                                                              i
                                                                              .
                                                                           
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        2
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         ∑
                                                         j
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           n
                                                                           
                                                                              .
                                                                              j
                                                                           
                                                                        
                                                                     
                                                                  
                                                                  
                                                                     
                                                                        2
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               n
                                                            
                                                         
                                                         
                                                            
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           It has a maximum value of one, indicating a perfect agreement between the two partitions, while its expected value of zero suggests a level of agreement due to chance. Moreover, R
                           
                              A
                            can take values on a larger range than [0, 1] and, in particular, may be negative [17,55]. Therefore, for two partitions to be in significant agreement, R
                           
                              A
                            must assume a non-negative value substantially away from zero.

The adjustedRand package contains two main classes: InputARand.java and AdjustedRand.java. The first is an extension of the abstract class InputMeasure.java and it provides input fields needed to compute the Adjusted Rand Index with the second class (see Fig. 2 again).

The AdjustedRand.java class allows to compute the Adjusted Rand Index in two modes: single and iterative. In single mode, the agreement is computed between two given partitions of D stored both in the clusterMatrix data format. In the iterative mode, the agreement is computed between a given partition (e.g. the gold solution) and a set of partitions of D produced by a clustering algorithm, for all k in [k
                           
                              min
                           , k
                           
                              max
                           ], i.e., 
                              
                                 C
                                 
                                    
                                       k
                                       min
                                    
                                 
                              
                              ,
                              …
                              ,
                              
                                 C
                                 
                                    
                                       k
                                       max
                                    
                                 
                              
                           .

In order to grant simplicity and uniformity, the computation of a set of partitions of D can be seen as the computation of a measure that returns nothing, i.e., it is a null measure. It takes as input D, a clustering algorithm A and two integers k
                           min, k
                           max (with 1<
                           k
                           min
                           ≤
                           k
                           max) and it stores in main memory the sequence of partitions of D, i.e., 
                              
                                 C
                                 
                                    
                                       k
                                       min
                                    
                                 
                              
                              ,
                              …
                              ,
                              
                                 C
                                 
                                    
                                       k
                                       max
                                    
                                 
                              
                           , obtained via the repeated application of A. The package implementing this idea contains two main classes: InputNullM.java and NullMeasure.java. The role of the first one is analogous to the InputARand.java class and, as in that case, it is an extension of the InputMeasure.java class. As for the second, it has the following three extensions:
                              
                                 •
                                 
                                    NullMeasureGeneric.java: it takes as one of its parameters the path name of a clustering algorithm binary executable, with input/output formats compatible with ValWorkBench. the clustering is then performed via that algorithm.


                                    NullMeasureHierarchical.java: the clustering is performed via Hierarchical Clustering, with an implementation internal to the library. Average, Complete and Single Link cluster merging are supported.


                                    NullMeasureHierarchicalInit.java: as pointed out in Section 2.1, the partition quality of some clustering algorithms may be improved by getting an external initialization. This class is analogous to the NullMeasureGeneric.java class in that it takes as a parameter a clustering algorithm external to the library. However, in addition to it, it provides a Hierarchical Clustering initiation to it. This latter feature is built-in.

The structure and content of this package is depicted in Fig. 3
                        . With reference to the description given in that figure, it is worth pointing out that, in addition to WCSS, the following classes also have extensions analogous to it: ConsensusClustering.java, GapStatistics.java, KrzanowskiLai.java and FigureOfMerit.java. Those extensions provide a standard implementation of a given measure along with variants that have been designed to be computationally efficient, via heuristics [23]. The advantages of such an approach are discussed and exemplified in Section 3. For brevity, we limit ourselves to discuss with some level of detail only the classes contained in the WCSS package. As anticipated earlier, a full description of the entire internal measures package is given in Additional File 3 at the supplementary material website [1].

WCSS measures the “goodness” of a cluster via its compactness, one of the most fundamental indicators of cluster quality. Indeed, for each k
                           ∈[2, k
                           
                              max
                           ], the method consists of computing the sum of the square distance between each element in a cluster and the centroid of that cluster. The “correct” number of clusters k
                           * is predicted according to the following rule of thumb. For values of k
                           <
                           k
                           *, the value of WCSS should be substantially decreasing, as a function of the number of clusters k. On the other hand, for values of k
                           *
                           ≤
                           k, the compactness of the clusters will not increase as much, causing the value of WCSS not to decrease as much. The following heuristic approach comes out [51]: Plot the values of WCSS, computed on a given set of clustering solutions, in the range [1, k
                           
                              max
                           ]; choose as k
                           * the abscissa closest to the “knee” in the WCSS curve.

The WCSS package contains two main classes. Specifically, the InputWCSS.java class is an extension of the abstract class InputMeasure.java that allows to provide input fields needed to compute WCSS by the WithinClustersSumSquares.java class. This latter, is an abstract class for:
                              
                                 •
                                 
                                    WCSSGeneric.java: it takes as one of its parameters the path name of a clustering algorithm binary executable, with input/output formats compatible with ValWorkBench. The computation of WCSS is then performed via that algorithm.


                                    WCSSFast.java: it is the implementation of a fast heuristic for the computation of WCSS. The interested reader can find details in [23].


                                    WCSSHierarchical.java: the computation of WCSS is performed via Hierarchical Clustering, with an implementation internal to the library. Average, Complete and Single Link cluster merging are supported.


                                    WCSSHierarchicalInit.java: this class is analogous to the WCSSGeneric.java class in that it takes as a parameter a clustering algorithm external to the library. However, in addition to it, it provides a Hierarchical Clustering initiation to it. This latter feature is built-in.

@&#RESULTS@&#


                     ValWorkBench has been designed with the aim to provide a generic programming paradigm for the development and testing of novel validation measures that takes full advantage of the Java programming features, including platform-independence. In order to illustrate the novelty offered by this library in the cluster validation literature, it results convenient to first summarize the main characteristics of the library, as they can be extrapolated from the technical presentation given in Section 2, then discuss the advantages they seem to offer to programmers and algorithm engineers, and finally compare ValWorkBench with existing validation software packages. The “user point of view” is also briefly discussed, for completeness, since it is not central to this paper.
                        
                           (1)
                           An abstraction design common to all measures. All modules of ValWorkBench are as “generic” as possible and structured within a hierarchy of classes. That is, a particular internal validation measure is an instance of a particular class, summarizing measures analogous to it. In turn, that class is an instance of a generic validation measure class that summarizes all of the internal/external validation measures.

Access/re-usability of the main building blocks. Given the architecture description provided in this paper, together with all of the Additional Files provided at the supplementary material website [1], all of the main modules and classes present in ValWorkBench are fully described for use by programmers interested in developing new measures, based on those building blocks. Moreover, being an open source library, it is possible to extend it by simply adding new code or functionality, inheriting data types and methods from the common base classes.

Clustering algorithm independence. Any clustering algorithm, provided as a binary executable and with input/output conventions compatible with the ones of ValWorkBench, can be executed within the library.

The sinergic combination of points (1) and (2) above makes possible: (a) the rapid “combination” and specialization of measures already implemented in ValWorkBench, even by programmers not part of the “project”, to obtain new ones; (b) the use of a basic set of methods that may bring to a substantial simplification of the development of entirely new measures.

Point (a) is best illustrated via the following examples, that have already been implemented.
                        
                           •
                           Extensions of a measure class. Examples of the further specialization of a measure, via extensions of a measure class, have been given for the WithinClustersSumSquares.java and NullMeasure.java classes. With reference to Section 2.5.1, we discuss the advantages of such an approach, using this latter class. Analogous considerations hold for all measures having extensions. The four extensions of the WithinClustersSumSquares.java class cover, in an orderly fashion, a spectrum of execution scenarios, providing a set of choices to the user and opportunities for further extensions to the programmer. WCSSGeneric.java covers the case in which a user wants to execute the measure with a clustering algorithm of her/his choice. WCSSHierarchicalInit.java covers the case in which an external hierarchical initialization is required by the clustering algorithm used to compute the measure. WCSSFast.java covers the case in which, for time efficiency reasons, the computation of a fast approximation of WCSS is needed. Likewise, the WCSSHierarchical.java class relates to efficiency of execution, when WCSS has to be computed by a Hierarchical Clustering algorithm. Indeed, in that class, the computation of the measure is interleaved with the bottom-up construction of the tree corresponding to the Hierarchical Clustering. Such a simple implementation detail results in major gains in execution time with respect to the execution of WCSS with a Hierarchical Clustering algorithm external to the class. Such an execution can take place via WCSSGeneric.java. However, the user has a more time-efficient alternative.

Combination of existing measure classes to create new ones. The DiffFom measure, defined in [23], is implemented via the DiffFOMGeneric.java class. The aim of DiffFom is to make FOM automatic in the prediction of the optimal number of clusters in a dataset. Indeed, as opposed to KL, the prediction using FOM is based on the analysis of the FOM curve (see [56,23] for details). Such a “visual inspection methodology”, although common to a few other highly appraised measures, is subjective and represents a limitation for that type of measures. Mathematically, given the analogy between FOM and KL, it is very natural to define a variant of FOM in which the automatic prediction rule defined for KL is extended to FOM. Such a natural extension of FOM was very easily realized via an extension of the KrzanowskiLai.java class that uses the FigureOfMerit.java class.

Development of entirely new measures. We now discuss point (b). This point is somewhat more difficult to illustrate via examples. However, it is essential to remark that the service layer of ValWorkBench offers a good variety of methods that, in our view, simplifies the development of a new measure, since the library is open source and all of its modules and classes are available for re-use. We limit ourselves to mention the following packages. The datatypes package offers several datatypes, pertinent to cluster validation and that may be of use for the implementation of a new measure. Moreover, it can be extended if a new datatype is needed. The nullmodels package offers the fundamental null models of use in cluster validation. Therefore, if a new method needs the generation of “random datasets”, usually done via the null models included in ValWorkBench, the software is already there. The graphics package offers various methods to display curves and diagrams. They can be of use for the development of a new measure, if the prediction of the optimal number of clusters in a dataset is done via the inspection of a curve.

As for point (3), apart from the obvious advantages that this offers in terms of its use for data analysis purposes, such a feature also offers the possibility to rapidly benchmark new clustering algorithms that have been developed independently of ValWorkBench. For illustrative purposes, we assume that one has a new clustering algorithm that needs to be evaluated. A sound experimental protocol to carry out such a validation task is outlined in [11,5,24] and involves the use of external indexes. We limit ourselves to describe the basic step, with the aid of methods available in ValWorkBench. One chooses datasets in which the gold solution is known, e.g., a collection of them is reported in [25]. One chooses an external index in ValWorkBench in order to measure the level of agreement between a partition generated by the algorithm that needs to be validated and the gold solution of a given dataset. The algorithm binary executable is given as an input parameter to the selected external index in order to obtain the result of the partition agreement. The interested reader can find additional details of the experimental protocol just outlined in [11,5,24]. Moreover, in those papers, there are also pointers to clustering algorithms binary executables, compatible with ValWorkBench, that can be used to establish how competitive is the new algorithm with respect to some basic existing ones.

As for a comparison with existing cluster validation software, a preliminary clarification concerning the programming environments in which those software packages have been developed, i.e., R and MatLab, is in order. In principle, both of those software environments can be used to design a library analogous to ValWorkBench, offering analogous advantages. Of course, it remains to be seen up to which degree those programming environments will support a full fledged object-oriented, generic, design that is naturally supported by Java.


                     Table 1
                      summarizes the cluster validation software libraries available in the Literature, grouping them by programming environment and mentioning how they compare with ValWorkBench with respect to its main features, summarized in (1)-(3) above. The fact that the software developed in R does not grant the same features of ValWorkBench is mostly due to the fact that no effort has been made to come-up with a unique R programmming framework in which to develop cluster validation measures. Indeed, those software packages have been programmed apparently with no higher level of coordination. As for Matlab and in particular CVAP [53], since all of the methods in that library are not accessible and can be used only via a GUI, software re-usability is not granted and the question of as to whether there is an abstraction design common to all measures becomes a meaningless question.

The absence of clustering independence in some of the software developed in R is again part of something that has not been considered during the design of the mentioned software libraries, the exception being the ones that allow the execution of the measure they implement via an external algorithm present in the R environment.

In terms of “the user point of view”, all of the measures implemented in ValWorkBench and in the other mentioned libraries have been proposed in the literature, where their success for microarray data analysis is well documented. For the convenience of the reader, Table 2
                      summarizes the measures that all of those libraries have in common with the library presented here. Since ValWorkBench is based on an earlier accurate benchmarking of the implemented measures [23,25,19], the potential user is given useful insights on the effectiveness of the proposed library.

@&#CONCLUSIONS@&#

We have presented a new software library with an efficient and generic design that addresses a wide range of problems in cluster analysis. Moreover, it is the very first software development platform for cluster validation analysis that has been specifically designed to provide full usability of all its building blocks. In fact, departing from the current state of the art, the major novelty of ValworkBench is to place the “developer point of view” at a par with the “user point of view”. Those features make it a unique programming platform for cluster analysis, in particular for microarray, in bioinformatics, computational biology and biomedicine.


                     ValWorkBench is under active development and we hope that it will become one of the standard platforms for algorithmic engineering at the interface of cluster analysis. We envision to extend the ValWorkBench framework including additional statistical tests for measuring the quality of a partition (e.g. Bayesian analysis), as well as including new building blocks (e.g. new data generation approaches and robustness analysis).

The authors declare that they have no conflict of interest.

@&#ACKNOWLEDGEMENTS@&#

Part of this work was supported by Italian Ministry of Scientific Research, FIRB Project “Bioinfomatica per la Genomica e la Proteomica” (project no. RBNE01F5WT), FIRB Project “Algoritmi per la Scoperta ed il Ritrovamento di Patterns in Strutture Discrete, con Applicazioni alla Bioinformatica” (project no. RBIN04BYZ7). Additional support to R.G. has been provided by Progetto di Ateneo dell’Universitá degli Studi di Palermo [2012-ATE-0298] ‘Metodi Formali ed Algoritmici per la Bioinformatica su ScalaGenomica’. The authors are also very grateful to the referees that, through their comments and constructive criticisms, have greatly helped in improving the presentations of our contributions.

@&#REFERENCES@&#

