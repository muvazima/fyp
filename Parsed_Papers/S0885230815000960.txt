@&#MAIN-TITLE@&#Speaker-adapted confidence measures for speech recognition of video lectures

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A new, particular logistic regression model is proposed to improve confidence measures for automatic speech recognition.


                        
                        
                           
                           Speaker-adapted models are proposed to further improve confidence measures.


                        
                        
                           
                           Empirical results are provided showing that speaker-adapted models outperform their non-adapted counterparts.


                        
                        
                           
                           The improvement of confidence measures shown to be useful on an interactive speech transcription application.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Confidence measures

Speech recognition

Speaker adaptation

Log-linear models

Online video lectures

@&#ABSTRACT@&#


               
               
                  Automatic speech recognition applications can benefit from a confidence measure (CM) to predict the reliability of the output. Previous works showed that a word-dependent naïve Bayes (NB) classifier outperforms the conventional word posterior probability as a CM. However, a discriminative formulation usually renders improved performance due to the available training techniques.
                  Taking this into account, we propose a logistic regression (LR) classifier defined with simple input functions to approximate to the NB behaviour. Additionally, as a main contribution, we propose to adapt the CM to the speaker in cases in which it is possible to identify the speakers, such as online lecture repositories.
                  The experiments have shown that speaker-adapted models outperform their non-adapted counterparts on two difficult tasks from English (videoLectures.net) and Spanish (poliMedia) educational lectures. They have also shown that the NB model is clearly superseded by the proposed LR classifier.
               
            

@&#INTRODUCTION@&#

Significant advances in the field of Automatic Speech Recognition (ASR) have been achieved over the last decades. Nowadays, automatic transcriptions of spontaneous speech in moderately noisy environments have reached an accurate enough quality (Rousseau, 2011; Sundermeyer et al., 2011; Swietojanski et al., 2013). This quality can be even better when ASR systems are adapted to specific scenarios (Leggetter and Woodland, 1995; Gales, 1998; Gauvain and Lee, 1994; Digalakis et al., 1995; Wiesler et al., 2014; Martinez-Villaronga et al., 2013). Nonetheless, ASR is still far from producing error-free transcriptions and, consequently, its performance in many applications is not completely satisfactory.

To further improve the usefulness and performance of the current technology, researchers have proposed to compute a normalised score or confidence measure (CM) to indicate the reliability of the ASR output. This score has been computed at different levels: phoneme, word, phrase or sentence. Nevertheless, CM at the word level has been the main focus in the literature due to its usefulness for the vast majority of applications (Wessel et al., 2001; Kim and Ko, 2005; Gao et al., 2009; Bardideh et al., 2007; Wang et al., 2010; Junfeng and Yeping, 2011; Yadav and Patil, 2013).

One widely used word-level CM has been word posterior probability (Wessel et al., 2001). From then on, many works have focused on combining word posterior with additional sources of knowledge. The combination has been addressed as a classification problem in the vast majority of the works. Most well-known classifier algorithms have been tried: linear, Gaussian mixtures, neural networks, decision trees, support vector machines, etc. For further reference, a still good comprehensive survey can be found in Jiang (2005).

In the framework of CM as a classification problem, significant improvements were achieved by means of a combination of word-dependent (specific) and word-independent (generalised) naïve Bayes (NB) classifiers (Sanchis et al., 2012). Nonetheless, NB is learned by means of a generative criterion, the maximum likelihood estimate (MLE), which involves some issues. In particular, MLE overfits due to the unseen data. This issue was addressed in NB work by using a complex backing-off smoothing technique. But still, MLE aims at modelling the distribution underlying a given sample, which does not guarantee the solution to be the best suited for classification. Indeed, better fitted criteria may improve overall performance. For instance, the maximum mutual information (MMI) (Heigold et al., 2010) aims at better discriminating between classes without explaining the data. This criterion has been widely exploited in the literature for the maximum entropy (ME) models (Guiasu and Shenitzer, 1985; Yu et al., 2011).

Nevertheless, despite the success of MMI training in many applications, there is no direct relationship between maximising the MMI and minimising the probability of classification error. Instead, there are better suited criteria, which guarantee the minimisation of the classification error rate (CER) such as the minimum classification error (MCE) or the mean squared error (MSE). Therefore, we propose a logistic regression (LR) model to be learnt by means of the MSE to surpass NB performance.

On the other hand, speaker model adaptation has proved to be very effective for the improvement of recognition performance (Leggetter and Woodland, 1995; Gales, 1998; Gauvain and Lee, 1994; Digalakis et al., 1995). However, adaptation of the CMs to the speaker is nowadays unexplored. There is an increasing number of interesting scenarios in which CMs can be very useful and information about the speaker is available, such as the online lecture repositories. These repositories usually count with a large number of speeches delivered by a reduced number of speakers. Improving CM performance in these academic repositories is highly motivated since manual transcription is not affordable for such a large amount of speeches. Moreover, ASR performance is usually poor due to the amount of technical concepts, very different native and non-native accents, etc. In this scenario, interactive speech transcription (IST) guided by CMs can help in massively producing acceptable transcripts for large amounts of videos with limited manual effort (Silvestre-Cerda et al., 2013).

Motivated by the scenario depicted above, we propose to adapt the CM models to the speaker in an attempt to improve CM classification and IST performance. To do so, we formulate the speaker adaptation to extend both the published NB and the proposed LR models.

The rest of the content is organised as follows: the inclusion of speaker dependence into the NB model is described in Section 2. Section 3 proposes the LR model and formulates its corresponding speaker-dependent version. Section 4 describes the evaluation of the proposed models on two challenging tasks based on ASR transcripts from videoLectures.net and poliMedia repositories. Comparative results are presented including also conditional random field (CRF) models (Seigel, 2013; Seigel and Woodland, 2011; Fayolle et al., 2010). Section 5 proves that the increased CM performance results in better amended transcripts for videoLectures.net when integrated into an IST application. Finally, Section 6 raises the conclusions.

In this section, we introduce a speaker-adapted confidence estimator model. The model is designed to extend the naïve Bayes (NB) approach that was successfully applied to speech recognition (Sanchis et al., 2012) as well as to machine translation (Sanchis et al., 2007). Thus, let us first briefly recall the speaker independent NB model.

The NB model is formulated on the framework of confidence estimation addressed as a classification problem. On this framework, the recognised words are labelled as correct (c
                     =1) or incorrect (c
                     =0) by means of the class posterior given the word (
                        w
                     ) and a vector of input scores (x):
                        
                           (1)
                           
                              
                                 
                                    c
                                    ˆ
                                 
                              
                              =
                              
                                 
                                    arg
                                    
                                    max
                                 
                                 c
                              
                              
                              p
                              (
                              c
                              |
                              w
                              ,
                              
                                 
                                    x
                                 
                              
                              )
                              =
                              
                                 
                                    arg
                                    
                                    max
                                 
                                 c
                              
                              
                              p
                              (
                              c
                              |
                              w
                              )
                              
                              p
                              (
                              
                                 
                                    x
                                 
                              
                              |
                              c
                              ,
                              w
                              )
                           
                        
                     where Eq. (1) is obtained by applying Bayes’ rule and then ignoring the class-independent term. Also, the values of all the involved variables in the latter equation are assumed to be discrete. Discretisation avoids the need of explicitly modelling the probability distribution of continuous-valued features, while it renders a more flexible and data-driven model. Details on discretisation and several different approaches can be found in Seigel (2013). Here, we just discretised by dividing the feature domain into a fixed number of evenly-spaced bins. The optimal number of bins was tested on the development set.

The estimation of 
                        p
                        (
                        
                           
                              x
                           
                        
                        |
                        c
                        ,
                        w
                        )
                      is usually biased due to the training data sparsity. More robust estimations can be obtained by simplifying the problem with the following strong independence assumption (the “naïve Bayes assumption”):
                        
                           (2)
                           
                              p
                              (
                              
                                 
                                    x
                                 
                              
                              |
                              c
                              ,
                              w
                              )
                              =
                              
                                 ∏
                                 d
                                 D
                              
                              p
                              (
                              
                                 x
                                 d
                              
                              |
                              c
                              ,
                              w
                              )
                           
                        
                     
                  

Therefore, the basic problem is to estimate 
                        p
                        (
                        
                           x
                           d
                        
                        |
                        c
                        ,
                        w
                        )
                      for each class-word pair and 
                        p
                        (
                        c
                        |
                        w
                        )
                      for each target word. Given N training samples 
                        
                           
                              {
                              (
                              
                                 
                                    
                                       x
                                    
                                 
                                 n
                              
                              ,
                              
                                 c
                                 n
                              
                              ,
                              
                                 w
                                 n
                              
                              )
                              }
                           
                           
                              n
                              =
                              1
                           
                           N
                        
                     , these probabilities can be computed as the maximum likelihood estimate (MLE):
                        
                           (3)
                           
                              p
                              (
                              c
                              |
                              w
                              )
                              =
                              
                                 
                                    N
                                    (
                                    c
                                    ,
                                    w
                                    )
                                 
                                 
                                    N
                                    (
                                    w
                                    )
                                 
                              
                              
                              p
                              (
                              
                                 x
                                 d
                              
                              |
                              c
                              ,
                              w
                              )
                              =
                              
                                 
                                    N
                                    (
                                    c
                                    ,
                                    w
                                    ,
                                    
                                       x
                                       d
                                    
                                    )
                                 
                                 
                                    N
                                    (
                                    c
                                    ,
                                    w
                                    )
                                 
                              
                           
                        
                     where {N(·)} are suitably defined event counts on a given training data set. However, the MLE quickly overfit the training data. In order to prevent this overfitting, a particular backing-off smoothing method was introduced in Sanchis et al. (2012).

We propose to extend the NB into a naïve Bayes speaker-adapted model (NB+spk). For that, a new variable s is introduced into Eq. (1) to identify the speaker:
                        
                           (4)
                           
                              
                                 
                                    c
                                    ˆ
                                 
                              
                              =
                              
                                 
                                    arg
                                    
                                    max
                                 
                                 c
                              
                              
                              p
                              (
                              c
                              |
                              w
                              ,
                              
                                 
                                    x
                                 
                              
                              ,
                              s
                              )
                              =
                              
                                 
                                    arg
                                    
                                    max
                                 
                                 c
                              
                              
                              p
                              (
                              c
                              |
                              w
                              ,
                              s
                              )
                              p
                              (
                              
                                 
                                    x
                                 
                              
                              |
                              c
                              ,
                              w
                              ,
                              s
                              )
                           
                        
                     
                  

Consequently, the problem is turned into computing 
                        p
                        (
                        
                           x
                           d
                        
                        |
                        c
                        ,
                        w
                        ,
                        s
                        )
                      for each class-word-speaker triplet and 
                        p
                        (
                        c
                        |
                        w
                        ,
                        s
                        )
                      for each word-speaker pair, which analogously can be simply estimated by means of their MLE:
                        
                           (5)
                           
                              
                                 
                                    p
                                    ˆ
                                 
                              
                              (
                              c
                              |
                              w
                              ,
                              s
                              )
                              =
                              
                                 
                                    N
                                    (
                                    c
                                    ,
                                    w
                                    ,
                                    s
                                    )
                                 
                                 
                                    N
                                    (
                                    w
                                    ,
                                    s
                                    )
                                 
                              
                           
                        
                     
                     
                        
                           (6)
                           
                              
                                 
                                    p
                                    ˆ
                                 
                              
                              (
                              
                                 x
                                 d
                              
                              |
                              c
                              ,
                              w
                              ,
                              s
                              )
                              =
                              
                                 
                                    N
                                    (
                                    c
                                    ,
                                    w
                                    ,
                                    
                                       x
                                       d
                                    
                                    ,
                                    s
                                    )
                                 
                                 
                                    N
                                    (
                                    c
                                    ,
                                    w
                                    ,
                                    s
                                    )
                                 
                              
                           
                        
                     As in the non-adapted (speaker-independent) NB approach, MLE overfitting can be prevented by using a straightforward extension of the backing-off smoothing method proposed in Sanchis et al. (2012).

In this section, we propose a new CM model based on logistic regression (LR) models. It is worth noting that, for binary classification problems such as the CM problem considered in this work, these models are equivalent to more general conditional random field (CRF) models. In what follows, after describing the proposed speaker-dependent LR models, we briefly discuss how to discriminatively learn them using the MSE training criterion (Section 3.1). In Section 4, this criterion is empirically compared with a similar yet different criterion that is commonly used in CRF training.

The proposed approach resembles the ones presented in Estienne et al. (2008). However, that work formulated the classification problem as a generative model, and only the posterior of the features was attempted to be learnt in a discriminative way. Furthermore, the purpose was to mimic NB, so no improvements were obtained. Hence, in contrast to Estienne et al. (2008), here we do model the class posterior; define simpler input functions for the LR model; introduce a standard L
                     2 regularisation to avoid the complex set of maximum entropy constraints with cut-offs; and use the MSE learning criterion, optimised with the simple and fast iRPROP+ (Igel and Hüsken, 2003) algorithm.

The assumption of a general LR distribution for the class posterior yields the following classification rule:
                        
                           (7)
                           
                              
                                 c
                                 ˆ
                              
                              =
                              
                                 
                                    arg
                                    
                                    max
                                 
                                 c
                              
                              
                              p
                              (
                              c
                              |
                              w
                              ,
                              
                                 
                                    x
                                 
                              
                              )
                              =
                              
                                 
                                    arg
                                    
                                    max
                                 
                                 c
                              
                              
                                 
                                    exp
                                    
                                       (
                                       
                                          ∑
                                          i
                                       
                                       
                                          λ
                                          i
                                       
                                       
                                          f
                                          i
                                       
                                       (
                                       c
                                       ,
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       )
                                       )
                                    
                                 
                                 
                                    Z
                                    (
                                    w
                                    ,
                                    
                                       
                                          x
                                       
                                    
                                    )
                                 
                              
                           
                        
                     where 
                        w
                      is the recognised word and x
                     =(x
                     1, …, x
                     
                        D
                     ) is a D-dimensional vector of discretised input features. On the other hand, 
                        Z
                      is a normalisation constant, which does not affect classification; λ
                     (·) are a set of data-driven parameters; and f
                     (·) a set of functions which give the model expressiveness.

As discussed before, the NB model in Sanchis et al. (2012) introduced several convenient assumptions: conditional independence amongst the D scores, discretisation of the continuous-valued scores, etc. Hence, we propose now a particular definition for the f
                     (·) functions to make the LR model behave similarly to the NB model in terms of classification.

Let i be the triplet of labels (
                        
                           
                              c
                              ˜
                           
                        
                        ∈
                        {
                        0
                        ,
                        1
                        }
                     , 
                        
                           
                              w
                              ˜
                           
                        
                        ∈
                        {
                        1
                        ,
                        …
                        ,
                        W
                        }
                     , 
                        
                           
                              
                                 x
                                 ˜
                              
                           
                           
                              
                                 d
                                 ˜
                              
                           
                        
                        ∈
                        {
                        1
                        ,
                        …
                        ,
                        
                           X
                           
                              
                                 d
                                 ˜
                              
                           
                        
                        }
                     ) indexing the classes, the known vocabulary and the values of the score number 
                        
                           
                              d
                              ˜
                           
                        
                        ∈
                        {
                        1
                        ,
                        …
                        ,
                        D
                        }
                      respectively. 
                        
                           X
                           
                              
                                 d
                                 ˜
                              
                           
                        
                      accounts for the total number of different possible discrete values of 
                        
                           x
                           
                              
                                 d
                                 ˜
                              
                           
                        
                     . For each possible triplet, let us define the following function:
                        
                           (8)
                           
                              
                                 f
                                 
                                    
                                       
                                          c
                                          ˜
                                       
                                    
                                    ,
                                    
                                       
                                          w
                                          ˜
                                       
                                    
                                    ,
                                    
                                       
                                          
                                             x
                                             ˜
                                          
                                       
                                       
                                          
                                             d
                                             ˜
                                          
                                       
                                    
                                 
                              
                              (
                              c
                              ,
                              w
                              ,
                              
                                 
                                    x
                                 
                              
                              )
                              =
                              
                                 δ
                                 
                                    
                                       
                                          c
                                          ˜
                                       
                                    
                                 
                              
                              (
                              c
                              )
                              ·
                              
                                 δ
                                 
                                    
                                       
                                          w
                                          ˜
                                       
                                    
                                 
                              
                              (
                              w
                              )
                              ·
                              
                                 δ
                                 
                                    
                                       
                                          
                                             x
                                             ˜
                                          
                                       
                                       
                                          
                                             d
                                             ˜
                                          
                                       
                                    
                                 
                              
                              (
                              
                                 
                                    x
                                 
                              
                              )
                           
                        
                     with δ(·) being the Kronecker delta and 
                        
                           δ
                           
                              
                                 
                                    
                                       x
                                       ˜
                                    
                                 
                                 
                                    
                                       d
                                       ˜
                                    
                                 
                              
                           
                        
                        (
                        
                           
                              x
                           
                        
                        )
                        ≡
                        
                           ∏
                           
                              
                                 d
                                 ′
                              
                           
                           D
                        
                        
                           δ
                           
                              
                                 
                                    
                                       x
                                       ˜
                                    
                                 
                                 
                                    
                                       d
                                       ˜
                                    
                                 
                              
                           
                        
                        (
                        
                           x
                           
                              
                                 d
                                 ′
                              
                           
                        
                        )
                        ·
                        
                           δ
                           
                              
                                 d
                                 ˜
                              
                           
                        
                        (
                        
                           d
                           ′
                        
                        )
                        =
                        
                           δ
                           
                              
                                 
                                    
                                       x
                                       ˜
                                    
                                 
                                 
                                    
                                       d
                                       ˜
                                    
                                 
                              
                           
                        
                        (
                        
                           x
                           
                              
                                 d
                                 ˜
                              
                           
                        
                        )
                     .

It becomes clear from the latter definition that the set of functions 
                        {
                        
                           f
                           
                              
                                 
                                    c
                                    ˜
                                 
                              
                              ,
                              
                                 
                                    w
                                    ˜
                                 
                              
                              ,
                              
                                 
                                    
                                       x
                                       ˜
                                    
                                 
                                 
                                    
                                       d
                                       ˜
                                    
                                 
                              
                           
                        
                        }
                      serves merely to activate the corresponding weights 
                        {
                        
                           λ
                           
                              
                                 
                                    c
                                    ˜
                                 
                              
                              ,
                              
                                 
                                    w
                                    ˜
                                 
                              
                              ,
                              
                                 
                                    
                                       x
                                       ˜
                                    
                                 
                                 
                                    
                                       d
                                       ˜
                                    
                                 
                              
                           
                        
                        }
                     . Thus, it is the set of weights alone which will render the classification, and they are to be learned exclusively from data, as detailed in Section 3.1. Also, it should be noted that each of the defined functions does not involve more than one score. This is precisely equivalent to assuming naïve Bayes over the scores, as in Eq. (2).

Furthermore, in order to prevent overfitting, additional weights and functions to be active independently of one or more label values are necessary:
                        
                           (9)
                           
                              
                                 
                                    
                                       
                                          f
                                          
                                             
                                                
                                                   c
                                                   ˜
                                                
                                             
                                             ,
                                             ∅
                                             ,
                                             ∅
                                          
                                       
                                       (
                                       c
                                       ,
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       )
                                       =
                                       
                                          δ
                                          
                                             
                                                
                                                   c
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       c
                                       )
                                    
                                 
                                 
                                    
                                       
                                          f
                                          
                                             
                                                
                                                   c
                                                   ˜
                                                
                                             
                                             ,
                                             ∅
                                             ,
                                             
                                                
                                                   
                                                      x
                                                      ˜
                                                   
                                                
                                                
                                                   
                                                      d
                                                      ˜
                                                   
                                                
                                             
                                          
                                       
                                       (
                                       c
                                       ,
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       )
                                       =
                                       
                                          δ
                                          
                                             
                                                
                                                   c
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       c
                                       )
                                       ·
                                       
                                          δ
                                          
                                             
                                                
                                                   
                                                      x
                                                      ˜
                                                   
                                                
                                                
                                                   
                                                      
                                                         d
                                                         ˜
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       (
                                       
                                          
                                             x
                                          
                                       
                                       )
                                    
                                 
                                 
                                    
                                       
                                          f
                                          
                                             
                                                
                                                   c
                                                   ˜
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                   ˜
                                                
                                             
                                             ,
                                             ∅
                                          
                                       
                                       (
                                       c
                                       ,
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       )
                                       =
                                       
                                          δ
                                          
                                             
                                                
                                                   c
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       c
                                       )
                                       ·
                                       
                                          δ
                                          
                                             
                                                
                                                   w
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       w
                                       )
                                    
                                 
                              
                           
                        
                     
                  

These terms enable a behaviour similar to the smoothing in the NB model, which backs off to less specific probabilities under certain conditions.

Finally, it should be noted that the presented model typically involves a huge number of weights to be estimated, of order 
                        O
                      (vocabulary×number of features×mean number of values per score). Fortunately, the computation time can be halved by defining a new set of weights 
                        
                           λ
                           
                              (
                              ·
                              )
                           
                        
                        ≡
                        
                           λ
                           
                              
                                 
                                    c
                                    ˜
                                 
                              
                              =
                              1
                              ,
                              (
                              ·
                              )
                           
                        
                        −
                        
                           λ
                           
                              
                                 
                                    c
                                    ˜
                                 
                              
                              =
                              0
                              ,
                              (
                              ·
                              )
                           
                        
                     , and the corresponding activation features:
                        
                           (10)
                           
                              
                                 
                                    
                                       
                                          f
                                          
                                             
                                                
                                                   w
                                                   ˜
                                                
                                             
                                             ,
                                             
                                                
                                                   
                                                      x
                                                      ˜
                                                   
                                                
                                                
                                                   
                                                      d
                                                      ˜
                                                   
                                                
                                             
                                          
                                       
                                       (
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       )
                                       =
                                       
                                          δ
                                          
                                             
                                                
                                                   w
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       w
                                       )
                                       ·
                                       
                                          δ
                                          
                                             
                                                
                                                   
                                                      x
                                                      ˜
                                                   
                                                
                                                
                                                   
                                                      d
                                                      ˜
                                                   
                                                
                                             
                                          
                                       
                                       (
                                       
                                          
                                             x
                                          
                                       
                                       )
                                    
                                 
                                 
                                    
                                       
                                          f
                                          
                                             ∅
                                             ,
                                             ∅
                                          
                                       
                                       (
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       )
                                       =
                                       1
                                    
                                 
                                 
                                    
                                       
                                          f
                                          
                                             ∅
                                             ,
                                             
                                                
                                                   
                                                      x
                                                      ˜
                                                   
                                                
                                                
                                                   
                                                      d
                                                      ˜
                                                   
                                                
                                             
                                          
                                       
                                       (
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       )
                                       =
                                       
                                          δ
                                          
                                             
                                                
                                                   
                                                      x
                                                      ˜
                                                   
                                                
                                                
                                                   
                                                      d
                                                      ˜
                                                   
                                                
                                             
                                          
                                       
                                       (
                                       
                                          
                                             x
                                          
                                       
                                       )
                                    
                                 
                                 
                                    
                                       
                                          f
                                          
                                             
                                                
                                                   w
                                                   ˜
                                                
                                             
                                             ,
                                             ∅
                                          
                                       
                                       (
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       )
                                       =
                                       
                                          δ
                                          
                                             
                                                
                                                   w
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       w
                                       )
                                    
                                 
                              
                           
                        
                     in this way, Eq. (7) adopts the following expression:
                        
                           (11)
                           
                              p
                              (
                              c
                              |
                              w
                              ,
                              
                                 
                                    x
                                 
                              
                              )
                              =
                              
                                 1
                                 
                                    1
                                    +
                                    exp
                                    
                                       (
                                       
                                          
                                             (
                                             −
                                             1
                                             )
                                          
                                          c
                                       
                                       
                                          ∑
                                          i
                                       
                                       
                                          λ
                                          i
                                       
                                       
                                          f
                                          i
                                       
                                       (
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       )
                                       )
                                    
                                 
                              
                           
                        
                     
                  

Speaker dependence can be easily introduced into Eq. (11), yielding a logistic regression speaker-adapted (LR+spk) model:
                        
                           (12)
                           
                              p
                              (
                              c
                              ∣
                              w
                              ,
                              
                                 
                                    x
                                 
                              
                              ,
                              s
                              )
                              =
                              
                                 1
                                 
                                    1
                                    +
                                    exp
                                    
                                       (
                                       
                                          
                                             (
                                             −
                                             1
                                             )
                                          
                                          c
                                       
                                       ·
                                       (
                                       
                                          ∑
                                          i
                                       
                                       
                                          λ
                                          i
                                       
                                       
                                          f
                                          i
                                       
                                       (
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       )
                                       +
                                       
                                          ∑
                                          j
                                       
                                       
                                          λ
                                          j
                                       
                                       
                                          f
                                          j
                                       
                                       (
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       ,
                                       s
                                       )
                                       )
                                       )
                                    
                                 
                              
                           
                        
                     where speaker dependence has been formulated as a separated sum over j for the sake of clarity. Now, the number of weights to be estimated is increased by S times, S being the number of known speakers. In this case, the new index j should map the triplet of labels (
                        
                           
                              w
                              ˜
                           
                        
                        ∈
                        {
                        ∅
                        ,
                        1
                        ,
                        …
                        ,
                        W
                        }
                     , 
                        
                           
                              
                                 x
                                 ˜
                              
                           
                           
                              
                                 d
                                 ˜
                              
                           
                        
                        ∈
                        {
                        ∅
                        ,
                        1
                        ,
                        …
                        ,
                        
                           X
                           
                              
                                 d
                                 ˜
                              
                           
                        
                        }
                     , 
                        
                           
                              s
                              ˜
                           
                        
                        ∈
                        {
                        1
                        ,
                        …
                        ,
                        S
                        }
                     ).

Thus, speaker adaptation results in the addition of the following functions:
                        
                           (13)
                           
                              
                                 
                                    
                                       
                                          f
                                          
                                             
                                                
                                                   w
                                                   ˜
                                                
                                             
                                             ,
                                             
                                                
                                                   
                                                      x
                                                      ˜
                                                   
                                                
                                                
                                                   
                                                      d
                                                      ˜
                                                   
                                                
                                             
                                             ,
                                             
                                                
                                                   s
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       ,
                                       s
                                       )
                                       =
                                       
                                          δ
                                          
                                             
                                                
                                                   w
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       w
                                       )
                                       ·
                                       
                                          δ
                                          
                                             
                                                
                                                   
                                                      x
                                                      ˜
                                                   
                                                
                                                
                                                   
                                                      d
                                                      ˜
                                                   
                                                
                                             
                                          
                                       
                                       (
                                       
                                          
                                             x
                                          
                                       
                                       )
                                       ·
                                       
                                          δ
                                          
                                             
                                                
                                                   s
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       s
                                       )
                                    
                                 
                                 
                                    
                                       
                                          f
                                          
                                             
                                                
                                                   w
                                                   ˜
                                                
                                             
                                             ,
                                             ∅
                                             ,
                                             
                                                
                                                   s
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       ,
                                       s
                                       )
                                       =
                                       
                                          δ
                                          
                                             
                                                
                                                   w
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       w
                                       )
                                       ·
                                       
                                          δ
                                          
                                             
                                                
                                                   s
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       s
                                       )
                                    
                                 
                                 
                                    
                                       
                                          f
                                          
                                             ∅
                                             ,
                                             
                                                
                                                   
                                                      x
                                                      ˜
                                                   
                                                
                                                
                                                   
                                                      d
                                                      ˜
                                                   
                                                
                                             
                                             ,
                                             
                                                
                                                   s
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       ,
                                       s
                                       )
                                       =
                                       
                                          δ
                                          
                                             
                                                
                                                   
                                                      x
                                                      ˜
                                                   
                                                
                                                
                                                   
                                                      d
                                                      ˜
                                                   
                                                
                                             
                                          
                                       
                                       (
                                       
                                          
                                             x
                                          
                                       
                                       )
                                       ·
                                       
                                          δ
                                          
                                             
                                                
                                                   s
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       s
                                       )
                                    
                                 
                                 
                                    
                                       
                                          f
                                          
                                             ∅
                                             ,
                                             ∅
                                             ,
                                             
                                                
                                                   s
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       w
                                       ,
                                       
                                          
                                             x
                                          
                                       
                                       ,
                                       s
                                       )
                                       =
                                       
                                          δ
                                          
                                             
                                                
                                                   s
                                                   ˜
                                                
                                             
                                          
                                       
                                       (
                                       s
                                       )
                                    
                                 
                                 
                                    
                                 
                              
                           
                        
                     
                  

As discussed in Section 1, the weights of the discriminative models can be estimated to minimise the MSE, which may be preferable for classification problems instead of the MMI criterion and the MLE criterion for generative models. Given N training samples 
                           
                              
                                 {
                                 (
                                 
                                    
                                       
                                          x
                                       
                                    
                                    n
                                 
                                 ,
                                 
                                    c
                                    n
                                 
                                 ,
                                 
                                    w
                                    n
                                 
                                 )
                                 }
                              
                              
                                 n
                                 =
                                 1
                              
                              N
                           
                        , the MSE can be formulated as an optimisation problem by means of the objective:
                           
                              (14)
                              
                                 
                                    F
                                    MSE
                                 
                                 (
                                 
                                    
                                       λ
                                    
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       n
                                       =
                                       1
                                    
                                    N
                                 
                                 
                                    
                                       (
                                       
                                          c
                                          n
                                       
                                       −
                                       
                                          p
                                          λ
                                       
                                       (
                                       
                                          c
                                          n
                                       
                                       =
                                       1
                                       |
                                       
                                          w
                                          n
                                       
                                       ,
                                       
                                          
                                             
                                                x
                                             
                                          
                                          n
                                       
                                       )
                                       )
                                    
                                    2
                                 
                              
                           
                        
                     

However, there is no closed form solution for the optimal λ under the minimum MSE constrain. Fortunately, any simple gradient descent based optimisation algorithm can succeed in finding the solution despite the MSE not being a convex criterion. In this work, we opted for the simpler iRPROP+ (Igel and Hüsken, 2003) iterative algorithm, which provides faster convergence than other more expensive methods such as generalised iterative scaling (GIS) (Darroch and Ratcliff, 1972). A recent evaluation of different optimisation algorithms on a large task can be found in Wiesler et al. (2013).

Another common issue of many training criteria, including MSE, is that they easily overfit the weights to the training data. Since there is no clear way to smooth discriminatively trained models, a typical amendment is to add a L
                        2 regularisation term to the objective:
                           
                              (15)
                              
                                 F
                                 (
                                 
                                    
                                       λ
                                    
                                 
                                 )
                                 =
                                 
                                    F
                                    MSE
                                 
                                 (
                                 
                                    
                                       λ
                                    
                                 
                                 )
                                 −
                                 
                                    C
                                    2
                                 
                                 
                                    ∑
                                    i
                                 
                                 
                                    
                                       (
                                       
                                          λ
                                          i
                                       
                                       −
                                       
                                          λ
                                          i
                                          
                                             (
                                             0
                                             )
                                          
                                       
                                       )
                                    
                                    2
                                 
                              
                           
                        where λ
                        (0) can be either a reliable estimation of the weights or simply 0.

For our model, 
                           
                              λ
                              i
                              
                                 (
                                 0
                                 )
                              
                           
                           =
                           
                              
                                 0
                              
                           
                         is a clever guess, since it prevents the features from having an overrated impact. During experimentation, the zero regularisation made the feature-independent term λ
                        ∅,∅ drop quickly to zero after a few iterations. This behaviour can be interpreted as an increased generalisation of the model, since λ
                        ∅,∅ is proportional to the logarithm of the class prior p(c) from the generative point of view. Thus, for two different models yielding the same performance on a certain test, the one with λ
                        ∅,∅ closer to zero is likely to perform better on a new test with different prior distribution.

@&#EXPERIMENTS@&#

The evaluation of the proposed models (NB+spk, LR and LR+spk) and the baseline model (NB) has been carried out over two difficult tasks from English (videoLectures.net) and Spanish (poliMedia) video lectures. These tasks have been used in the context of the EU-funded project transLectures, which had the aim of developing innovative, cost-effective tools for the automatic transcription and translation of online educational videos (Silvestre-Cerdà et al., 2012). The English task has been defined over the free and open access educational video lecture repository VideoLectures.NET (VL). In VL, the recorded lectures are mostly delivered by distinguished scholars and scientists at important conferences, summer schools, workshops, etc. Currently, VL hosts more than 16,000 lectures from 12,698 speakers. The Spanish task has been defined over Polimedia (PM), which is a recent, innovative service for the creation and distribution of multimedia educational content at Universitat Politècnica de València (UPV). PM is designed primarily to allow UPV professors to record their courses in video blocks lasting up to 10min, accompanied by time-aligned slides. PM hosts more than 9000 lectures from 1300 speakers with a duration of 2100hours.

The state-of-the-art ASR TLK toolkit (T.T.-U. team) has been used for the experiments. Acoustic models (AM) were learned using TLK by means of a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture, in a similar fashion to Dahl et al. (2012). Speaker adaptation was implemented using constrained MLLR (CMLLR) features (Stemmer et al., 2005; Giuliani et al., 2004). The speech data to train the English AM consisted of out-of-domain corpora (TED-LIUM (Rousseau et al., 2012), EPPS (Rousseau, 2011; Ramabhadran et al., 2007; TC-STAR Evaluation Report) and Voxforge), as well as in-domain VL speeches. In contrast, only in-domain PM speech data was used for Spanish. Additionally, it should be noted that the speakers related to the AM data are different from those selected to evaluate the CM models. The statistics of the AM train data are summarised in Table 1
                        .

On the other hand, the language model (LM) consisted of 5-gram models computed with the SRILM toolkit (Stolcke, 2002). It is worth mentioning that a common LM was used for all the lectures of the VL task. However, a different LM was used for the PL task depending on the speaker who delivered the speech. Each different LM was adapted to the speaker by exploiting the textual content in the slides available for these PM lectures (Martinez-Villaronga et al., 2013).

The evaluation of CMs has been carried out over a distinct corpus from the data used to build the ASR systems. This corpus was split into training, development and test partitions in a balanced way for each of the speakers (statistics are summarised in Table 2
                        ). As a measure of the difficulty of the task, it should be noted that about 25% of the words of each test are not found in the training sets. The word error rates (WER) on the automatic transcripts of the VL and PL test sets were 29.97% and 11.83%, respectively.

For the purpose of evaluation, the recognised words must be labelled as correct or incorrect. The labelling was computed as the tagging error on the automatic transcripts compared to the reference transcripts based on the optimal Levenshtein alignments. Additionally, class prediction (correct, c
                        =1, or incorrect, c
                        =0) is carried out by minimising the Bayes risk as follows:
                           
                              (16)
                              
                                 
                                    c
                                    *
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   correct
                                                
                                                
                                                   if
                                                   
                                                   p
                                                   (
                                                   c
                                                   =
                                                   1
                                                   |
                                                   w
                                                   ,
                                                   
                                                      
                                                         x
                                                      
                                                   
                                                   ,
                                                   s
                                                   )
                                                   >
                                                   τ
                                                
                                             
                                             
                                                
                                                   incorrect
                                                
                                                
                                                   ow
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Note that the speaker dependence in Eq. (16) is not present in the case of speaker independent models. The threshold τ can be empirically estimated on the development set. However, this was only necessary for the generative models, because the optimal threshold for the discriminative models (LR and LR+spk) resulted always very close to 0.5 due to the MSE training criterion.

The performance of CMs has been tested based on the following evaluation metrics:
                           
                              •
                              Classification error rate (CER): The relative number of wrongly classified samples on an evaluation sample set, given the rule in (16). It is the direct natural metric to assess the performance of two classifiers: the higher the value, the worse. A simple way to estimate the goodness of a classifier is to compare the CER value to the relative number of incorrect samples produced by the system (usually referred as the “baseline”). Unfortunately, the CER as a metric has some flaws: results cannot be directly compared for different tests sets; and the CER is very sensitive to the test set itself, not only to the classifier.

Area under the ROC curve (AROC): The area under the Receiving Operating Characteristic (ROC) curve (Fawcett, 2006). Briefly, the ROC curve is the set of points in the False Positive Rate (FPR)–True Positive Rate (TPR) space, yielded by the classification for every possible different value of the classification threshold τ. The AROC is usually normalised within [0, 100], 100 being a perfect classification and 50 a random classification. The AROC has been a commonly used metric to evaluate the replicability of the CER results. Nonetheless, this metric has been severely criticised since it can give potentially misleading results if ROC curves cross, and it is incoherent in terms of misclassification costs (Hand, 2009).


                                 h-measure (Hand, 2009): Normalised metric which is proportional to the overall misclassification loss incurred when using an optimal threshold (which depends on the costs) averaged by a certain function u(c) over the cost ratio c
                                 ∈[0, 1], c
                                 =
                                 c
                                 0/(c
                                 0
                                 +
                                 c
                                 1) and (c
                                 0, c
                                 1) being the misclassification costs. For the common case in which it cannot be derived which kind of misclassifications are preferable (false positive, or false negatives, etc.), the author proposes a normalised symmetric function u(c)∝
                                 β(c
                                 ;2, 2)∝(c
                                 −
                                 c
                                 2). This measure was proposed to avoid the issue of the AROC metric, since it is proportional to the expectation of the overall misclassification loss weighted by a function depending on the distribution of the scores. Thus, the weight function to measure the AROC depends on the classifier to be tested.


                                 Normalised cross entropy (NCE)
                                 : Metric proportional to the cross entropy of the classified set. This metric is related to the average log distance of the score to the true class. NCE equals 1 for a perfect classification in which the predicted posteriors of the correct class score 1 for the correct samples and 0 for the incorrect. Unfortunately, the lowest value is unbounded, since it involves the sum of the logarithm of zero or arbitrarily low values for samples which scored high on the opposite class to the true one. Despite this flaw (noticed shortly after its publication (Wessel et al., 1998)), it is still widely used.

@&#RESULTS@&#

Experiments have been carried out computing the set of input scores that performed the best for the NB model in Sanchis et al. (2012).
                           
                              •
                              SP: Word acoustic log-score per time frame (10-ms).

D: Duration (in ms.) of the word per phone.

NL: Length of the N-gram in which the word has been decoded.

PAvg: Word posterior probability computed as the average of frame-based posteriors (Wessel et al., 2001).

PMax: Like PAvg but using the maximum instead of the average (Wessel et al., 2001).

The NL score is not exactly the same as that used in Sanchis et al. (2012), since the length of the N-gram is used instead of the Boolean feature representing the LM back-off behaviour.


                        Table 3
                         summarises the performance of the proposed models on the VL and PL test sets in terms of the different metrics presented in Section 4.2. We also include results from additional experiments using Conditional Random Field (CRF) models which, as stated in recent publications, are of particular importance (Seigel, 2013; Seigel and Woodland, 2011; Fayolle et al., 2010; Lavergne et al., 2010).
                           1
                        
                        
                           1
                           Both, CRF++ and wapiti toolkits were tested. Results presented here correspond to wapiti toolkit (https://wapiti.limsi.fr/), which in turn outperformed CRF++. The optimisation algorithm used was RPROP+ too with L2 regularisation. The optimisation criterion was Maximum log-Likelihood conditional Estimate.
                         It is worth noting that all models have been compared under identical conditions. To assess statistical significance of results, 95% confidence intervals are included for the CER% evaluation metric.

From the results in Table 3, it can be stated that speaker-adapted models outperform their non-adapted counterparts. This is true, indeed, for all models and all evaluation metrics, and also holds for both, VL and PL tasks. Statistically speaking, this statement is significant to a great extent, especially in the case of VL. In this case, in terms of CER%, the best results are: 14.99, with CRF+spk, and 14.82 with LR+spk. These figures are clearly below the lower limit of the 95% confidence intervals for CRF and LR, respectively. On the other hand, the results on PL are similar, though the CRF+spk result overlap the CER% confidence interval for CRF at its lower half, and the same happens with LR+spk. This might be influenced by the comparatively low values of CER% on PL for all models.

Another conclusion that can be drawn from Table 3 is that the NB model is clearly superseded by CRF and LR, and that this also holds for their speaker-adapted versions. Given that the LR model is designed as a discriminatively trained version of NB, this result was well expected. On the other hand, although LR(+spk) results are slightly but consistently better than those of CRF(+spk), there is no clear statistical evidence to support its superiority. Indeed, the main difference between them is the training criterion used which, from our experiments, has little effect on the results.

The ROC curves of the NB(+spk), CRF(+spk) and LR(+spk) models are depicted in Figs. 1 and 2
                        
                         for VL and PL, respectively. The classification thresholds adjusted from the development data (operating points) and the optimal ones are also plotted. As can be observed, the speaker-adapted models show better performance than their basic, non-adapted counterparts for nearly all possible classification thresholds.


                        Table 4
                         shows detailed results on the VL test, at speaker level, using the CER evaluation metric. As above, the best results are achieved by LR+spk and CRF+spk. The results at speaker level using other evaluation metrics are similar and are omitted for simplicity.

With the aim of measuring the benefits of the LR+spk model in a practical application, we have evaluated its performance in an interactive speech transcription (IST) setting applied within the EU project transLectures. In this setting, users devote a limited amount of effort to supervising a given percentage of words of the automatic transcriptions. User effort is optimised by ordering the speech segments selected for supervision from lower to higher reliability based on CMs.

The VL test set has been used for the assessment of the NB and LR+spk models. Corrections were performed by means of a simulated user in a similar way to Sanchez-Cortina et al. (2012).

The final quality (measured in WER) of partially supervised transcriptions resulting for different percentages of supervised words is depicted on Fig. 3
                     . The figure assesses the behaviour when using the NB, LR+spk or CRF+spk (wapiti+spk) models to compute CMs. A random strategy corresponding to a sequential supervision of the words is also depicted.

From Fig. 3, it can be stated that the LR+spk and CRF+spk models perform similarly, and that they both outperform the NB model for any level of user effort (percentage of supervised words). In particular, for the reasonable range of percentages from 10% to 20%, the LR+spk and CRF+spk produce relative WER improvements between 2% and 7%.

@&#CONCLUSIONS@&#

We have introduced a new particular logistic regression model to improve the reliability of the confidence measures for automatic speech recognition. Also, as a main contribution, we have proposed the use of speaker-adapted models.

The experiments have shown that speaker-adapted models outperform their non-adapted counterparts on two difficult tasks from English (videoLectures.net) and Spanish (poliMedia) educational lectures. The proposed logistic regression model achieved comparatively good results.

Finally, a simple real application of interactive speech transcription guided by confidence measures has confirmed that the gains obtained by the proposed models translate into a noticeable improvement of the resulting semi-supervised transcriptions for an equal level of user effort.

@&#ACKNOWLEDGEMENTS@&#

The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement no 287755. Also supported by the Spanish MINECO (iTrans2 TIN2009-14511 and Active2Trans TIN2012-31723) research projects and the FPI Scholarship BES-2010-033005.

@&#REFERENCES@&#

