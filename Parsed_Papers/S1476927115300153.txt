@&#MAIN-TITLE@&#Impact of heuristics in clustering large biological networks

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           In this paper, we have proposed two new heuristics for clustering biological networks.


                        
                        
                           
                           We have incorporated these heuristics into a celebrated clustering algorithm called SPICi to get three new clustering algorithms.


                        
                        
                           
                           We have conducted extensive experiments and analysis to analyze the performance of the new algorithm and the results are found to be promising.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Algorithms

Biological networks

Clustering

Heuristics

@&#ABSTRACT@&#


               
                  
                  Traditional clustering algorithms often exhibit poor performance for large networks. On the contrary, greedy algorithms are found to be relatively efficient while uncovering functional modules from large biological networks. The quality of the clusters produced by these greedy techniques largely depends on the underlying heuristics employed. Different heuristics based on different attributes and properties perform differently in terms of the quality of the clusters produced. This motivates us to design new heuristics for clustering large networks. In this paper, we have proposed two new heuristics and analyzed the performance thereof after incorporating those with three different combinations in a recently celebrated greedy clustering algorithm named SPICi. We have extensively analyzed the effectiveness of these new variants. The results are found to be promising.
               
            

@&#INTRODUCTION@&#

Clustering is an important tool in biological network analysis. However, traditional clustering algorithms do not perform well in the analysis of large biological networks being either extremely slow or even unable to cluster (Song and Singh, 2009). On the other hand, recent advancement of the state of the art technologies along with computational predictions have resulted in large scale biological networks for numerous organisms (Brun et al., 2004). As a result, faster clustering algorithms are of tremendous interest. There exist a number of clustering algorithms that work well on small to moderate biological networks. For instance, a number of algorithms in the literature can guarantee that they would generate clusters with some specific properties (e.g., Cfinder (Adamcsek et al., 2006; Palla et al., 2005; Colak et al., 2009; Georgii et al., 2009)). They are however computationally very intensive and hence do not scale well as the size of the biological network increases. Algorithms like WPNCA (Peng et al., 2014) and ClusterONE (Nepusz et al., 2012) are new approaches to handle weighted biological networks of moderate size. But in many cases they fail or require a large amount of time to cluster.

To this end, some more efficient approaches have been introduced most of which are based on greedy techniques (e.g., SPICi (Jiang and Singh, 2010), DPClus (Altaf-Ul-Amin et al., 2006), etc.). However, algorithms like MGclus (Frings et al., 2013) suit relatively well for clustering large biological networks with dense neighborhood. In most cases, clusters produced by greedy approaches are highly dependent on the heuristic(s) employed. It is expected that a better heuristic will yield even more improved results. This motivates us to search for a better heuristic for a well-performing greedy approach to devise an even better clustering algorithm that not only runs faster but also provides quality solutions.

SPICi (Jiang and Singh, 2010) is a relatively recent and new approach among the greedy techniques that can cluster large biological networks. After carefully studying and analyzing the implementation of SPICi, we have discovered that some essential modification in the heuristics employed can bring drastic change in the clusters’ quality. In this paper, we have proposed a couple of new heuristics for SPICi with an aim to devise an even better clustering algorithm. We have implemented three new versions of SPICi and have conducted extensive experiments to analyze the performance our new implementations. Experimental results are found to be promising with respect to both speed and accuracy. Some preliminary results of this paper have been presented at Shafin et al. (2015).

@&#BACKGROUND@&#

We start this section with preliminaries on some related notions followed by a discussion on the algorithmic framework of SPICi. We will also briefly review the heuristics used in SPICi. A biological network is modeled as an undirected graph G
                     =(V, E) where each edge 
                        (
                        u
                        ,
                        v
                        )
                        ∈
                        E
                      has a confidence score 
                     
                        (
                        0
                        <
                        
                           w
                           
                              u
                              ,
                              v
                           
                        
                        ≤
                        1
                        )
                     , also called the weight of the edge. We say that, 
                        
                           w
                           
                              u
                              ,
                              v
                           
                        
                        =
                        0
                     , if the two vertices 
                        u
                        ,
                        v
                      have no edge between them. The weighted degree of each vertex u, denoted by 
                        
                           d
                           w
                        
                        (
                        u
                        )
                     , is the sum of the confidence scores of all of its incident edges, i.e., 
                        
                           d
                           w
                        
                        (
                        u
                        )
                        =
                        
                           ∑
                           
                              (
                              u
                              ,
                              v
                              )
                              ∈
                              E
                           
                        
                        
                           w
                           
                              u
                              ,
                              v
                           
                        
                     . Based on the confidence scores or weights of the edges, we can define the term density for a set of vertices S
                     ⊆
                     V as follows. The density 
                        D
                        (
                        S
                        )
                      of a set S
                     ⊆
                     V of vertices is defined as the sum of the weights of the edges that have both end vertices belonging to S divided by the total number of possible edges in S. Formally,
                        
                           
                              D
                              (
                              S
                              )
                              =
                              
                                 
                                    
                                       ∑
                                       
                                          u
                                          ,
                                          v
                                          ∈
                                          S
                                       
                                    
                                    
                                       w
                                       
                                          u
                                          ,
                                          v
                                       
                                    
                                 
                                 
                                    |
                                    S
                                    |
                                    ×
                                    (
                                    |
                                    S
                                    |
                                    −
                                    1
                                    )
                                    /
                                    2
                                 
                              
                           
                        
                     
                  

For each vertex u and a set S
                     ⊆
                     V, support of u by S, denoted by 
                        S
                        (
                        u
                        ,
                        S
                        )
                     , is defined as the sum of the confidence scores of the edges of u that are incident to the vertices in S. Formally,


                     
                        
                           
                              S
                              (
                              u
                              ,
                              S
                              )
                              =
                              
                                 ∑
                                 
                                    v
                                    ∈
                                    S
                                 
                              
                              
                                 w
                                 
                                    u
                                    ,
                                    v
                                 
                              
                           
                        
                     
                  

Given a weighted network, the goal of SPICi is to output a set of disjoint dense sub-graphs. SPICi uses a greedy heuristic approach that builds one cluster at a time and expansion of each cluster is done from an original protein seed pair. SPICi depends on two parameters, namely, the support threshold, T
                     
                        s
                      and the density threshold, T
                     
                        d
                     . The use of these two parameters will be clear shortly. Now, we briefly review how SPICi employs its heuristic strategies. In fact, SPICi first selects two seed nodes and then attempt to expand the clusters.

While selecting the seed vertices, SPICi uses a heuristic. Very briefly, at first it chooses a vertex u in the network that has the highest weighted degree. Then it divides the neighboring vertices of u into five bins according to their edge weights, namely, (0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8] and (0.8, 1.0]. Then the vertex with the highest weighted degree belonging to the highest non-empty bin is chosen as the second seed, 
                           v
                        . The edge 
                           (
                           u
                           ,
                           v
                           )
                         is referred to as the seed edge.

For cluster expansion, SPICi follows a procedure similar to that of Altaf-Ul-Amin et al. (2006). It works with a vertex set S for the cluster initially containing the two selected seed vertices. It uses a heuristic approach to build the clusters and it builds one cluster at a time. In the cluster expansion step, SPICi searches for the vertex u such that 
                           S
                           (
                           u
                           ,
                           S
                           )
                         is maximum amongst all the unclustered vertices that are adjacent to a vertex in S. If 
                           S
                           (
                           u
                           ,
                           S
                           )
                         is smaller than a threshold then u is not added to S and 
                           D
                           (
                           S
                           )
                         is updated accordingly. However, if the calculated 
                           D
                           (
                           S
                           )
                         turns out to be smaller than the density threshold T
                        
                           d
                         then SPICi does not include u in the cluster and output S.

The heuristics employed by SPICi are developed based on an observation that two vertices are more likely to be in the same cluster if the weight of the edge between them is higher (Jiang and Singh, 2010).

The two heuristics that SPICi employs are implemented in the form of two procedures, namely, Search and Expand. In the Search procedure, node with the highest outdegree is chosen as the seed and in Expand, node with the highest support is selected as the candidate to be added to the cluster. In this paper, we have proposed two heuristics and we combine our heuristics with the heuristics of SPICi to have three new versions of SPICi. We will refer to these three versions as SPICi
                     
                        
                           
                           +
                           1
                        
                     , SPICi
                     
                        
                           
                           +
                           2
                        
                      and SPICi
                     
                        
                           
                           +
                           12
                        
                     . To be specific, we employ a new heuristic and modify the Expand procedure of SPICi to get Expand+. Similarly, we employ another new heuristic and modify the Search procedure of SPICi to get Search+. In SPICi
                     
                        
                           
                           +
                           1
                        
                     , we combine Expand+ with Search while in SPICi
                     
                        
                           
                           +
                           2
                        
                     , we combine Expand with Search+. Both the heuristics are replaced by Search+ and Expand+ in SPICi
                     
                        
                           
                           +
                           12
                        
                     . In essence, our first heuristic is to choose the node with the highest weighted degree among the neighbors as the first seed and second one is to choose the node with the highest average weighted degree as the candidate to join the cluster.

Note that, the heuristics employed by SPICi are developed based on an observation that two vertices are more likely to be in the same cluster if the weight of the edge between them is higher (Brohee and Helden, 2006). In what follows, we describe our heuristic strategies along with the motivation and rationale behind those.

Consider Fig. 1
                        . Here assume that, the current cluster set is S
                        ={1, 2, 3} and the set of candidate nodes is {4, 5}. The goal at this point is to expand the current cluster. Now SPICi calculates 
                           S
                           (
                           4
                           ,
                           S
                           )
                           =
                           1.4
                         and 
                           S
                           (
                           5
                           ,
                           S
                           )
                           =
                           1.5
                         and since 
                           S
                           (
                           5
                           ,
                           S
                           )
                           >
                           S
                           (
                           4
                           ,
                           S
                           )
                        , it will include node 5 in the cluster. However, two vertices are more likely to be in the same module if the weight on the edge between them is higher (Jiang and Singh, 2010). In Fig. 1, we can see that the average weight with which node 4 is connected with nodes 2 and 3 is higher than the same with which node 5 is connected with nodes 1, 2 and 3. So, a cluster comprising the set {4, 2, 3} seems more meaningful than a cluster comprising the set {5, 1, 2, 3}. Hence, although node 4 is not connected to node 1, it seems more useful to include node 4 in the current cluster. To make a better decision, we introduce a new heuristic measure which we refer to as the AverageEdgeWeight as follows. For each node u and a set S
                        ⊆
                        V, let Q
                        ⊆
                        S be the set of vertices u is connected with. The average edge weight of u by S is defined as follows,
                           
                              
                                 AverageEdgeWeight
                                 (
                                 u
                                 ,
                                 S
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             v
                                             ∈
                                             Q
                                          
                                       
                                       
                                          w
                                          
                                             u
                                             ,
                                             v
                                          
                                       
                                    
                                    
                                       |
                                       Q
                                       |
                                    
                                 
                              
                           
                        
                     

Now let us refer back to the scenario illustrated in the Fig. 1. Using our new heuristic measure, we calculate AverageEdgeWeight(4, S)=0.7 and AverageEdgeWeight(5, S)=0.5. Since AverageEdgeWeight(4, S)>
                        AverageEdgeWeight(5, S), in contrast to SPICi, we choose node 4 as desired. In Expand procedure, as we move from a dense region to a less dense region, it is more sensible to select nodes having higher values for average edge weight. This statement can be supported by the illustration of biological networks of Figs. 3 and 4, where we observe that nodes from less dense areas have more edge connectivity than the nodes in more dense areas. Despite having less connectivity, the edge weights of the nodes in dense areas carry better values. This encourages us to identify the dense area nodes using the AverageEdgeWeight as it focuses on the edge values.

In order to observe how this heuristic affects clustering, we searched for candidate nodes that were selected to be included into the cluster having a higher AverageEdgeWeight but lower support value than other candidate nodes. The results are shown in Table 1
                         and we can see that the case we consider here comes up frequently.

The modified Expand procedure, i.e., Expand+ is presented in form of Algorithm 1.


                        
                           Algorithm 1
                           Expand+(u, v) 
                                 
                                    
                                       
                                       
                                          
                                             initialize the cluster 
                                                   S
                                                   =
                                                   {
                                                   u
                                                   ,
                                                   v
                                                   }
                                                
                                             
                                          
                                          
                                             initialize CandidateQ to contain vertices neighboring u or 
                                                   v
                                                
                                             
                                          
                                          
                                             initialize AverageEdgeWeightHeap to contain vertices neighboring u or 
                                                   v
                                                
                                             
                                          
                                          
                                             
                                                while 
                                                CandidateQ is not empty do
                                             
                                          
                                          
                                             
                                                find the largest non-empty bin of Average Edge
                                             
                                          
                                          
                                             
                                                
                                                Weight Heap nodes in CandidateQ among the bins with AverageEdgeWeight(t, S) range of (0.8, 1],(0.6, 0.8],(0.4, 0.6],(0.2, 0.4],(0.0, 0.2].
                                          
                                          
                                             
                                                extract t from CandidateQ with highest support(t, S) and t belongs to largest non-empty bin of AverageEdgeWeightHeap
                                             
                                          
                                          
                                             
                                                
                                                if 
                                                support(t, S)≥
                                                T
                                                
                                                   s
                                                
                                                ×|S|×
                                                density(S) and density(S
                                                ∪{t})>
                                                T
                                                
                                                   d
                                                
                                             
                                          
                                          
                                             
                                                
                                                
                                                S
                                                =
                                                S
                                                +{t}
                                          
                                          
                                             
                                                
                                                increase the support for vertices connected to t in CandidateQ
                                             
                                          
                                          
                                             
                                                
                                                for all unclustered vertices adjacent to t, insert them into CandidateQ if not present
                                          
                                          
                                             
                                                
                                                for all unclustered vertices adjacent to t, update AverageEdgeWeightHeap
                                             
                                          
                                          
                                             
                                                
                                                break from loop
                                          
                                          
                                             
                                                
                                                end if
                                             
                                          
                                          
                                             
                                                end while
                                             
                                          
                                          
                                             return S
                                             
                                          
                                       
                                    
                                 
                              
                           

For each vertex u, the weighted degree of its neighbors, denoted by 
                           
                              A
                              w
                           
                           (
                           u
                           )
                         is simply the summation of the weighted degrees of all of its neighbors. So, we have 
                           
                              A
                              
                                 w
                                 (
                                 u
                                 )
                              
                           
                           =
                           
                              ∑
                              
                                 (
                                 u
                                 ,
                                 v
                                 )
                                 ∈
                                 E
                              
                           
                           
                              d
                              w
                           
                           (
                           v
                           )
                        . To illustrate the usefulness of this heuristic measure, let us consider Fig. 2
                        . While selecting the first seed SPICi groups the set of nodes with similar outdegrees. In Fig. 2, SPICi would have two group of nodes {1, 2, 3, 5} and {4}. The grouping is as such, because node 1 has outdegree of 2.4 which is rounded off to 2 and similarly the outdegrees of 2, 3, 5 are 1.5, 2, 1.5 which are also rounded to 2. As these nodes have the same outdegree after rounding off, they are in the same group. And node 4 has outdegree of 1.4 so it is rounded off to 1 and it creates a new group. Now, SPICi will select a node from the highest weight group. This node selection cannot be predicted because there is no sequencing or sorting in SPICi, it is dependent on how the nodes are traversed and entered in the group. Thus, we cannot predict which node will be selected beforehand. Suppose 5 is selected. But, clearly 5 is in a weak neighborhood, i.e., it does not have a dense group around it. But if we use the weighted degree of all the neighbors for a particular node then for node 1 we have the highest weighted degree 
                           (
                           
                              A
                              w
                           
                           (
                           1
                           )
                           =
                           2
                           +
                           1.4
                           +
                           1.5
                           +
                           1.6
                           =
                           6.5
                           )
                        . And, choosing the node with the highest weighted degree of neighbors is likely to enhance the probability to select the most promising node as the first seed. This ensures that we are selecting the node from a dense neighborhood, so in the expand process we will always start in a dense population.

From the visualization of significant portions of STRING Human and STRING Yeast networks (Figs. 3 and 4
                        
                        ) we can infer that, nodes of less dense areas have the property of being connected to many nodes from different dense areas which may help them to get selected as the seed node. However, seed node should be selected from a dense area to uncover quality functional module. The modified Search procedure, i.e., Search+ is provided in form of Algorithm 2 (Table 2
                        ).

To analyse a bit further, we have checked the topology in the input biological networks and seek how many nodes are in a weak neighborhood. We search for those nodes which has strictly higher outdegrees but lower neighbor weights than the nodes adjacent to them. If any node falls in such condition then they are in a weak neighborhood with high outdegree which may lead SPICi to avoid a clever selection.

From the results we see that almost half of the nodes of STRING networks are in weak neighborhood. This suggests that the heuristics we proposed will make a better choice in many situations. The presence of such topology also indicates the necessity of improved heuristics for greedy clustering algorithms.


                        
                           Algorithm 2
                           Search+
                                 
                                    
                                       
                                       
                                          
                                             Initialize DegreeQ to be V
                                             
                                          
                                          
                                             
                                                while DegreeQ is not empty do
                                             
                                          
                                          
                                             
                                                Extract u from DegreeQ with largest weighted degree of neighbors
                                          
                                          
                                             
                                                
                                                if 
                                                u has adjacent vertices in DegreeQ then
                                             
                                          
                                          
                                             
                                                
                                                Find from u's adjacent vertices the second seed protein 
                                                   v
                                                
                                             
                                          
                                          
                                             
                                                
                                                
                                                
                                                   S
                                                   =
                                                   Expand
                                                   (
                                                   u
                                                   ,
                                                   v
                                                   )
                                                
                                             
                                          
                                          
                                             
                                                
                                                else
                                             
                                          
                                          
                                             
                                                
                                                
                                                S
                                                ={u}
                                          
                                          
                                             
                                                
                                                end if
                                             
                                          
                                          
                                             
                                                
                                                V
                                                =
                                                V
                                                −
                                                S
                                             
                                          
                                          
                                             
                                                Delete all vertices in S from DegreeQ
                                             
                                          
                                          
                                             
                                                For each vertex in t in DegreeQ that is adjacent to a vertex in S, decrement its weighted degree by support (t, S)
                                          
                                          
                                             
                                                end while
                                             
                                          
                                       
                                    
                                 
                              
                           

In this section we analyze the time complexity of our new implementations based on the newly proposed heuristics. In particular, we implement three improved versions of SPICi, namely, SPICi
                     
                        
                           
                           +
                           1
                        
                     , SPICi
                     
                        
                           
                           +
                           2
                        
                      and SPICi
                     
                        
                           
                           +
                           12
                        
                     . More specifically, in SPICi
                     
                        
                           
                           +
                           1
                        
                     , we plug in Expand+ in SPICi instead of Expand; in SPICi
                     
                        
                           
                           +
                           2
                        
                      we plug in Search+ in SPICi instead of Search; and in SPICi
                     
                        
                           
                           +
                           12
                        
                      we plug in both Expand+ and Search+ instead of Expand and Search respectively. Table 3
                      shows the summary of these three implementations along with the original version, i.e., SPICi. In what follows, we assume that the graph has a total of n vertices and m edges.


                     
                        
                           –
                           
                              SPICi
                              
                                 
                                    
                                    +
                                    1
                                 
                              : In SPICi
                              
                                 
                                    
                                    +
                                    1
                                 
                              , we use three binary heap data structures, namely, DegreeQ (for the Search procedure), CandidateQ and AverageEdg- eWeightHeap (for the Expand+ procedure). Each of the operations of Insert, ExtractMax, DecreaseKey, IncreaseKey and Delete on CandidateQ, DegreeQ and AverageEdgeWeightHeap runs in O(log
                              n) time. For initial seed selection, each vertex is inserted in DegreeQ once in the Search procedure and hence there are n 
                              Insert operations. In Expand+, the neighbors of the nodes that are already in a cluster are the candidate nodes for further cluster formation and are inserted in CandidateQ using the Insert operation based on the AverageEdgeWeight (described in Section 3.1) of each candidate node stored in the AverageEdgeWeightHeap. Subsequently, the values are updated using IncreaseKey and DecreaseKey operations. These operations together yield a total of n 
                              Insert and update (i.e., IncreaseKey and DecreaseKey) operations. Furthermore, the nodes that are inserted in CandidateQ and DegreeQ are extracted needing n 
                              ExtractMax or Delete operations. Hence, the running time of Expand+ is O(2n
                              log
                              n) depending on CandidateQ and AverageEdgeWeightHeap. Now to read the graphs O(m) time is needed. So the total time complexity of SPICi
                              
                                 
                                    
                                    +
                                    1
                                 
                               is O(3n
                              log
                              n
                              +
                              m)=
                              O(n
                              log
                              n
                              +
                              m).


                              SPICi
                              
                                 
                                    
                                    +
                                    2
                                 
                              : In Search+, we have implemented the Weighted Degree of Neighbors heuristic (described in Section 3.2). In Search+, we have used Fibonacci Heap (Fredman and Tarjan, 1987) on which we need to apply Insert, DecreaseKey, Delete and ExtractMax operations. Here, each of the first three operations can be done in constant time whereas the last one (i.e., ExtractMax) runs in O(log
                              n) time. Note that, for the initial calculation of the Weighted Degree of Neighbors, we need O(m) time. Now, there will be a total of n 
                              ExtractMax operations. On the other hand, In Expand there are n 
                              IncreaseKey operations. Hence the running time of Search+ is O(n
                              log
                              n
                              +
                              m). So, after including the time to read the graph, the total running time of SPICi
                              
                                 
                                    
                                    +
                                    2
                                 
                               becomes O(n
                              log
                              n
                              +2m)=
                              O(n
                              log
                              n
                              +
                              m).


                              SPICi
                              
                                 
                                    
                                    +
                                    12
                                 
                              : The running time of SPICi
                              
                                 
                                    
                                    +
                                    12
                                 
                               can be easily deduced by summing up the respective running times of Expand+ and Search+ procedures. The running time of Expand+ is O(2n
                              log
                              n) and that of Search+ is O(n
                              log
                              n
                              +
                              m). Therefore, considering the time to read the graph, the total running time of SPICi
                              
                                 
                                    
                                    +
                                    12
                                 
                               becomes O(3n
                              log
                              n
                              +2m)=
                              O(n
                              log
                              n
                              +
                              m).

@&#EXPERIMENTS AND RESULTS@&#

We have conducted our experiments on a PC having Intel 2.40GHz core i5 processor with 4GB memory. The coding has been done in C++ using Codeblocks 10.05 IDE. All the experiments have been run in Linux (Ubuntu 12.04) environment. The source codes of our proposed heuristics are freely available at http://goo.gl/e9du1Y. For all the experiments we set both T
                     
                        s
                      and T
                     
                        d
                      to 0.5, the same value that was used in SPICi. For our experiments we had to convert the gene names and we used various sources to convert and extract the gene names from STRING-Known and Predicted (2015), THE SYNERGIZER (2014) and BioGRID (2014). The overall conversion of the gene names affects the analysis. But as all the experiments were done using the same procedure, the overall penalty borne by the clusters are the same. The reported runtimes of SPICi, SPICi
                        
                           
                           +
                           1
                        
                      and SPICi
                        
                           
                           +
                           2
                        
                      are wall clock times since the same was done in Jiang and Singh (2010).

We have used four networks: two networks for yeast and two for human. These are the same networks used by Brohee and Helden (2006) to experimentally evaluate SPICi. The properties of these networks are reported in Table 4
                        . Experimentally determined physical and genetic interactions are found in the two Biogrid networks (Chatr-aryamontri et al., 2013). On the other hand, functional association between proteins that are derived from data integration are found in the two STRING networks (Jensen et al., 2009). These datasets are available at (SPICi, 2014). Brohee and Helden (2006) conducted another analysis on the Human bayesian network which we could not get from the authors. For the Biogrid networks all non-redundant interaction pairs are extracted which include the protein genetic and physical interactions and all weighted interactions for the STRING networks were used as reported in Jiang and Singh (2010).

The GO analysis conducted here is based on the analysis done in Jiang and Singh (2010). They used a framework described in (Song and Singh, 2009) to evaluate the obtained clusters. We have used the same framework to compare among the clusters we have obtained using different heuristics. To construct the functional modules’ reference set we have used Gene Ontology (GO) in the same way as it was done in Jiang and Singh (2010). The Gene Ontology (Ashburner et al., 2000) is an external measure to derive functional modules. For a GO biological process (BP) or cellular component (CC) functional term, a module contains all the proteins that are annotated with that term. Evaluation of clustering algorithms is done by judging how well the clusters correspond to the functional modules as derived from either GO BP or GO CC annotations. Following the work of Jiang and Singh (2010), we have considered the GO terms that annotate at most 1000 proteins for each organism. For a particular GO annotation A, G
                        
                           A
                         is the functional module set having all genes that are annotated with A. To measure the similarity between GO functional modules and derived clusters, Song and Singh (2009) uses the following three measures:
                           
                              –
                              Jaccard: Consider a cluster C. With each GO derived functional module group G
                                 
                                    A
                                  the Jaccard value of C is computed as 
                                    
                                       
                                          |
                                          C
                                          ∩
                                          
                                             G
                                             A
                                          
                                          |
                                       
                                       
                                          |
                                          C
                                          ∪
                                          
                                             G
                                             A
                                          
                                          |
                                       
                                    
                                 . The maximum Jaccard value we get for cluster C over all GO term A is considered the Jaccard value of C.

PR (Precision Recall): Consider a cluster C. With each GO derived functional module G
                                 
                                    A
                                 , its PR value is computed as 
                                    
                                       
                                          |
                                          C
                                          ∩
                                          
                                             G
                                             A
                                          
                                          |
                                       
                                       
                                          |
                                          
                                             G
                                             A
                                          
                                          |
                                       
                                    
                                    ×
                                    
                                       
                                          |
                                          C
                                          ∩
                                          
                                             G
                                             A
                                          
                                          |
                                       
                                       
                                          |
                                          C
                                          |
                                       
                                    
                                 . The maximum PR value we get for cluster C over all GO term A is considered the PR value of C.

Semantic density: Average semantic similarity within each pair of annotated proteins is computed for each cluster. For two proteins p1 with annotations A(p1) and p2 with annotations A(p2). The semantic similarity of their GO annotations is defined as:
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      2
                                                   
                                                
                                                ×
                                                
                                                   
                                                      
                                                         min
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            a
                                                         
                                                      
                                                      ∈
                                                      
                                                         
                                                            A
                                                         
                                                      
                                                      (
                                                      
                                                         
                                                            
                                                               p
                                                               1
                                                            
                                                         
                                                      
                                                      )
                                                      ∩
                                                      
                                                         
                                                            A
                                                         
                                                      
                                                      (
                                                      
                                                         
                                                            
                                                               p
                                                               2
                                                            
                                                         
                                                      
                                                      )
                                                   
                                                
                                                log
                                                (
                                                
                                                   
                                                      p
                                                   
                                                
                                                (
                                                
                                                   
                                                      a
                                                   
                                                
                                                )
                                                )
                                             
                                             
                                                
                                                   
                                                      
                                                         min
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            a
                                                         
                                                      
                                                      ∈
                                                      
                                                         
                                                            A
                                                         
                                                      
                                                      (
                                                      
                                                         
                                                            
                                                               p
                                                               1
                                                            
                                                         
                                                      
                                                      )
                                                   
                                                
                                                log
                                                (
                                                
                                                   
                                                      p
                                                   
                                                
                                                (
                                                
                                                   
                                                      a
                                                   
                                                
                                                )
                                                )
                                                +
                                                
                                                   
                                                      
                                                         min
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            a
                                                         
                                                      
                                                      ∈
                                                      
                                                         
                                                            A
                                                         
                                                      
                                                      (
                                                      
                                                         
                                                            
                                                               p
                                                               1
                                                            
                                                         
                                                      
                                                      )
                                                   
                                                
                                                log
                                                (
                                                
                                                   
                                                      p
                                                   
                                                
                                                (
                                                
                                                   
                                                      a
                                                   
                                                
                                                )
                                                )
                                             
                                          
                                       
                                    
                                 where p(a) is the fraction of annotated proteins with annotation a in the organism (Lord et al., 2003), (Song and Singh, 2009). For semantic density calculations, all GO terms that are annotating even more than 1000 proteins are also considered (Jiang and Singh, 2010).

The measures above have a range between 0 and 1. The higher the values, the better is the result of uncovering clusters satisfying the functional modules corresponding to GO. We calculate Jaccard, PR and semantic density for each cluster and for both BP and CC ontology. These measures are attributed to all proteins in the cluster. Singleton cluster genes are penalized by assigning 0 to each of the three measure. Lastly, we compute average value of each of the six measures (three CC and three BP) over all proteins of the network. The results of the GO analysis are presented in Table 5
                        . Here we have analyzed the 3 new variants, namely, SPICi
                           
                              
                              +
                              1
                           
                        , SPICi
                           
                              
                              +
                              2
                           
                         and SPICi
                           
                              
                              +
                              12
                           
                         along with SPICi as well as some other celebrated algorithms from the literature, namely, MGclus (Frings et al., 2013), ClusterONE (Nepusz et al., 2012) and WPNCA (Peng et al., 2014).

From Table 5, we observe that the changes in the heuristic of the algorithm affects the quality of the clusters. For moderate sized networks like Biogrid Human and Biogrid Yeast, SPICi
                           
                              
                              +
                              1
                           
                         has higer values in most of the cases. But for large networks the values are close to the values of SPICi. The same property holds with SPICi
                           
                              
                              +
                              2
                           
                         but when we use both heuristics together in SPICi
                           
                              
                              +
                              12
                           
                         it dominates SPICi in most of the cases increasing the quality of the functional modules. Algorithms like WPNCA (Peng et al., 2014) and MGclus (Frings et al., 2013) are unable to cluster large biological network while ClusterONE (Nepusz et al., 2012) shows good performance in large biological network but has the overhead of time complexity which is a major concern for fast clustering algorithms.

To analyze the different sizes of the functional modules uncovered by the algorithms we have also performed the cluster-size analysis. The clusters are divided into five different bins depending on the number of proteins in the cluster. The first four bins are, [0,5), [5,15), [15,50], [50,150) which resembles the lower and upper limit of the size of the clusters. The last bin contains the number of clusters having more than 150 proteins. The result of this analysis has been presented in Table 6
                        . After observing the values of Table 6, we notice that greedy algorithms like SPICi uncovers modules of medium
                           2
                        
                        
                           2
                           Cluster size between 5 and 50.
                         size mostly. One of the reasons for this can be attributed to the fact that these algorithms are non-overlapping. On the other hand ClusterOne and MGclus uncovers large functional modules for large networks. For the large “STRING Human” network, SPICi finds 9444 clusters in total whereas SPICi
                           
                              
                              +
                              12
                           
                         finds a fewer number of clusters (9340). So the average size of clusters of SPICi
                           
                              
                              +
                              12
                           
                         should be larger than that of SPICi.

While clustering with SPICi
                           
                              
                              +
                              12
                           
                        , it uncovers clusters that are on average larger in size maintaining the density threshold. So, it uncovers clusters which are better in size and thus for the GO analysis while we calculate the intersection values between clusters and derived functional modules it gives us a better GO analysis result. Hence, we can understand that uncovering clusters having larger size on an average is a major reason for SPICi
                           
                              
                              +
                              12
                           
                         to have better GO analysis results than SPICi.

The running times of the algorithms run during our experiments are reported in Table 7
                        . Note that, an ideal clustering algorithm should cluster large biological networks reasonably fast while maintaining the quality of the cluster. MGclus (Peng et al., 2014), ClusterOne (Nepusz et al., 2012) and WPNCA (Peng et al., 2014) are not time efficient as compared to SPICi and the three new variants as the process of clustering is very slow with these algorithms. In our experiments, MGclus gave a runtime error when clustering STRING Human network and the process WPNCA could not provide any output for the same network even after 12hours. ClusterOne took 1hour to cluster STRING yeast network and seven hours to cluster STRING human network. Although ClusterOne has achieved good GO analysis results for large biological networks, its high running time makes it infeasible for large networks.

Robustness analysis of the improved algorithms are performed using the process of Brohee and Helden (2006) which tests the algorithms ability to restate the MIPS complexes from synthetic test network data. To perform the robustness analysis, we have created networks from the MIPS complexes of Saccharomyces cerevisiae available at (Mewes et al., 2002). The nodes of the created network resembles to the protein of these complexes and the edges are created between any two proteins in the same complex. Suppose the network created this way holds |E| edges. Now, the networks are modified using two factors, edge addition rate p
                        
                           a
                         and edge deletion rate p
                        
                           d
                        . At first the values of p
                        
                           a
                         and p
                        
                           b
                         are chosen over 10 different rates and then on the generated network p
                        
                           a
                        
                        ×|E| are added prior to the removal of p
                        
                           d
                        
                        ×|E| edges which are selected randomly. The two measures used by Brohee and Helden (2006) are Accuracy and Separation. Accuracy indicates how well the algorithm can recover the gold standars of MIPS complexes. On the other hand the Separation measure provides how specific the mapping is between the clusters to the MIPS gold standard set. For a more precise definition of measures and process description please refer to Brohee and Helden (2006).

The robustness of each variant (i.e., SPICi
                        
                           
                              
                              +
                              1
                           
                        , SPICi
                        
                           
                              
                              +
                              2
                           
                         and SPICi
                        
                           
                              
                              +
                              12
                           
                        ) has been compared with SPICi. The combination of 10 different addition rates and 10 different deletion rates was used to generate the test networks. This test provides the separation and accuracy measures (Brohee and Helden, 2006) which helps us to deduce the robustness of the algorithm. The output of the robustness analysis has been presented in Tables 8–13
                        
                        
                        
                        
                        
                        . Among these tables, Table 8 (Table 9), Table 10 (Table 11) and Table 12 (Table 13) present the accuracy (separation) measure, respectively, for SPICi
                           
                              
                              +
                              12
                           
                        , SPICi
                           
                              
                              +
                              2
                           
                         and SPICi
                           
                              
                              +
                              1
                           
                         against SPICi. In each table, addition and deletion rates are placed row-wise and column-wise respectively. Each cell represents a particular measure, i.e., accuracy or separation, which is obtained from the test networks generated using the addition and deletion rate of the corresponding row and column. In each cell of the tables, a value higher than 1.00 indicates that the corresponding variant (i.e., SPICi
                           
                              
                              +
                              12
                           
                         (in case Tables 8 and 9) or SPICi
                           
                              
                              +
                              2
                           
                         (in case Tables 10 and 11) or SPICi
                           
                              
                              +
                              1
                           
                         (in case Tables 12 and 13)) performed better than SPICi.

At this point a brief discussion on the robustness analysis performed is in order. In Tables 8 and 9, we have compared SPICi
                           
                              
                              +
                              12
                           
                         against SPICi. After observing the values in Table 8 (i.e., accuracy) and Table 9 (i.e., separation), we can realize that the level of accuracy of these 2 algorithms is similar. Note that the accuracy and separation values are not very large than 1.00, where the value of 1.00 means that both the algorithms handled the noise in the network in a similar manner. But in most cases, the separation values of SPICi
                           
                              
                              +
                              12
                           
                         reflects better performance. Tables 10 and 11 presents the accuracy and separation values of SPICi
                           
                              
                              +
                              2
                           
                         against SPICi. For higher addition and deletion rates the accuracy of SPICi
                           
                              
                              +
                              2
                           
                         is almost 1.00 whereas for lower addition and deletion rates it performs slightly better in comparison. With respect to the separation values, SPICi
                           
                              
                              +
                              2
                           
                         performs better than SPICi. Finally, Tables 12 and 13 denote accuracy and separation measures of SPICi
                           
                              
                              +
                              1
                           
                         against SPICi. Unlike the accuracy values of SPICi
                           
                              
                              +
                              12
                           
                         and SPICi
                           
                              
                              +
                              2
                           
                         (Tables 8 and 10), the accuracy values of SPICi
                           
                              
                              +
                              1
                           
                         are mostly less than 1.00. Even in a few cases, the value is near 0.75. This indicates slightly superior robustness of SPICi against SPICi
                           
                              
                              +
                              1
                           
                        . But for the separation measure, SPICi
                           
                              
                              +
                              1
                           
                         still performs well.

As both SPICi
                           
                              
                              +
                              1
                           
                        , SPICi
                           
                              
                              +
                              2
                           
                         and SPICi
                           
                              
                              +
                              12
                           
                         are similar to the original algorithm (i.e., SPICi), they all are expected to have almost the same level of robustness. To summarize the observations of Tables 8–13, we notice that the robustness of SPICi
                           
                              
                              +
                              1
                           
                         and SPICi
                           
                              
                              +
                              2
                           
                         is similar to that of SPICi. However, it can be seen that for low addition and deletion rates SPICi
                           
                              
                              +
                              1
                           
                         and SPICi
                           
                              
                              +
                              2
                           
                         performs better. The separation values from the Tables 9, 11 and 13 suggest that the new variants have uncovered clusters that are more accurately mapped to the MIPS gold standard as in most cases the values for the new variants are higher than 1.00 indicating a better mapping. So, the overall results of robustness analysis shows that the improved heuristics handles the noise in the network in a similar fashion of SPICi but gives a better mapping to the gold standards.

@&#CONCLUSION@&#

In bio-informatics clustering algorithms are considered to be one of the most important tools. Although there are a number of clustering algorithms that can cluster biological network, most of them fail to cluster large biological networks. In this paper we have proposed two new greedy heuristics and have presented three new implementations of a celebrated greedy clustering algorithm called SPICi (Jiang and Singh, 2010). Our experimental results and analyses indicate promising results.

From our investigation we have found that in greedy algorithms, the heuristic used is the key that helps the algorithm to use the structures and foundation of input networks. In this paper we have seen, SPICi
                        
                           
                           +
                           1
                        
                     , SPICi
                        
                           
                           +
                           2
                        
                      and SPICi
                        
                           
                           +
                           12
                        
                      use different sets of heuristics to provide outputs of different quality. This suggests that better heuristics yield better results. Hence, it presents a new scope of designing heuristics that evaluates biological network in a better way.

As a future work, we plan to investigate whether our heuristic approaches are useful in other relevant clustering issues like multi-network clustering, overlapping cluster identification, active module identification etc. as has been discussed in a recent review at Mitra et al. (2013).

@&#REFERENCES@&#

