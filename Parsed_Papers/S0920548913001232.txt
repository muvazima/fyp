@&#MAIN-TITLE@&#Partition-based fuzzy median filter based on adaptive resonance theory

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This paper presents a novel partition-based fuzzy median filter.


                        
                        
                           
                           The proposed neural network model based on ART is developed.


                        
                        
                           
                           Experimental results confirm the efficient removal of impulse noise.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Impulsive noise

Fuzzy rule

Adaptive resonance theory

Median filter

Least mean square

@&#ABSTRACT@&#


               
               
                  This paper presents a novel partition-based fuzzy median filter for noise removal from corrupted digital images. The proposed filter is obtained as the weighted sum of the current pixel value and the output of the median filter, where the weight is set by using fuzzy rules concerning the state of the input signal sequence to indicate to what extent the pixel is considered to be noise. Based on the adaptive resonance theory, the authors developed a neural network model and created a new weight function where the neural network model is employed to partition the observation vector. In this framework, each observation vector is mapped to one of the M blocks that form the observation vector space. The least mean square (LMS) algorithm is applied to obtain the optimal weight for each block. Experiment results have confirmed the high performance of the proposed filter in efficiently removing impulsive noise and Gaussian noise.
               
            

@&#INTRODUCTION@&#

Digital image processing technologies of all kinds have been pervasively utilized in various professional areas such as geographical analysis, diagnostic imaging, image-based control and instrumentation, etc. so as to enhance the quality of common people's lives. The performance of image processing tasks such as edge detection, image retrieval, and image segmentation strictly depends on the quality of the digital images input. Unfortunately, due to a number of imperfections in the imaging process, recorded images are often contaminated by impulsive noise during image acquisition or transmission. Noise is usually caused by either a faulty imaging system (i.e., sensor noise) or an imperfect medium (i.e., random scattering and absorption) [12,27], both happening every now and then. Therefore, the removal of noise from digital images has received a lot of research interest in the last decade. However, methods currently available for the restoration of digital images corrupted by noise unfortunately introduce undesirable blurring effects [30].

Based on order statistics, the median filter and its derivatives have shown good efficiency in suppressing impulsive noise [3,4,9,13,15–19,25,27,29,31,36,41]. However, these approaches are location-invariant in nature, which can easily bring significant blurring effects into the image details. Recently, to avoid modifying noise-free pixels, decision-based median filters with thresholding operations have been developed [10,11,21,28,32,33,35,37,38]. They mainly employ a decision-making mechanism to determine whether the input pixel to a given filtering window is noisy. After the noise filtering operation, only the pixels which are identified as corrupted by the decision-making mechanism are processed. Pixels that are classified as noise-free are left unchanged. These methods enhance the noise removal performance and reduce the distortion effects. However, one disadvantage of these hard switching schemes is that they have trouble exploiting fixed decision-making processes since the threshold parameters are obtained at a pre-assumed noise density level. To remedy the problem, Lin and Yu thus proposed their adaptive two-pass median filter (ATM) based on support vector machines (SVM) [22]. Additionally, Tsai, Chang, and Lin introduced such concepts as decision tree, particle swarm optimization, as well as SVM and designed a median-type filter [43]. These filters can enhance the noise removal performance and reduce the distortion effects.

Alternatively, some filtering methods for the removal of noise from digital images use neural-fuzzy systems [1]. Neural-fuzzy median filters are controlled by fuzzy rules that judge the possibility of the existence of impulsive noise. Neural-fuzzy median filters are designed in a way that they can be trained over a reference image and yield weighting coefficients. It is thus not necessary to obtain any membership functions to represent the fuzzy rules. These filters have proven capable to provide excellent robustness in filtering images outside the training set. However, the weighting coefficients can vary as a result of the choice of reference image; in other words, the generalization capability is poor.

On the other hand, the concept of partition-based filtering has been studied and applied to reduce Gaussian noise [2]. Based on the partition concept, an extension to the neural-fuzzy median filter, named the partition fuzzy median (PFM) filter, has been developed [23]. This filter is obtained as the weighted sum of the current pixel value and the output of the median filter. In such an adaptive scheme, the optimal weights of the mutually exclusive blocks are obtained by training over a reference image. To achieve a good balance between noise suppression and detail preservation, Lin proposed an adaptive center weighted median (ACWM) filter using the partition concept [20]. Though the weights are derived from the reference image and a relatively large number of partition parameters, both PFM and ACWM filters have shown good efficiency in suppressing noise.

In this paper, a novel partition-based fuzzy median (PBFM) filter with the adaptive resonance theory (ART) applied is to be proposed to fix the drawbacks of the above hard switching schemes and adaptive median filters with trial-and-error parameters. According to fuzzy rules, the PBFM filter partitions the observation vector space using a neural network model based on the adaptive resonance theory (NNM-ART) and thus is able to provide standards for the use of a fuzzy-based median filter that does image restoration. The proposed fuzzy filter is obtained as a weighted sum of the current pixel value and the output of the median filter. The weight of each block is optimized only for the data that fall within that block. This is done by training the filter over a reference image with the least mean square (LMS) algorithm [14]. Such a design of the proposed fuzzy filter helps a lot in simplifying the setup of fuzzy-based median filters as well as the arrangement of the software components concerned. To keep the mean square error down to the minimum, the noise filtering operation is progressively applied through several iterations. Experiment results have demonstrated that the proposed filter significantly outperforms many well-accepted median-based filters. Moreover, the proposed filter also does a great job in removing Gaussian noise as well as mixed Gaussian and impulsive noise.

The rest of this paper is organized as follows. In Section 2, the basic principles of the PBFM filter are introduced. Then, the design of the proposed PBFM filter is detailed in Section 3. In Section 4, the results of some experiments are given to demonstrate the performance of the proposed scheme. Finally, the conclusions are given in Section 5.

Let C
                     ={k
                     =(k
                     1,k
                     2)|1≤
                     k
                     1
                     ≤
                     H,1≤
                     k
                     2
                     ≤
                     W} denote the pixel coordinates of a two-dimensional image, where H and W are the image height and width, respectively. A noisy signal at location k
                     ∈
                     C is then denoted as x(k). A sample observed, or filter window w{k}, is defined in terms of the image coordinates symmetrically surrounding the current pixel x(k). The size of the filter window K
                     =2n
                     +1, where n is a non-negative integer, can be given by:
                        
                           (1)
                           
                              
                                 w
                                 
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          x
                                          f
                                       
                                       
                                          k
                                       
                                       :
                                       f
                                       =
                                       1
                                       ,
                                       
                                       2
                                       ,
                                       
                                       ⋯
                                       ,
                                       
                                       n
                                       ,
                                       
                                       n
                                       +
                                       1
                                       ,
                                       
                                       ⋯
                                       ,
                                       
                                       K
                                    
                                 
                                 ,
                              
                           
                        
                     where the input pixel x(k)=
                     x
                     
                        n
                        +1(k) is the central pixel. For example, Fig. 1
                      shows a 3×3 filter window (i.e., K
                     =9) whose center is at pixel x
                     5(k). This filter window is used throughout this work. The filter window slides across the image pixels in a raster scanning fashion from left to right and from top to bottom.

A schematic diagram of the proposed adaptive median filter based on fuzzy rules is shown in Fig. 2
                     . It is composed of three parts: a median filter, fuzzy rules, and a membership function to help set the degree of noise suppression and image-detail preservation the filter is to perform. The output value y(k) of the adaptive median filter at pixel x(k) can be obtained as follows:
                        
                           (2)
                           
                              
                                 y
                                 
                                    k
                                 
                                 =
                                 m
                                 
                                    k
                                 
                                 +
                                 α
                                 
                                    k
                                 
                                 
                                    
                                       x
                                       
                                          k
                                       
                                       −
                                       m
                                       
                                          k
                                       
                                    
                                 
                                 ,
                              
                           
                        
                     where m(k) denotes the output of the median filter and α(k) denotes the membership function indicating to what extent x(k) is not considered an impulsive noise [1,23]. Each linear combination of the current pixel value and the output of the median filter is taken as an estimate and used for image restoration. If α(k)=0, an impulsive noise is considered to be located at pixel x(k), and the output is then the median value m(k) If α(k)=1, however, then no impulsive noise is considered to exist at x(k), and the output value is the same as that of the original pixel, namely x(k) If α(k) can only be either 0 or 1, the filter functions as a switching median filter; that is, it is a decision-based filter. However, it is oftentimes difficult to make a clear-cut judgment as to whether or not impulsive noise exists at pixel x(k) Therefore, α(k) should take a continuum from 0 to 1 according to fuzzy rules. In other words, deciding the value of the membership function α(k) is the major issue of the proposed PBFM filter.

The membership function α(k) can be set according to the local characteristics of the input signals. In general, the amplitude of most impulsive noise is much more prominent than the fine differences of adjacent signals. Thus, the following three linguistic variables can be defined to generate fuzzy rules.
                           Definition 1
                           The value a(k) denotes the absolute difference between input x(k) and median value m(k) as follows [20,23]:
                                 
                                    (3)
                                    
                                       
                                          a
                                          
                                             k
                                          
                                          =
                                          
                                             
                                                x
                                                
                                                   k
                                                
                                                −
                                                m
                                                
                                                   k
                                                
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           

According to Definition 1, the following fuzzy rule presented in the if-then-else format can be used to judge whether impulsive noise exists.
                           
                              Rule A:
                              If a(k) is SMALL, then no impulsive noise is assumed present; else an impulsive noise is assumed present [1,20,23].

Rule A implies that a large a(k) value indicates that the input x(k) is corrupted by impulsive noise; that is, x(k) is dissimilar to the median value of filter window w{k}. Thus, the linguistic variable a(k) is a measure for detecting the possibility of a contaminated input x(k). However, there can be misjudgments if Rule A is the only criterion used to judge whether impulsive noise exists. For example, suppose x(k) is located on a line component with no impulsive noise in the filter window w{k}. In this case, with only Rule A applied, pixel x(k)will be mistakenly classified as noisy because a(k) is large. To avoid such misjudgments, it is necessary to add other rules.


                              
                                 
                                    (4)
                                    
                                       
                                          b
                                          
                                             k
                                          
                                          =
                                          
                                             
                                                
                                                   
                                                      x
                                                      
                                                         k
                                                      
                                                      −
                                                      
                                                         x
                                                         
                                                            c
                                                            1
                                                         
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                
                                                +
                                                
                                                   
                                                      x
                                                      
                                                         k
                                                      
                                                      −
                                                      
                                                         x
                                                         
                                                            c
                                                            2
                                                         
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                
                                             
                                             2
                                          
                                          ,
                                       
                                    
                                 
                              
                           

where |x(k)−
                              x
                              
                                 c1(k)|≤|x(k)−
                              x
                              
                                 c2(k)|≤|x(k)−
                              x
                              
                                 i
                              (k)|,1≤
                              i
                              ≤
                              K, i
                              ≠0,c1,c2 [20,22,23].

Note that x
                        
                           c1(k) and x
                        
                           c2(k) are selected to be the pixel values closest to that of x(k) among its adjacent pixels in the filter window w{k}. According to Definition 2, Rule B can be derived and added to the filter.
                           
                              Rule B:
                              IF b(k) is SMALL, then no impulsive noise is assumed present; else impulsive noise is assumed present.

When a line component appears in the filter window w{k}, b(k) must be small. In this case, if Rule B is applied, a correct judgment will be made that no impulsive noise is located at pixel x(k).

However, when a pixel exists in an edge area, as Fig. 3
                                  shows, it will be misjudged as impulsive noise since the a(k) value is large. Therefore, it is important to distinguish noise components from edge components for effective noise filtering and edge preservation.


                              
                                 
                                    (5)
                                    
                                       
                                          
                                             w
                                             
                                                d
                                                0
                                             
                                          
                                          
                                             k
                                          
                                          =
                                          MED
                                          
                                             
                                                
                                                   x
                                                   1
                                                
                                                
                                                   k
                                                
                                                ,
                                                
                                                ⋯
                                                ,
                                                
                                                
                                                   x
                                                   4
                                                
                                                
                                                   k
                                                
                                                ,
                                                
                                                
                                                   d
                                                   0
                                                
                                                ⋄
                                                
                                                   x
                                                   5
                                                
                                                
                                                   k
                                                
                                                ,
                                                
                                                ⋯
                                                ,
                                                
                                                
                                                   x
                                                   9
                                                
                                                
                                                   k
                                                
                                             
                                          
                                          ,
                                       
                                    
                                 
                              where MED{x
                              1(k),⋯,x
                              4(k),d
                              0
                              ⋄
                              x
                              5(k),⋯,x
                              9(k)}
                                 
                                    
                                       
                                          =
                                          MED
                                          
                                             
                                                
                                                   x
                                                   1
                                                
                                                
                                                   k
                                                
                                                ,
                                                
                                                ⋯
                                                ,
                                                
                                                
                                                   x
                                                   4
                                                
                                                
                                                   k
                                                
                                                ,
                                                
                                                
                                                   
                                                      
                                                         
                                                            x
                                                            5
                                                         
                                                         
                                                            k
                                                         
                                                         ,
                                                         
                                                         ⋯
                                                         ,
                                                         
                                                         
                                                            x
                                                            5
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      ⏟
                                                   
                                                   
                                                      
                                                         d
                                                         0
                                                      
                                                      
                                                      times
                                                   
                                                
                                                ,
                                                
                                                ⋯
                                                ,
                                                
                                                
                                                   x
                                                   9
                                                
                                                
                                                   k
                                                
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           

Here MED denotes the median operation, d
                        0 denotes the non-negative integer weight, and d
                        0
                        ⋄
                        x
                        5(k) means that there are d
                        0 copies of the input pixel x(k) [22].
                           Definition 4
                           
                              
                                 
                                    (6)
                                    
                                       
                                          l
                                          
                                             k
                                          
                                          =
                                          
                                             
                                                x
                                                
                                                   k
                                                
                                                −
                                                
                                                   w
                                                   3
                                                
                                                
                                                   k
                                                
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           

If the variable l(k) is applied, whose center weight d
                              0 equals 3, then the edge components in the filter window w{k} will not be detected as noise since l(k) is small [22]. Therefore, in this case, the decision made is that no impulsive noise is located at pixel x(k). Thus, Rule C can be derived as follows:

IF l(k) is SMALL, then no impulsive noise is assumed present; else impulsive noise is assumed present.

According to the knowledge of domain experts, the action α(k) can take fuzzy sets {VERY LOW, LOW, MEDIUM, HIGH} of natural language as its values. The behavior of the detection filter can be described using eight combination fuzzy rules based on Rules A, B and C as follows:
                           
                              Rule 1:
                              IF a(k) is LARGE, b(k) is LARGE and l(k) is LARGE, then α(k) is VERY LOW.

IF a(k) is LARGE, b(k) is LARGE and l(k) is SMALL, then α(k) is LOW.

IF a(k) is LARGE, b(k) is SMALL and l(k) is LARGE, then α(k) is LOW.

IF a(k) is LARGE, b(k) is SMALL and l(k) is SMALL, then α(k) is MEDIUM.

IF a(k) is SMALL, b(k) is LARGE and l(k) is LARGE, then α(k) is LOW.

IF a(k) is SMALL, b(k) is LARGE and l(k) is SMALL, then α(k) is MEDIUM.

IF a(k) is SMALL, b(k) is SMALL and l(k) is LARGE, then α(k) is MEDIUM.

IF a(k) is SMALL, b(k) is SMALL and l(k) is SMALL, then α(k) is HIGH.

Note that a(k) denotes the membership function indicating to what extent impulsive noise is not considered present at pixel x(k). Now, the problem is how to design function α(k) for a(k), b(k) and l(k). Among the eight fuzzy rules above, six fuzzy sets, namely LARGE, SMALL, HIGH, MEDIUM, LOW, and VERY LOW, can be expressed in the form of one-dimensional shape membership functions. Therefore, the membership function α(k) can be obtained by using fuzzy approximate reasoning with the membership function of fuzzy sets LARGE and SMALL and its relationship to a(k) [1,23,39]; that is, a(k) can be viewed as a synthetic membership function of fuzzy sets. However, in real-world image restoration processes, it is difficult to obtain precise membership functions for a(k), b(k), l(k) and to accurately execute fuzzy approximate reasoning. In the next subsection, a novel partition-based learning approach is presented to solve this problem.

In order to design the membership function α(k), a method for the partitioning of the observation vector space is needed, and a learning approach to set the value of each block is required so to minimize the mean square error of the filtering output. With the three linguistic variables a(k), b(k), and l(k) [42] applied, the observation vector can be expressed by:
                           
                              (7)
                              
                                 
                                    X
                                    
                                       k
                                    
                                    =
                                    
                                       
                                          a
                                          
                                             k
                                          
                                          ,
                                          
                                          b
                                          
                                             k
                                          
                                          ,
                                          
                                          l
                                          
                                             k
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Partitioning happens when the observation vector space S, which is a subset of R
                        3, is classified into a set of M mutually exclusive blocks, defined as Ω
                        1,Ω
                        2, ⋯, Ω
                        
                           M
                        , given by:
                           
                              (8)
                              
                                 
                                    
                                       Ω
                                       i
                                    
                                    =
                                    
                                       
                                          X
                                          
                                             k
                                          
                                          ∈
                                          S
                                          :
                                          n
                                          
                                             
                                                X
                                                
                                                   k
                                                
                                             
                                          
                                          =
                                          i
                                       
                                    
                                    ,
                                    
                                    i
                                    =
                                    1
                                    ,
                                    
                                    2
                                    ,
                                    ⋯
                                    ,
                                    
                                    M
                                    ,
                                 
                              
                           
                        where the classifier n(⋅) is now defined as a function of the observation vector X(k). That is, these M blocks Ω
                        
                           i
                        , i
                        =1,2,⋯,M satisfy:
                           
                              (9)
                              
                                 
                                    S
                                    =
                                    
                                       
                                          ∪
                                          
                                             i
                                             =
                                             1
                                          
                                          M
                                       
                                       
                                          Ω
                                          i
                                       
                                    
                                    
                                    and
                                    
                                    
                                       Ω
                                       i
                                    
                                    ∩
                                    
                                       Ω
                                       j
                                    
                                    =
                                    ϕ
                                    ,
                                    
                                    for
                                    
                                    i
                                    ≠
                                    j
                                    .
                                 
                              
                           
                        
                     

Then, each input pixel x(k) corresponding to its X(k)∈
                        S is classified into one of the M blocks according to the classifier n(X(k))=
                        i.

To design the function α(k), a classifier n(⋅) based on the adaptive resonance theory (ART) is developed in this study. For various types of input patterns, Grossberg and Carpenter proposed two types of adaptive resonance architecture: ART1, which provides a formation in response to arbitrary sequences of binary input patterns, and ART2 in response to arbitrary sequences of analog (continuous-valued) input patterns [5–8,24]. Now, based on the ART2 algorithm, a neural network model (NNM) called the NNM-ART neural network is developed for designing classifier n(⋅). The framework of the NNM-ART architecture is shown in Fig. 4
                        . The NNM-ART network is composed of an input layer and a cluster layer, which is a simple net for determining the nearest cluster exemplar. A synaptic prototype weight wj
                         that has the same dimension as the input data X(k) is associated with each neuron j in the cluster layer as shown in Fig. 4.

The NNM-ART network functions as follows. The input layer of nodes is solely for the input X(k). The cluster layer, on the other hand, is the competitive layer where the nodes compete against one another (winner takes all) based on the given inputs. NNM-ART uses the Manhattan distance Uj
                        , which is the difference between the input data X(k) and the weight vector of the jth node in the competition.
                           
                              (10)
                              
                                 
                                    
                                       U
                                       j
                                    
                                    =
                                    
                                       
                                          
                                             X
                                             
                                                k
                                             
                                             −
                                             
                                                w
                                                j
                                             
                                          
                                       
                                       1
                                    
                                    ,
                                    
                                    j
                                    =
                                    1
                                    ,
                                    
                                    2
                                    ,
                                    
                                    ⋯
                                    ,
                                    
                                    M
                                    .
                                 
                              
                           
                        
                     

When all Uj
                         are determined, the nodes in the competitive layer begin to compete. The node with the smallest Manhattan distance Uj
                         wins. Suppose that the qth node is the winner. The output ij
                         of the jth node in the competitive layer is:
                           
                              (11)
                              
                                 
                                    
                                       i
                                       j
                                    
                                    =
                                    
                                       
                                          
                                             
                                                1.0
                                                ,
                                                
                                                j
                                                =
                                                q
                                                ,
                                             
                                          
                                          
                                             
                                                0.0
                                                ,
                                                
                                                j
                                                ≠
                                                q
                                                .
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

In other words, the competitive output layer is a “winner-take-all” type of net. Consider the minimum Manhattan distance between the input vector X(k) and w
                        
                           j
                        ,j
                        =1,2,⋯,M. The nodes in the cluster layer compete for the input vector to be classified. The node with the smallest Manhattan distance Uj
                         is the winner and sends a signal 1 to the output node ij
                        .

In the NNM-ART learning algorithm, the first training vector is X(k), and w
                        1
                        =
                        X(k) is used directly to establish the first cluster node. Then, the next input training vector is compared with the first cluster node. It is assigned to the first cluster node if its Manhattan distance is smaller than the vigilance parameter [24]. Otherwise, a new cluster node is generated. Consider the minimum Manhattan distance between the input vector X(k) and w
                        
                           j
                        ,j
                        =1,2,⋯,c. Notably, c is the current number of cluster layer neurons. That is, NNM-ART places the input vector into the most similar cluster node. This process is repeated for all training input vectors. Ultimately, NNM-ART classifies all input patterns into M clusters.

Each training vector is entered to the input layer. The nodes in the cluster layer compete (winner takes all) for the input vector to be classified. The NNM-ART network determines the winner node for the training vector in the competitive layer. Then, the weight vector wq
                        , which directs from the winner (qth node) in the competitive layer to the input layer, is updated by applying the following learning rule.
                           
                              (12)
                              
                                 
                                    
                                       w
                                       q
                                    
                                    
                                       
                                          t
                                          +
                                          1
                                       
                                    
                                    =
                                    
                                       w
                                       q
                                    
                                    
                                       t
                                    
                                    +
                                    
                                       1
                                       
                                          
                                             N
                                             q
                                          
                                          +
                                          1
                                       
                                    
                                    
                                       
                                          
                                             
                                                1
                                                −
                                                
                                                   U
                                                   q
                                                
                                             
                                          
                                          X
                                          
                                             k
                                          
                                          −
                                          
                                             w
                                             q
                                          
                                          
                                             t
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where the cluster q has Nq
                         members. Notably, the weight vectors pointing to the loser nodes stay unchanged. The NNM-ART algorithm for the partitioning of the observation vector space is as follows.
                           Algorithm
                           
                              
                                 
                                    (1)
                                    Set the initial weight w
                                       1
                                       =
                                       X(k).

Set the vigilance δ.

Set the maximum cluster node M.

Do
                                          
                                             (a)
                                             Compute Manhattan distance Uj
                                                .
                                                   
                                                      
                                                         
                                                            
                                                               U
                                                               j
                                                            
                                                            =
                                                            
                                                               
                                                                  X
                                                                  
                                                                     k
                                                                  
                                                                  −
                                                                  
                                                                     w
                                                                     j
                                                                  
                                                               
                                                            
                                                            ,
                                                            
                                                            j
                                                            =
                                                            1
                                                            ,
                                                            
                                                            2
                                                            ,
                                                            
                                                            ⋯
                                                            ,
                                                            
                                                            c
                                                            .
                                                         
                                                      
                                                   
                                                
                                             

Decide the winner node q, where q is the smallest Uj
                                                .

If (Uj
                                                
                                                ≤
                                                δ) OR (number cluster nodes J
                                                ≥
                                                M)then update winner weight.
                                                   
                                                      
                                                         
                                                            
                                                               w
                                                               q
                                                            
                                                            
                                                               
                                                                  t
                                                                  +
                                                                  1
                                                               
                                                            
                                                            =
                                                            
                                                               w
                                                               q
                                                            
                                                            
                                                               t
                                                            
                                                            +
                                                            
                                                               1
                                                               
                                                                  
                                                                     N
                                                                     q
                                                                  
                                                                  +
                                                                  1
                                                               
                                                            
                                                            
                                                               
                                                                  
                                                                     
                                                                        1
                                                                        −
                                                                        
                                                                           U
                                                                           q
                                                                        
                                                                     
                                                                  
                                                                  X
                                                                  
                                                                     k
                                                                  
                                                                  −
                                                                  
                                                                     w
                                                                     q
                                                                  
                                                                  
                                                                     t
                                                                  
                                                               
                                                            
                                                            .
                                                         
                                                      
                                                   
                                                
                                             

If (Uq
                                                
                                                >
                                                δ) AND (number cluster nodes J
                                                <
                                                M)then add one cluster node J.
                                                   
                                                      
                                                         
                                                            
                                                               w
                                                               J
                                                            
                                                            =
                                                            X
                                                            
                                                               k
                                                            
                                                            .
                                                         
                                                      
                                                   
                                                
                                             

While (stable cluster exists)

A stable cluster appears when the input training vector is classified into the same cluster as before, so it is not necessary to decide the total number of iterations to process in advance. In the NNM-ART training process, the centroid of the data in a cluster can be computed by:
                           
                              (13)
                              
                                 
                                    
                                       w
                                       j
                                    
                                    =
                                    
                                       1
                                       
                                          N
                                          j
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             N
                                             j
                                          
                                       
                                       
                                          
                                             X
                                             i
                                             j
                                          
                                          
                                             k
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Note that cluster j has Nj
                         members and that X
                        
                           i
                        
                        
                           j
                        (k) denotes the input observation vector X(k) in cluster j. When an input observation vector X(k) is applied to the network at time t, the NNM-ART algorithm rewards the winner neuron q. According to Uj
                         with a centroid, the winner weight w
                        
                           j
                        
                        (t) can be updated as follows:
                           
                              (14)
                              
                                 
                                    
                                       
                                          
                                             w
                                             q
                                          
                                          
                                             t
                                          
                                          =
                                          
                                             1
                                             
                                                
                                                   N
                                                   q
                                                
                                                +
                                                1
                                             
                                          
                                          
                                             
                                                
                                                   N
                                                   q
                                                
                                                ×
                                                
                                                   w
                                                   q
                                                
                                                
                                                   
                                                      t
                                                      −
                                                      1
                                                   
                                                
                                                +
                                                
                                                   
                                                      1
                                                      −
                                                      
                                                         U
                                                         q
                                                      
                                                   
                                                
                                                X
                                                
                                                   k
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                          =
                                          
                                             w
                                             q
                                          
                                          
                                             
                                                t
                                                −
                                                1
                                             
                                          
                                          +
                                          
                                             1
                                             
                                                
                                                   N
                                                   q
                                                
                                                +
                                                1
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      1
                                                      −
                                                      
                                                         U
                                                         q
                                                      
                                                   
                                                
                                                X
                                                
                                                   k
                                                
                                                −
                                                
                                                   w
                                                   q
                                                
                                                
                                                   
                                                      t
                                                      −
                                                      1
                                                   
                                                
                                                .
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Finally, the iterative learning rule of wj
                         in NNM-ART can be derived as follows:
                           
                              (15)
                              
                                 
                                    
                                       w
                                       j
                                    
                                    
                                       t
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                   j
                                                
                                                
                                                   
                                                      t
                                                      −
                                                      1
                                                   
                                                
                                                +
                                                
                                                   1
                                                   
                                                      
                                                         N
                                                         j
                                                      
                                                      +
                                                      1
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            1
                                                            −
                                                            
                                                               U
                                                               j
                                                            
                                                         
                                                      
                                                      X
                                                      
                                                         k
                                                      
                                                      −
                                                      
                                                         w
                                                         j
                                                      
                                                      
                                                         
                                                            t
                                                            −
                                                            1
                                                         
                                                      
                                                   
                                                
                                                ,
                                                
                                                if
                                                
                                                j
                                                =
                                                q
                                                
                                                is
                                                
                                                a
                                                
                                                winner
                                                ,
                                             
                                          
                                          
                                             
                                                
                                                   w
                                                   j
                                                
                                                
                                                   
                                                      t
                                                      −
                                                      1
                                                   
                                                
                                                ,
                                                
                                                otherwise
                                                .
                                             
                                          
                                       
                                    
                                    
                                 
                              
                           
                        
                     

According to the partitioning of the observation vector space, the membership function for pixel x(k) can be expressed as α
                        
                           i
                        (k),i
                        =1,2,⋯,M. The setting of the optimal αi
                        (k) determines the filtering results. Here, each independent block weight αi
                        (k) can be obtained using the least mean square (LMS) algorithm [14,34]. Fig. 5
                         shows the weight controller for the membership functions. The LMS algorithm, which is a gradient-decent learning algorithm, tends to minimize the cost function 
                           
                              
                                 
                                    ε
                                    ^
                                 
                                 i
                              
                              
                                 k
                              
                           
                         with respect to the block Θ
                        
                           i
                        .
                           
                              (16)
                              
                                 
                                    
                                       
                                          ε
                                          ^
                                       
                                       i
                                    
                                    
                                       k
                                    
                                    =
                                    
                                       
                                          ∑
                                          k
                                       
                                       
                                          
                                             
                                                
                                                   d
                                                   
                                                      k
                                                   
                                                   −
                                                   y
                                                   
                                                      k
                                                   
                                                
                                             
                                             2
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where d(k) denotes the desired output and y(k), expressed as Eq. (2), denotes the physical output of the PBFM filter. Optimal weights are sought to minimize the mean square error. Since the M blocks are mutually exclusive, the total minimum mean square error can be expressed as:
                           
                              (17)
                              
                                 
                                    ε
                                    =
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          M
                                       
                                       
                                          
                                             
                                                ε
                                                ^
                                             
                                             i
                                          
                                          
                                             k
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Therefore, the total error ε can be minimized by individually minimizing each 
                           
                              
                                 
                                    ε
                                    ^
                                 
                                 i
                              
                              
                                 k
                              
                              .
                           
                         By differentiating Eq. (16), the updated rule associated with the training algorithm can be formulated as:
                           
                              (18)
                              
                                 
                                    
                                       α
                                       i
                                       
                                          
                                             t
                                             +
                                             1
                                          
                                       
                                    
                                    
                                       k
                                    
                                    =
                                    
                                       α
                                       i
                                       
                                          t
                                       
                                    
                                    
                                       k
                                    
                                    −
                                    
                                       l
                                       i
                                       
                                          t
                                       
                                    
                                    e
                                    
                                       k
                                    
                                    
                                       
                                          
                                             x
                                             
                                                t
                                             
                                          
                                          
                                             k
                                          
                                          −
                                          
                                             m
                                             
                                                t
                                             
                                          
                                          
                                             k
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Here, l
                        
                           i
                        
                        (t) denotes the learning rate, which is a convergence factor, e(k) is the difference between the desired output d(k) and the physical output y(k), and t is the time when the pixel x(k) is processed. The time is t
                        +1 when the next pixel x(k) is also included in the i th block. When αi
                        (k) sufficiently converges, the value of αi
                        (k) becomes optimal, giving the minimum mean square error. Based on the assumptions presented in Ref. [14] for the derivation of LMS convergence, the following conditions are sufficient for the convergence onto the mean square:
                           
                              (19)
                              
                                 
                                    0
                                    <
                                    
                                       l
                                       i
                                    
                                    <
                                    
                                       2
                                       
                                          E
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                         
                                                            k
                                                         
                                                         −
                                                         d
                                                         
                                                            k
                                                         
                                                      
                                                   
                                                   2
                                                
                                                |
                                                n
                                                
                                                   
                                                      X
                                                      
                                                         k
                                                      
                                                   
                                                
                                                =
                                                i
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    i
                                    =
                                    1
                                    ,
                                    
                                    2
                                    ,
                                    
                                    ⋯
                                    ,
                                    
                                    M
                                    ,
                                 
                              
                           
                        where E[⋅|⋅] is the conditional expectation.

Finally, the output y(k) at the pixel x(k) can be denoted as follows:
                           
                              (20)
                              
                                 
                                    y
                                    
                                       k
                                    
                                    =
                                    
                                       α
                                       i
                                    
                                    
                                       k
                                    
                                    x
                                    
                                       k
                                    
                                    +
                                    
                                       
                                          1
                                          −
                                          
                                             α
                                             i
                                          
                                          
                                             k
                                          
                                       
                                    
                                    m
                                    
                                       k
                                    
                                    ,
                                    
                                    i
                                    =
                                    1
                                    ,
                                    
                                    2
                                    ,
                                    
                                    ⋯
                                    ,
                                    
                                    M
                                    ,
                                 
                              
                           
                        where i indicates that the pixel x(k) is classified into block Ω
                        
                           i
                        . The partition index i is determined using the NNM-ART neural network. A detailed block diagram illustrating the operation of the PBFM filter is shown in Fig. 6
                        . Note that the parts of fuzzy rules and membership functions introduced earlier in Fig. 2 have now been updated to observation vector extraction and weight controller respectively in Fig. 6. Meanwhile, αi
                        (k) is associated with block Ω
                        
                           i
                         and denotes the ratio function indicating to what extent impulsive noise is considered present when the input x(k) is classified into this block. For each block Ω
                        
                           i
                        , a specific filtering operation is independently performed.

Various noise rates and types were considered in the experiments. In general, impulse is independent of image intensity. By definition, impulsive noise at noise ratio r means that r percent of pixels are replaced with impulse. Impulse (random-valued impulse) uniformly distributed over the range of [0, 255] was applied to the 8-bit gray-scale images. Zero-mean additive Gaussian noise with a standard deviation of σ was also considered in the experiments. In addition, the mixture of Gaussian noise and impulsive noise was also used as a third type of noise in the evaluation.

Extensive experiments were conducted on a variety of 512×512 test images to evaluate the performance of the proposed PBFM filter. Throughout the experiments, 3×3 filter windows were used with the peak signal-to-noise ratio (PSNR) employed to quantitatively measure the restoration performance, where larger PSNR values suggest better image restoration. In the experiments, a reference image “Couple” corrupted by 20% impulsive noise was taken as the training image.


                     Fig. 7
                      shows the vigilance effect shown on the cluster number. The figure indicates that if the vigilance is set low, then a small similarity can be tolerated, and a smaller set of clusters will form. On the other hand, if the vigilance is high, then a large number of clusters will form. The NNM-ART set the vigilance as δ
                     =0.93, and the network dynamically generated 523 cluster layer nodes in the training process. To stay computationally reasonable, the maximum number of clusters was set to be 150. In other words, all the membership functions α
                     
                        i
                     (k),i
                     =1,2,⋯,150 in the observation vector space could be independently trained using Eq. (18).

To improve the image restoration performance, progressive filtering was implemented in the PBFM filter. The first experiment was conducted to assess the effectiveness of the proposed strategy of progressive filtering. Fig. 8
                         shows the iterative filtering process conducted on various test images corrupted by 20% impulsive noise. In the extensive experiments, the filtering processes were progressively applied through several iterations to achieve convergence. It is interesting to notice that, as Fig. 8 shows, satisfactory results can be obtained when the number of iterations is two. For this reason, the final results were said to be achieved at two iterations throughout all the experiments.

The noise removal capability of the proposed PBFM filter was extensively evaluated. To assess the effectiveness on various corrupted images, the proposed filter was compared with a number of other median-based filters, including the median (MED) filter, the switching scheme I (SWM-I) filter [38], the progressive switching median (PSM) filter [40], the tri-state median (TSM) filter [11], the fuzzy median (FM) filter [1], the adaptive center weight median (ACWM) filter [20], and the partition fuzzy median (PFM) filter [23]. The parameters of each filtering method tested were tuned exhaustively to obtain the best results possible. As for the training process, the same “Couple” image corrupted by 20% impulse was used as the training image for the FM filter, the ACWM filter, and the PFM filter. Table 1
                         presents the PSNR comparison results of removing 20% impulsive noise. As Table 1 reveals, the PBFM filter gave the highest PSNR. Fig. 9
                         compares the image restoration results for the “F16” image corrupted by 20% impulsive noise. The PBFM filter clearly achieved better impulse removal and provided a more visually pleasant image.

The experiments also demonstrated the robustness of the PBFM filter against various percentages of impulsive noise. Training image “Couple” with 20% noise was used to obtain the membership function ai
                        (k), independent of the actual corruption percentage. That is, the optimal membership function ai
                        (k) of block Θ
                        
                           i
                         was independent of the noise intensity in all experiments. Fig. 10
                         shows the PSNR comparison results of filtering the “Boat” image, where the noise ratio varied from 5% to 30%. The proposed filter obviously outperformed the other methods at all noise ratios. Fig. 11
                         shows the outstanding restoration results obtained by the PBFM filter for the “Boat” image corrupted by 5% to 30% impulsive noise.

Although the PBFM filter was originally designed to remove impulsive noise, it also turns out capable of effectively reducing Gaussian noise and mixed noise. In the Gaussian noise removal experiments, restoration results were compared among the partition-based weighted sum (PWS) filter [2], some other median-based filters such as MED, FM, PFM, and ACWM, as well as the PBFM filter. Tables 2 and 3
                        
                         show that the proposed filter did give the best PSNR to images corrupted by Gaussian noise (σ
                        =30) and a mixture of Gaussian (σ
                        =20) and impulsive noise (20%). Fig. 12
                         shows a comparison of image restoration results on the “Lena” image corrupted by Gaussian noise (σ
                        =30). The PBFM filter provided the most visually pleasant image.

@&#CONCLUSION@&#

In this study, a partition-based fuzzy median filter was proposed to improve the performance of median-based filters in term of noise removal from corrupted digital images. According to fuzzy rules, the concept of observation vector space partitioning with the adaptive resonance theory applied was developed to obtain the membership function so as to indicate to what extent to consider noise present. In addition, the LMS algorithm was used to obtain the optimal weight for each block. As a result, a filtering operation could be activated as a weighted sum of the current pixel value and the output of the median filter. Besides, to achieve better performance, progressive implementation was researched. The efficacy of the proposed filter was experimentally demonstrated, and its superiority over many well-accepted methods in terms of PSNR as well as perceived image quality was tested and proven.

@&#REFERENCES@&#

