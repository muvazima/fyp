@&#MAIN-TITLE@&#Cloud-based power estimation and power-aware scheduling for embedded systems

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We predict power consumption of embedded systems to extend their service time.


                        
                        
                           
                           A cloud model is proposed.


                        
                        
                           
                           A power-aware scheduling organizes system calls into DAGs.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Embedded system

Power profiling

Power-aware cluster scheduling

Wireless sensor networks

Cloud computing

Directed acylic graph

@&#ABSTRACT@&#


               
               
                  Power efficiency is a crucial issue for embedded systems, and effective power profiling and prediction tools are in high demand. This paper presents a cloud-based power profiling (CPP) tool for recording system calls and their associated parameters to predict hardware power consumption when running target applications. Based on hardware power consumption and system profiling from the operating system (OS) kernel, the proposed network model can effectively summarize running behavior of the target applications and the relationship among system calls. This model is also used to develop an energy efficient cluster scheduling for user-inactive processes to reduce the power consumption and extend the service time of embedded systems. These profiling data can be integrated into a cloud model to be maintained by software designers or OS developers to accommodate power estimation and scheduling data for a variety of platforms.
               
            

@&#INTRODUCTION@&#

Energy consumption has emerged as one of the most critical issues for embedded devices (e.g., smartphones, wearable devices and vehicle monitor/control systems) and their applications. Recent developments in embedded system design have focused on multi-core architectures which perform applications in parallel, resulting in more complex energy consumption patterns than those found in single-chip devices. Although many improvements have been proposed for the design of low-power hardware, an application-dependent approach to energy management should be considered in higher system levels [1]. For example, a sensor network application that periodically returns sensing data can save a considerable amount of energy by temporarily turning off its radio device when sensing environment or computing data. Modern OSs can profile system calls to provide insight into application execution behavior. Such insight, in turn, can be used to predict and reduce hardware power consumption.

As transistors become more tightly packed, power and heat density levels on single chips operating at very high frequencies become unsustainably high. Therefore, heat and power-efficiency issues have emerged as major challenges for the development of single core processor systems. One solution to address the power and heat barriers in CMOS technology is to reduce the operating frequency and increase the number of computing cores. This provides greater system density and performance per watt as compared to single core processors, and thus allows new levels of performance and scalability in computer systems. In addition, multicore processors allow parallel execution of the application threads and hence offer better concurrency. For example, MediaTek® clocked an octa-core mobile device SoC (MT6752) at 1.7GHz, capable of concurrently processing tasks over eight cores [2]. MT6595 is an octa-core SoC with LTE connectivity capabilities, powered by four Cortex-A17 cores with heterogeneous multi-processing capabilities. However, this rapid progress in hardware technology has raised the urgent need for effective scheduling in multicore systems in terms of architecture-side technological advances including multiprocessing, symmetric multithreading, non-uniform memory access (NUMA) [3], virtualization, etc. Overall scheduling fairness such as the workload balance between user responsiveness and overall utilization must be considered. A scheduler needs to consider not only the distribution of threads on cores, but also the distribution of execution load. System calls and hardware-supported instruction profiling, available on modern OS platforms, can help to handle the scheduling problem. They provide similar functions on different platforms, providing information for characterizing process execution behavior. Linux provides rich opportunities for scheduling at the user level, along with information about the system state and workload. System calls provide an indispensable mechanism, allowing modern OSs to recognize most actions taken by applications. This mechanism is situated at an appropriate level of granularity to understand application execution behavior. At the system-call level, applications can be viewed as a set of fine-grained tasks containing a sequence of system calls, which can be used to make power-aware scheduling decisions.

Linux has already been widely applied in smartphones, wireless sensor networks, embedded devices, vehicle infotainment systems, and televisions. For example, Android [3,4] is built on top of the Linux kernel and was deployed in 79.3% of smartphones sold worldwide in 2013 [5]. The Linux kernel handles process control, networking, and peripheral and file system access. Device drivers are either integrated directly with the kernel or added as modules which are loaded while the system is running.

Cloud computing [6] is a model for on-demand network access to a group of configurable resources, such as communication network, storage, computing capability, applications and services. Cloud computing presents a promising business model as it allows users to outsource the provisioning of tasks while ensuring access to up-to-date resources or services on demand. Currently, Google Play® and the Apple App Store® respectively serve 1.3 and 1.2 million applications. Applications in the public cloud exhibit diverse execution behaviors as well as energy consumption properties. However, it is difficult to predict hardware power consumption based on application-level profiling in a variety of platform and software combinations. A generalized model of computing support for applications could be useful to record and analyze hardware power consumption and application execution behavior.

List scheduling is a standard technique used to schedule tasks with precedence constraints [7,8]. The tasks employed by classic list scheduling are scheduled on m identical processors or without power awareness. In a system with identical processors, each processor has the same specification and runs at identical speeds.

This paper presents the concept for a repository used for recording data about the relationship among system calls, hardware instructions and their power consumption. This repository could be situated in a cloud-based environment and equipped with powerful servers to analyze the recorded data. These data are then processed using a set of cloud-based power profiling tools called CPP to estimate the system power consumption of target applications. These analysis results are also applied to develop an energy-efficient multicore list scheduling algorithm. The proposed power prediction method relies on the proposed tools with a network model and table-based computation along with the repository which analyze the profiling data, and therefore provide improved precision for power prediction. Given numerous and diverse target applications and platforms, the proposed model will include a variety of tables and should be transferred to servers in a cloud computing environment to perform data intensive computing. In this paper we propose:
                        
                           1.
                           Tools which predict power consumption for target applications by means of profiling (retrieving) their system calls, essential micro instructions and parameters;

a cloud computing model for analyzing the profiling data for a variety of application and platform combinations to improve power consumption predictions; and

a cluster-based power-aware scheduling for user-interactive processes which organizes system calls using a network model to shorten execution time.

The remainder of this paper is organized as follows: Section 2 reviews the literature on power profiling. The proposed cloud-based model and power profiling are introduced in Section 3. Section 4 introduces a power-aware list scheduling. Section 5 presents the experimental results, and we conclude this paper in Section 6.

@&#LITERATURE REVIEW@&#

Power profiling can help researchers understand the energy consumption behavior of computer devices and improve their energy efficiency. In general, power profiling is categorized into two classes. As shown in Fig. 1
                     (a), measurement-based profiling [9,10] probes device circuits using instruments such as Volt-Ohm-Milliammeters (VOMs), oscilloscopes or data acquisition (DAQ). It is used to record system power consumption corresponding to time instances, and thus analyzes power consumption behavior corresponding to specific program execution. PowerScope [9] proposed by Flinn et al., measures the power consumption of embedded systems while recording system events. Their proposed software profiles the target systems and associates the execution time with measured parameters, so as to compute device energy consumption. Sun et al. [10] proposed an energy profiling framework that combines the high-speed complex programmable logic device (CPLD) bus signaling capability of mPlatform® with the smart TwinStar® power board. They constructed diverse modes for the target system by applying an energy management unit (EMU) built in CPLD, used to monitor representative events and weight power consumption.

As shown in Fig. 1(b), model-based profiling [11–17] has a pre-defined instructions/functions table associated with individual CPU power consumption. Table content usually changes with hardware, platform and OS specifications. Whenever an application is performed, the profiling system records its execution instructions/functions and derives the corresponding CPU power consumption from the table. The estimated result is then used as a guideline to choose applications for installation, or to determine the usage patterns of embedded systems. Application developers use these results to estimate and optimize the energy consumption of their programs. Model-based profiling can be categorized as low-level [11,12] and high-level models [13–18]. Low-level models are associated with power consumption patterns derived from the system architecture level or processor instruction level, and evaluates CPU power consumption by simulation or through the use of virtualization software. Brooks et al. proposed the Wattch model [11] for architecture-level power analysis and optimizations based on the simulation software SimpleScalar [12]. Wattch modularizes the basic units of the embedded processors such as the control unit (CU), arithmetic logic unit (ALU), registers and cache unit, and records their individual utilization times and durations. High-level models estimate the CPU power consumption based on system-level or function-level information gathered on the target systems. Tiwari et al. [13] simulated the base cost of each CPU instruction with a variety of instruction formats and addressing modes. Using simulated power consumption and by monitoring the times and durations of instruction execution, this method can be used to evaluate the power cost of embedded software, and thus help to determine if a design meets its specified power constraints. Qu et al. [14] proposed a function-level power analysis model that counts the number of individual functions called by the target processes during their execution time. It stores function invocations in a database and computes the average power cost for each function. Blume et al. [15] proposed a hybrid power model for embedded systems at the function and architecture levels. The model records the executions of both processor instructions and functions to increase the accuracy of power consumption profiling and prediction. Chung et al. [16] proposed a power profiling tool called ANEPROF for Android® devices to compute the function-level power distribution and distinguish the hardware power consumption of individual threads, Java methods, and JVM services. Jung et al. [17] proposed an online power modeling tool for smartphones called DevScope, which controls components in accordance with the battery monitoring unit (BMU) updating rate, and automatically derives the component power model by analyzing changes to the power state. DevScope is not an invasive model which can be employed without changing system architecture and kernel configuration. Shye [18] proposed a user-centric power estimation model based on an accurate linear regression method to characterize the power consumption of individual peripherals with respect to user activity patterns. The method is suitable for use with applications that interact frequently with users, while the method proposed in the current paper is favorable for characterizing execution patterns of user-inactive programs.

In modern dual-mode OSs, applications that access most system resources involve with system calls. At the system level, the execution behavior of running processes can be retrieved and analyzed by monitoring their invoked system calls and essential microinstructions. Analyzing information about system calls and the execution behavior of programs can be used to estimate power consumption and improve task scheduling and thus improve application energy efficiency.

The CPP tools capture the properties of profiling data from target devices and applications in accordance with essential CPU microinstructions such as load/store or invoked system calls. These massive data sets can potentially be stored and analyzed in a cloud-based model as shown in Fig. 2
                     , which deals with the power and execution behaviors of the programs associated with different platforms.

Applications normally access peripherals through system calls and interrupts, which are among the smallest I/O units at the application function level. For example, whenever an application invokes a write function such as writing data to disk, NIC or output ports, the Linux kernel invokes the system call write( ). The first CPP tool is called system calls monitor (SCM) and keeps track of the invoked system calls along with their parameters from the Linux kernel. In Fig. 3
                        , SCM categorizes and analyzes the invoked system calls and their time instances, and identifies resources used for currently executed applications along with usage duration. The other CPP tool is called multicore process-status retrieval (mPSR) which keeps track of process status and memory cycles, and reports to SCM by recognizing CPU instructions, because SCM cannot accurately compute power consumption simply by counting system call invocations. The memory bus cycle usually refers to CPU cycle, and takes time periods for reading and/or writing data to and from memory [19]. Timer interrupt frequency is an important parameter for determining the duration for which the processors execute applications or access main memory [20]. During the power-on phase, Linux reads the initial clock from real-time clock (RTC) and determines the number of timer interrupts per second (generally, 100 per second). Whenever a timer interrupt is issued, the value of global variable jiffies is increased by one to trigger a context switch and control the process execution time. To accurately compute power consumption due to processor execution or memory access, mPSR is designed to recognize the timer interrupt and memory access instructions emerging from the system calls of the target applications. A number of kernel settings such as CONFIG_HZ, NO_HZ, HIGH_RES_TIMERS and High Precision Event Timer (HPET) affect the kernel timer interrupts. mPSR retrieves the current timer configurations using a batch shown below [20].
                           
                              
                                 
                                 
                                    
                                       $ cat/boot/config-‘uname-r’ ∣ grep HZ
                                    
                                    
                                       # CONFIG_HZ_1000 is not set
                                    
                                    
                                       # CONFIG_HZ_300 is not set
                                    
                                    
                                       CONFIG_MACHZ_WDT=m
                                    
                                    
                                       CONFIG_NO_HZ=y
                                    
                                    
                                       CONFIG_HZ=100
                                    
                                    
                                       CONFIG_HZ_100=y
                                    
                                    
                                       # CONFIG_HZ_250 is not set
                                    
                                    
                                       $ cat/boot/config-‘uname-r’ ∣ grep HIGH_RES_TIMERS
                                    
                                    
                                       CONFIG_HIGH_RES_TIMERS=y
                                    
                                 
                              
                           
                        
                     

The timer interrupt frequency can determine the power and duration required by the processor. We can find timer initial process for Linux kernel in the path using:

<linux>/init/main.c

and <linux> is the source code directory of Linux kernel. We then modify main.c after function time_init( ).
                           
                              
                                 
                                 
                                    
                                       timekepping_init();
                                    
                                    
                                       time_init();
                                    
                                    
                                       prompt_to_mPSR( );
                                    
                                 
                              
                           
                        
                     

mPSR can derive the frequency of timer interrupt from the function ACTHZ( ) defined in <linux>/include/linux/jiffies.h as.

#define ACTHZ (SH_DIV (CLOCK_TICK_RATE, LATCH, 8)).

mPSR obtains essential CPU instructions to count CPU and memory cycles as required by the target applications. It also computes the memory and CPU power consumption per unit of time. SCM constructs power-consumption tables for the given information and system calls, and builds a repository to save the precedence relations among the application system calls. Fig. 4
                         shows the structure of the CPP tools, with the resident programs SCM and mPSR recording system events from the smartphone (left side) and reporting them to a PC (right side). They also record system call parameters and essential CPU instructions such as memory access instructions associated with system calls and process statuses from ordinary application executions. They then aggregate all this data and report periodically to the analysis side. Meanwhile, a DAQ is used to probe variations in voltage, power and temperature, and reports on the fly to LabVIEW® on the analysis side. After parsing the data from DAQ and SCM, the data is aggregated into tables and stored in the analysis side repository.

To derive accurate data from system calls, SCM performs most system calls individually 100,000 times with various parameters to compute the average hardware power consumption. Before this, SCM determines the parameter combinations for each system call by monitoring the execution of target applications. The ten most-frequently used combinations are stored in a table SC_A_P for application A on platform P. For example, Table 1
                         shows the execution time in the second row of system call write( ) with predetermined parameters, and the average power consumption is shown in Table 2
                        . In Fig. 5
                        , SCM derives the average execution time and power consumption through DAQ and LabVIEW®.

In Fig. 6
                        , a detailed framework for SCM and mPSR is composed of initial and operational stages. In the lower region, SCM and mPSR collect data such as hardware power, parameters, execution time of system calls and memory access. This offline data is uploaded to a cloud environment to analyze the used system calls, because different systems may have various hardware specifications and OS versions. The microinstructions for memory access are regarded as a part of the individual system calls and are also stored in the temporary repository. Those data will be used to compute hardware power required by the system calls in the operational stage.

In the upper region of Fig. 6, SCM and mPSR monitor the CPU events and the invocations of system calls regarding their process id (PID), and obtain their parameters. Before that, the following processes should be performed in the cloud environment: In Table 3
                        , SCM counts the number of invocations for individual system calls and stores their information in the repository. In this table, the system call ID is given by the OS and facilitates mapping of the data from the temporary repository. When a system call is invoked and traced by SCM in the lower region, its parameters and ID are retrieved to construct a dynamic system-call dispatching (SCD) table for individual applications. In addition, the average power consumption of the current system calls in the repository is associated with the individual ID and the combination of their individual most-frequently used parameters. In the meantime, mPSR recognizes CPU status and memory access cycles for the current process PID and associates this with their durations and system call information in the repository. For example, Table 4
                         presents the results computed by SCM and mPSR which trace system calls and CPU status, and employ the data in the temporal repository to estimate the hardware power consumption of the target applications. In the bottom of the table, Standby_time and Exec_time respectively denote the accumulated standby time and execution time of a process, while Idle_time and Wait_time are respectively the idle and waiting times. To calculate CPU power consumption, mPSR uses the results derived from SCM and adds the resulting power consumption from Eq. (1).
                           
                              (1)
                              
                                 Standby
                                 _
                                 time
                                 ×
                                 
                                    
                                       p
                                    
                                    
                                       stand
                                    
                                 
                                 +
                                 Exec
                                 _
                                 time
                                 ×
                                 
                                    
                                       p
                                    
                                    
                                       exe
                                    
                                 
                                 +
                                 (
                                 Idle
                                 _
                                 time
                                 +
                                 Wait
                                 _
                                 time
                                 )
                                 ×
                                 
                                    
                                       p
                                    
                                    
                                       idle
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    p
                                 
                                 
                                    stand
                                 
                              
                              ,
                              
                              
                                 
                                    p
                                 
                                 
                                    exe
                                 
                              
                           
                         and 
                           
                              
                                 
                                    p
                                 
                                 
                                    idle
                                 
                              
                           
                         respectively denote power consumption per unit of time defined by RTC while CPU is in standby, execution and idle state. The upper region labeled by system calls denotes their power consumption, which is also derived from the repository. The power consumption of the system call is approximately 978mW, which is derived from the upper region of the table. In the bottom line of the table, CPU power is approximately 270mW as derived by mPSR from Eq. (1) during application execution.

The SCD tables and system call relation (SCR) graphs in the repository are shown in the lower region of Fig. 7
                        . To generate the SCD tables, the first stage defines the relationship among system calls in accordance with their functions, invocation times, and precedence. Their relationships are weighted as follows: inherent
                        =3, triggered
                        =2, unrelated
                        =1 and itself
                        =0. For example, the traced results from an application are shown in Table 5
                        (a). The number of invocations to open( ) and close( ) is the same and their precedence relations are found in the repository. Therefore, their relationship is set to inherent (i.e., 3). After completing a pair of open( ) and close( ), Linux uses Getuse( ) to check the status of the resources manipulated by the open( ) and close( ) pair, and thus the relationship is triggered (i.e., 2). Moreover, Mmap( ) is immediately invoked to check the memory mapping, and thus both open( ) and close( ) have triggered relations with Getusage( ) and Mmap( ). According to the temporal repository, the invocation to Fork( ) is not affected by open( ) and vice versa, and thus it has an unrelated relation with open( ). An example of these relations is shown in Table 5(b).

Secondly, system calls are grouped according to relevance and precedence, and are assigned in a topological order. For example, open( ), write( ) and close( ) are regarded as a group for writing data to an object. Their topological order is shown in Fig. 8
                         using a directed acyclic graph (DAG). A grouping can simplify the SCR graph to focus on significant power/time consumption and decreased computational complexity. For example, Fig. 9
                         shows detailed system calls grouped by SCM achieving a resource query purpose; this can be represented as a node in the DAG in Fig. 10
                        . To generate the final SCR graph for an application, SCM records and analyzes a series of system calls to organize each group, called system call groups (SC_groups), and estimates their hardware power consumption. The memory access information is also considered part of SC_groups, and thus power consumption due to memory access is also included in the system. Fig. 10 shows an example of SC_group DAGs for an application, and a complete flow for the proposed tools is illustrated in the upper region of Fig. 7.

The data volume for each combination of applications and platforms amounts to at least 35 megabytes. About 190 system calls are invoked by Linux which defines them in syscall.h and allows for up to 7 parameters to be stored in its default registers. mPSR can also record a maximum of 20 events such as memory access instructions, process status, kernel timer settings such as CONFIG_HZ, NO_HZ, HIGH_RES_TIMERS, HPET and timer interrupts. To enroll the information for program A on platform P in the repository, we construct a set of tables SC_A_P for the system calls with their most frequently used parameter combinations along with additional information including average power consumption and duration, and thus each table contains a maximum of 2100 items. Each item needs an average of 20 bytes of data storage using string or double type. To produce the SC_A_P tables for program A on P, the servers must construct a repository to pre-process the raw data derived from the execution instances in individual platforms. That is, for the execution of program A on platform P, we construct an execution instance table Exec_A_P to store its invoked events and parameters. In the experiment, each Exec_A_P table reserves 100,000 records and each instance is performed for 20min or until no records are available. Given the parameters required by individual system call events and CPU/memory instructions, each record stores an additional 15 fields, and thus requires about 1,600,000 items (32 Mbytes) for an Exec_A_P table. The inclusion of additional platforms and applications will significantly increase server memory and storage capacity requirements. In accordance with tables Exec_A_P, we can construct tables SC_A_P and use the cloud-based servers to compute the precedence relationship among system calls and their associated frequencies. When the number of target applications and platforms increases, the processing time is determined by server performance. Additional storage space is needed for the SCD table and SCR graphs which require a 210*210 table which requires 880 K bytes for a combination of the OS and platform, and the SC_group graphs require powerful computing to build and maintain their data structures with respect to individual platforms.

In this section, the proposed algorithm for SCD graph is used to improve list scheduling for multi-threaded programs without user input. Each user-inactive application is granulated and regarded as multiple groups of system calls to be scheduled in accordance with the duration, precedence and hardware power consumption of SC_group DAGs. Due to the power awareness of the SCD graph, the proposed list scheduling method is energy-efficient and reduces application completion time.

Firstly, list scheduling on m uniform processors system is introduced in which processors/cores have the same specification but can have different speed settings. List scheduling moves tasks to the front of list L as soon as they become ready. Each task is represented by a worst-case execution time (WCET). Initially, task k is removed from the list, where 1⩽
                        k
                        ⩽
                        m, and scheduled on processor (or cluster) k. Upon the completion of task k, the first unscheduled task in L (i.e., task m
                        +1) is removed and scheduled to processor k which has the earliest available idle time. This process repeats until all tasks in the list are finished. Fig. 11
                         shows an example for list scheduling with identical processors.

In this section, we propose a system-call-grouped task model for the energy-aware multicore scheduling on uniform processors. This model makes the following assumptions: (1) each application/process T is composed of several groups of system calls 
                           
                              
                                 
                                    τ
                                 
                                 
                                    i
                                 
                              
                           
                        , that is, 
                           
                              T
                              =
                              {
                              
                                 
                                    τ
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    τ
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    τ
                                 
                                 
                                    n
                                 
                              
                              }
                           
                        , and (2) the number of execution speeds for each processor is discrete. A modified graph of SC_groups is shown in Fig. 12
                        (a), and each node executed at a speed of f
                        =0.5 denotes a group of system calls to be labeled with WCET. Along with the information from the SCD tables, device power consumption and remaining service time can be predicted easily. To reduce the hardware power consumption for applications, the proposed scheduling approach uses the network algorithm called critical path method (CPM) [21] on the graphs of SC_groups. This determines the shortest time possible to complete the applications, and its schedule can thus be improved using our cluster scheduling algorithm. Fig. 12(b) shows an example of the SCD graph with CPM.

In accordance with SCD graph, the number of invocations, power consumptions and duration of the system calls with memory access and CPU execution are derived and contribute to scheduling decisions. The cores for executing an application are divided into a high-speed and a low-speed cluster. Initially, the high-speed cluster consists of a single core while the low-speed cluster consists of the remaining available cores. Cores in the same cluster operate at an identical speed, with low-speed clusters executing at Ref-speed which is determined by the tangent slope of the relational curve between core frequencies and power. In Fig. 13
                        , for example, the slope of the curve from 0 to 0.4GHz is not decreased until the frequency reaches 0.5GHz while the high-speed cluster is assigned to 1GHz. Fortunately, Ref-speed is determined quickly because most microprocessors are made using CMOS processes, in which the power consumption is at least a quadratic function of the processor speed [21]. The tangent slope of the convex function can be derived by its derivative.

High-speed clusters with high utilization rates will cause a delay in the critical path and thus prolong the schedule span. Our empirical results show that the resulting power consumption exceeds that of the ordinary list schedule when the high-speed cluster utilization rate exceeds 80%, mainly due to the quadratic relation between core speed and power. This would also delay application completion time. Therefore, the proposed cluster algorithm shown in Fig. 14
                         adds an additional core to the high-speed cluster when its current utilization exceeds 80%.

The dispatching algorithm shown in Fig. 15
                         assigns the tasks on the critical path to the high-speed cluster while the others are assigned to the low-speed cluster. In Fig. 16
                        (a), the tasks in Fig. 12(b) are dispatched according to the Task_dispatching algorithm. Processor P1 with a speed 1.0GHz is assigned to the high-speed cluster while P2, P3 and P4, all with speeds of 0.5GHz, belong to the low-speed cluster. Compared to the list scheduling in Fig. 12(a) with identical speed, the proposed algorithms result in a 25% decrease in total execution as compared to the ordinary schedule. In addition, the ordinary list schedule in Fig. 16(a) with a speed of 0.5GHz executes for 39ms and idles for 10ms, consuming 13.7mW. In Fig. 16(b), the proposed schedule executes for a total of 19ms at 0.5GHz and 10ms at 1.0GHz, and idles for 8.5ms, consuming 9.31mW.

@&#EXPERIMENTAL RESULTS@&#

The experiments use an ARM Cortex A9 and DMA-PAD4421 platform with an Exynos 4412 quad-core processor. The profiling hardware uses DAQ NI 9129 produced by National Instrument®, and LabVIEW® Pro 2012 is used for profiling analysis. The repository and tables are manipulated using MySQL® performed on VMware ESXi® virtual machines running on an IBM X3650 machine with two octo-core XEON Processors (E5-2620) with a total of 24GB main memory.

We first analyze popular applications including Facebook®, Google Chrome® and Gmail® on Android based on Linux 2.6.36. The accuracy of the proposed profiling method is compared to that of ANEPROF [14], which provides high-level power distribution and a power breakdown among threads and system functions. During power estimation experiments, total processor utilization does not exceed 60% to reduce the power correlation from other system components or services. When CPU loading is high (e.g., >60%), the context switching and peripheral I/O from other applications introduce a large number of additional system calls or interrupts which may interfere with DAQ profiling and thus decrease CPP accuracy. A detailed study by Shye et al. [22,23] reports that the CPU utilization rate of Android-based phones is usually either below 10% or at 100%, and they spend most of their time at the former case. To understand the CPU utilization for the target smartphone applications on Android, we used the CPP tools and built a small android program using TOP command for a simple test. They were installed in smartphones belonging to 20 undergraduates and reported CPU utilization to our cloud server. Their smartphones were turned on with fully charged batteries without GPS and telephone function and continuously accumulated their log data for maximum 3days. The average CPU utilization obtained for every 20min was about 14.7%, while the maximum CPU utilization was 49.1%. Therefore, in the typical case for smartphone applications, the proposed model can provide good accuracy. We compare the results achieved with the proposed tools against a real measurement profiling with overhead control and time synchronization [14]. On the experimental platform, the length of an active interval is set to at least 20min. Each experiment was started after launching the target applications. As shown in Figs. 17 and 18
                        
                        , both Facebook and Gmail are frequently refreshed by their server connections and the system hardly ever enters dormant mode. The refreshing frequency depends on a variety of application parameters such as browser type, number of friends, friend posting frequency, friend replies, “check-in” frequency, forum usage, games usage, mailbox settings and advertisements. Before the processor enters the idle state, the Linux kernel has to first determine when the processor will enter a dormant mode in accordance with the current scheduling policy [3,20] which considers fairness, CPU/set affinity, interactivity boosting, break-even time, etc. Therefore, frequent connection refreshing may prevent the processor form entering dormant mode or deeper sleep. User interaction with the applications triggers hardware interrupts associated with the items in the data structure irq_desc to IRQ and ISR in the Linux kernel. Therefore, most interactions from users can be captured and analyzed by the proposed tools to produce accurate power consumption predictions. In the experiment, when Google Chrome is activated, it launches a static webpage with hyperlinks which the user then clicks every 60s. Fig. 17 presents the percentage of system power consumption with respect to the system calls invoked by Facebook. System call SendTo( ) is used to send a message on a socket and consumes the greatest amount of power, because Facebook continuously sends messages to its server. Mincore( ) continuously checks processor status to ensure the application has sufficient resources for execution. It indicates whether pages of the calling process’s virtual memory are resident in main memory, and so will not cause a page fault in OS if referenced. Fork( ) is repeatedly invoked by the application to continue the current process and thus avoid service termination.

In Fig. 18, Getrusage( ) consumes the highest power during the execution of Chrome. It is used to confirm current resource use so as to ensure sufficient resources are reserved for computing and storing current webpages.

In Fig. 19
                        , Gmail invokes system calls and power consumption patterns similar to those of Facebook. However, unlike Facebook, Gmail continuously duplicates threads using Fork( ), and reads the created threads using Read( ). Thereafter, it sends messages frequently to the Gmail servers using SendTo( ) to confirm the latest updates on mails.


                        Fig. 20
                         shows the average power consumption varying with system utilization. Actual power profiling is used to compare system power consumption forecasts for applications using the CPP and ANEPROF [14]. Actual power profiling uses DAQ to derive real hardware power consumption, which is then used as a yardstick for the experiments. Actual profiling errors produced by the CPP increase slightly in proportion to system utilization because the overhead for shared resources such as CPU context switches and network interfaces will reduce estimation accuracy. In Fig. 21
                        , the applications Facebook, Gmail and Chrome are individually run 100 times for 20min each time. The figure shows a difference of approximately 1.7% between actual profiling and the CPP method for executing Facebook. This error is primarily attributed to two main reasons. Firstly, the tools are themselves resident programs in the operational stage. Secondly, a time difference (i.e., clock drift) exists between the examined device and examining computers in the initial stage [14]. ANEPROF incorporates the OS profiler, VM profiler and framework profiler which record JVM and system activities and are coordinated by a runtime controller (daemon). Experimental results show that ANERPROF may overestimate actual power consumption for Facebook, Gmail and Chrome by about 3.5%, 5.7% and 7.5%, respectively.

In this section, we investigate completion time and power consumption of target programs under different scheduling methods. To eliminate interference from user-input latency and frequency, the scheduling experiments use the user-inactive command line instructions (CLI) program speedtest-cli [24] to check internet speed on Linux. In the experiment, speedtest-cli is performed without any input argument, and it automatically reports download and upload speeds from the closest Speedtest server. We performed this test 100 times for each simulated setting, and executed each task for a maximum of 10 s. Task information is derived from the proposed repository, and the task precedence relationship is determined according to the SC_group graph. Four cores are used in the simulation with DVFS capability and discrete speed/frequency levels as shown in Fig. 13. We compare the proposed method with the cluster-based scheduling methods proposed in [25] with maximum speed and DVFS, respectively abbreviated as VM-max and VM-DVFS.


                        Fig. 22
                        (a) shows the results in terms of average task completion time. We apply CPM to speed up tasks on the critical path of SCD graphs to significantly decrease application execution times. In addition, to further shorten completion time, CPM uses idle time in available cores, thus reducing leakage overhead. Moreover, the suitable speed of the low-speed cluster is determined according to the slope of the tangent of the power/frequency curve. The proposed method prevents low-speed tasks (i.e., non-critical tasks) from executing at high speed. This also contributes to a decrease in dynamic power consumption. Fig. 22(b) shows the average system power consumption for the underlying methods. The proposed method respectively reduces power consumption for VM-max and VM-DVFS by 22.5% and 13%.

In Fig. 23
                        , the average completion time of the underlying methods varies with the number of tasks per application. When the number of tasks is less than 30, there are tiny differences between the results of our method and VM-max. With an increasing number of tasks, non-critical tasks in VM-max gradually occupy high-speed cores and thus hamper the completion of critical tasks. In Fig. 24
                        , the proposed method saves up to 15% and 23% more energy than VM-DVFS and VM-max, respectively, and saves more power than other methods when the number of tasks increases. It shows that the differences of power consumption between the proposed method and VM-max and VM-DVFS increase when the number of the tasks increases; the proposed method is found to outperform both VM-max and VM-DVFS.

@&#CONCLUSIONS AND FUTURE WORK@&#

We propose power estimation model and power-aware scheduling method for multicore embedded systems which can be deployed in a cloud computing environment. First, we propose a cloud-based power profiling framework and accompanying tools that can precisely forecast power consumption by mobile devices. In particular, the proposed tool monitors and analyzes hardware power consumption associated with CPU instructions, memory access and system calls used by Linux to produce system-call groups and the corresponding DAGs. This notion is implemented in power profiling tools to provide precise predictions for mobile device power consumption when total processor utilization is not high (less than 60%). The proposed graph algorithms and repositories also contribute to a new cluster-based power-aware scheduling which reduces task completion time and power consumption up to 9% and 13% as compared to established methods. Future work will focus on the development of periodic and sporadic real-time scheduling for multicore architectures, seek to create a model to address user-interactive scheduling problems, and propose power-aware scheduling based on machine-learning methods. On the basis of the relationship patterns among hardware power, user behavior and social network settings, we will also study leakage-aware scheduling to save more energy.

@&#ACKNOWLEDGEMENTS@&#

We are indebted to the anonymous reviewers for providing insightful comments and providing useful suggestions throughout this paper. Without the anonymous reviewers supportive work this paper would not have been possible. This work was supported in part by the Ministry of Science and Technology of the Republic of China under Grant MOST 102-2221-E-025-003.

@&#REFERENCES@&#

