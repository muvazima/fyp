@&#MAIN-TITLE@&#Efficient 3D object recognition using foveated point clouds

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Object recognition: foveation speedup 7x compared to the non-foveated approach.


                        
                        
                           
                           True recognitions rates are kept high with false recognitions at 8.3%.


                        
                        
                           
                           Faster setups with 91.6% recognition rate and 14x improvement were also achieved.


                        
                        
                           
                           The slowest configuration still shows almost 3x faster computing times


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Point cloud

3D object recognition

Moving fovea

@&#ABSTRACT@&#


               
                  Graphical abstract
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

With current developments experienced in hardware technologies, computer vision systems would be ideally able to capture 3D data of the world and process this data in order to take advantage of their inherent depth information. However, nowadays, most current computer vision systems are still based on 2D images while the use of 3D data can offer more details about geometric and shape information of captured scenes and consequently, of general objects of interest. In this way, the development of 3D object recognition systems has been an active research topic over the last years [1].

Recent technology advances have enabled the construction of devices, as for example the Microsoft Kinect [2], that capture 3D data from the real world. The Kinect is a consumer grade RGB-D sensor originally developed for entertainment that has enabled several novel works for research including robotics, commercial, and gaming applications. Mobile phone manufactures have also started to shipping smartphones with stereo vision cameras in the recent years. Other manufacturers already announced camera sensors with depth information as a 4th channel. Furthermore, the price reduction of equipment is driving a wide adoption of 3D capture systems.

Although the amount of data provided by 3D point clouds is very attractive for object recognition, it requires intensive computing algorithms that could render systems based on this type of data computationally prohibitive, mainly if real time interaction is needed. Hardware accelerators and optimizations are frequently used for real time computing, however object recognition is still an open research field with several challenging research opportunities, especially when real time performance is desired. One software solution consists in processing point clouds efficiently using algorithms that compute local geometric traits. One example of such system is depicted in Section 4, which enumerates advantages of correspondence grouping algorithms.

We are interested on accelerating object retrieval using 3D perception tools and data acquisition from real images (not synthetic images). For this purpose, we propose the usage of a moving fovea approach to downsample 3D data and reduce the processing of the object retrieval system from point clouds. An example of foveated cloud can be seen in Fig. 1
                     . Experimental results show that our system offers up to seven times faster recognition time computing without compromising recognition performance. We also provide two web based tools to interactively view and manipulate point clouds and to capture Kinect point clouds without the need to install any software, which has been used within the Collage Authoring System.

This article is structured as follows: Section 2 presents the theoretical background used in this work with reviews of some related works on 3D object retrieval and the moving fovea approach. Section 3 describes 3D moving fovea applied to the object recognition problem and its formulation in the context of our work. Section 4 depicts both the system that forms the base of our implementation and also the proposed scheme, along with implementation considerations. Section 5 describes the experiments, including a performance evaluation that can be executed with the Collage authoring environment, while Section 6 closes the article with our final remarks.

Three-dimensional object recognition is a multi-disciplinary area of research, with major contributions originating from the Pattern Recognition, Computer Vision, Robotics and Computer Graphics communities. In this section, relevant contributions from each of these subareas are briefly enumerated, emphasizing the data acquisition method employed in each of them.

Vision is so far the most important sensing resource for robotics tasks that can be executed based on devices like web cameras and depth sensors. Unfortunately, the huge amount of data to be processed is limited by the processing time that is a restriction for doing reactive robotics. Several approaches represent image with non-uniform density using a foveated model that mimics the retina mapping to the visual cortex in order to deal with this amount of data [3–9]. The fovea is the area of retina with greatest visual acuity, so foveated models have high resolution nearby the fovea and decrease the resolution according to the distance from the fovea.

The foveation process is performed either by subsampling in software [3,4], by hardware with reduced sampling [10] or by using a system with 2 or more cameras, where one is used for peripheral vision and another one is used for foveated vision [11,12]. The software foveation allows greater ease of modification and easily implementable in conventional hardware, but is slower than hardware solutions which are usually more expensive and difficult to change. In terms of coverage, solutions that use specific cameras to peripheral and foveated vision are more open to stimuli of the whole environment by using a wide angle peripheral camera, what would require a huge resolution camera in the case of a single camera system due to high resolution fovea needs. However, a camera specific for foveated vision requires movement of physical devices and a large difference between peripheral and fovea cameras suppress stimuli appearing on a intermediate level of resolution because these are not in the fovea camera field of view neither in the peripheral camera. In this work, the foveation is performed by software. It is important to note that most of these models allow free movement of the fovea, what does not happen at the biological eye's retina. Otherwise, all the vision resources should be moved in order to keep the object at foveal region.

In a dynamic and cluttered world, all information needed to perform complex tasks are not completely available and not processed at once. Information gathered from a single eye fixation is not enough to complete these tasks. In this way, in order to efficiently and rapidly acquire visual information, our brain decides not only where we should look but also what is the sequence of fixations [13]. This sequence of fixations, and therefore the way the fovea is guided, is related to cognition mechanisms controlled by our visual attention mechanism. Several works propose saliency maps from which fixations can be extracted [14].

It is also known that the human vision system has two major visual attention behaviors or dichotomies [15]. In the top-down attention approach, the task in hand guides attention processing. On the other hand, in bottom-up attention, external stimuli drive attention. Text reading is an example of the top-down behavior of attention, where visual fixations are done systematically, passing through the paper in a character by character and line by line movement. On the opposite, if a ball is thrown toward the same reader, this bottom-up stimulus will make the reader to switch attention to the dangerous situation.

Besides in robotic vision, several foveated systems are proposed in order to reduce the amount of data to be coded/decoded also in real-time video transmission [8,6]. In this kind of application, an image should be encoded with foveation thus keeping higher resolution in regions of interests. In a similar way, Basu [16] proposes a foveated system to 3D visualization with limited bandwidth restriction, where the fovea position controls the objects’ texture quality and resolution.

Early object recognition systems acquired data from expensive and rarely available range sensors, such as laser scanners [17,18] and structured light patterns [19]. Ashbrook et al. [17] describe an object recognition system that relies on similarities between geometric histograms extracted from the 3D data and the Hough Transform [20]. Johnson and Hebert popularized the Spin Images descriptor [18,19], which was used as the basis to an object recognition algorithm that groups correspondences of Spin Images extracted in a given query model and those extracted in the scene data that share a similar rigid transformation between the model and the scene [18]. Data from 3D scanners and also from synthetic CAD 3D models are employed in the work of Mian et al. [21].

Until recently, 3D object recognition systems processed data mostly in an off-line fashion, due to long computing times involved [22]. This paradigm has started to shift as algorithms have been proposed in the Robotics community [23,24] to enable real-time manipulation and grasping for robotic manipulators. In fact, algorithms designed to describe 3D surfaces through histograms of various local geometric traits evaluated on point clouds became a major trend in the last years [25–27,23]. Consequently, faster and more accurate 3D object recognition systems based on keypoint matching and descriptors extracted in the scene and in the sought object point clouds were developed. After being established, point correspondences are grouped by hypotheses sharing a common transformation, which is estimated by voting [28,29], multi-dimensional clustering [30,31] or RANSAC [32] (also used to detect shapes on 3D data [33]). The presence of the object of interest is then inferred if certain conditions are met, such is the number of votes, cluster size, or the number of RANSAC inliers.

With the wider availability of consumer-grade depth sensors such as the Microsoft Kinect, several works on 3D object recognition are proposed employing this class of sensor [34–38,24]. Aldoma et al. [24] proposed the global feature coined Clustered Viewpoint Feature Histogram (CVFH) to improve performance of object recognition for robotics. Machine learning based approaches [37,38] were formulated to perform 3D object recognition making heavy use of depth information, without any computation on point clouds involved.

Aldoma et al. [34] highlight how algorithms that are part of the Point Clouds Library (PCL) software package could be used to form 3D object recognition systems based on local and global features. There are also 3D object classification/categorization systems, as in the works of Wohlkinger et al. [35,36] and of Lai et al. [37]. In this latter class of systems, every chair in a scene should be labeled as the object of type “chair”, whereas in object recognition only the specific chair being sought should be retrieved from the scene.

This work proposes the use of a foveated point cloud in order to reduce the processing time of object detection. The idea is that the point density is higher nearby the fovea and that this density decreases according to the distance from the fovea. In this way, it is possible to reduce the total number of the points reducing also the processing time at the same time that the density around the fovea is enough to keep feasible the object detection. Parts of the point cloud with reduced density may be useful in providing other stimuli which may be part of a context of visual attention. For example, a saliency map can be computed in the foveated cloud in order to drive bottom-up or top-down stimulus. This can be very useful in the context of robotic vision, since the robot can be aware to multiple simultaneous stimuli in the environment.

The foveated point cloud proposed here is based on the 2D foveated model proposed by Gomes [4]. This model transforms an image into a set of smaller images with same size but with different resolutions. In order to achieve that, the model defines image patches from the original image that are arranged in a sequence of levels. The first level is a mapping of the whole original image while the last one is a mapping of a patch placed at the original image centered at a fovea. This patch has the same size of each image level. The result is a set of small images that composes a foveated image.

In the 3D case, instead of resampling concentric image patches, the foveated point cloud is achieved by downsampling the original point cloud using concentric boxes, each one representing a level as shown in Fig. 2
                        . Each box specifies a point cloud crop each one with a different point cloud density. The outer box has one of its corners placed at a specific 3D coordinate and it defines the model coordinate system. See the axes in Fig. 2. All points outside this box are discarded. Inside it, smaller boxes are linearly placed. The smallest box is centered at a parameter called fovea: a 3D coordinate where the point cloud density is maximum. A downsampling schema is applied in this smallest box. Each bigger box is also downsampled but with a level by level decreasing point cloud density up to the outer box, where the point cloud density is minimum.

The proposed foveated point cloud is formalized as follows. We define m+1 3D boxes of size 
                           
                              
                                 S
                              
                              
                                 k
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 3
                              
                           
                        , with 
                           k
                           =
                           0
                           ,
                           1
                           ,
                           …
                           ,
                           m
                         representing each level. Each level of the foveated point cloud model changes the point cloud density. The first level (level 0) has a density reduction by d
                        0 and the last one (level m) has a density reduction by d
                        
                           m
                        . The density reduction of intermediate levels is given by linear interpolation between d
                        0 and d
                        
                           m
                        .

The largest box has three parameters: size (S
                        0), orientation and position (denoted by 
                           Δ
                        ). Usually, if the whole point cloud should be covered, it is possible to automatically set these three parameters as the bounding box of the entire scene. However in some applications, it could be interesting to place it in a part of a huge point cloud. The last two parameters determine the model coordinate system.

The smallest box is guided by a fovea 
                           F
                         at that box center. For formalization convenience, the fovea coordinate system origin is (0, 0, 0) at the largest box center. In this way, 
                           F
                           =
                           F
                           ′
                           −
                           
                              
                                 S
                              
                              
                                 0
                              
                           
                           /
                           2
                        , where 
                           F
                           ′
                         is the fovea at model coordinate system.

Let 
                           
                              
                                 δ
                              
                              
                                 k
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 3
                              
                           
                         be the displacement of box at level k, then 
                           
                              
                                 δ
                              
                              
                                 0
                              
                           
                           =
                           (
                           0
                           ,
                           0
                           ,
                           0
                           )
                         and 
                           
                              
                                 δ
                              
                              
                                 m
                              
                           
                           +
                           
                              
                                 S
                              
                              
                                 m
                              
                           
                           /
                           2
                           =
                           F
                           ′
                        .

The displacement of each box using linear interpolation is given by
                           
                              (1)
                              
                                 
                                    
                                       δ
                                    
                                    
                                       k
                                    
                                 
                                 =
                                 
                                    
                                       k
                                       (
                                       
                                          
                                             S
                                          
                                          
                                             0
                                          
                                       
                                       −
                                       
                                          
                                             S
                                          
                                          
                                             m
                                          
                                       
                                       +
                                       2
                                       F
                                       )
                                    
                                    
                                       2
                                       m
                                    
                                 
                              
                           
                        Note that 
                           
                              
                                 δ
                              
                              
                                 k
                              
                           
                         is defined only for 
                           m
                           >
                           0
                        ; in other words, the foveated model should have at least 2 levels.

The size of each k-th box using linear interpolation is given by
                           
                              (2)
                              
                                 
                                    
                                       S
                                    
                                    
                                       k
                                    
                                 
                                 =
                                 
                                    
                                       k
                                       
                                          
                                             S
                                          
                                          
                                             m
                                          
                                       
                                       −
                                       k
                                       
                                          
                                             S
                                          
                                          
                                             0
                                          
                                       
                                       +
                                       m
                                       
                                          
                                             S
                                          
                                          
                                             0
                                          
                                       
                                    
                                    
                                       m
                                    
                                 
                              
                           
                        
                     

Here, we introduce a fovea growth factor 
                           G
                           =
                           (
                           
                              
                                 s
                              
                              
                                 x
                              
                           
                           ,
                           
                              
                                 s
                              
                              
                                 y
                              
                           
                           ,
                           
                              
                                 s
                              
                              
                                 z
                              
                           
                           )
                           ∈
                           
                              
                                 R
                              
                              
                                 3
                              
                           
                        , where 
                           
                              
                                 s
                              
                              
                                 x
                              
                           
                           ,
                           
                              
                                 s
                              
                              
                                 y
                              
                           
                           ,
                           
                              
                                 s
                              
                              
                                 z
                              
                           
                         are the scale factors applied to directions x, y and z, respectively (see Fig. 3
                        ). As detailed in Section 4, this factor increases the number of points by enlarging levels volumes. Observe that this model behaves like there is no foveation when 
                           G
                         goes to 
                           ∞
                        .

In this way, each level is bounded by the lower limit of maximum between 
                           
                              
                                 δ
                              
                              
                                 k
                              
                           
                           −
                           G
                         and (0, 0, 0) and the upper limit of the minimum between 
                           
                              
                                 δ
                              
                              
                                 k
                              
                           
                           +
                           
                              
                                 S
                              
                              
                                 k
                              
                           
                           +
                           G
                         and S
                        0. These minimum and maximum limit the levels to the original point cloud boundary.

After foveated levels boundaries computation, the point cloud is downsampled in order to change the point cloud density. In this step, there are two possibilities of point cloud storage: to create a single point cloud joining all the downsampled points from each level or to store each downsampled point cloud from each level independently. Note that both ways can be adopted simultaneously.

However, by joining all points in a single cloud leads to geometric distortions, probably imperceptible on a visual inspection, if the downsampling algorithm modifies the points coordinates. A possible solution to this issue is to join all points from a level that do not belong to an inner level. This way points from a level do not mix with points from another one. In order to ensure the disjunction between levels, it is enough to test if the point from a level k to be inserted in the foveated point cloud is not inside the box k+1 (
                           k
                           ≠
                           m
                        ) as depicted in Algorithm 1. Example of a foveated cloud point can be seen in Fig. 4
                        
                        
                           Algorithm 1
                           Processing steps applied to foveate a point cloud.
                                 
                                    
                                       
                                       
                                          
                                             
                                                
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           

As explained before, one of the parameters of the foveated point cloud is the fovea position vector. A dense point cloud is more suitable to successful correspondence matching. If the object is far from the fovea then fewer keypoints are extracted and less correspondences are found. Thus, it is desirable to keep the fovea near the object. In order to achieve better results, the proposed architecture includes a visual attention module that guides the fovea placement.

First, a function evaluates if the object is detected by the system. If the object is detected, then the fovea is moved to the object's centroid. Otherwise, if the object is not detected, then some strategy may be applied in order to recover the fovea position. A sequence of fixations can also be used along the time or at once in order to detect where the object is. Once the object is detected, a tracking module can be applied so that the fovea is always in the desirable place.

One straightforward strategy is to disable foveation until the object is found. This temporarily increases the processing time, but the original point cloud is used and, then, the object can be found at foveated peripheral areas. Another strategy is to gradually increase the growth fovea factor. By using this strategy, it is possible to gradually increase the number of cloud points and thus avoiding having a processing time peak. Another possible strategy is to use a bottom-up attention strategy. In this case, the fovea is moved to the most salient region, which can be computed considering the class of objects to be found.

If the scene has more than one object, then it is possible to foveate each object at a time and process them in sequence. In other words, if two objects, for example, ask for top-down attention, then the visual process pay attention to one in a frame and to the another one in the next frame.

As some of these issues are not the main contribution of the current work, we neglect it to be treated in a future work. We just wanted to remark that it is possible to apply several strategies based on visual attention in order to properly place the fovea.

In this section, we discuss the core framework that our system is based, the correspondence grouping algorithm [29]. After showing the standard method, the foveated scheme to recognize objects is presented, along with the modifications and implications that were needed to maximize performance using multiresolution data.

The proposed object recognition scheme works with point clouds (set of 3D points referenced in a fixed frame) representing the object model and the scene to be processed. Positions in this reference frame supposedly having an instance of the sought object are given as outputs. We note here that our system recognizes objects in a scene if and only if the model of the query object is available, implying that it does not perform object classification/categorization or retrieve similar objects from a previously computed database (as is the case of some systems enumerated on the work of Tangelder and Veltkamp [22]). Put differently, the query object is searched in the scene and not vice versa. As a consequence, this allows the system to find multiple instances of the same object in a single scene.

We have chosen to build our system based on the local 3D features framework, which exploits local geometric traits at key positions in point clouds in order to extract discriminative descriptors employed to establish point correspondences between keypoints from the model and from the scene. These point correspondences are further processed to infer possible presences of the object. Moreover, this class of system presents some desirable and important properties, such as robustness to occlusion and scene clutter, dispensing the need to elaborate extensive and cumbersome training stages (mandatory for machine learning approaches) and ability to process point clouds acquired from RGB-D sensors like the Kinect in an efficient manner.

The system is based on the correspondence grouping approach of Tombari and Di Stefano [29] (with implementation publicly available [39]), in which a model object is recognized in the scene if, after keypoint correspondences being established, enough evidence for its presence in a given position is gathered. This scheme is shown in Fig. 5
                        a. For the sake of completeness, every step of the system is described as follows.

The first step in the correspondence grouping algorithm is to describe both the scene and model point clouds. For this, the normal vector for each point is computed considering a surface generated by a neighborhood of size k
                           
                              n
                            around each point. Then, a uniform downsampling algorithm is applied to extract keypoints as the centroid of all points contained within a radius r
                           
                              k
                           . After this, SHOT (Signature of Histograms of OrienTations) descriptors [27] are computed, assembling a histogram of the normals within a neighborhood of radius r
                           
                              s
                            as the signature of a keypoint. The last step to fully describe point clouds is a very important stage encompassing the estimation of a Local Reference Frame (LRF) for keypoints of the model and scene. Thus, the principal axes spanning a LRF within a neighborhood of radius r
                           
                              l
                            in each keypoint position are estimated robustly by the algorithm of Petrelli and Di Stefano [40]. The result of this computation (three unit vectors for each principal direction) is associated with each keypoint and will be employed in the final stage of the object recognition scheme. Different values for the parameters are set in the scene and in the model, allowing more precise recognition tasks. For clarity, the process which extracts descriptors for the model and scene is shown in Algorithms 2 and 3 respectively, with parameters 
                              
                                 
                                    k
                                 
                                 
                                    nm
                                 
                              
                              ,
                              
                                 
                                    r
                                 
                                 
                                    km
                                 
                              
                              ,
                              
                                 
                                    r
                                 
                                 
                                    sm
                                 
                              
                              ,
                              
                                 
                                    r
                                 
                                 
                                    lm
                                 
                              
                            used for the model and 
                              
                                 
                                    k
                                 
                                 
                                    ns
                                 
                              
                              ,
                              
                                 
                                    r
                                 
                                 
                                    ks
                                 
                              
                              ,
                              
                                 
                                    r
                                 
                                 
                                    ss
                                 
                              
                              ,
                              
                                 
                                    r
                                 
                                 
                                    ls
                                 
                              
                            used for the scene.
                              Algorithm 2
                              Processing steps applied to the model point cloud. See text for parameter details.
                                    
                                       
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

Processing steps applied to the scene point cloud. See text for parameter details.
                                    
                                       
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

For each keypoint and respective descriptor and LRF in the model a match in the scene is searched by finding the closest scene point (in the Euclidean sense) in the n-dimensional space containing the SHOT descriptors. Search procedures are employed in a kd-tree to handle the cumbersome routine involved. If the squared distance between the SHOT descriptors is smaller than a threshold 
                              
                                 
                                    d
                                 
                                 
                                    max
                                 
                                 
                                    2
                                 
                              
                           , a point correspondence is established. This process is highlighted in Algorithm 4.
                              Algorithm 4
                              Keypoint matching. See text for parameter details.
                                    
                                       
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

Since each model and scene keypoint have a LRF associated, a full rigid body transformation modeled by a rotation and a translation can be estimated between the LRFs associated with each keypoint correspondence. Accordingly, a 3D Hough Space is used to gather evidence of the object presence through a voting process. After the rigid body transformation is applied, the cell of the Hough Space containing this 3D position is calculated and its accumulator incremented. Finally, after repeating these steps for all correspondences, object instances are deemed found at each cell having the number of votes larger than a predefined threshold V
                           
                              h
                           . The size of each cell is controlled by a parameter L
                           
                              h
                           . Algorithm 5 illustrates the object recognition scheme through correspondence grouping.
                              Algorithm 5
                              Object recognition based on correspondence grouping. See text for parameter details.
                                    
                                       
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

We note that the normal vectors are evaluated considering neighborhoods formed by all points in the clouds, whereas the SHOT descriptors and LRFs are computed at the neighborhoods of the keypoints. In this way, computation time is saved, while the local 3D geometric traits are still kept discriminative.

To enhance the 3D object recognition capabilities of the correspondence grouping approach, the cloud foveation algorithm is employed after some adaptations. A complete scheme of the proposed 3D object recognition system is shown in Fig. 5b.

A foveated model is applied to the cloud acquired from the depth sensor according to Algorithm 1 and the parameters of Table 2. Normal estimation can be done before or after foveation. In the first case, the computation is more expensive, but the captured geometric traits of the scene are less distorted. In the current version of the system, we opted to conserve scene geometry.

Since the keypoint extraction (uniform downsampling) would extract keypoints with a single radius (originally r
                        
                           ks
                        ), the multiresolution of the scene cloud would not be respected, as shows Fig. 6
                        b. Consequently, we adapted the keypoint extraction to be also dependent on different and specified levels of resolution, possibly differing of the used downsampling radii 
                           
                              
                                 d
                              
                              
                                 0
                              
                           
                           …
                           
                              
                                 d
                              
                              
                                 m
                              
                           
                        . The correspondence grouping algorithm was then modified to accommodate keypoint extraction with foveated point clouds. The scene points are downsampled using a different radius r
                        
                           k
                         for each level 
                           k
                           =
                           0
                           ,
                           …
                           ,
                           m
                        . The first level (level 0) uses a radius of r
                        0 and the last (level m) uses a radius of r
                        
                           m
                        . All other radii from the intermediate levels are linearly interpolated. Thus, keypoints can be extracted respecting each level of the foveated scene resolution, as shows Fig. 6c.

There are two major consequences about this approach. First, there is a considerable time saving due to the keypoint reduction both in descriptors computation and correspondence step, since different values for the extraction radius are employed instead of a (possibly small) single value. Second, it is possible to greatly increase the keypoint density near the fovea position without significantly increasing the total number of original scene points. This peripheral decrease and foveal increase in the number of points also reduces the number of false descriptors correspondences, improving thus the object detection success rate if the fovea is properly placed.

In the foveated version of the recognition scheme, Algorithm 3 (scene processing) would be modified to include the scene foveation after the normal estimation and to extract keypoints using a radius value r
                        
                           k
                         for each resolution level instead of the r
                        
                           ks
                         (see Fig. 5b).

@&#EXPERIMENTS AND RESULTS@&#

@&#IMPLEMENTATION DETAILS@&#

The proposed object recognition system is implemented in C++ on an Ubuntu Linux environment, making use of the functionalities provided by the Point Cloud Library (PCL) [34]. The experiments are evaluated on a non-optimized version of the system running at a laptop PC with an Intel Core i5 2.5Ghz processor and 4GB of RAM.

The availability of ground truth data allows a more in depth evaluation of the proposed object recognition method by directly comparing the object data with the algorithm output. Hence, through the use of a model object that can be described analytically and also be accurately and easily identifiable on a given scene, we can proceed with a more thorough analysis regarding the number of instances of the query object found and its retrieved position in the scene. To cope with this, an object model in the form of a sphere is utilized as ground truth after some computation steps are executed in each scene to collect its actual position in the global reference frame and also its radius. For this, a RANSAC [41] procedure is applied aiming to fit a 3D sphere model in each scene point cloud. Accordingly, this simple yet efficient procedure is able to correctly identify the sphere in scenes with variable levels of clutter and occlusion accurately enough to suffice our needs. After carefully placing the sphere in the scene, we manually move a Kinect sensor, capturing a total of 12 frames of scenes with increasingly level of clutter from different views, at the same time that the parameters of the detected sphere are gathered and annotated in a ground truth file. Fig. 7
                         shows 4 samples of the acquired scenes and their respective ground truth.

In this experiment, the correspondence grouping 3D object recognition method is compared with our proposed moving fovea object recognition. As can be seen in Section 4.1, the standard correspondence grouping algorithm has several parameters which should be properly tuned according to the operation scenario for a successful recognition procedure. This can be explained because the scene/object dimensions should be taken into account and also because there are not (yet) any methods allowing automatically parameterization. Table 1
                         shows all parameters involved in the process and their respective default values. These used default values are determined empirically after executing the algorithm for various scenes gathered with a similar sensor (3D Kinect), setup (objects on a table top) and distances between the sensor and the scene.

The foveated recognition scheme has parameters to be set, specifying the region in 3D space and the desired resolution levels. The moving fovea parameters are shown in Table 2
                        .

We aim to show experimentally that the foveated approach decreases the total running time per frame of the correspondence grouping process while keeping high successful recognition rates. For this, various foveated and non-foveated setups are instantiated by varying the different parameters involved. These different configurations are divided into 4 groups. The first group uses three different sizes for the highest resolution level 
                           
                              
                                 S
                              
                              
                                 m
                              
                           
                         keeping constant values for the downsampling radii, 
                           
                              
                                 r
                              
                              
                                 0
                              
                           
                           =
                           0.01
                         and 
                           
                              
                                 r
                              
                              
                                 m
                              
                           
                           =
                           0.5
                        . All three setups are shown in Table 3
                        . The second group (Table 4
                        ) employs three possibilities for r
                        
                           m
                         with 
                           
                              
                                 S
                              
                              
                                 m
                              
                           
                           =
                           (
                           0.1
                           ,
                           0.1
                           ,
                           0.1
                           )
                         and 
                           
                              
                                 r
                              
                              
                                 0
                              
                           
                           =
                           0.01
                        . Three different configurations varying r
                        0, with 
                           
                              
                                 S
                              
                              
                                 m
                              
                           
                           =
                           (
                           0.25
                           ,
                           0.25
                           ,
                           0.25
                           )
                         and 
                           
                              
                                 r
                              
                              
                                 m
                              
                           
                           =
                           0.3
                         were used for the third group of fovea settings, as shows Table 5
                        . All these foveated configurations are built with three resolution levels (m=2). The last group is composed by five non-foveated setups. We decide to keep all the parameters of the standard correspondence grouping constant, employing different values only for the radius to choose the scene keypoints, r
                        
                           ks
                        . Table 6
                         summarizes the last group.

Since no object recognition methods are free of resulting in false detections (false positives), some criterion to classify a detection as successful (true positive) or not has to be established. To comply with this, the Euclidean distance between the ground truth sphere position and the detected sphere position is measured. The sphere is stated as detected if this distance is below a given threshold. It is important to note that the used correspondence grouping algorithm (standard and foveated) is able to detect multiple instances of the same object in the scene. Thus, if multiple detections are triggered within a given radius around the ground truth position, only one is counted. Both distance thresholds were set with a value of 8cm. The object detection configurations are run in all 12 scenes with the number of true positives and false positives per configuration being kept. There is only one instance of the sought object in each scene, summing up a total of 12 true positives.

The results for all the 14 configurations in the 12 scenes are shown in Table 7
                        . For the first group (having varying fovea size 
                           
                              
                                 S
                              
                              
                                 m
                              
                           
                        ), only the configurations with ID 1 and 2 were able to successfully detect the object. The small fovea size (10cm in each box dimension) and large r
                        
                           m
                         (50cm) used in the configuration 0 severely degrades the scene resolution to a point that becomes impossible to extract relevant local features and descriptors. Consequently, not enough point correspondences could be stabilized to detect the object.

Though we use the smallest fovea size of the first group in all configurations of the second group, smaller values for r
                        
                           m
                         are set, enabling correct detections, except for the configuration 5. A relatively large (14cm) for r
                        
                           m
                         explains this fact. The setup number 3 detected the sphere in all 12 scenes and the setup 4 in only 3 of them. This is expected because a smaller r
                        
                           m
                         value decreases the uniform sampling radius from intermediate levels (interpolated between r
                        0 and r
                        
                           m
                        ). Also, it increases the number of keypoints throughout the levels and thus increases the number of successful detections.

In the third group, only configuration number 6 triggered the detection of the sought object in the scenes, though in not all of them (11 out of 12). A larger r
                        0 was set, resulting in overly downsampled point clouds and thus, no detections for setups 7 and 8.

Only the non-foveated configurations 9 and 10 detect the object. This latter setup detect the sphere in all scenes without any false detections, and hence can be elected the gold standard against which we can compare our algorithm. Curiously, the configuration that results in the largest number of false detections is the non-foveated with the highest resolution (configuration 9) and the foveated with the largest fovea size (configuration 2). In some sense, we could state that the overall downsampling process applied by the fovea plays an important role possibly removing redundancy and undesired features from the scene point cloud. The largest recognition rate achieved by foveated setups is also 12 true detections, but with one false positive (setups 1 and 3) and two false positives (setup 2).

Although the configuration number 10 has had the best recognition performance in terms of detection ratio, it is the second slowest of all setups, with an average of 7.201s per scene. A successful recognition in the foveated scenario is achieved with an average of 0.364s per frame by the setup 4, but with only 0.33% of success ratio. The fastest average processing time achieved (0.507s) with recognition rates higher than 50% is computed by the setup 6, with 91.6% of true positive rate and only 8.3% of false positive rate. This setup is also the fastest showing the smallest false positive rate with at least 50% of success. The setup showing the fastest computing times with true positive rate at 100% is the setup 1, with an average of 1.058s per frame and false positive rate of 8.3%. Of all foveated configurations, the worst average belongs to the setup 2. In terms of gain in computation performance, the fastest setup shows an increase of 
                           19.78
                           x
                        , while the most accurate offers 
                           6.8
                           x
                         and the slowest is still 
                           2.75
                           x
                         faster than the better standard correspondence grouping configuration.

Obviously, the slowest average computation time of all configurations is computed with the non-foveated which has used the smallest radius to extract the scene keypoints. Also, we can see that as the scene resolution degrades, what is dictated mainly by the two interpolating radii rather than the fovea size, the computation times and recognition rate decrease.

These results emphasize that using the moving fovea, one can opt to dramatically decrease recognition times providing a small increase in false detections (if acceptable) or to decrease recognition times keeping false detections to a minimum.

Due to the criterion used in stabilizing an object detection as true positive and false positive, we show in Figs. 8 and 9
                        
                         how sensitive to the detection threshold (distance between the detected object and the ground truth) are the best four foveated setups (IDs 1, 2, 3 and 6) and the best non-foveated one (10). As it can be seen in Fig. 8, the best foveated setup in terms of overall confiability is also the most capable to detect the object closer to the ground truth position. In fact, it is even less sensitive to the threshold than the gold standard configuration, being able to recognize the object in all 12 scenes using as threshold 2.5cm, while the best non-foveated only recognized the same number with the threshold set as 3.8cm. In Fig. 9, we analyze the threshold employed to classify a detection as false positive also for the top five configurations. We note that using a low value (e.g. 1cm), all object detection setups find multiple instances of the same object around its true position. The non-foveated setup required the smallest threshold (
                           4
                           
                           cm
                        ) to avoid false detections while the setup number three was the foveated configuration which required the smallest value. The most reliable foveated setup stopped triggering false detections only after 13.5cm.

We also analyze how the fovea placement in the scene would influence the overall object detection performance. For this, we have chosen 4 samples scenes with varying amount of clutter and run the foveated correspondence grouping algorithm with varying distances between the ground truth position and the fovea position along one axis. Distances in the range of 0–35cm were used. The fixed foveated parameters were m=2, 
                           
                              
                                 S
                              
                              
                                 m
                              
                           
                           =
                           (
                           0.3
                           ,
                           0.3
                           ,
                           0.3
                           )
                        , 
                           
                              
                                 S
                              
                              
                                 0
                              
                           
                           =
                           (
                           1.0
                           ,
                           1.0
                           ,
                           1.0
                           )
                        , 
                           
                              
                                 r
                              
                              
                                 0
                              
                           
                           =
                           0.01
                         and 
                           
                              
                                 r
                              
                              
                                 m
                              
                           
                           =
                           0.06
                        . The results of the experiments can be seen in Fig. 10
                        . For each scene, the figure shows if detection of the sphere succeeded or not, according to the distance being discussed. Clearly, as the fovea moves far from the (true) object center, less correct correspondences are found, resulting also in less true object detections. Nevertheless, with all other parameters kept constant, the moving fovea method correctly recognizes the object if it is positioned within a distance of at most 18cm.

Experiments available to the user are built using the Collage Authoring Environment for Executable Papers from Elsevier, allowing a complete and interactive experience of the proposed 3D object recognition method directly from the web. Functionalities such as data acquisition, data visualization through an interactive 3D point cloud visualizer, modification of algorithm parameters, execution of the object recognition system and code editing are readily available. If desired, the source codes can be downloaded to be used with other systems.

We verify that users frequently find difficulties while trying to execute their first tests with 3D models, specially related to capture and visualization. In light of this, besides the executable system that is available from Collage, we also offered two minor contributions to this authoring environment to help users to start experiment with 3D object capture and manipulation quickly and easily: a data acquisition module and a tool to manipulate and visualize 3D point clouds.

We have developed a data acquisition module that allows visualization and capture of 3D point clouds using the Microsoft Kinect Sensor by simply using a web browser, so users do not need any specific software installed on their machine (with the exception of the Kinect sensor device drivers). This system can be easily used to generate 3D models of the scenes and objects of interest for further processing.

In order to be able to capture data from the Kinect Sensor, we developed a web based Kinect capture system, which relies on the Java Web Start (JWS) technology to allow anyone to use it with a single click on a web page. We used the OpenNI library [42] and Java Native Interface (JNI) abstractions to be able to connect to the Kinect sensor plugged on the user's computer.

The system generates 3D point clouds in the PCD (point cloud data) file format, a portable, simple and open format for 3D data introduced by the Point Cloud Library (PCL) [34]. It consists of a standard ASCII text file containing a 3D point information per each line in a format separated by spaces (X Y Z RGB), so any point and its color can be represented by a single line of ASCII numbers.

Another contribution is the PCD point cloud opener for Collage. We have adapted and worked with the Collage team to deploy a PCL web viewer (kindly granted to use by the PCL development team) on Collage, so any Collage Experiment can open and manipulate point clouds with a simple and intuitive interface. The PCL viewer uses the WebGL system, a recent standard for 3D object rendering directly on web pages. WebGL is supported on most modern browsers, thus, in order to view and interact with the point clouds, a browser with WegGL support must be used.

The object recognition experiment available (Fig. 11
                        ) to the user in the Collage Environment is now explained in detail.

In order to provide his/her own input point clouds to our experiment, users are able to use the data acquisition module described in Section 5.6.1. Observe that this module is completely independent from the Collage Authoring Environment.

Our Collage Experiment contains two experiments: Model Segmentation Experiment and Object Recognition Experiment. The first one is responsible for taking one point cloud representing a scene composed by only one object lying on a table. The Model Segmentation Experiment then generates the Model Point Cloud, containing only the point cloud representing the object. The Object Recognition Experiment takes as input the Model Point Cloud generated by the previous experiment and a point cloud of a scene containing this object, among others. Then, it executes the Correspondence Grouping and the Foveated Point Cloud approaches. The user is able to see the visual result of this execution and also text files with details about it.

The workflow presented in the Model Segmentation Experiment will be followed, in order to capture and segment the object model (according to Sections 5.6.1 and 5.8.1 respectively). A readme1.txt text file with general instructions on how to use the system is available as Experiment Data Item 1, while a readme2.txt (Experiment Data Item 2) explains how to proper acquire and segment the object model. The scene captured containing only the object of interest upon a table is then shown in the Experiment Data Item 3, an interactive point cloud visualizer (see Section 5.7). Experiment Data Item 4 shows the source code files. The Experiment Code Item 1 contains Bash commands used to compile and execute the source code. After performing the actual segmentation process, the resulting point cloud containing the model data is shown in another interactive window at the Experiment Data Item 5, with textual output from the program shown at the Experiment Data Item 6. If desired, the source code (Experiment Data Item 4) of the segmentation procedure can be edited (or downloaded). In the case that the user chooses not to capture data, a previously specified PCD file penguin.pcd is used as default value for the Experiment Data Item 3. Alternatively, a PCD file assumed to be already stored in the user's file system can also be supplied by modifying the Experiment Data Item 4. We note here that this PCD file should contain the object point cloud already segmented.

After the model acquisition and segmentation, the actual Object Recognition Experiment can be executed. A text file readme3.txt (Experiment Data Item 7) instructs the user on how to proceed, while the source code can be visualized, edited or downloaded by manipulation of the Experiment Data Item 9. The user can supply a scene containing the object to be recognized or use the default scene.pcd through the Experiment Data Item 8. The experiment executes the correspondence grouping algorithms in its default version (Section 4.1) and foveated version (Section 4.2), resulting in two outputs presented for each execution: a point cloud visualization of the recognition result, highlighting the recognized instances in the scene and also the object of interest and a textual output, printing important information of the execution such as correspondences founds and execution time. These two outputs can be inspected in the Experiment Data Item 10 and Experiment Data Item 11 (default visualization and text output) and Experiment Data Item 12 and Experiment Data Item 13 (foveated visualization and text output).

In the case that the user opts to supply the object point cloud via the Kinect, an object segmentation procedure has to be applied to the data, aiming to extract only the points belonging to the object of interest. To this end, after positioning the model object over a plain surface (e.g. a table) without any other object on the field of view of the sensor, the point cloud is processed by:
                              
                                 1.
                                 removing points that are at a distance z
                                    
                                       max
                                     from the sensor;

removing remaining points classified as contained on a plane until a given fraction F
                                    
                                       points
                                     of all points is reached through RANSAC [41] plane segmentation;

clustering remaining points by Euclidean Distance, implying that starting from a single point, points with distance smaller than a threshold d
                                    
                                       cluster
                                     are grouped together.

This simple procedure was verified experimentally to work well for several objects, as can be seen in Fig. 12
                           a and b.

User collaboration in placing the object as indicated is necessary to reduce the number of points to be processed and also to avoid points from the object model being incorrectly discarded. Some shortcomings such as incorrect point clouds as results can easily be solved with proper parameter tuning.

@&#CONCLUSIONS@&#

We have presented in this article the usage of the moving fovea approach to efficiently execute 3D object recognition tasks. We note that although this paper explores the usage of the foveated approach to a specific object recognition algorithm, the proposed technique is well suitable to be used with any other 3D object recognition or retrieval system.

One key feature of the proposed approach is the usage of a multiresolution scheme, which uses several levels of resolution ranging from the highest resolution, possibly equal to the resolution of the original scene, to lower resolutions. Lower resolutions are obtained by reducing the point cloud density according to the fovea level, which consequently reduces the processing time. This setup is similar to the human vision system, which focus attention and processing on objects of interests at the same time that keeps attention and processing to peripheral objects, but with lower resolution.

Experimental analysis shows that the proposed method dramatically decreases the processing time used to recognize 3D objects on scenes with considerable level of clutter, while keeping accuracy loss to a minimum. In comparison to a state-of-the-art recognition method, a true positive recognition rate of 91.6% was achieved with an improvement of seven fold performance gain in terms of average recognition time per frame. These results are well suitable for usage on mobile and embedded systems with low computational resources or on applications that need faster object recognition processing time, such as robotics.

As a future work, we plan to explore the usage of the foveated multiresolution system to best find the fovea position according to possible objects identified on lower scales. As the object is found on the lower scale, then the fovea can focus and process detailed information of the object at the best fovea position.

@&#ACKNOWLEDGEMENTS@&#

The authors would like to thank the support from the National Research Council (CNPq), Brazilian sponsoring Agency for research and also the PCL development team that has allowed the use of PCL web viewer tool.

Supplementary data associated with this article can be found in the online version at 10.1016/j.cag.2013.03.005.

Note from publisher: this material was originally submitted as part of the Collage Executable Paper pilot, please visit http://www.elsevier.com/executablepaper for more information.


                     
                        
                           
                              Note from publisher: this material was originally submitted as part of the Collage Executable Paper pilot, please visit http://www.elsevier.com/executablepaper for more information.
                           
                           
                        
                     
                  

@&#REFERENCES@&#

