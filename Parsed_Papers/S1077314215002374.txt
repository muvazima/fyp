@&#MAIN-TITLE@&#RIMOC, a feature to discriminate unstructured motions: Application to violence detection for video-surveillance

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel and compact feature discriminating structuredness of observed motions.


                        
                        
                           
                           A feature embedded in a weakly supervised learning framework.


                        
                        
                           
                           An efficient method for real-time violence detection in on-board video-surveillance.


                        
                        
                           
                           Ability of the learned model to generalize training data for varied contexts.


                        
                        
                           
                           A new dataset representative of the targeted application for extensive evaluation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Violence detection

Aggression detection

Violent event detection

Unstructured motion

Abnormality detection

Video-surveillance

@&#ABSTRACT@&#


               
               
                  In video-surveillance, violent event detection is of utmost interest. Although action recognition has been well studied in computer vision, literature for violence detection in video is far sparser, and even more for surveillance applications. As aggressive events are difficult to define due to their variability and often need high-level interpretation, we decided to first try to characterize what is frequently present in video with violent human behaviors, at a low level: jerky and unstructured motion. Thus, a novel problem-specific Rotation-Invariant feature modeling MOtion Coherence (RIMOC) was proposed, in order to capture its structure and discriminate the unstructured motions. It is based on the eigenvalues obtained from the second-order statistics of the Histograms of Optical Flow vectors from consecutive temporal instants, locally and densely computed, and further embedded into a spheric Riemannian manifold.
                  The proposed RIMOC feature is used to learn statistical models of normal coherent motions in a weakly supervised manner. A multi-scale scheme applied on an inference-based method allows the events with erratic motion to be detected in space and time, as good candidates of aggressive events.
                  We experimentally show that the proposed method produces results comparable to a state-of-the-art supervised approach, with added simplicity in training and computation. Thanks to the compactness of the feature, real-time computation is achieved in learning as well as in detection phase. Extensive experimental tests on more than 18 h of video are provided in different in-lab and real contexts, such as railway cars equipped with on-board cameras.
               
            

@&#INTRODUCTION@&#

Automated vision-based analysis of human behaviors is an important task with many applications in surveillance, ambient assisted living, customer behavior analysis, gaming, concept-based video retrieval, automatic video annotation and summarization. The problem of generic human behavior interpretation is still very complex and cannot yet be solved with one type of approach. Most of the time, it is the application with its specific context and final objective that drive the type of approach to use. As reported by Salah et al., contextual information constrains the problem that involves complex spatiotemporal and semantic reasoning. Context can include scene geometry (view point and 3D composition), scene type (outdoor, indoor, street, football field), object types in scene, people in scenes (number and appearance), knowledge of prior actions and interaction semantics [1].

In this paper, we focus on the problem of violent behavior detection for video-surveillance applications. Whereas action recognition has been widely studied, the literature about violence detection is much sparser. Violent or aggressive behaviors are complex to model and detect, especially because: they have not a unique definition and show great variability; they may need specific interpretation relative to the context to be confirmed; and their intensity may be assessed with some subjectivity. As for any human action, there exist large variances in body poses, non-rigid body movements, camera angles, clothing textures, lighting conditions. The additional difficulty is that violent behavior cannot be always summarized by a precise action or gesture, but is generally reflected by a set of human interactions. Aggression on the street, bag snatching with violence, vandalism and riot are pretty different from gesture of fighting, boxing or sumo wrestling. In addition, complex real-life environments (dynamic and cluttered background, high people density, occlusions) make scene analysis more difficult.

Researchers have addressed violence detection with many contexts of application. Some studies are devoted to violence detection in movies for automatic tagging of violent scenes. Both audio and visual cues (e.g., explosion, car-braking, crash, gunshot, flame, skin with blood) are generally detected. By the way, this task has been part of the annual challenge MediaEval since 2011 [2–4]. To qualify violent scenes in this task, either a subjective definition is adopted (scenes one would not let an 8-year-old child see because they contain physical violence) or a more objective one (physical violence or accident resulting in human injury or pain). But both have very high-level of semantic, very difficult to translate in the computer vision world. Other studies address the similar application of web video content analysis for automatic rating, tagging and filtering [5]. In all these articles, the multimedia analysis means that not only visual cues are used, but also audio cues, textual tags, etc. When comparing the importance of each modality separately, audio cues are the most efficient [6]. However, in video-surveillance applications, audio and color are not necessarily available, as pointed out by Deniz et al. [7]. And explosions, blood and running are useful cues for action movies, but rare in real-world situations.

Other authors focus on fight detection, but in general, with few (two) people in a facilitated context (isolated in front of uncluttured background) [8]. Well-defined gesture or unitary interactions can then be modeled and detected. Even if it is not their primary focus, many datasets for general action recognition containing some instances of fight can then be used (e.g., “Punching” or “Sumo wrestling” actions from UCF101 dataset). On the contrary, a dedicated dataset for fight detection in hockey games was proposed by Bermejo et al. [9]. Still, Deniz et al. [7] noticed that the detection on this dataset was easier than on action movie dataset because fights in televised hockey footage were more consistently similar and also because the supervised classifier was biased by other cues (e.g., camera zoom-in during fights contrasting with wide-angle shots during non-fight segments). Thus, this highlights the importance of the training and testing data and, consequently, raises the question of which application the violence detector is intended for.

Our work focus on violence detection for video-surveillance as automated alert systems are necessary to filter the useful information for the security operator monitoring a wall of CCTV screens. Some studies dealing with this topic use again audio and visual cues [10–15]. Fewer tackle the problem by vision only. Interestingly, even within surveillance-purpose, the developed approaches can be very different according to the type of scene present in the dataset. For example, to detect violence in a crowd, Hassner et al. consider the crowd as a dynamic texture as they assume people appear in low resolution from a viewpoint far from the scene [16].

Even if different levels of analysis were studied (body pose, body part detection, trajectory analysis...), most of authors worked on developing low-level features used in a learning framework to detect violent event in a more efficient and generic manner. Often, the detection problem is transformed into a classification problem by considering video clips pre-segmented from a video stream. The most frequently adopted framework is a supervised binary classifier trained on numerous examples of violent and non-violent events, i.e., the direct “reduction” to two classes of the multi-class classification problem tackled in generic action recognition. The features used can be generic descriptors for describing any action (MoSIFT [17] or descriptors computed on spatiotemporal interest points STIP [18]) combining appearance and/or motion information. The major drawbacks of supervised approaches are: to gather a representative training dataset with balanced number of positive and negative examples, to manually label them which is tedious and expensive. Furthermore, it is difficult to have pure positive instances of the violent behaviors in real-world environments because a temporal and spatial annotation (segmentation) is necessary to train only on pure events without influence of background normal (non-violent) elements.

More rarely, one-class classification is used for violence detection [19]. Nevertheless, violence in video-surveillance is a low frequency yet high impact event. Consequently, abnormal event detection approaches seem totally appropriate for such a rare event. Indeed, these methods do not require data for the rare (abnormal) event to be detected but only normal events in order to model, during a training stage, what will not be detected. This weakly supervised learning approach is particularly interesting in an applicative point-of-view because violence data is much more difficult to collect and annotate than non-violent data. The main drawback is that even if abnormal detected events are rare and unexpected [20], they are not necessarily interesting for the purpose of violence detection if the feature used is not appropriate, i.e., if it cannot discriminate violent from non-violent events. Indeed, the choice of descriptors substantially determines the type of rare events that will be detected (e.g., crowd escape panic [21,22], traffic-jam, wrong way or tail-gating detection [23–26]). Thus, it is generally more efficient to define features specific to targeted events.

This is why we propose in this paper a novel Rotation-Invariant feature modeling MOtion Coherence (RIMOC) that intends to code structured motions and discriminate them from erratic or jerky motions, assumed to be present in violent behaviors. A codebook of visual words representing this unitary structured motion is learned offline on non-violent data, as well as their space-time statistical configurations. Then, these models feed an abnormal event detection framework, and a multi-scale scheme applied on an inference-based method allows the events with jerky motion to be detected in space and time, as good candidates of aggressive events.

This feature should be specific to violent events but also generic enough so as the learned models not to be too much dependent on the specific appearance of the normal events observed during the training stage. Indeed, training data will hardly represent all the normal non-violent events. As pointed by Deniz et al., “appearance also may confuse the detector” and overfitting the training dataset is a risk [7]. Ideally, the learned models could be transferred from one camera view to another for whole adaptivity of the method. Some other important criteria when dealing with efficiency of a method for a surveillance application are the real-time processing (e.g., “detection alert should be made within a few seconds of the outbreak of violence” [16]) and the compactness in memory (e.g., too large feature make unfeasible the learning of a visual codebook on a large training dataset, even with a parallel K-means [7]).

The most relevant contributions of this paper are the following:

                        
                           •
                           it is one of the rare studies tackling the video-based detection of aggressive events in complex and realistic surveillance environment, such as railway cars with dynamic cluttered background and occlusions;

it defines a novel and compact low-level feature space that codes the structuredness of the observed motions;

it provides a first attentional module for a greater cascade system of violence detection by embedding this feature in a multiscale scheme to detect good candidates of aggressive events occurring at different locations and scales;

it provides experimental tests showing that the use of the proposed feature space embedded in a statistical anomaly detection framework can produce results comparable to a state-of-the-art supervised approach, with added training and computation simplicity;

it provides extensive experimental tests using more than 18 h of video and 276 aggressions, both from real and lab-recorded sequences and from different contexts, some with poor-quality CCTV cameras.

it provides temporally annotated ground truth for violent event and jerky motion detection in a publicly available dataset, as well as a new dataset for real-life violence for video-surveillance applications and results on a new on-board violence surveillance dataset.

thanks to the simplicity and compactness of the feature, and an easy training process, a real-time implementation of both learning and detection phases has been achieved.

This paper is organized as follows. Section 2 reviews related work. Section 3 briefly explains the overall method proposed in this paper. Section 4 presents the novel feature space used to code the low-level structure of the motions. The learning and detection frameworks utilized in the experiences are described in Section 5. Section 6 describes the existing datasets and those used to get the experimental results presented in Section 7. A discussion of the results, jointly with considerations for future work is presented in Section 8 and Section 9 concludes the paper.

@&#RELATED WORK@&#

The state-of-the-art literature dealing with the problem of detecting aggressive events from complex environments of video-surveillance, and especially in public transportation, is quite scarce. Here we review studies that aim at detecting some kind of aggressive behavior, even if in different contexts, and studies that are somehow related to the approach we follow. More generally, the literature dealing with human behavior analysis can be roughly categorized in two families of methods: human action recognition framework and abnormal event detection framework.

Most of studies consider human action recognition as a multi-class classification problem and tackle it using a supervised learning framework [27–34]. These methods strive to be robust against severe camera motions and an arbitrary view point. Also, most of the multi-class approaches are designed to perform classification using pre-segmented sequence and use a somewhat balanced number of samples from each class. Some of these varied classes can be specific aggressive actions, like fighting, punching, kicking, wrestling or boxing. This means that they generally do not include a largely dominant background class from which events of interest need to be separated. If one naively applies such techniques in a surveillance scenario, the risk of a high false detection rate is very probable, as there are mostly non-aggressive situations.

Different levels of analysis are also possible, from low-level (e.g., local features) to higher-level features such as body boundaries, interactions, pose, contact, detected and tracked body parts, trajectories,...with possible use of a-priori kinematic models. Low-level analysis has the advantages of not depending on body part detectors or people trackers and being more robust to context for not making assumption of number of people. In the literature, low-level approaches either describe the entire frame or sub-images or focus their analysis on interest points [35,36] by using descriptors around them [37–39] or by tracking them and describing these tracklets. The first category includes optical flow-based [40,41] methods or dynamic texture-based [42,43] methods, mostly devoted to crowded scene analysis. The second category uses descriptors generally computed in spatio-temporal volumes using gradients, intensities, flows or other local features, possibly at multiple scales. A survey of such descriptors is given in [44]. A video clip can then be represented by diverse techniques, among them the commonly used bag-of-features [39,45].

Marín-Jiménez et al. [27] show that by using space-time interest points, jointly with a bag-of-words model, one can achieve competitive results for human interaction recognition. Kai Guo et al. [34] use a dense set of spatio-temporal feature vectors aggregated in an empirical covariance matrix to compactly represent the actions of interest. They propose two supervised learning methods: (i) the nearest-neighbor classification using a suitable Riemannian metric for covariance matrices and (ii) the reconstruction of a query covariance matrix by a sparse linear combination of the training covariance matrices. Their method shows some robustness to the action variability, viewpoint change, and low object resolution, but is generally tested in fairly simple environments.

Derpanis et al. [33] perform spatio-temporal detection and localization of human actions in video, based on the visual space-time oriented energy measurements. Although the approach can accommodate variable appearances of the same action, the actions considered comprise only very structured motions, e.g., pick-up, jumping jacks, push elevator button, one-handed wave and two-handed wave.

Wang et al. [30] proposed the use of dense trajectories and motion boundary descriptors, with very promising results, in the context of action classification using pre-segmented videos. Because this work has received much attention from the community, we extended it in order to work with uncut videos and use it as the state-of-the-art supervised baseline. Later, Wang et al. improved over the previous work by proposing trajectories with better invariance resistance to camera motions [31]. Although this achieved better results in the datasets they used, it is not so important in our case when dealing with static surveillance cameras.

Very recently, Gaidon et al. [32] proposed a method to learn hierarchical representations of small trajectories, i.e. tracklets, of activity videos in an unsupervised manner. It uses spectral embedding to compute the similarity between tracklets and a nested bag-of-features for classification. Their method improves over unstructured activity models but adds computational complexity to the learning and classification process. Furthermore, it is not clear that, in the case of unstructured motions, such as aggressions, the method would produce meaningful tracklets.

Interest point-based methods are in general very robust to camera motion and occlusion and showed excellent performances on many benchmarks [39,45,46]. However, they can have difficulties in providing efficient representations when videos contain too few interest points (e.g., too little motion) or too much motion (as in real video-surveillance sequences where background is complex). In addition, feature point tracking may undergo illumination and reinitialization problems.

Even if violence detection from video is far less studied than action recognition, some authors [9,16,47] tackled the problem as a binary supervised problem. One of the commonly used descriptors in violence detection is the space-time interest points (STIP) [18], an extension of Harris corner to space-time used with either Histogram of Oriented Gradient (HOG), Histogram of Optical Flow (HOF), or combination (HNF). Yet, Chen et al. notice that in practice, true spatial-temporal corners are quite rare [19]. Another local STIP to detect periodic movements has been proposed [35]. The question is whether complex actions can be represented by periodic motions alone. Chen et al. proposed the MoSIFT [17] as an extension of SIFT with a HOF-like part.

The contexts in which violence is detected can be very different: movies, generic multimedia videos on the web [5,47], sport TV broadcast videos [9], and video surveillance [16]. This has great importance because the general supervised methods used are somehow specialized to a specific context.


                           Violence detection in movies and multimedia videos Violent scene detection in movies has been rather well studied in the literature, in particular, with the creation of specialized benchmarks as MediaEval [2,6,7,9,48–54]. Most of these methods rely on audio (e.g., explosions, gunshot, car-braking) and video cues (flame, skin with blood, motion levels) given by dedicated detectors, as well as textual cues as comments. Interestingly, Giannakopoulos et al. [6] assume a correlation between violence and video activity, and, to this aim, track objects (frontal or profile faces, full or upper body) to derive a metric for their motion (Average Motion and Motion Orientation Variance). Nonetheless, they report that video modality separately performs worse (59% accuracy) than the other modalities (75% for text, 79% for audio). More recently, by observing the blur, i.e. an image shift to low-frequencies, created by violent actions, Deniz et al. propose a specific feature for extreme acceleration pattern detection based on Radon transform of the power spectrum of consecutive frames [7].


                           Violence in TV broadcast videos A dedicated dataset for fight detection in hockey games was proposed by Bermejo et al. [9] who compared binary SVM classifiers based on bag-of-words from STIP and MoSIFT. Deniz et al. [7] noticed that the detection on this dataset was easier than on action movie dataset because fights in televised hockey footage were more consistently similar in appearance and also because the supervised classifier was biased by other efficient cues (e.g., camera zoom-in during fights contrasting with wide-angle shots during non-fight segments). Thus, this highlights the influence of the training data and, consequently, this limits the ability of too specialized classifiers to apply on different types of datasets.


                           Violence detection in video-surveillance Unlike in movies, in video-surveillance, audio and color are not necessarily available. Moreover, explosions, blood and running are useful cues for action movies, but rare in real-world situations, as claimed by [7]. We cannot rely neither on zooming effects in an automated surveillance setup. The studies in this particular application context are far fewer.

In a pioneer work, Datta et al. aim at detecting fight of person-on-person [8]. They rely on motion trajectory information and on orientation information of a person’s limbs. After extracting person’s blob, they detect and track head and limbs to measure an Acceleration Measure Vector (AMV) composed of direction and magnitude of motion as well as its derivative, jerk. A detector confirms the presence of skin. The method assumes only two people with upright silhouettes are fighting in front of a static background without occlusion. Wrestling instead of hitting each other is not handled, nor falling down or group violence. Chen et al. proposed binary local motion descriptors representing shape and motion of space-time volumes (cubes) around interest points that have high contrast in both space and time [19]. Based on a bag-of-words method, they learn normal behaviors by one-class SVM and detect aggressions as outliers. They apply the method on the dining room of an elderly care home.


                           Violence detection by audio-visual sensory. Although not always available, audio sensory was used together with video for detecting violence in public space [10–15]. In CASSANDRA project, Zajdel et al. detect lively and aggressive behaviors of people on a train platform [10]. They follow a fusion scheme with scream-like cues and a pseudo-kinetic energy computed from the difference between velocity of a person and the velocity of tracked keypoints inside the bounding box of that person. The method assumes all people have been previously detected and tracked. Lefter et al. as well as Yang in his PhD dissertation tackle the problem of multi-modal aggression detection in trains [12–15]. If the context is identical to ours, these approaches mainly deal with the fusion and aim at modeling a rule-based system using expert knowledge of the train and passengers. Yang uses third-party software, or state-of-the-art methodologies as low-level detectors, background subtraction, face detector and tracker, as well as much prior knowledge on the geometry and different zones of the train compartment. He finally concludes [15] that “much improvement of the system can be achieved by adopting better low-level detection algorithms, or algorithms that are better trained on the train environment.” His dataset is unfortunately not available.


                           Violent crowd behavior. The previous methods cannot be applied for violence detection in crowd because of the very high density of people viewed with low resolution and partially hidden by occlusions. That is why Hassner et al. propose an approach based on dynamic textures [16]. A “violent flow” descriptor based on temporal change of optical flow magnitude can be either used as a global descriptor of a video clip and directly classified by an SVM learning-based binary classifier. Or it can be used by a bag-of-features. The dataset they gathered contains 246 very short videos (3.6 s on average). This type of method is not designed for videos with people viewed at a shorter distance and in higher resolution.

To conclude, the problem of detecting aggressive motions for video-surveillance in complex environment, as the confined space of a railway car, is not much addressed by the state-of-the-art. Most of methods focus on recognizing actions on datasets very different from video-surveillance context (e.g., movies, sports). The methods proposed in the context of video-surveillance are in general supervised and require large and representative labeled datasets. Their ability to apply to a dataset different from the training is not demonstrated because test and training data always come from the same dataset.

Considering the rarity, the variety and irregular property of the aggressive behaviors, an abnormality detection framework seems to be appropriate for aggressive event detection. Although the detection of aggressive behaviors is commonly tackled using supervised techniques, the characteristics of the problem suggest that one could also see it as an abnormality detection problem. Abnormality detection is used to detect situations that are not observed frequently. Although the term abnormality cannot be defined explicitly, all such systems are based on the implicit assumption that events that occur occasionally are potentially suspicious, and thus, may be considered as being abnormal. In recent years, the detection of unusual or suspicious activities, uncommon behaviors, or irregular events in a scene has become a very active field in computer vision [21–26,55–58].

Inside public transportation systems, one expects to have a high degree of overlapping between people, being thus a difficult scenario to accurately perform tracking. For that reason, we only mentioned methods with a focus on local spatio-temporal anomalies in videos, and that do not use (people) trajectories. Among them, Boiman’s work [56] is one of the first that proposes the detection of local spatio-temporal anomalies by modeling the lower-level 3D patch distribution inside a bigger ensemble. The “normality” score is obtained from evaluating how well one can reconstruct the new data from all the bigger ensembles previously seen, i.e. from a database of normal samples, discarding their spatio-temporal location. Although accurate, this method proves to be computationally prohibitive to use with online approaches. More recently, Roshtkhari et al. [24,58] use the same idea but, instead of gathering all the previously seen data, they only keep a small number of representative patch models, i.e. codewords, and statistically model their organization using probability density functions in a (near) real-time algorithm.

Unlike the previously described methods, other authors [23,26] model the location-specific properties of the image region to detect anomalies. Nevertheless, those studies are quite different from the problem of aggression detection because the type of motions and interactions considered are simpler.

Commonly used low-level feature descriptors include optical flow-based [21,25,59–61], gradient-based [22,26,60], dynamic texture-based [57,62,63], and frequency-based [64,65] descriptors.

If the previously cited methods have the advantage on supervised methods, of requiring only normal non-violent – thus easily available – events, they rather detect generic rare events than specific aggressive events. The detected events depend mainly on the feature and the normal data used to learn the model. Thus, to use efficiently this abnormality framework, the feature used should be specific enough to discriminate targeted events, and also with good generalizing ability so as the trained normality model to represent all possible non-violent events.

This work proposes a novel method to model the structure of motions by means of a new low-level feature space based on the eigenvalues of Histograms of Optical Flow (HOF). This process adds invariance to rotations on the image space and is maximally discriminant between the motions of interest and the environment. The feature space is further embedded into a statistical abnormality detection methodology, as shown in Fig. 1
                     .

We start by dense sampling the input video and constructing local Spatio-Temporal Volumes (STV) around each sampled point. Each frame of the STV is coded as a HOF, from which the eigenvalues are computed. During training we learn a codebook from data comprising normal situations, and use the codebook to further learn spatio-temporal configurations of the words, c. The spatio-temporal configurations are learned as probability distributions over the relative locations, δ, of the words on larger regions. This process is exemplified in the region of Fig. 1 labeled as learning.

During inference we evaluate if the query instance 
                        E
                      has been generated using the estimated models, by maximizing the likelihood over the possible configurations at different scales. The process is based on the key assumption that normal data instances occur in high probability regions of the learned stochastic models, while anomalies occur in low probability regions. The inference formulation is based on the method proposed by Roshtkhari and Levine [24], with the addition of a multiscale fusion step before thresholding the similarity value. Indeed, the likelihood of observed motion configuration being explained by the model is minimized over scales in order to detect abnormality at any scale. The process is exemplified in the region of Fig. 1 labeled as inference.

Our intention is to separate two classes of motions, aggressive or unstructured motions and normal ones. Contrary to the normal motions, that exhibit some temporal coherence, the aggressive ones have less structure, i.e. exhibiting quicker changes in direction and amplitude. Generally speaking, normal motions, when observed in short temporal windows, can be broadly divided into five categories: (i) no motion, (ii) constant motion, (iii) slight direction change, (iv) slight speed change, (v) slight speed and direction changes. Conversely, aggressive motions are erratic and unstructured, leading to persistent abrupt changes, characterized by fast and considerable changes in both speed and direction.

The descriptions above are general, thus not associated with the motion specifics, such as the direction and distance to the camera. It is our intention to derive a low-level feature space as invariant as possible to the specificities of the motion. Ideally, feature vectors of constant motions should be the same for whichever direction, and similarly for slight changes and different execution distances. By the rotation-invariance of the RIMOC, a straight line constant motion in any direction will be coded the same way – generalization – and will avoid learning each direction motion during training. The rationale is that the feature space should be discriminative between the normal motions and the aggressive motions, no matter the motion specificities of a specific event.

We start by explaining how the features are sampled and how we code the low-level motion specificities based on Histograms of Optical Flow, in Section 4.1. In Section 4.2, we show how to use the HOF vectors to construct a feature space that explores some of the desired invariance properties and in Section 4.3, we present the tools needed to work on the spherical space induced by the L
                     2-normalized HOF vectors. We finalize by giving an intuitive interpretation of the feature space in Section 4.4, through an analogy with the Harris [66] corner detection algorithm and an illustration of the feature space behavior.

Because the appearance of the scene is not important in our context, we only use motion information, e.g. optical flow vectors, as input to compute the features. To extract dense optical flow vectors, we use the Farnebäck algorithm [67], which embeds a translation motion model between neighborhoods of two consecutive frames and exhibits a good compromise between speed and accuracy [30].

Also, we want to be able to deal with motions coming from different objects. Thus we transform the input flow vectors into Histograms of Optical Flow (HOF) for local regions of the image. Furthermore, recent research shows that dense sampling is preferable to sparse interest point detectors [68]. Therefore, we compute the HOF vectors over a dense grid of points. Fig. 2
                         shows the type of spatio-temporal regions (Spatio-Temporal Volumes – STVs) used to compute the HOF vectors.

Each STV models the motion locally and in a short-term perspective, as a (δx, δy, δt) volume of video around a sampled point. For each frame of the STV, a non-normalized HOF vector X
                        
                           i
                         is computed. Each STV is thus a set of HOF vectors 
                           
                              X
                              =
                              {
                              
                                 X
                                 1
                              
                              ,
                              
                              
                                 X
                                 2
                              
                              ,
                              
                              …
                              ,
                              
                              
                                 X
                                 n
                              
                              }
                              ,
                           
                         where n is the number of frames comprised in the temporal window δt. For δt ≤ ε, with ε small enough, perceived normal motions will tend to one of the five broad categories described above, i.e. changes in direction and speed are always slight or nonexistent. Concurrently, when considering aggressive motions, it is expected that it exists a 
                           
                              δ
                              t
                              =
                              τ
                              ∈
                              [
                              0
                              ,
                              
                              
                                 ɛ
                              
                              ]
                           
                         that maximizes the difference between the perceived motion of both classes. By optimizing τ this way, we build a X descriptor that is most sensitive to this difference. This value is optimized experimentally in Section 7.3. For now, let us consider that δt is such that both classes are maximally discriminable.


                        Fig. 3
                         exemplifies the HOF vectors computation for one STV. Each X
                        
                           i
                         vector is a representation of the motion inside the patch for a specific frame i. In order for this vector to be invariant to the scale, i.e. motions performed at different distances from the camera, we L
                        2-normalize the HOF vectors by computing the vector 
                           
                              
                                 X
                                 i
                                 
                                    2
                                    ¯
                                 
                              
                              =
                              
                                 X
                                 i
                              
                              /
                              
                                 
                                    ∥
                                    
                                       X
                                       i
                                    
                                    ∥
                                 
                                 2
                              
                           
                        . This way, the difference in amplitude of the optical flow vectors induced by the same motion performed at different distances from the camera is eliminated. Note that, for this invariance to work correctly, we need to consider STVs of different spatial scales as described in Section 5.1.1.

Because the L
                        2-normalization
                           1
                        
                        
                           1
                           The L
                              1 normalization is not well suited because it would introduce a distortion in the invariance properties, as will become clear next Section.
                         embeds the vectors 
                           
                              
                                 X
                                 i
                              
                              ∈
                              
                                 R
                                 p
                              
                           
                         into the unit sphere 
                           
                              
                                 S
                                 
                                    p
                                    −
                                    1
                                 
                              
                              ,
                           
                         all the statistics need to be computed in that spherical manifold, as presented in Section 4.3. First, let us present the feature space used and the invariance properties.

In order to understand the invariance properties of the proposed feature space, let us work in the continuous case. Consider the 2π-periodic functions 
                           
                              f
                              ,
                              g
                              :
                              R
                              →
                              R
                              ,
                           
                         such that 
                           
                              f
                              (
                              θ
                              +
                              2
                              π
                              n
                              )
                              =
                              f
                              (
                              θ
                              )
                           
                         and 
                           
                              g
                              (
                              θ
                              +
                              2
                              π
                              n
                              )
                              =
                              g
                              (
                              θ
                              )
                              ,
                           
                         with 
                           
                              n
                              ∈
                              Z
                              ,
                           
                         to be representations of the probability density function of the flow field for image regions. It is trivial that, due to the periodicity of the functions, the continuous inner product of the two functions on a period

                           
                              
                                 
                                    
                                       〈
                                       f
                                       ,
                                       g
                                       〉
                                    
                                    =
                                    
                                       ∫
                                       
                                          −
                                          π
                                       
                                       π
                                    
                                    f
                                    
                                       (
                                       θ
                                       )
                                    
                                    g
                                    
                                       (
                                       θ
                                       )
                                    
                                    d
                                    θ
                                 
                              
                           
                        doesn’t change under shifts. Thus: 
                           
                              
                                 〈
                                 f
                                 ,
                                 g
                                 〉
                              
                              =
                              
                                 〈
                                 
                                    f
                                    ′
                                 
                                 ,
                                 
                                    g
                                    ′
                                 
                                 〉
                              
                              ,
                           
                         for 
                           
                              
                                 f
                                 ′
                              
                              
                                 (
                                 θ
                                 )
                              
                              =
                              f
                              
                                 (
                                 θ
                                 −
                                 η
                                 )
                              
                           
                         and 
                           
                              
                                 g
                                 ′
                              
                              
                                 (
                                 θ
                                 )
                              
                              =
                              g
                              
                                 (
                                 θ
                                 −
                                 η
                                 )
                              
                              ,
                           
                         ∀η. A shift of the density function of the flow field is the result of rotations on the image, i.e. adding a fixed angle to all the flow vectors.

For the discrete case of the HOF vectors, X
                        
                           i
                         ≃ f, Y
                        
                           i
                         ≃ g are approximations of true density functions, thus ⟨X
                        
                           i
                        , Y
                        
                           i
                        ⟩ ≃ ⟨f, g⟩ and the inner product of the shifted version is also similar, 
                           
                              
                                 〈
                                 
                                    X
                                    i
                                 
                                 ,
                                 
                                    Y
                                    i
                                 
                                 〉
                              
                              ≃
                              
                                 〈
                                 
                                    X
                                    i
                                 
                                 +
                                 η
                                 ,
                                 
                                    Y
                                    i
                                 
                                 +
                                 η
                                 〉
                              
                              ,
                           
                         with the quality of the approximation depending on the discretization used. This fact is important because the inner product of two vectors,

                           
                              
                                 
                                    
                                       〈
                                       
                                          X
                                          i
                                       
                                       ,
                                       
                                          Y
                                          i
                                       
                                       〉
                                    
                                    
                                       =
                                       ∥
                                    
                                    
                                       X
                                       i
                                    
                                    
                                       
                                          ∥
                                       
                                       2
                                    
                                    
                                       
                                          ∥
                                          
                                             Y
                                             i
                                          
                                          ∥
                                       
                                       2
                                    
                                    cos
                                    
                                       (
                                       ϕ
                                       )
                                    
                                    ,
                                 
                              
                           
                        is a measure of the angle, ϕ, between them.

As the shifting operator doesn’t change the norm, the fact of having a similar inner product between shifted HOF vectors means that they maintain their configuration. This means that a rotation of the images or, locally, of a specific STV, will introduce a similar change on all the HOF vectors, thus maintaining the relative angles. As relative angles are proportional to the distance computed on the spheric manifold, the covariance matrix, 
                           
                              
                                 Σ
                                 
                                    X
                                    
                                       2
                                       ¯
                                    
                                 
                              
                              =
                              c
                              o
                              v
                              
                                 (
                                 
                                    X
                                    
                                       2
                                       ¯
                                    
                                 
                                 )
                              
                              ,
                           
                         computed on such manifold maintains its eigenvalues.

This way, by using the eigenvalues of 
                           
                              
                                 Σ
                                 
                                    X
                                    
                                       2
                                       ¯
                                    
                                 
                              
                              ,
                           
                        
                        
                           
                              (1)
                              
                                 
                                    
                                       λ
                                    
                                    =
                                    [
                                    
                                       λ
                                       1
                                    
                                    ,
                                    
                                    
                                       λ
                                       2
                                    
                                    ,
                                    
                                    …
                                    ,
                                    
                                    
                                       λ
                                       n
                                    
                                    ]
                                    ,
                                 
                              
                           
                        with λ
                        1 > λ
                        2 > ⋅⋅⋅ > λn
                        , as the low-level STV vector representation, we achieve rotation invariance in each STV separately. This feature, named Rotation-Invariant feature for Motion coherence (RIMOC), adds the desired invariance with relation to the motion specificities related to the direction of motion, while maintaining a good discrimitation with relation to the structure of the motion.

Notice that, unlike the descriptor cov3D [69] defined as a spatio-temporal covariance based on gradient and optical flow, RIMOC only uses eigenvalues of HOF covariance matrix, which allows a very compact description of the elementary motion pattern.

In Section 5.1 we will show how to integrate this feature in an abnormality detection framework that will allow us to locate aggressions in the spatio-temporal domain. Next, we present the tools to compute the desired covariance matrix on the spheric manifold.

By using L
                        2-normalized histograms, the manifold is the positive orthant of the unit sphere, i.e. the set of points 
                           
                              
                                 S
                                 +
                                 
                                    p
                                    −
                                    1
                                 
                              
                              =
                              
                                 {
                                 x
                                 ∈
                                 
                                    R
                                    +
                                    p
                                 
                                 :
                                 
                                    x
                                    T
                                 
                                 x
                                 =
                                 1
                                 }
                              
                           
                        . This is actually a Riemannian submanifold of the Euclidean space 
                           
                              R
                              p
                           
                         
                        [70], that constitutes a smooth and differentiable manifold. For such manifold, one can define a Riemannian metric as the family of inner products in the tangent space, 
                           
                              
                                 ϑ
                                 
                                    x
                                    0
                                 
                              
                              :
                              
                                 T
                                 
                                    x
                                    0
                                 
                              
                              
                                 S
                                 +
                                 
                                    p
                                    −
                                    1
                                 
                              
                              ×
                              
                                 T
                                 
                                    x
                                    0
                                 
                              
                              
                                 S
                                 +
                                 
                                    p
                                    −
                                    1
                                 
                              
                              →
                              R
                              ,
                           
                         with 
                           
                              
                                 x
                                 0
                              
                              ∈
                              
                                 S
                                 +
                                 
                                    p
                                    −
                                    1
                                 
                              
                           
                        . Informally, the tangent space of a Riemannian manifold is a linear space, with the same dimension of the manifold, defined at each point of the manifold. In our case, a tangent space, 
                           
                              
                                 T
                                 
                                    x
                                    0
                                 
                              
                              
                                 S
                                 +
                                 
                                    p
                                    −
                                    1
                                 
                              
                              ,
                           
                         can be constructed for all 
                           
                              
                                 x
                                 0
                              
                              ∈
                              
                                 S
                                 +
                                 
                                    p
                                    −
                                    1
                                 
                              
                              ,
                           
                         defining the linear space of all the tangent vectors to 
                           
                              S
                              +
                              
                                 p
                                 −
                                 1
                              
                           
                         at x
                        0.

Considering the unit sphere embedded in 
                           
                              
                                 R
                                 p
                              
                              ,
                           
                         the inner product can be inherited from the standard inner product in 
                           
                              R
                              p
                           
                         as

                           
                              (2)
                              
                                 
                                    
                                       ϑ
                                       
                                          x
                                          0
                                       
                                    
                                    =
                                    
                                       
                                          〈
                                          u
                                          ,
                                          v
                                          〉
                                       
                                       
                                          x
                                          0
                                       
                                    
                                    =
                                    
                                       u
                                       T
                                    
                                    v
                                    ,
                                 
                              
                           
                        for 
                           
                              
                                 x
                                 0
                              
                              ∈
                              
                                 S
                                 +
                                 
                                    p
                                    −
                                    1
                                 
                              
                           
                         and 
                           
                              u
                              ,
                              v
                              ∈
                              
                                 T
                                 
                                    x
                                    0
                                 
                              
                              
                                 S
                                 +
                                 
                                    p
                                    −
                                    1
                                 
                              
                           
                        .

Defining an affine connection, one can connect nearby tangent spaces, allowing to differentiate the tangent vectors. Because the manifold is equipped with a Riemannian metric, the Levi–Civita connection is the natural choice. This enables us to transport vectors along the manifold and, thus, compute distances on it: the geodesics. A geodesic is the minimum distance between any two points in the manifold, computed along the manifold. Being able to compute distances on the manifold is the final requisite to represent our data in a linear space, the tangent space, so that the covariance matrices can be later computed. This can be done using the exponential map.

For Riemannian manifolds, the exponential map is a function that maps the points from the tangent space to the manifold, maintaining the distances. In the case of the unit sphere, the exponential map is 
                           
                              e
                              x
                              
                                 p
                                 x
                              
                              :
                              
                                 T
                                 x
                              
                              
                                 S
                                 +
                                 
                                    p
                                    −
                                    1
                                 
                              
                              →
                              
                                 S
                                 +
                                 
                                    p
                                    −
                                    1
                                 
                              
                           
                        . Conversely, the inverse exponential map projects points from the manifold onto the tangent space. For the unit sphere, all points in the manifold have the same velocity, i.e. the norm of the tangent vectors, thus the exponential map only depends on the tangent point.

In our case, the exponential map, expressed as function of point x
                        0, the tangent point belonging both to the manifold and the tangent space, is

                           
                              (3)
                              
                                 
                                    e
                                    x
                                    
                                       p
                                       
                                          x
                                          0
                                       
                                    
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    cos
                                    
                                       (
                                       ∥
                                       x
                                       ∥
                                       )
                                    
                                    
                                       x
                                       0
                                    
                                    +
                                    sin
                                    
                                       (
                                       ∥
                                       x
                                       ∥
                                       )
                                    
                                    
                                       x
                                       
                                          ∥
                                          x
                                          ∥
                                       
                                    
                                 
                              
                           
                        
                     

And the inverse exponential space

                           
                              (4)
                              
                                 
                                    e
                                    x
                                    
                                       p
                                       
                                          
                                             x
                                             0
                                          
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       (
                                       x
                                       )
                                    
                                    =
                                    
                                       θ
                                       
                                          s
                                          i
                                          n
                                          (
                                          θ
                                          )
                                       
                                    
                                    
                                       (
                                       x
                                       −
                                       
                                          x
                                          0
                                       
                                       cos
                                       
                                          (
                                          θ
                                          )
                                       
                                       )
                                    
                                 
                              
                           
                        with 
                           
                              θ
                              =
                              
                                 cos
                                 
                                    −
                                    1
                                 
                              
                              
                                 〈
                                 
                                    x
                                    0
                                 
                                 ,
                                 x
                                 〉
                              
                           
                        .

After defining the correct tangent space for the data points, the desired statistics on that linear vector space can be computed, e.g. the covariance matrices, as it were on the manifold.

The last thing to define is the point where to compute the tangent space. Obviously, that point should be the distribution mean. In order to compute a mean on a manifold, one normally uses the Karcher mean [71], that is a generalization of centroids to metric spaces. The method uses an iterative process to minimize the Fréchet function, which is computationally expensive to perform densely across the image. In our case, as we work on the positive orthant of the unit sphere, the L
                        2-normalized Euclidean mean of the points is actually a good approximation and much faster to compute.


                        Fig. 4
                         shows an example of the covariance structure of HOF points computed on the tangent space of 
                           
                              S
                              2
                           
                        . Note that, when transforming the space, from 
                           
                              S
                              p
                           
                         into the eigenvalues space, one obtains invariance to rotations with relation to the origin of the HOF space. Such rotations can represent, as shown in Section 4.2, rotations in the image space.

Using the aforementioned method to compute the second-order statistics on the spheric manifold, one can directly compare them for different STVs. In practice, each STV covariance matrix is computed at a different mean, thus on a different tangent space, but their structure is equivalent to the structure computed on the manifold.

Generally, using the sample matrix as the estimator of the true covariance matrix is not a good choice if the ratio p/n is not negligible, leading to ill-conditioned matrices. This, in turn, implies error amplification, when inverting the matrix and computing their eigenvalues. In our experiences, that was not a problem, probably because we were only interested in separating coherent from erratic motions, not in correctly estimating the motion itself. Nonetheless, if this becomes a problem, one can use well-conditioned estimators, such as the one proposed in [72], claimed to be more accurate than the sample covariance matrix, even for very small numbers of observations and variables.

From the prerequisite of using a small enough τ, as postulated in Section 4.1, the perceived normal motions will slightly change. Let us try to understand the implications on the final feature vectors.

We can think of the model for the computed HOF vector as 
                           
                              
                                 
                                    X
                                    t
                                 
                                 ˜
                              
                              =
                              
                                 X
                                 t
                              
                              +
                              
                                 ν
                              
                              ,
                           
                         where 
                           
                              
                                 X
                                 t
                              
                              ∈
                              
                                 R
                                 p
                              
                           
                         is the true HOF vector and 
                           ν
                         represents the noise present in the image, e.g. in the optical flow computation. For simplicity, let us consider the noise to be a zero mean p-variate normal 
                           
                              
                                 ν
                              
                              ∼
                              N
                              (
                              0
                              ,
                              Q
                              )
                              ,
                           
                         with an isotropic covariance matrix 
                           
                              Q
                              =
                              I
                              
                                 σ
                                 2
                              
                              ,
                           
                         were I stands for the identity matrix. It is easy to see that the covariance matrix 
                           
                              
                                 Σ
                                 ˜
                              
                              =
                              c
                              o
                              v
                              
                                 (
                                 
                                    X
                                    ˜
                                 
                                 )
                              
                           
                         has the contribution from a random variable, 
                           
                              Σ
                              =
                              c
                              o
                              v
                              
                                 (
                                 
                                    ν
                                 
                                 )
                              
                              =
                              I
                              
                                 σ
                                 2
                              
                              ,
                           
                         and from a deterministic variable that depends on the motion of the objects in the image, 
                           
                              
                                 Σ
                                 0
                              
                              =
                              c
                              o
                              v
                              
                                 (
                                 X
                                 )
                              
                           
                        . No matter the law governing X
                        
                           t
                        , 
                           Σ
                        
                        0 can be considered as computed from a degenerated random vector that is independent of the noise component, leading to 
                           
                              
                                 Σ
                                 ˜
                              
                              =
                              
                                 Σ
                                 0
                              
                              +
                              Σ
                           
                        .

Thus, structure of the covariance matrix becomes:

                           
                              
                                 
                                    
                                       
                                          
                                             Σ
                                             ˜
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                Σ
                                                0
                                             
                                             +
                                             Σ
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                V
                                                0
                                             
                                             Δ
                                             
                                                V
                                                0
                                                T
                                             
                                             +
                                             V
                                             I
                                             
                                                σ
                                                2
                                             
                                             
                                                V
                                                T
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                V
                                                0
                                             
                                             Δ
                                             
                                                V
                                                0
                                                T
                                             
                                             +
                                             
                                                σ
                                                2
                                             
                                             I
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                V
                                                0
                                             
                                             
                                                (
                                                Δ
                                                +
                                                
                                                   σ
                                                   2
                                                
                                                I
                                                )
                                             
                                             
                                                V
                                                0
                                                T
                                             
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        
                     

This means that if the noise energy is small enough when compared to the deterministic signal, the final energy of the signal will be spread along the direction of the trajectory formed by the set of HOF vectors, and coded in 
                           Δ
                        . Recalling what is considered as normal motions:

                           
                              •
                              no motion changes: that is the same as considering no deterministic motion, thus the final covariance structure will be dominated by the noise;

slight changes: in this case the HOF vectors will exhibit a simple and coherent trajectory, leading to changes in a small number of space directions.

Our main claim is that for the abnormal situations, the HOF trajectory will not be coherent, thus exhibiting changes in more directions than in the normal case. This is due to the fact that the STV will contain more abrupt changes. In this case, the data will spread along several directions and the structure of the covariance matrix will reflect this fact.


                        Fig. 5 shows the eigenvalues extracted at a specific STV over several frames, comparing two normal walking motions and an aggression. First, we note that for both walking motions, the eigenvalues energy is either very small or concentrated along the first eigenvalue. This is in line with the rotation invariance property, that should produce similar results for similar motions in different directions, and with the claim of coherence of the HOF trajectory, by exhibiting energy concentration in only one direction. Note that most of the eigenvalues for the normal cases are not visible as their values are almost zero because there is no motion change. In the aggression case, as consequence of the incoherence of the HOF trajectory, the energy is distributed across several eigenvalues, leading to a different feature signature.

An interesting analogy with Harris [66] image corner detection algorithm can be made. The corner detection method uses the covariance matrix of 2D spatial gradients between points in an image patch to measure the cornerness of the center point. This is done by analyzing its eigenvalues and, in general, interpreted as:

                           
                              
                                 
                                    
                                       
                                          
                                             
                                                λ
                                                1
                                             
                                             ,
                                             
                                                λ
                                                2
                                             
                                             ∼
                                             0
                                          
                                       
                                       
                                          ⇒
                                       
                                       
                                          
                                             Mostly
                                             
                                             a
                                             
                                             uniform
                                             
                                             region
                                             .
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                λ
                                                1
                                             
                                             
                                             large
                                             ,
                                             
                                                λ
                                                2
                                             
                                             ∼
                                             0
                                          
                                       
                                       
                                          ⇒
                                       
                                       
                                          
                                             Region
                                             
                                             as
                                             
                                             a
                                             
                                             strong
                                             
                                             edge
                                             .
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                λ
                                                1
                                             
                                             ,
                                             
                                                λ
                                                2
                                             
                                             
                                             large
                                          
                                       
                                       
                                          ⇒
                                       
                                       
                                          
                                             Corner
                                             
                                             or
                                             
                                             textured
                                             
                                             region
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Our case differs in two ways: (i) instead of using 2D image gradients, we use p-dimensional vectors (HOF) and (ii) instead of estimating the covariance matrices across the 2D spatial image dimensions, we estimate them across the 1D temporal dimension of the STV. Whereas corner detection tries to characterize how image intensity changes across several directions, coherent motion modeling intends to characterize motion direction changes cross time.

In our case, we have 
                           
                              p
                              −
                              1
                           
                         eigenvalues, due to the L
                        2 embedding, and an intuitive interpretation is the following:

                           
                              
                                 
                                    
                                       
                                          
                                             
                                                λ
                                                1
                                             
                                             ,
                                             …
                                             ,
                                             
                                                λ
                                                
                                                   p
                                                   −
                                                   1
                                                
                                             
                                             ∼
                                             0
                                          
                                       
                                       
                                          
                                             
                                             ⇒
                                             
                                          
                                       
                                       
                                          
                                             Mostly
                                             
                                             zero
                                             
                                             or
                                             
                                             constant
                                             
                                             motion
                                             .
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                λ
                                                1
                                             
                                             
                                             
                                                large
                                                ,
                                             
                                             
                                                λ
                                                2
                                             
                                             ,
                                             …
                                             ,
                                             
                                                λ
                                                
                                                   p
                                                   −
                                                   1
                                                
                                             
                                             ∼
                                             0
                                          
                                       
                                       
                                          ⇒
                                       
                                       
                                          
                                             Slight
                                             
                                             motion
                                             
                                             changes
                                             .
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                λ
                                                1
                                             
                                             ,
                                             …
                                             ,
                                             
                                                λ
                                                
                                                   m
                                                   −
                                                   1
                                                
                                             
                                             
                                             
                                                large
                                                ,
                                             
                                             
                                                λ
                                                m
                                             
                                             ,
                                             …
                                             ,
                                             
                                                λ
                                                
                                                   p
                                                   −
                                                   1
                                                
                                             
                                             ∼
                                             0
                                          
                                       
                                       
                                          ⇒
                                       
                                       
                                          
                                             
                                                Abrupt
                                                
                                                motion
                                                
                                                changes
                                                .
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Albeit the desired characteristics of the proposed low-level feature space are adapted to the problem of detecting unstructured motions, the information coded by the dense spatio-temporal volumes is still quite local. Using information from larger spatio-temporal regions of the image is desirable in order to learn patterns of complex motions, as is usually the case when people interact. We follow on the idea introduced by Boiman et al. [56], of using spatio-temporal compositions of local STVs, and evaluating the new data abnormality by measuring the reconstruction error.

In order to make a decision about new observations in a reasonable time, we rather use the Roshtkhari and Levine’s [24] formulation of the problem. Their work is focussed on two aspects of the problem: (i) how to efficiently store the learned information regarding the spatio-temporal volumes and their relative arrangement and (ii) how to perform fast and accurate inferences on new data. Also, we add a new multiscale procedure inspired from the object detection community. Our approach to the learning process is briefly described in Section 5.1.

Because supervised methods are more commonly used when classifying specific activities, e.g. fighting scenarios, and in order to validate the applicability of an abnormality framework to the problem at hand, we compare its results against the state-of-the-art supervised classification framework proposed by Wang et al. [30]. Improvements of this method were proposed in [31], where features have better robustness to camera motions. Though, as our application context contains only static surveillance cameras, these improvements are not necessary. For this reason, comparison of our method is made with the former version, as described in Section 5.2.

Similarly to [24], we represent the video as compact ensembles of spatio-temporal volumes, and use the same probabilistic framework. The main idea is to learn the typical arrangements of the STVs, in normal situations, and use this information to draw inference about normality of newly observed videos.

In order to achieve near real-time performance, the 3D STVs are grouped into similar sets, forming a codebook. Due to the large number of STVs present in the video, the codebook is learned using an online formulation of the K-means clustering (see [24] Section 3.1.2 for details). Using the learned codewords, an approximation of the posterior probability of assigning each word to a new STV is defined as the normalized assignment weight

                           
                              (5)
                              
                                 
                                    P
                                    
                                       (
                                       c
                                       |
                                       
                                          λ
                                       
                                       )
                                    
                                    =
                                    
                                       1
                                       
                                          
                                             ∑
                                             i
                                          
                                          
                                             1
                                             
                                                D
                                                (
                                                
                                                   c
                                                   i
                                                
                                                ,
                                                
                                                   λ
                                                
                                                )
                                             
                                          
                                       
                                    
                                    ×
                                    
                                       1
                                       
                                          D
                                          (
                                          c
                                          ,
                                          
                                             λ
                                          
                                          )
                                       
                                    
                                    ,
                                 
                              
                           
                        where D(c, 
                           λ
                        ) stands for the Euclidean distance between center of word c and the STV feature vector 
                           λ
                         defined in Eq. (1).

After learning the codebook, a larger video volume containing several STVs is considered: the ensemble of STVs. During training, each ensemble is considered as a set of words arranged in a specific spatio-temporal fashion. This is done by assigning the nearest word to the feature vector to the corresponding STV and computing its relative position with respect to the center of the ensemble. The relative position is then coded in the variable δ. The model is similar to a star-graph with one central node, as represented in Fig. 6
                        .

In addition to the appearance term P(c|
                           λ
                        ), that measures the visual similarity of the STV, the star-graph adds a structural term P(δ|cα, c). Star-graph models have been used in part-based object detection, with the main difference here being that the parts, i.e. the STVs, are fixed in the spatio-temporal video volume, e.g. they correspond to the densely sampled points. The structural term is learned as a probabilistic model of the relative position of the words, by constructing a histogram over δ for every pair of possible words {cα, c}.

During inference, one intends to compute the similarity between newly observed ensembles, the query, and the normal ensembles observed during training. This is solved as a reconstruction problem by computing the distance to the most similar topology learned during training. The set of hypotheses describing the topology of an ensemble can be defined as

                           
                              
                                 
                                    H
                                    =
                                    
                                       ⋃
                                       
                                          
                                             
                                                
                                                   c
                                                   ∈
                                                   C
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      c
                                                      α
                                                   
                                                   ∈
                                                   C
                                                
                                             
                                          
                                       
                                    
                                    
                                       {
                                       
                                          (
                                          δ
                                          ,
                                          
                                             c
                                             α
                                          
                                          ,
                                          c
                                          )
                                       
                                       }
                                    
                                    ,
                                 
                              
                           
                        and the topology of the query as 
                           
                              E
                              =
                              
                                 
                                    {
                                    
                                       
                                          Δ
                                       
                                       k
                                    
                                    ,
                                    
                                       
                                          λ
                                       
                                       α
                                    
                                    ,
                                    
                                       
                                          λ
                                       
                                       k
                                    
                                    }
                                 
                                 
                                    k
                                    =
                                    1
                                 
                                 K
                              
                              ,
                           
                         where 
                           Δ
                        
                        
                           k
                         stands for the relative position of the kth STV. The aim is thus to search all previously observed ensembles and compute the posterior probability of the most similar hypotheses 
                           H
                        . This is done by maximizing the posterior probability

                           
                              (6)
                              
                                 
                                    
                                       max
                                       H
                                    
                                    P
                                    
                                       (
                                       H
                                       |
                                       E
                                       )
                                    
                                    =
                                    
                                       max
                                       
                                          
                                             
                                                
                                                   c
                                                   ∈
                                                   C
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      c
                                                      α
                                                   
                                                   ∈
                                                   C
                                                
                                             
                                          
                                       
                                    
                                    P
                                    
                                       (
                                       δ
                                       ,
                                       
                                          c
                                          α
                                       
                                       ,
                                       c
                                       |
                                       E
                                       )
                                    
                                    ,
                                 
                              
                           
                        with 
                           C
                         representing the learned codebook. Considering independence between each STV, codeword entries and locations, the maximization above can be written as

                           
                              (7)
                              
                                 
                                    
                                       max
                                       
                                          
                                             
                                                
                                                   c
                                                   ∈
                                                   C
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      c
                                                      α
                                                   
                                                   ∈
                                                   C
                                                
                                             
                                          
                                       
                                    
                                    P
                                    
                                       (
                                       δ
                                       ,
                                       
                                          c
                                          α
                                       
                                       ,
                                       c
                                       |
                                       E
                                       )
                                    
                                    =
                                    
                                       max
                                       
                                          
                                             
                                                
                                                   c
                                                   ∈
                                                   C
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      c
                                                      α
                                                   
                                                   ∈
                                                   C
                                                
                                             
                                          
                                       
                                    
                                    
                                       ∏
                                       
                                          k
                                          =
                                          1
                                       
                                       K
                                    
                                    P
                                    
                                       (
                                       
                                          δ
                                          k
                                       
                                       |
                                       
                                          c
                                          α
                                       
                                       ,
                                       c
                                       )
                                    
                                    P
                                    
                                       (
                                       c
                                       |
                                       
                                          
                                             λ
                                          
                                          k
                                       
                                       )
                                    
                                    P
                                    
                                       (
                                       
                                          c
                                          α
                                       
                                       |
                                       
                                          
                                             λ
                                          
                                          α
                                       
                                       )
                                    
                                    .
                                 
                              
                           
                        
                     

See reference [24] for the complete derivation. This procedure is performed for every pixel in each image, resulting in a spatio-temporal similarity map, that can be used to detect spatio-temporal abnormal regions. Those regions are the ones that are not well explained by the learned models, thus exhibiting a low similarity value. In our case, we perform detection by simply thresholding the similarity value, but more complex methods could potentially be used.

Contrary to the method proposed in Ref. [24], that includes several scales during learning by adding STVs of different spatio-temporal sizes to the same ensemble, we opted to learn only one STV scale in each ensemble. Our approach jointly resizes the spatial size of the ensemble and the STVs, i.e. we use a fixed ratio between the size of the ensembles and the STVs. We start by defining the larger scale, Sc
                           0, that depends on the size of the biggest person in the video we are interested in, and then use smaller scales of size 
                              
                                 S
                                 
                                    c
                                    n
                                 
                                 =
                                 S
                                 
                                    c
                                    0
                                 
                                 
                                    (
                                    1
                                    −
                                    n
                                    s
                                    )
                                 
                                 ,
                              
                            with 
                              
                                 n
                                 =
                                 {
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 }
                              
                            and 
                              
                                 s
                                 =
                                 0.2
                              
                           . In our test, we set the reference scale Sc
                           0 to roughly 1/3 of the biggest person size and the STV as 25% of the ensemble size.

This approach is inspired from the pedestrian detection literature, that normally scales the models during test. Also we take advantage from the fact that the optical flow is already computed over several scales [67], meaning that, although each ensemble only includes one STV scale, the information coded inside the STV can potentially come from different scales. The fact that we work with normalized histograms, not pixel values, already accounts for not having exact scales during learning.

During detection, this process turns the method scale-invariant, meaning that the previously trained models can potentially be used with very different scales. This allows, for example, to use pre-trained models in quite different contexts (see Section 7 for examples). Also, it is possible to train the same model with sequences at different scales, potentially from different contexts, by only setting the biggest scale.

The detection process is performed separately for each scale and the fusion done by minimizing the response over the scales

                              
                                 (8)
                                 
                                    
                                       
                                          
                                             Sim
                                          
                                          E
                                       
                                       =
                                       
                                          min
                                          
                                             S
                                             c
                                          
                                       
                                       
                                          (
                                          
                                             max
                                             H
                                          
                                          P
                                          
                                             (
                                             
                                                H
                                                |
                                             
                                             
                                                E
                                                
                                                   S
                                                   c
                                                
                                             
                                             )
                                          
                                          )
                                       
                                       ,
                                    
                                 
                              
                           with 
                              
                                 E
                                 
                                    S
                                    c
                                 
                              
                            standing for the query ensemble topology at scale Sc. We first compute a per-scale similarity, with relation to the topology that better explains the query, and then use the scale that finds less support in the training data, i.e. that is worst explained.


                           Fig. 7
                            shows the influence of scales in the detection. The top image shows the result of the fusion over the scales, and the bottom images the similarity value computed in each scale separately. In this example, although the largest scales have a high similarity value, the smallest scale exhibits a low similarity that is retained during fusion. On the contrary, the problem of mixing different scales in the same ensemble would be that the low similarity of the correct scale would be diluted in the overall topology maximization of Eq. (7), by the inclusion of inappropriate scales. This phenomenon could artificially increase the similarity value, making the detection of the events of interest difficult.

As supervised approach, we use the method proposed by Wang et al. [30]. This method is originally used to classify activities in a multiclass setting and has received much attention due to its state-of-the-art performance. The method tracks a densely sampled grid of points across several frames, and at several spatial scales, by median filtering in a dense optical flow field. Then, several trajectory-aligned local descriptors are computed around the tracked point’s trajectory.

Several descriptors are used: (i) trajectory shape, (ii) histogram of gradients (HOG), (iii) histogram of optical flow (HOF) and (iv) motion boundary histogram (MBH). In order to evaluate the dense trajectories descriptors, a standard bag-of-features approach is applied, by learning a codebook of 4000 words for each descriptor. Finally, a video volume is represented by a set of histograms of visual word occurrences, one for each descriptor. See [30] for details.

As classification procedure, a non-linear SVM with an RBF-χ
                        2 kernel combining the different histogram features in a multi-channel approach [73] is used:

                           
                              
                                 
                                    K
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       ,
                                       
                                          x
                                          j
                                       
                                       )
                                    
                                    =
                                    exp
                                    
                                       (
                                       −
                                       
                                          ∑
                                          c
                                       
                                       
                                          1
                                          
                                             A
                                             c
                                          
                                       
                                       D
                                       
                                          (
                                          
                                             x
                                             i
                                             c
                                          
                                          ,
                                          
                                             x
                                             j
                                             c
                                          
                                          )
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              D
                              (
                              
                                 x
                                 i
                                 c
                              
                              ,
                              
                                 x
                                 j
                                 c
                              
                              )
                           
                         is the χ
                        2 distance between the histograms for video i and j with respect to the cth
                         feature, and Ac
                         a per-channel normalization factor.

In order to use the method in a detection framework, rather than in a classification one, the actions need to be temporally localized. Similarly to [74], we use a set of sliding windows of various sizes 
                           
                              S
                              =
                              {
                              2
                              ,
                              4
                              ,
                              6
                              ,
                              8
                              }
                           
                         s, with a 2-s step. Instead of using a non-maximum suppression (NMS) to delete windows that have an overlap greater than 20% with higher scoring windows [74], we opted to maximize the response across the scales, for each 2-s fragment of video.

Such an approach will tend to produce less miss detections in the middle of well detected events, but somewhat less accurate event starts and ends. Nonetheless, due to the inherent subjectivity of defining a precise beginning and ending in an aggressive behavior, we opted to use a maximum over scales for its better temporal consistence.

Regarding datasets, there exists no available database for specifically testing aggressive event detection in onboard public transportation
                      contexts. Commonly used datasets to perform human activity classification include fighting or other aggressions as one of their classes, such as: Behave
                        2
                     
                     
                        2
                        
                           http://groups.inf.ed.ac.uk/vision/BEHAVEDATA/INTERACTIONS/
                        
                     , CAVIAR
                        3
                     
                     
                        3
                        
                           http://groups.inf.ed.ac.uk/vision/CAVIAR/CAVIARDATA1/
                        
                     , Hollywood2
                        4
                     
                     
                        4
                        
                           http://www.di.ens.fr/~laptev/actions/hollywood2/
                        
                     , SDHA(2010)
                        5
                     
                     
                        5
                        
                           http://cvrc.ece.utexas.edu/SDHA2010/
                        
                      and UCF101
                        6
                     
                     
                        6
                        
                           http://crcv.ucf.edu/data/UCF101.php
                        
                     . Other datasets have been proposed for violence detection in some specific context: MediaEval Violent Scenes
                        7
                     
                     
                        7
                        
                           http://www.multimediaeval.org/mediaeval2014/violence2014/
                        
                     , Hockey fights
                        8
                     
                     
                        8
                        
                           http://visilab.etsii.uclm.es/personas/oscar/FightDetection/index.html
                        
                      and ViolentFlows
                        9
                     
                     
                        9
                        
                           http://www.openu.ac.il/home/hassner/data/violentflows/
                        
                     .

Although all those datasets include aggression motion patterns that could be found on aggressive behaviors inside public transportation, they all comprehend completely different contexts. Hockey fights, ViolentFlows and Hollywood2 datasets comprise pre-segmented sequences of arbitrary view point and severe camera motion, gathered from broadcast hockey matches, YouTube clips and Hollywood movies, respectively. Differently, the purpose of SDHA(2010), Behave and CAVIAR datasets is to make available a test bed for learning and modeling human interactions in a surveillance scenario. They are specially designed to distinguish high-level and precise interactions such as: shake-hands, pointing, pushing, kicking, punching, fighting, groups meeting, chasing, walking, browsing, resting, leaving bags, and so on. Samples are very well structured, with people viewed at a distance, but they don’t really reflect the context we are interested on, i.e. confined view in public transportation rolling stocks with cluttered background and occlusions.

When considering abnormality detection, the sequences available also present different contexts. Typically used sequences comprise the Subway, Train, Belleview and Boat-Sea sequences
                        10
                     
                     
                        10
                        
                           http://www.cse.yorku.ca/vision/research/anomalous-behaviour-data/
                        
                      
                     [57]. The Subway sequence includes, as normal behaviors, people entering and exiting a subway station and, as abnormal behaviors, people moving in the wrong direction or avoiding payment, i.e. jumping hover. In the Train and Boat-Sea sequences, the abnormal detection involves the detection of people and boats, respectively, the normal behavior being the non-existence of objects in the scene. The Belleview sequence involves cars moving through an intersection, the abnormalities being cars entering thoroughfare from left or right. Other used sequences are: the UCSD pedestrian datasets
                        11
                     
                     
                        11
                        
                           http://www.svcl.ucsd.edu/projects/anomaly/dataset.html
                        
                      that deal with the detection of carts, cyclist or skaters in a normally pedestrian zone and the UMN
                        12
                     
                     
                        12
                        
                           http://mha.cs.umn.edu/Movies/Crowd-Activity-All.avi
                        
                     , where the aim is to detect crowded escape scenes. In such sequences, the abnormality detection is related to the direction, intensity and existence of motion patterns, but not violent events.

There exists no dataset available to test the detection of aggressive behaviors in the context of public transportation rolling stocks. For that reason, we created a large dataset comprising sequences from two different sites (Table 1): (i) from an in-lab fake train
                     
                        13
                     
                     
                        13
                        
                           http://projet-degiv.com/projet/
                        
                      and (ii) from a real underground railway line, real train.

Furthermore, in order to test the generalization capabilities of our method, we use 15 different sequences with real life contexts, that comprise a set of 13 sequences with aggressive events, that are publicly available on YouTube and which references we gathered for further comparison, and two other publicly available sequences exhibiting only normal behaviors, from an airport and an underground railway station. We also tested on the Train Station dataset of the University of Amsterdam, a publicly available dataset with violence in a public space for surveillance application with challenging conditions like dynamic background (train passing) and occlusions during group violence.

The structure of the fake train and real train datasets is as follows:

The recorded sequences contain aggressive events of different intensity: (i) hostility, (ii) violence and (iii) fight. The intensity grows from hostility to fight. Hostility comprises simple disputes without too much contact between people, whereas violence adds contact but, contrary to fight, excludes kicks or punches. Although we don’t aim at differentiating between those classes, they build variability to the dataset.

Train Station, fake train and real train have ground truth labeling, temporally segmenting the events.

This dataset comprises 3 different contexts recorded inside the in-lab fake train and viewed by 3 different cameras. Fig. 8
                        
                         shows the layout of the fake train car.

All cameras recorded the scene during the same period of time, at 30 fps, with three contexts: (i) aggressions with low people density, (ii) aggressions with high people density and (iii) normal situations with varying people density. The sequences with aggressive examples are uncut sequences with the aggression events within normal activities, as desired for an event detection setup and opposed to a recognition setup. Table 2 summarizes the structure of the dataset.

Those 9 sequences, 3 contexts and 3 cameras, are further separated into train and validation sets as described in Section 7.1. Examples of frames from all views are presented in Fig. 16, one view per-line. Note that, while there exists some superposition between views, the scale, viewpoint and object overlap changes greatly from one camera to another.

This dataset comprises recordings performed in a real underground railway train viewed from four different cameras placed on two different cars, as shown in Fig. 9.

The dataset comprises thus four uncut sequences with aggression events within normal activities, similarly to the fake train sequence. It is a real sequence with an initial part composed of different human actions while the car is parked at the dock station, followed by a complete train journey. The variety of aggressive events in this dataset is similar to the one in the fake train dataset. Table 3
                         summarizes the structure of the dataset.

All the sequences from this dataset are only used for testing. Not only the size of the images is different from the aforementioned dataset, but is was also recorded with different cameras and at a different frame rate, 13fps in this case with low-quality CCTV cameras. In order to remove noise and video artifacts, a Gaussian blur is applied to each image of this dataset before processing, using a standard deviation of 13. In addition, there are times where the line changes from an underground to an outside track, leading to huge light changes, as shown in the first row of Fig. 20. Fig. 20 also shows some examples of aggressive behaviors.

Although the primary purpose of Train Station dataset
                           14
                        
                        
                           14
                           
                              http://www.gavrila.net/Datasets/Univ__of_Amsterdam_Multi-Cam_P/UvA_Multi-Camera_Multi-Person_/uva_multi-camera_multi-person_.html
                           
                         is precise multi-person tracking with overlapping cameras [75], it contains violent events in a video-surveillance setting with dynamic background and occlusions in group violence. These sequences have been used in the Cassandra project for audio-visual processing. As precise ground truth for beginning and end of each violent event was not available, we annotated the sequences and make them available for other researchers under request. The dataset contains 14 sequences of in total 8543 frames, recorded on a train platform. Between two and five actors enact various situations ranging from persons waiting for a train to fighting hooligans (see Fig. 23). The scenes have dynamic backgrounds with trains passing by and people walking on the train platform. Lighting conditions vary significantly over time. The area of interest is 7.6 × 12 m and is viewed by three overlapping, frame synchronized cameras. Frames have a resolution of 752 × 560 pixels, recorded at 20 fps.

In order to test the generalization and invariance capabilities of our method, we gathered sequences from different contexts. This dataset is composed of 13 small sequences including only aggressive events and two larger sequences without aggressive events.

The sequences from this set are composed of publicly available YouTube videos fragments
                              15
                           
                           
                              15
                              
                                 https://www.youtube.com/watch?v=BT6MzEb97X8, https://www.youtube.com/watch?v=FeCX1IF7RFk, https://www.youtube.com/watch?v=3sIAbDPqx2o, https://www.youtube.com/watch?v=nnw2C_1n8Bg, https://www.youtube.com/watch?v=WhGO4XFGkso, https://www.youtube.com/watch?v=a-Wwmj2FOZA, https://www.youtube.com/watch?v=egeIT-P6Bc4, https://www.youtube.com/watch?v=hfqRlAVijlc, https://www.youtube.com/watch?v=GuaKfU7RQ7w, https://www.youtube.com/watch?v=bpEWYNpeBxM, https://www.youtube.com/watch?v=9TUBZukYTFQ, https://www.youtube.com/watch?v=9jdFIUZc2XA, https://www.youtube.com/watch?v=DF0_r-yfxHM
                              
                            that contain aggressions, mostly recorded by surveillance cameras, but also by mobile phones. The viewpoint, image quality and type of behavior greatly change between sequences, as shown in Fig. 24.

With this rather different and challenging dataset, we want to show that the proposed method is applicable to sequences that exhibit large differences from the training sequences, suggesting its good generalization and invariance capabilities. The dataset is composed of 13 sequences, each one containing one aggressive behavior with duration between 9 and 49 s. Figs. 24 show sample images from those sequences.

This set comprises two sequences from the i-LIDS dataset
                              16
                           
                           
                              16
                              i-LIDS Multiple Camera Tracking and Abandoned Baggage Detection scenarios, CAST, United Kingdom’s Home Office: www.ilids.co.uk and www.eecs.qmul.ac.uk/~andrea/avss2007_d.html
                              
                            that exhibit only normal events, one from an airport and one from an underground railway. Both sequences where recorded at 25 frames per second, with a duration of 2.5 min for the airport sequence (cf. Fig. 25a) and 21.75 min for the underground sequence (cf. Fig. 25b). The underground sequence is recorded with a low-quality CCTV camera and for that reason we pre-process each frame with a Gaussian blur, using a standard deviation of 13, in order to remove noise and video artifacts.


                           Fig. 25 displays examples from each sequence, showing that, although there is no aggressive event, the sequences are fairly complex, exhibiting extended overlapping between people, luggage carrying, train motion, etc.

@&#EXPERIMENTAL RESULTS@&#

This section evaluates the RIMOC feature space with the abnormality inference process described in Section 5.1 on four presented datasets. We start by describing the evaluation metrics used in Section 7.1 and the division of the datasets to train and test the two learning methods in Section 7.2. In Section 7.3, we describe the parameter optimization during learning and quantitatively compare the results for both learning methods on the fake train dataset. Sections 7.4 and 7.5 present the results on the real train and the Train Station datasets, respectively, whereas Sections 7.6 and 7.7 present a qualitative evaluation of the proposed method on real-life context dataset corresponding to real aggressions and normal situations, respectively. At the end of each section, we present samples that illustrate the results obtained when using the proposed RIMOC feature space. Some details about computation times are given in Section 7.8.

To evaluate our event detection method, we use ROC curves plotting the true positive rate versus the false positive rate. We use a per-frame criterion, as normally used in object detection, with the true positive rate as the number of correctly detected frames over the total number of aggression frames and the false positive rate the number of wrongly detected frames over the total number of negative frames.

Because the per-frame ROC curve doesn’t measure the sensitivity of the algorithm with relation to the event we want to detect, we also use a per-event metric on the true positive rate, measuring the rate of correctly detected events with relation to the total number of events. This implies a definition of a detected event. Similarly to [76], we use an event overlap criterion that measures the overlap percentage between the detected event frames and the true event duration to decide if the event has been correctly classified. For example, if an overlap criterion of 30% is used, at least 30% of the frames of the event (from the ground truth) have to be detected to consider a true event detection.

For the false positive rate, we cannot measure the per-event rate because true negative events are not countable in uncut videos or online video streams. For this reason, we use ROC curves mixing the per-event true detection with the per-frame false detection rate for a specific event overlap criterion. We could represent the results as per-event precision-recall curves in both axes, as proposed by [76], but this would imply the definition of a false positive event by counting, for example, the number of consecutive frames detected. As both these definitions and the precision-recall curves are more relevant at a system level, we prefer to analyze the low-level performances using ROC curves.

Our aim is to evaluate both the recognition performances and the generalization capabilities of the methods. For that reason, we use a fixed training set, and never re-train the methods nor change the parameters when testing in different contexts. We decided to evaluate the performances using this challenging scenario in order to validate that our abnormality procedure compares positively with the supervised one and that the proposed feature space exhibits the desired invariance properties. Furthermore, when using the supervised approach, re-training the models in each context is not a realistic scenario due to the necessity of manually labeling the positive examples.

As is discussed in Section 8, the abnormality learning paradigm could easily be re-trained for each specific context, as it only requires negative examples, easily available, which would expectedly improve the accuracy. Also, the abnormality procedure is inherently online, which allows a continuous model adaptation, as suggested in [24].

The datasets are thus used in the following manner:

                           
                              
                                 Abnormality training comprises all the normal sequences of the fake train dataset (see Section 6.1), i.e. 70, 290 frames extracted from 0.66 h of video without aggressive events.


                                 Supervised training comprises all the normal sequences plus half of each sequence exhibiting aggressive events of the fake train dataset (see Section 6.1). This adds to a total of 566, 220 frames extracted from 5.7 h of video with 116 aggressive events.


                                 Validation comprises the half of the sequences exhibiting aggressive events that are not used in the supervised training, from the fake train dataset (see Section 6.1), i.e. 542, 790 frames extracted from around 5 h of video with 116 aggressive events.


                                 Test comprises all sequences from the real train and Train Station datasets (see Sections 6.2 and 6.3), adding respectively, 327, 600 and 25, 629 frames extracted from 7 h and 21 min of video with 31 and 33 aggressive events.


                                 Qualitative test comprises all the sequences from the real life contexts, corresponding to both the real aggressions and the normal events, with a total of 45, 550 frames extracted from 0.4 hours of video and comprising 13 aggressive events.

We use this dataset to optimize the learning parameters of the methods and as a first comparison of both learning paradigms.

When using spatio-temporal compositions, several parameters have to be chosen, e.g. the size of the spatio-temporal volumes, the size of the ensembles and the size of the codebook. When using our feature, we fixed the spatial size of the patches as 25% of the spatial size of the ensembles. We use 100 pixels as the spatial size of the ensembles for the first scale, with a reference of 350 pixels for the size of a standing person, and 40 frames for the temporal size, at 15 fps. This means that we use 25 pixels for the spatial size of the patches for the first scale and we also set at 25 pixels the spatial step for the patches, i.e. no spatial overlap between patches. We use 3 scales in all the tests.

Those values were defined empirically, considering the sizes and scale of the training images, the problem at hand and the real-time requisites of our system. The other parameters were optimized experimentally, namely: (i) the size of the codebook, (ii) the temporal size of the patches and (iii) the number of bins used in the HOF computation step.

From our tests, we didn’t gain much by using more than 10 words. In the other hand, the performance decreases for lower values, with around a 2% decrease in the detection rate when using 7 words instead of 10. For this reason, we chose to use 10 words for all tests. This result is in line with the results presented in [24], that show that more than 20 words do not increase the accuracy of their method. Despite the arguably more complex scenes of our scenarios, the inferior number of words needed in our case can be explained by the invariance properties of our space, as discussed in Section 4.2.

As claimed in Section 4.1 there exists an optimal value for τ, the temporal size of the patches, that maximizes the separation between both classes. We optimize this value experimentally, by computing the ROC curves for different values and choosing the one that achieves better detection rate. Fig. 10
                            exemplifies the optimization of this value, obtained by fixing the false alarm rate at 0.01 and varying the temporal size from 20 to 4 frames. We consistently obtained the best value at 12 frames even for different values of the false alarm rate.

The number of bins used to compute the HOF vectors is optimized in the same way. Fig. 11
                            shows the ROC curves when using HOF vectors computed with 4, 8, 12 and 16 bins. From this result, it is clear that better detection rates are achieved with fewer bins in the histogram, i.e. 4.

From those tests, we fixed the following parameters for our feature as: 10 words, 
                              
                                 τ
                                 =
                                 12
                              
                            and 4-bin HOF vectors, and use them throughout all the tests. Furthermore, except when stated otherwise, all detection examples are obtained with a fixed threshold
                              17
                           
                           
                              17
                              This value corresponds to the maximization of the negative log in Eq. (7)
                              
                            of 770. For the original method [24] we use as sizes, 
                              
                                 
                                    V
                                    s
                                 
                                 =
                                 
                                    (
                                    7
                                    ,
                                    7
                                    ,
                                    4
                                    )
                                 
                              
                            for the spatio-temporal volumes, 
                              
                                 
                                    E
                                    s
                                 
                                 =
                                 
                                    (
                                    50
                                    ,
                                    50
                                    ,
                                    50
                                    )
                                 
                              
                            for the ensembles and 13 words to create the codebook, as suggested in the paper.


                           Fig. 12
                            compares the ROC curves obtained for the feature used in [24] and our feature, both per-frame and per-event. From those figures, one can see that the difference, especially at the low false alarm rate region, is quite large, with our feature achieving a per-frame detection rate of 50%, against around 10% for the original feature at a 0.005 false positive rate. For the same points, if we compare the per-event detection rate, at a 10% overlap requirement, we observe a difference of more than 50% in the true recognition rate, i.e. around 90% for our feature, against less than 40% for the original feature.

The supervised learning described in 5.2 is performed using pre-segmented volumes of video of sizes {4, 6} s, with the correct labels, for the supervised training set described in Section 7.1. During the test, we allow the method to evaluate more temporal windows as this introduces flexibility in the classification. In order to understand the influence of the temporal integration during classification, we compare the results obtained when using different sizes for the temporal sliding windows.

The per-frame results plotted in Fig. 13 show that classifying with more temporal windows always improves the results for the supervised approach. This is related to the difference in duration of the different aggressions, so that with more temporal scales one can more accurately segment the aggressions. Although this improves the per-frame results, due to a better temporal segmentation, it also introduces a bigger delay in the response, equal to the bigger scale, that is 8 s in this case, and an increase in the computational burden by adding more features to create and classify.

Note that the abnormality framework only uses one temporal scale, around 2.7 s (i.e. 40 frames), with its per-frame results somewhere in between the ones obtained with only 2 and 4 s for the supervised approach. For the supervised approach, and because it achieves better results, we choose to use all four temporal scales in all tests from now on. Fig. 14 compares the supervised and abnormality approaches with relation to the event detection, with 10% overlap requirement. Although the supervised clearly outperforms the abnormality method when regarding the per-frame detection, that doesn’t translate into a better per-event detection. This suggests that the supervised approach achieves a clearly better temporal segmentation of the aggressive events but does not detect more events if the overlap requirement is small enough.

In order to better understand
                            this behavior, Fig. 15
                            shows the evolution of the per-event detection rate when varying the overlap requirement, for a false positive rate fixed at 0.005. Note that, if using a different value for the false positive rate, one would achieve different results. For example, for values between 0.001 and 0.005, the difference between the algorithms is larger in favor of the abnormality method. In either case, one finds a value for the overlap requirement that leads to a better performance of the supervised approach for values greater that this value and the inverse for smaller values. At a 0.005 false positive rate, we achieve similar results for both methods for an overlap of 15% with the abnormality method performances decreasing faster than the supervised approach, when the overlap requirement increases.

In a scenario where we intend to quickly raise an alarm or send a signal to a higher level system, the abnormality framework appears appropriate. This could be done, for example by simply defining a minimum number of detected frames needed or by also analyzing the spatio-temporal extent of the abnormality classification. A discussion of such enhancements is presented in Section 8.


                           Fig. 16
                            shows sample results on the validation set for the abnormality framework. Abnormality values are superimposed on top of the original image by using a color map from green to red representing lower to higher values. When the value is bigger than the threshold defined in Section 7.3.1, a white ellipse is displayed to signal a detection. The detection is simply performed by checking if there is a pixel greater than the threshold. This threshold corresponds to a false positive rate of around 0.0025 in the ROC curves of Figs. 13 and 14 (represented by a blue circle).

The results show that the method is able to both detect aggressions close and far away from the camera. See Fig. 16 rows 6 and 7 for close events and rows 1 and 2 for distant events. Also, aggressions of different intensities are correctly detected: compare rows 4 and 6 exhibiting low intensity, to rows 3 and 7, that display intense contact with large motion displacements.

Using both the abnormality and the supervised frameworks previously trained, we compare the results on the real train test set. As done with the validation set, we compare the results both per-frame and per-event. Fig. 17
                         compares the per-frame results and shows that the supervised approach is superior. When analyzing the per-event results in Fig. 18
                        , similarly to the results obtained with the validation set, the abnormality framework becomes better, in this case for a false positive rate greater than 0.003. Note that this result is obtained at an overlap requirement of 10%. In these conditions, as we change our working point by increasing the false positive rate, the abnormality framework exhibits a larger increase in the recognition rate than the supervised framework.

The influence of the overlap requirement is shown in Fig. 19
                         where the detection rate is a function of the overlap requirement for two false positive rate values, 0.005 and 0.05. As with the validation set, the abnormality framework achieves better recognition for smaller overlap requirement values. In addition, when working at a 0.05 false positive rate, the abnormality framework is better until a value of 45% for the overlap requirement. In our opinion, although working at a high false positive rate is not desirable, better results could be achieved by complementing the abnormality framework with a higher level system able to improve the simplistic detection rule used here. Such discussions are provided in Section 8. Notice that the dataset reflects real-life conditions and is really challenging. In addition, by directly using the models (codebook, statistic models for inference and parameters) trained on the fake train dataset only, we show the ability of generalization of the method.


                        Fig. 20
                         shows sample results of the abnormality framework when applied to the real train test set. The correspondence between the used thresholds and the working point for this dataset is represented by a blue circle in Figs. 17 and 18. Although the threshold is the same value for all tests, for these sequences it corresponds to a working point with a smaller recognition rate when compared to the validation set, 42% against 90.4%. This is due to the large differences between the sequences. For example, the fact that the image noise is larger, that the background is dynamic (due to the train motion) and that the illumination varies, has a big influence on the results.

The first row of Fig. 20 shows the large illumination change when the train exits the underground tunnel to enter aerial railway, and a correct no detection of the method. The method is also stable under the dynamic background with shadow motion and train crossing. Rows 2 to 5 show examples of detection during aggressions. Videos with results are shown on our website.
                           18
                        
                        
                           18
                           
                              http://www.kalisteo.eu/en/demonstration_video.htm
                           
                         
                        Section 8 provides a discussion on the influence of the threshold on the detection, the main difficulties and the possibility of continuous learning to deal with such problems.

As in [10], all aggressive motion events were considered (excited supporters harassing people, lively argument, fighting and aggression on machine…). Regarding the per-frame performances, the detector has an accuracy of 82.2% on the whole dataset. Figs. 21
                         and 22
                         show the ROC curves per frame and per event, respectively. This last figure shows that for a false positive rate of 0.13%, around 80% of the events are detected. Notice again that these very good results (see examples in Fig. 23) have been obtained by directly applying the models trained on the fake train dataset, which shows the ability of generalization of the method in relation to the datasets.

Using the same model trained on the fake train dataset, 12 out of the 13 aggressive events present in the real aggressions set were detected. In order to perform this test, we chose a suitable scale, i.e. the size of the larger scale of the detector (maximum human height in the foreground), for each one of the sequences, and tested with the same parameters as before. In Fig. 24 is shown sample images of the obtained results.

The only not detected aggression is presented in the 12th line of Fig. 24, where the method is not able to detect the kick. One reason for this is that it is a very quick motion that doesn’t allow the system to respond. Despite the poor quality of the images (many artifacts), varied viewpoints and distances to the aggression, and different motion speeds, the method shows very good ability for generalization. See Section 8 for a discussion of the characteristics of the method and possible enhancements. See full video results on our website.

When testing on the two sequences of the normal situations set, we intend to be able to not detect anything as the sequences only comprise normal motions. For the airport sequence, the method doesn’t detect any aggression. As shown in Fig. 25
                        
                        
                        a, although the motions present in this sequence are complex and with many people crossing, the learned models are resistant to these types of difficulties.

For the underground station sequence, we obtained a false positive rate of 0.026% with 5 false positives during the 21 min (18,900 frames) of the challenging sequence containing trains entering the stations, high people density with lots of crossing and inter-person occlusions, and people running to catch their train. Correct true negatives are shown in Fig. 25b, where the models are able to ignore complex people crossing each other. Notice again that no training phase was performed on the video. The same models trained on the fake train dataset were directly applied. See video results on our website.

The algorithm was implemented in C++ on a PC and runs in real time (15 fps on a QuadCore processor Intel CoreTM i7-4710HQ @ 2.50GHz). On a typical 640 × 480 image, dense optical flow is computed with Farnebäck’s method in around 53 ms by frame. An average time of 8.7 ms per frame is needed for the feature computation and codeword assignment whereas inference at 3 scales with likelihood map fusion is performed in 4.3 ms in average. Total processing is achieved at 15 fps (see Table 4).

The results obtained by embedding the proposed feature space RIMOC within an abnormality detection framework are very promising, exhibiting several advantages when compared to a supervised learning methodology. Firstly, it provides detection results with a very quick response time. While the delay of the supervised method is 3/4 of the largest scale, i.e. 6 s (due to the 2-s step and the half of the largest scale), the delay of the abnormality framework is half the temporal size of the ensemble, i.e. around 1.3 s. Secondly, it naturally provides a spatial segmentation of the events of interest. Such segmentation is of utmost importance specially when considering a complete surveillance system, because it allows the system to trigger further processing on the detected area, e.g. allowing the posterior tracking of the implicated people or driving attention of a pan-tilt-zoom camera. Thirdly, there is no need to label positive examples for training phase, making the system much easier and cheaper to train. Furthermore, the rarity of the aggressive events makes it even more difficult the gathering of the data needed to train a supervised method. Finally, the fact that the method is able to achieve a good trade-off between true detections and false positives in very complex environments, even without learning them specifically, is specially promising.

There are still some challenging cases, causing false positives. Fig. 26a shows a false detection when one of the people dances while listening to music. This is the typical case in which a low-level method cannot interpret the detected erratic motion. Higher-level processing or other modality (as audio) are then necessary to confirm or not the detection. Other types of false positives can arise when the scale used is not adapted to the true scale of the people in image, as shown in Fig. 26b, from the normal situations set. In this case, another range of scales should have been applied, taking into account the variation of apparent human heights in the image. One false positive on real train is presented in Fig. 26c and can be explained by the fact that train crossing was not learned during training step in laboratory and parameters were kept unchanged in spite of the image poor-quality (many artifacts), the heavy contrasts caused by sunlight and shadow and the irregular framerate due to acquisition.

Some of the problems described above are of different nature and the way to tackle them should also be different. For example, re-training and/or complementing training on the actual context of use should solve problems such as the train crossing. Also, as suggested in [24], it is possible to continue adapting the models on-line, during detection, providing a continuous adaptation to the environment changes. Using some kind of weak calibration, in order to drive the scale choice in each image region, could solve problems such as the one present in Fig. 26b. People crossing each other is inherently difficult, but the fact that the scales used are too big for the size of the people present in the image causes the low-level feature to wrongly code the motion as incoherent. Because our method does not include any filtering step, the inclusion of a post-processing of the abnormality computation, as the use of a median filtering as in [24], could eliminate some false positives. Furthermore, the use of a higher-level method, that analyzes the outputted spatio-temporal likelihood map, should produce even more coherent detections. Also, higher-level analysis of trajectories could help solve the “aperture problem” of analyzing motions only at a low-level.

@&#CONCLUSION@&#

This work tackles the problem of detecting realistic aggressions in onboard video surveillance setups, where occlusions as well as dynamic and cluttered background make the scene so complex to analyze. We defined a novel and compact low-level feature, RIMOC, that codes the structuredness of the observed elementary motions. By using a codeword learning approach and an abnormal event detection framework that learns statistical space-time motion configurations, RIMOC shows good ability to discriminate structured from non-structured motions. Due to the absence of public onboard violence datasets, we built a structured database comprising more than 18 hours of video and 276 aggressions, both from real and lab recorded sequences and from different contexts, some with poor-quality CCTV cameras, and some with real-life violence. Extensive experimental tests showed that the use of the proposed method can produce results comparable to a state-of-the-art supervised approach, with added training and computation simplicity. Indeed, examples of violence are not necessary during the training phase of the proposed method. Experiments demonstrated that the method had a good ability to generalize the trained model, as the same model trained in laboratory was successfully applied on many different contexts. Finally, the proposed method is real-time and localizes aggressive motions in time and space. As future work, a surveillance system could take the good candidates to violence detection given by this method as a first attentional module, and then confirm the alerts with higher-level methods.

@&#ACKNOWLEDGMENTS@&#

This work was supported by the research projects Secur-ED (http://www.secur-ed.eu/ funded by European Commission FP7 program – GA no.261605) and DéGIV (http://projet-degiv.com/ co-funded by French FUI-BPI France and Conseil Général de l’Essonne). In addition, we would like to thank Dr. Bertrand Luvison for the very useful discussions, as well as Damien Raffard, Rhalem Zouaoui and Stéphanie Joudrier.

@&#REFERENCES@&#

