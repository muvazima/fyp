@&#MAIN-TITLE@&#Data-driven approach for assessing utility of medical tests using electronic medical records

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We define the information content of medical tests using entropic measures.


                        
                        
                           
                           Data are from the electronic medical record of patients.


                        
                        
                           
                           Blood test results are coupled to second surgery within 30days.


                        
                        
                           
                           Results correspond well to clinical knowledge.


                        
                        
                           
                           Tests with a substantial negative impact will benefit from having the information content specified.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Electronic medical records

Medical informatics

Colorectal surgery

Information theory

Pattern recognition

@&#ABSTRACT@&#


               
               
                  Objective
                  To precisely define the utility of tests in a clinical pathway through data-driven analysis of the electronic medical record (EMR).
               
               
                  Materials and methods
                  The information content was defined in terms of the entropy of the expected value of the test related to a given outcome. A kernel density classifier was used to estimate the necessary distributions. To validate the method, we used data from the EMR of the gastrointestinal department at a university hospital. Blood tests from patients undergoing surgery for gastrointestinal surgery were analyzed with respect to second surgery within 30days of the index surgery.
               
               
                  Results
                  The information content is clearly reflected in the patient pathway for certain combinations of tests and outcomes. C-reactive protein tests coupled to anastomosis leakage, a severe complication show a clear pattern of information gain through the patient trajectory, where the greatest gain from the test is 3–4days post index surgery.
               
               
                  Discussion
                  We have defined the information content in a data-driven and information theoretic way such that the utility of a test can be precisely defined. The results reflect clinical knowledge. In the case we used the tests carry little negative impact. The general approach can be expanded to cases that carry a substantial negative impact, such as in certain radiological techniques.
               
            

@&#INTRODUCTION@&#

At any point in the patient pathway, the health care provider can perform a test to increase their knowledge about the state of the patient. The nature of a test can vary widely, from simple examinations to expensive or burdensome patient scans. Examples of tests include blood samples, radiology, invasive procedures, echo-cardiography, endoscopies, and many more. The result of a test is expected to help the physician in the decision-making process on whether to provide a therapy, or perform a procedure. The decision-making process is inherently associated with uncertainties and risks. It is expected that the test results assist the physician in reducing such uncertainties. Physical examination of the patient reveals complimentary information about the patient, such as blood pressure, pulse, temperature, pain, color of skin. Together with prior medical history these present the essential information for making clinical decisions [1]. Often the decision to test is driven by routines, clinical knowledge and suspected issues with the patient. A quantitative way to decide whether or not to test is to employ a Bayesian approach and estimate possible posterior probabilities in the range of possible test results. If there is no posterior that would make the practitioner change their course of action, the test should not be done [2]. Both establishing a prior and compute posteriors accordingly are difficult tasks and render such rules difficult to employ in clinical practice.

Due to the increased use of electronic medical records (EMRs) [3], and the resulting availability of observational data, we believe one could find data-driven methodologies to define the information content and utility of medical tests. We have devised an approach and conducted a set of experiments to validate this hypothesis. Our experiments conducted in the context of tests and outcomes related to surgery show that one can effectively measure the information content of various medical tests by employing data-driven methods on longitudinal EMR data of patients. By utilizing the large volumes of data in the EMR, providers can immediately get support that improves the quality of care based on their own data [4,5].

New information about a patient is gathered at a cost of conducting tests. The benefit of a test is in terms of the added information it provides about the patient. This is the information content of the test. Here information is used in the sense of the physician gathering more data about the patient, thereby learning more about the patient. There is also a distinct but related mathematical concept of information, phrased in terms of entropic measures that quantifies the surprise of learning an outcome. For an outcome with probability p, the surprise is defined as −log
                     p. Thus, for a distribution p(x) for discrete outcomes x, the expected surprise, also known as Shannon entropy or simply entropy, is S(X)=−∑
                        x
                     
                     
                     p(x)log
                     p(x) with the convention that 0log0=0 [6]. The base of the logarithm is arbitrary, and we use base 2 throughout such that the entropy is measured in bits. This precisely quantifies the expected surprise of learning a piece of information. If the information is known prior to testing (p(x)=1 for one value of x, zero else), entropy is zero. The entropy is maximized when each outcome is equally likely.

We define the utility of a test as the increase in information content over the cost of the test. Cost can be in terms of economy, time, risk or discomfort to the patient, with a corresponding range of quantifications. Also, there may be secondary costs, e.g., in the case of false positives leading to unnecessary follow-ups or patient distress. Our goal is to precisely define the information gain due to a test and measure this with the information available in the EMR. Having a precise quantification of the expected information content of a test at a given time in a patient’s pathway can be useful in designing data-driven decision support systems for use in clinical practice. Thereby one accomplishes using real-world data to continuously improve clinical practice [7].

The use of information theory in medical testing dates back to 1973 [8]. In 1981 Diamond et al. used information theoretic ideas to quantify the value of ECG tests [9]. Rifkin used an approach that utilized the clinical value of multivalued test results rather than just binary ones [10]. Both approaches relied on theoretical justification or known clinical properties of tests such as their specificity and sensitivity. The information theoretic approaches were compared to the newly introduced concept of expected value of perfect information 
                        [11], for which an efficient computational algorithm was recently developed by Strong and Oakley [12]. A simple analysis of relative entropy as a measure of the information in diagnostic tests was done by Benish [13], and more recently McCabe et al. used information gain to develop risk scores [14]. Other approaches to using information theory to quantify the information content of tests has been suggested several times [15–19]. Our contribution is providing a data-driven methodology for quantifying the information gain and the utility of medical tests at different stages in a patient’s clinical pathway.

There is a general similarity between this problem and experimental design. In experimental design, usually there is an underlying model, and the task is to choose the experiment that provides the most information at optimal cost. The model has a certain parameter set, θ and a choice has to be made as to what experiment 
                           
                              ξ
                              
                              ∈
                              
                              Ξ
                           
                         needs to be conducted to gain the most amount of knowledge about the parameters, where 
                           
                              Ξ
                           
                         denotes a set of available experiments. Mutual information, defined as
                           
                              
                                 I
                                 (
                                 X
                                 ,
                                 Θ
                                 )
                                 =
                                 S
                                 (
                                 Θ
                                 )
                                 -
                                 
                                    
                                       S
                                    
                                    
                                       X
                                    
                                 
                                 (
                                 Θ
                                 |
                                 X
                                 )
                                 ,
                              
                           
                        has been suggested as a useful measure for correlating test results to outcomes [20,21]. Liepe et al. argue that mutual information is the best measure for determining the optimal experiment [22]. Here, 
                           
                              Θ
                           
                         is the random variable associated with the model parameter θ, X is the output data from a particular experiment and 
                           
                              S
                              (
                              Θ
                              )
                           
                         is the entropy of the distribution π(θ). A definition of what it means for an experiment to be preferable to another experiment was made clear in statistical terms by Goel and Ginebra [23].

Notably, by using the information theoretic approach, the test result X can be of any type, and the information content of various tests would be comparable in terms of the entropy irrespective of the original parameters. It is acknowledged that improving parameter estimates do not necessarily improve model predictions, and an alternative is to maximize the mutual information between predictions and data rather than parameters and data.

In this chapter we define the methods we use to quantify the information gain in a test at a given point in a patient’s pathway. The methods are influenced by other techniques [19,22–24], but have been adapted to the context of medical tests. To our knowledge, the framework presented here has not been used in this context before. In order to quantify the information gain for a given medical test, a patient model is needed. The patient model is described by a set of parameters θ. Prior information about these parameters, are encoded in the prior distributions π(θ). The purpose or complexity of the patient model is highly problem specific. In the simplest example it might be a single, binary parameter where 
                           
                              θ
                              
                              ∈
                              
                              {
                              0
                              ,
                              
                              1
                              }
                           
                         depends on a binary outcome, or the patient model may be arbitrarily complex with a large number of heterogeneous parameters. Obviously, for any chosen patient model, there must be sufficient data available to support the modeling framework.

At any point in time for a given patient model, and based on the prior distribution π(θ) the prior entropy is 
                           
                              S
                              (
                              Θ
                              )
                           
                        . When a certain test ξ is chosen, the result of the test y has a distribution 
                           
                              π
                              (
                              Y
                              |
                              Θ
                              ,
                              ξ
                              )
                           
                        . Using Bayes’ rule the posterior is
                           
                              
                                 π
                                 (
                                 Θ
                                 |
                                 Y
                                 ,
                                 ξ
                                 )
                                 =
                                 
                                    
                                       π
                                       (
                                       Y
                                       |
                                       Θ
                                       ,
                                       ξ
                                       )
                                       π
                                       (
                                       Θ
                                       )
                                    
                                    
                                       π
                                       (
                                       Y
                                       |
                                       ξ
                                       )
                                    
                                 
                                 .
                              
                           
                        
                     

Thus we are able to compute the expected information content. When little is known about the particular patient, the prior can be computed using population averages, or using a flat or uninformative prior. As we perform tests and gain more knowledge about the patient, the prior is updated accordingly. To learn how a test result couples to an outcome, we need to employ a decision rule. Using retrospective data, one can compute the decision rule by observing (θ,
                        y) for every patient, and use a classifier (in case θ takes discrete values) or regression model (when θ takes continuous values), such that the classifier or regression model yields 
                           
                              π
                              (
                              Θ
                              |
                              Y
                              ,
                              ξ
                              )
                           
                        . Another option is to inform the decision rule by clinical knowledge, where the domain expert gives the distributions for each test, which corresponds to a knowledge-driven approach. Notably, the posterior can serve as a prior for subsequent tests, though care should be taken if the tests are not independent. E.g., if the tests are performed using the same or related method (e.g., two computed tomography tests), reasons for false positives might be recurrent in the second test.

In the following we will use data-driven decision rules, where we have access to some retrospective test results from the EMR and the outcome associated with those results. We will denote the decision rule as a classifier, but it is understood that if θ takes continuous values, it will be a regression model [25].

We define the information gain in a similar fashion to how information gain was defined in terms of choosing the best experiment as suggested by Sebastiani and Wynn [24]. For a certain test ξ the distribution of the parameters given test results π(Θ|Y) are computed. In the following we omit explicitly referring the test ξ as this is fixed throughout. The expected entropy of this distribution over the possible values of the test is [24]:
                           
                              (1)
                              
                                 Λ
                                 =
                                 
                                    
                                       E
                                    
                                    
                                       Y
                                    
                                 
                                 [
                                 S
                                 (
                                 Θ
                                 |
                                 Y
                                 )
                                 ]
                                 .
                              
                           
                        
                     

The information gain is defined as how the entropy is reduced compared to the prior,
                           
                              (2)
                              
                                 Δ
                                 (
                                 ξ
                                 )
                                 =
                                 S
                                 (
                                 Θ
                                 )
                                 -
                                 Λ
                                 ,
                              
                           
                        which can be computed once the prior 
                           
                              π
                              (
                              Θ
                              )
                           
                         and classifier yielding 
                           
                              π
                              (
                              Θ
                              |
                              Y
                              )
                           
                         are provided. After a test is performed with result Y
                        =
                        y′, the prior changes to take that value into account 
                           
                              π
                              (
                              Θ
                              )
                              ↦
                              π
                              (
                              Θ
                              |
                              Y
                              =
                              
                                 
                                    y
                                 
                                 
                                    ′
                                 
                              
                              )
                           
                         according to the classifier. Thus, there is a patient progression under which the estimates of the parameter updates, and the optimal test may be updated accordingly. The patient progression is divided into P
                        >1 phases divided by times t
                        =
                        t
                        0, t
                        1, ⋯, tP
                        . Once a phase p has progressed, the classifier is run on the data from that phase, giving 
                           
                              
                                 
                                    π
                                 
                                 
                                    k
                                    ,
                                    p
                                 
                              
                              (
                              Θ
                              |
                              Y
                              )
                           
                         for each patient k. This way, we obtain a value for the delta information for a patient k in phase p
                        
                           
                              (3)
                              
                                 
                                    
                                       Δ
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 
                                    
                                       t
                                    
                                    
                                       p
                                    
                                 
                                 )
                                 =
                                 S
                                 
                                    
                                       
                                          
                                             
                                                π
                                             
                                             
                                                k
                                                ,
                                                p
                                             
                                          
                                          (
                                          Θ
                                          |
                                          Y
                                          )
                                       
                                    
                                 
                                 -
                                 Λ
                                 .
                              
                           
                        
                     

The delta information for individual patients can finally be averaged to obtain an expected information gain in each of the disease phases, 
                           
                              
                                 
                                    Δ
                                 
                                 
                                    ¯
                                 
                              
                              (
                              
                                 
                                    t
                                 
                                 
                                    p
                                 
                              
                              )
                           
                        .

In our experiments we focus on assessing the utility of blood test in the context of colorectal cancer (CRC) surgery. For patients with colorectal cancer, surgery is the only curative option, and is associated with a high rate of complications and readmissions. Up to 20% of surgeries result in complications, many of which are severe such as infection or anastomosis leakage that require a second surgery [26]. Diagnosing a complication or adverse events at an early stage is preferred as it improves the patient outcome, i.e., it decreases pain, discomfort, length of stay, readmission rate and potentially improves quality of life [27,28]. There are three main reasons for a second surgery after the primary CRC surgery. Firstly, an anastomosis leakage or abscess formation is a severe complication associated with poor outcome. Secondly, superficial wound infections remains the single most important reason for a second surgery, and imposes significant extra cost for the patients and for health care. Thirdly, post surgical bleeding is a severe complication, but is usually low in numbers. Overall, surgical site infection and wound infection are the most common causes for a second surgery. We focus particularly on anastomosis leakage and post surgical bleeding (the latter defined as code JWE: Reoperation for deep haemorrhage in gastroenterological surgery according to the NOMESCO classification used in the hospital) since these are potentially fatal complications and it is particularly important to detect these as early as possible [27,28].

The data were extracted from the QUAKE database, which contains data from the gastrointestinal department at the University Hospital of North Norway 2004–2014. The data source contains structured and unstructured data on patients undergoing surgery at the department in 2004–2012 [29]. We defined a cohort of 1083 patients who underwent CRC surgery and extracted all laboratory test results in the days 5 preoperatively through 20 postoperatively. A summary of the cohort is shown in Table 1
                        .

We present results for four different tests; Hemoglobin, Sodium, C-reactive protein (CRP) and Albumin. These tests were chosen based on that out of the test results considered potentially relevant for the purpose, these had the greatest numbers in the database. Blood tests carry little negative impact, and their negative impact is equal across the different tests. Hence cost is not included in the model.


                        CRP (C-reactive protein) is used mainly as a marker of inflammation, and increases during an inflammatory response. It will always increase in the postoperative phase due to the surgical trauma, but will further increase due to an postoperative abscess such as an anastomosis leakage [30,31]. Albumin is the main protein of human blood plasma. Low albumin may be caused by a number of conditions while high albumin is almost always caused by dehydration. Sodium is an essential nutrient that regulates blood volume, blood pressure, osmotic equilibrium and pH. Unusually low or high sodium levels may be caused by physical factors associated with ageing or illnesses involving vomiting or diarrhea. In the postsurgical phase its associated with loss of body fluids. Hemoglobin decrease with or without an absolute decrease of red blood cells, leads to symptoms of anemia. In the postsurgical phase, disturbances in the hemoglobin level are usually caused by either bleeding or dehydration.

In our case, the test results are numerical values yielded by the tests, and the patient model has just one binary parameter reflecting whether the patient is readmitted or not. For the classifier we used a kernel density classification, i.e.,
                           
                              
                                 π
                                 (
                                 θ
                                 =
                                 i
                                 |
                                 Y
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             μ
                                          
                                          
                                             i
                                          
                                       
                                       
                                       
                                          
                                             
                                                
                                                   f
                                                
                                                
                                                   ˆ
                                                
                                             
                                          
                                          
                                             h
                                          
                                       
                                       (
                                       y
                                       |
                                       θ
                                       =
                                       i
                                       )
                                    
                                    
                                       
                                          
                                             μ
                                          
                                          
                                             0
                                          
                                       
                                       
                                       
                                          
                                             
                                                
                                                   f
                                                
                                                
                                                   ˆ
                                                
                                             
                                          
                                          
                                             h
                                          
                                       
                                       (
                                       y
                                       |
                                       θ
                                       =
                                       0
                                       )
                                       +
                                       
                                          
                                             μ
                                          
                                          
                                             1
                                          
                                       
                                       
                                       
                                          
                                             
                                                
                                                   f
                                                
                                                
                                                   ˆ
                                                
                                             
                                          
                                          
                                             h
                                          
                                       
                                       (
                                       y
                                       |
                                       θ
                                       =
                                       1
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    
                                       
                                          f
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    h
                                 
                              
                              (
                              y
                              |
                              θ
                              =
                              i
                              )
                              =
                              
                                 
                                    ∑
                                 
                                 
                                    k
                                 
                              
                              
                                 
                                    K
                                 
                                 
                                    h
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             y
                                          
                                          
                                             k
                                          
                                          
                                             (
                                             i
                                             )
                                          
                                       
                                       -
                                       y
                                    
                                 
                              
                           
                         is the kernel density estimate for class i with a bandwidth parameter h when 
                           
                              
                                 
                                    y
                                 
                                 
                                    k
                                 
                                 
                                    (
                                    i
                                    )
                                 
                              
                           
                         are the observed data points belonging to class i 
                        [25]. μi
                         are the prior probabilities for each class, which we set uniformly to μi
                        
                        =1/2. The only parameter choice for our classifier is the bandwidth h of the selection rule, and the type of kernel [32]. Several solutions exists for data-driven bandwidth selection such as the Sheather Jones plug-in method [33] or even multiscale approaches [34,35]. However, in our case it is unlikely that there are relevant features on small scales, so we have chosen a value of h that significantly oversmoothes the density, while not smoothing away the entire signal. For this purpose, we heuristically chose a Gaussian kernel with a fixed bandwidth at h
                        =(max
                        y
                        –min
                        y)/40 equal to the standard deviation of the kernel. In order to have sufficient data to train the classifier, we have set a threshold of four samples from each class in order to estimate π(θ
                        =
                        i|Y).

To assess the uncertainty in the estimates of test utility, the following procedure was used. The classes are typically imbalanced, and we performed random subsampling of the majority class 100 times. For each sampling a classification rule 
                           
                              π
                              (
                              Θ
                              |
                              Y
                              )
                           
                         was computed. The mean of each sample result were computed and used as the estimate of 
                           
                              π
                              (
                              Θ
                              |
                              Y
                              )
                           
                         in Eq. (1). The 10th and 90th percentile over the samples are denoted qL
                        (y) and qU
                        (y) respectively. The information content is expected to be lower bounded by that of following classification rule,
                           
                              (4)
                              
                                 
                                    
                                       π
                                    
                                    
                                       C
                                    
                                 
                                 (
                                 y
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         U
                                                      
                                                   
                                                   (
                                                   y
                                                   )
                                                
                                                
                                                   if
                                                   
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         U
                                                      
                                                   
                                                   (
                                                   y
                                                   )
                                                   <
                                                   .
                                                   5
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         L
                                                      
                                                   
                                                   (
                                                   y
                                                   )
                                                
                                                
                                                   if
                                                   
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         L
                                                      
                                                   
                                                   (
                                                   y
                                                   )
                                                   >
                                                   .
                                                   5
                                                
                                             
                                             
                                                
                                                   .
                                                   5
                                                
                                                
                                                   else.
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

@&#RESULTS@&#

In the following we quantify the information gain with respect to second surgery within 30days of the index surgery for patients with colorectal cancer. All data analysis was performed using R [36].

The mutual information between test result distribution for tests performed on a specific day and resurgery within 30days are shown in Fig. 1
                        . Since there are a different number of tests performed each day (e.g., many more on the day of surgery than the day before), we need to adaptively assess significance. We generate random samples of equal size to the actual set of tests, and indicate in the graph the 95th percentile of these random samples. Thus, when the mutual information is beyond the 95th percentile, we can conclude that the MI is higher than would be expected by random chance. According to these results we see that CRP tests do have high mutual information with resurgery after day two post surgery. Serum Albumin tests have an immediate distinction, while sodium has little information content with respect to this outcome.

The MI is informative of the outcome, but it does not relate to a particular patient model, nor are we quantifying how much we learn relative to prior knowledge using MI.

Again, we use a patient model with a single binary parameter, that is 
                           
                              θ
                              
                              ∈
                              
                              {
                              0
                              ,
                              
                              1
                              }
                           
                        , depending on whether the patient is at risk of resurgery or not. The parameter is assumed to be a priori unknown, thus estimated by a prior μ
                        0
                        =
                        μ
                        1
                        =1/2 with 0 representing a no-risk patient, and 1 a patient at risk.

In Fig. 2
                         we shows the delta information for the combinations of the four tests and outcomes. We divided the clinical pathway into seven phases, and computed the information gain relative to the starting prior. The classes are unbalanced (about 11% of patients undergo resurgery), and we balanced the classes by subsampling the majority class. The resampling was performed 100 times to check for stability, which provided estimates for uncertainty as described earlier. The delta information is shown for some combinations of the different tests and types of resurgery, indicating on what day of surgery the test reveals the most important information.

In Fig. 3
                        , the classification rules are shown for the different phases. Thin gray lines indicate the classification rule for each of the 100 samples, and the 90th percentiles are shown as shaded gray. The mean classification rule is shown as thick black lines. The dashed line in the lower right plot of Fig. 3, shows the rule used for the lower estimate rule πC
                        (y), which provides the estimate for the lower bound of the point labeled 6 in the top left plot of Fig. 3.

@&#DISCUSSION@&#

There is a significant literature on the utility of medical tests, though little addresses the use of an information theoretic way of quantifying tests based on real world data such as the EMR. In this study we have shown how to quantify this utility in terms of an information theoretic approach. In the case of a second surgery, we see that the utility of typical tests is substantial immediately post index surgery.

In the case studied here, the tests carry little negative utility for the patient and provider, and are therefore done routinely throughout the clinical history. In other cases the negative impact is substantial, and determining the utility of tests in the different phases is important. Often the impact is constant throughout the patient trajectory, and thus the dynamics of the utility for a test depends only on the information content. To compare utility across different tests, any differences in negative impact must be taken into account. Examples of tests with real negative impact include screening, scans or biopsies that are costly and carry risk to varying degree. Generally, radiology tests are expensive, time consuming and uncomfortable for the patient. Additionally, there is a concern in terms of overuse resulting in unnecessary radiation exposure [37] and risk associated with interventional procedures. The principles for when to utilize the tests are often unclear. However, the challenge is that interpreting the radiology is far more complex compared to the simple numerical value associated with blood samples. Using either image analysis on the images or text analysis on the radiologists’ report are tracks to compute the test utility in these cases.

We use blood tests for the analysis, but obviously the methodology is not limited to these. The framework demonstrated here is easily adapted to any case where there is a clearly defined endpoint and copious legacy data. In particular, as long as the data source is sufficient for a statistical classifier, the test utility can be computed.

We have investigated each test individually, but combining tests can yield more powerful gains, more accurate classifiers and eventually more accurate estimates for the information gain at a possible loss of interpretability. Also, other information about the patient, such as demographic variables is likely to be informative about the outcome, but is not included in the current work. Such data should be included when designing full prediction models for the clinical problem in question.

We balance the classes before training the classifier. Naïvely balancing the classes is an arguable approach for a decision support system that attempts classification, e.g., distinguishing at-risk or no-risk groups where the at-risk group is typically much smaller. However, here the purpose of balancing is to understand how differentiable the classes are rather than provide posteriors that directly reflect probabilities for each of the classes.

Test experiments are particularly prone to confounding by practitioners. If the result of the test indicates risk for a known complication, the practitioner will take necessary action to avoid the complication. If this intervention successfully prevents a complication from occurring, the class labels will be confounded. This issue needs to be properly accounted for, which is only possible if the confounding intervention is known [38]. Test requests may to some extent be driven by physicians who have concerns about the patient at hand, and thus be a biased sampling of the patient population. In the context considered here tests are also taken routinely, e.g., on day three post surgery, which alleviates some of the bias.

The automatic learning of the decision rule used in this paper can be changed as appropriate. It needs not be data-driven but can be guided by clinical knowledge in a knowledge-driven approach, or some hybrid method. Learning can be combined with knowledge mining from authoritative sources. In the cases of decision rules learned from outside sources other than the data themselves, the estimation of 
                        
                           π
                           (
                           Θ
                           |
                           Y
                           )
                        
                      must be augmented by the actual data to obtain meaningful estimates of the information gain.

It would not be reasonable or practically possible for practitioners to integrate the thinking outlined here in their daily routines. However this work along with others that use big, real-word healthcare data to provide insight will be a foundation for future decision support tools.

@&#CONCLUSIONS@&#

We have shown how to quantify the information content in medical tests based on data available in the EMR. For the case of surgery for colorectal cancer, the information content of blood test results reflect clinical reality well. We quantify the expected value of the test at different clinical phases. It is possible to expand the methodology to other clinical scenarios and other types of tests where the impact of the test is larger.

SOS and KMA are supported by Tromsø Telemedicine Laboratory (TTL) funded by the Research Council of Norway Grant No. 174934. SOS is supported by the Regional Health Authority of North Norway research Grant No. HST1182-14.

@&#ACKNOWLEDGMENTS@&#

The authors acknowledge surgeons at University Hospital of North Norway, Rolv-Ole Lindsetmo, Arthur Revhaug and Kim Erlend Mortensen who performed the majority of surgeries studied here and who are essential for the data generation. Kristian Hindberg is thanked for his role in data extraction and data management.

@&#REFERENCES@&#

