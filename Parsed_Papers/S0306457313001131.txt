@&#MAIN-TITLE@&#Preferences in Wikipedia abstracts: Empirical findings and implications for automatic entity summarization

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We empirically study how Wikipedians summarize entity descriptions in practice.


                        
                        
                           
                           We compare entity descriptions in DBpedia with their Wikipedia abstracts.


                        
                        
                           
                           We analyze the length of a summary and the priorities of property values.


                        
                        
                           
                           We analyze the priorities of, diversity of, and correlation between properties.


                        
                        
                           
                           Implications for automatic entity summarization are drawn from the findings.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

DBpedia

Entity summarization

Feature selection

Property ranking

Wikipedia

@&#ABSTRACT@&#


               
               
                  The volume of entity-centric structured data grows rapidly on the Web. The description of an entity, composed of property-value pairs (a.k.a. features), has become very large in many applications. To avoid information overload, efforts have been made to automatically select a limited number of features to be shown to the user based on certain criteria, which is called automatic entity summarization. However, to the best of our knowledge, there is a lack of extensive studies on how humans rank and select features in practice, which can provide empirical support and inspire future research. In this article, we present a large-scale statistical analysis of the descriptions of entities provided by DBpedia and the abstracts of their corresponding Wikipedia articles, to empirically study, along several different dimensions, which kinds of features are preferable when humans summarize. Implications for automatic entity summarization are drawn from the findings.
               
            

@&#INTRODUCTION@&#

Entity-centric structured data such as Google’s Knowledge Graph, Facebook’s Open Graph, and W3C’s RDF and Linked Data, has become an important component of the Web. It describes the attributes of and the relationships between entities. The description of an entity, or an entity description for short, is composed of property-value pairs, a.k.a. features (Cheng, Tran, & Qu, 2011), as illustrated by the right-hand side of Fig. 1
                     . The value of a property could be a data value (e.g. an integer), another entity, or a class (usually being the type of the entity). The volume of such data increases rapidly on the Web. Entities are associated with more and more features. For instance, the RDF description of Sydney provided by Freebase
                        1
                        
                           http://www.freebase.com/m/06y57. Last accessed: 08/31/2013.
                     
                     
                        1
                      contains several hundred features. When showing such an entity description to the user, to avoid information overload, practical applications like Knowledge Graph on Google’s search results pages present not all but only a limited number (e.g. top-k) of features, called a summary of this entity description, or an entity summary for short. Then, the problem arises as to which features are best for constituting an entity summary to be used in a particular application (or, how to rank features), and the term entity summarization was coined to describe this problem (Cheng et al., 2011).

This emerging problem has received attention from researchers in the areas of information retrieval (Zhang, Zhang, & Chen, 2012), database (Fakas, 2011), and Semantic Web (Cheng et al., 2011). To develop an effective approach to entity summarization, a key issue to consider is how humans summarize entity descriptions by ourselves. However, to the best of our knowledge, there is a lack of extensive empirical studies on this topic, which motivates our work. In this article, we aim to explore, via a large-scale empirical analysis, which kinds of features are preferable when humans summarize entity descriptions for generic use. Implications for developing approaches to automatic entity summarization will be drawn from the findings.

To achieve this, instead of inviting a few human experts to summarize entity descriptions, which can hardly be extended to a large scale, we intend to analyze millions of entity descriptions and the summaries thereof given by a large and representative population. Therefore, we choose DBpedia (Bizer et al., 2009), a well-known dataset at the center of Linked Data. It treats the topic of each Wikipedia article as an entity, and extracts its structured description from the article. The results are represented as RDF triples (Klyne & Carroll, 2004), each of which comprises an entity, a property, and a value, or in other words, a feature of an entity, as illustrated by the right-hand side of Fig. 1. Thanks to the encyclopedic topics and massive amount of information offered by Wikipedia, DBpedia has collected tens of millions of RDF triples describing several million entities. To obtain general-purpose summaries of these entity descriptions, we exploit the first section of each Wikipedia article (i.e. the text prior to the table of contents), called a Wikipedia abstract, which provides an abstract of the article and thus is regarded as a generic textual summary of the corresponding entity description, as illustrated by the left-hand side of Fig. 1. Then, we identify the features in each entity description that are mentioned in its corresponding Wikipedia abstract, as illustrated by Fig. 1, by using an automatic approach. The results constitute an entity summary, with which we can analyze and reveal humans’ preferences in choosing features. In particular, these preferences belong to not a small population but the large Wikipedia community, thereby making our findings more generalizable.

Our major contributions are:
                        
                           •
                           a large-scale multi-dimensional statistical analysis of humans’ preferences in selecting features into an entity summary, and

a number of implications drawn for automatic entity summarization.

To be specific, we will investigate the length of entity summaries, analyze the priorities of, diversity of, and correlation between properties in summarization, and explore the preferences in choosing property values. Based on the empirical findings, several heuristics are recommended to be considered in future research on entity summarization.

The remainder of this article is organized as follows. Section 2 reviews the literature. Section 3 describes the dataset to use. Section 4 introduces and evaluates several strategies for identifying which features are mentioned in a Wikipedia abstract. Section 5 presents a multi-dimensional statistical analysis of the features mentioned in Wikipedia abstracts to explore humans’ preferences, from which implications for automatic entity summarization are drawn in Section 6. Finally, Section 7 concludes the article with a discussion of future work.

@&#LITERATURE REVIEW@&#

Entity summarization has proven to be important to entity search engines (Bai, Delbru, & Tummarello, 2008; Cheng & Qu, 2009), where query-biased entity summaries, consisting of features that contain query keywords, are created to be shown on search engine results pages. Zhang et al. (2012) used a learning-based approach to rank features according to their relevance to the query. Google (2012), to summarize an entity description in Knowledge Graph, may have utilized query logs to find the properties that have been asked more often in Google Search.

Recent interests mainly focused on generating a summary that can best characterize the underlying entity for generic use. RELIN (Cheng et al., 2011) employs a random surfer model to rank features according to their informativeness and relatedness. The measurement of informativeness is based on the self-information of features, and the measurement of relatedness is based on the co-occurrence of properties and of values on the Web. Thalhammer, Toma, Roa-Valverde, and Fensel (2012) considered a feature of an entity important if it is shared with the entity’s nearest neighbors. The distance between entities is defined based on their rates given by human users in certain applications. DIVERSUM (Sydow, Pikuła, & Schenkel, 2013) extends the notion of entity summary to be an arbitrary connected subgraph surrounding the entity in graph-structured data. A summary is constructed in a greedy manner by successively adding the edge that has both a short distance to the entity and a property frequently used in the data. In particular, features sharing a common property are not allowed to appear in a summary together, to improve the diversity.

The database community tackle a similar problem when providing keyword search in relational databases. As Fakas (2011) discussed, a single query-relevant tuple returned by keyword search does not comprise a complete result; additional related information is needed to comprise a meaningful result. To address this issue, a key challenge is to quantify the affinity between relations and between attributes in order to decide which tuples and attributes to include in the search results. A solution was proposed in Fakas (2011), and efficient implementations were presented in Fakas, Cai, and Mamoulis (2011).


                        Evaluating the approaches to entity summarization is also a challenge. Existing experiments were mainly based on a small number of hand-crafted gold-standard summaries. Thalhammer, Knuth, and Sack (2012) proposed to establish gold-standard summaries via crowdsourcing based on a quiz game.

Whereas major efforts have been made to study novel approaches to automatic entity summarization, to the best of our knowledge, no empirical study has been carried out to extensively analyze how humans summarize entity descriptions in practice, which can provide empirical support and inspire future research. This distinguishes our work from previous efforts.

Entity summarization is related to several other summarization problems.

Existing approaches to entity summarization are more or less inspired by the large body of work on text summarization (Spärck Jones, 2007), where the data to be summarized is a text document or multiple documents. Major efforts in this field have been made to summarize in an extractive manner, namely to treat documents as a set of sentences and then select a subset as a summary. Existing solutions create summaries for supporting various tasks. Among others, to generate a summary for generic use, sentences central to the topic of the document are preferred (Erkan & Radev, 2004; Radev, Jing, Styś, & Tam, 2004). Besides, sentences in a summary are generally expected to be dissimilar to each other, to reduce redundancy and improve diversity (Carbonell & Goldstein, 1998).

Entity summarization usually deals with graph-structured data, where the entity to be summarized appears as a node in the graph. The more general problem of graph summarization aims to find a compact representation that is representative of the entire graph. Existing solutions mainly follow a non-extractive manner, to find a higher-level abstraction that provides an insight into a coarse-level structure of the graph (Navlakha, Rastogi, & Shrivastava, 2008; Tian, Hankins, & Patel, 2008). By contrast, major approaches to ontology summarization transform an ontology into a graph, and follow an extractive manner to create a summary by choosing the most salient nodes (Wu, Li, Feng, & Wang, 2008) or subgraphs (Ge, Cheng, Li, & Qu, 2013; Zhang, Cheng, & Qu, 2007).

Our empirical study was based on a combination of two well-known datasets: DBpedia and Wikipedia.

DBpedia (Bizer et al., 2009), at the center of Linked Data, extracts structured information from Wikipedia and makes it accessible on the Web in RDF form. It describes millions of entities covering a wide range of encyclopedic topics like people, places, organizations, and species. Our experiments were based on the English version of DBpedia 3.7,
                        2
                        
                           http://wiki.dbpedia.org/Downloads37. Last accessed: 08/31/2013.
                     
                     
                        2
                      which was the latest version at the time of experimentation. Seven core datasets were imported, namely Ontology Infobox Types, Ontology Infobox Properties, Titles, Geographic Coordinates, Homepages, Persondata, and PND, which collectively provide 42.3 million RDF triples describing more than 3 million entities. Other core datasets were not imported for different reasons, e.g. Images concerning multimedia data which we have no interest in this work, Raw Infobox Properties and the like which are of very low quality. However, the reader is reminded that, although DBpedia is one of the most popular entity-centric structured datasets, it mainly consists of data extracted from Wikipedia infoboxes, which thereby may not provide comprehensive entity descriptions that include all the major features of each entity. This may affect the accuracy and generalizability of our findings.

To enable a fine-grained study in a comparative manner, we decided to particularly focus on the entities from ten popular classes. These classes were manually selected from those containing the largest numbers of entities based on the following principles. Firstly, upper classes describing very general concepts like Work and Species were ignored (but their subclasses might be selected). Secondly, classes sharing many overlapping entities were not chosen together, e.g. Animal and its subclass Insect. Table 1
                      presents the ten classes finally selected. They collectively contain 1.34 million entities, covering a considerable portion of the entities in DBpedia. The entities from these classes are pairwise disjoint except for only a negligible number of 468 entities from both ArchitecturalStructure and EducationalInstitution.

The Wikipedia abstracts used in the experiments were obtained from the same version of DBpedia, or to be more specific, from the Extended Abstracts dataset, which provides Wikipedia abstracts as well as the correspondences between entity descriptions and Wikipedia abstracts.

Before analyzing the preferences in Wikipedia abstracts, a critical step is to identify which features in an entity description are mentioned in the corresponding Wikipedia abstract. Fig. 1 illustrates this identification task; in this example, three features are mentioned while the other three are not. To carry out this task for 1.34 million entities, in this section we will discuss a number of automatic approaches, and find the most accurate one to use based on an extensive evaluation.

We consider that a feature of an entity is mentioned in the corresponding Wikipedia abstract if the textual form of the property value matches part of the Wikipedia abstract. Specific to RDF data, the textual form of a property value is obtained as follows.
                           
                              •
                              If the value of the property (e.g. name, long, and lat in Fig. 1) is a literal (viz. a data value in RDF), its textual form is the lexical form of this literal, or

if the value of the property (e.g. type and country in Fig. 1) is an entity or a class, its textual form is the name of this entity or class.

To determine whether the textual form of a property value matches part of the Wikipedia abstract, in light of the great variety of natural language, we firstly transform both of them into normalized word sequences in the following steps.
                           
                              1.
                              Text is broken up into word sequences at whitespace and punctuation characters.

Words in camel case (e.g. PopulatedPlace) are split into separate words (e.g. Populated and Place).

Letters are lowercased.

Stop words are removed.

Then, we devise three different strategies for deciding whether there is a match.
                           
                              
                                 SEQ. In this strategy, the textual form of a property value matches part of a Wikipedia abstract if the word sequence of the former is a subsequence of the word sequence of the latter. For example, ‘south east coast’ matches part of the Wikipedia abstract in Fig. 1, whereas ‘east south’ and ‘south coast’ do not.


                                 SET_ALL. In this strategy, the textual form of a property value matches part of a Wikipedia abstract if all the words in the word sequence of the former are contained in the word sequence of the latter, without considering the order of the words. For example, ‘east south’ and ‘south coast’ match part of the Wikipedia abstract in Fig. 1, whereas ‘east north’ does not.


                                 SET_ANY. In this strategy, the textual form of a property value matches part of a Wikipedia abstract if any word in the word sequence of the former is contained in the word sequence of the latter. For instance, even ‘east north’ matches part of the Wikipedia abstract in Fig. 1.

We also develop two optional extensions of the above strategies for comparing words.
                           
                              
                                 WN. In this extension, two different words will be considered identical if they share a common sense according to WordNet (Miller, 1995), i.e., they may be synonyms. For instance, by extending SEQ, ‘south east seashore’ will also match part of the Wikipedia abstract in Fig. 1 because ‘seashore’ and ‘coast’ share a common sense. However, since words are not disambiguated, errors may be introduced by this extension.


                                 STEM. In this extension, two different words will be considered identical if they share a common stem according to Snowball.
                                    3
                                    
                                       http://snowball.tartarus.org/. Last accessed:08/31/2013.
                                 
                                 
                                    3
                                  For instance, by extending SEQ, ‘south east coasts’ will also match part of the Wikipedia abstract in Fig. 1 because ‘coasts’ and ‘coast’ share a common stem, namely ‘coast’.

The two extensions can be applied together. That is, two different words will be considered identical if the one in the textual form of a property value shares a common sense with some word that shares a common stem with the one in a Wikipedia abstract.

Finally, three strategies (viz. SEQ, SET_ALL, or SET_ANY) and the optional extensions (viz. WN, STEM, both WN and STEM, or none) give rise to twelve combinations, which will be evaluated in the next subsection.

@&#EVALUATION@&#

To evaluate different combinations of the proposed strategies and their extensions, we randomly selected 200 entity descriptions and their corresponding Wikipedia abstracts from our dataset as test cases, under the constraint that an entity description selected must contain more than ten features to avoid too short (and thus trivial) entity descriptions. These test cases were divided and distributed to three human experts to manually identify all the features in each entity description that are mentioned in the corresponding Wikipedia abstract, or in other words, to manually perform the identification task as illustrated in Fig. 1. Given a test case, let 
                           
                              
                                 
                                    F
                                 
                                 
                                    A
                                 
                              
                           
                         and 
                           
                              
                                 
                                    F
                                 
                                 
                                    E
                                 
                              
                           
                         be the sets of features identified by an approach and an expert, respectively. We evaluated the quality of identification completed by an approach by using the well-known precision, recall, and F-measure metrics:
                           
                              (1)
                              
                                 
                                    
                                       
                                       
                                          
                                             Precision
                                             =
                                             
                                                
                                                   |
                                                   
                                                      
                                                         F
                                                      
                                                      
                                                         A
                                                      
                                                   
                                                   ∩
                                                   
                                                      
                                                         F
                                                      
                                                      
                                                         E
                                                      
                                                   
                                                   |
                                                
                                                
                                                   |
                                                   
                                                      
                                                         F
                                                      
                                                      
                                                         A
                                                      
                                                   
                                                   |
                                                
                                             
                                             ,
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             Recall
                                             =
                                             
                                                
                                                   |
                                                   
                                                      
                                                         F
                                                      
                                                      
                                                         A
                                                      
                                                   
                                                   ∩
                                                   
                                                      
                                                         F
                                                      
                                                      
                                                         E
                                                      
                                                   
                                                   |
                                                
                                                
                                                   |
                                                   
                                                      
                                                         F
                                                      
                                                      
                                                         E
                                                      
                                                   
                                                   |
                                                
                                             
                                             ,
                                          
                                       
                                    
                                    
                                       
                                       
                                          
                                             F
                                             -
                                             measure
                                             =
                                             
                                                
                                                   2
                                                   ·
                                                   Precision
                                                   ·
                                                   Recall
                                                
                                                
                                                   Precision
                                                   +
                                                   Recall
                                                
                                             
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        
                     


                        Fig. 2
                         shows the average precision and recall scores achieved by each approach on all the test cases. SET_ALL and SEQ obtained similar precision scores, but SET_ALL produced superior recall and thus F-measure scores. Compared with SET_ALL, SET_ANY traded precision for recall. By using the two extensions, in general, recall only improved slightly at the expense of precision. Therefore, we decided to use SET_ANY without any extension in the subsequent analysis because this combination achieved the highest F-measure score (0.899). Such a high F-measure score guaranteed the quality of the subsequent analysis.

It is possible to obtain even better results if the above combinations are further combined in an appropriate manner, e.g. a linear combination. However, limited by the size of “training data” crafted by human experts (only 200 cases), it is difficult to avoid overfitting when tuning the model. Therefore, we left it as future work.

Having identified the features mentioned in each Wikipedia abstract, in this section we present a multi-dimensional statistical analysis of these features to explore humans’ preferences. Firstly, in Section 5.1, we investigate the length of entity summaries. Then, from Sections 5.2, 5.3, 5.4, as a major focus of our work, we analyze the preferences in choosing properties, including not only their priorities but also their diversity and correlation. Finally, in Section 5.5, we explore the preferences in choosing property values. As discussed in Section 3, we group our results by the class of entity, and present them in a comparative manner.

Most approaches to entity summarization (Cheng & Qu, 2009; Cheng et al., 2011; Sydow et al., 2013; Thalhammer, Toma, et al., 2012; Zhang et al., 2012) require providing the length of a summary to generate by specifying the maximum number of features to include. Whereas the optimum length probably depends on the specific application and even on the specific user, we are still interested in the typical number of features mentioned in a Wikipedia abstract, which can serve as a reasonable default value for automatic approaches.

As shown in Fig. 3
                        , the average number of features mentioned in a Wikipedia abstract varies widely with the class of entity (from 4.52 to 11.16). We partially attribute this to the wide variation in the number of features in original entity descriptions (from 14.23 to 22.45). In fact, the average proportion of features that are mentioned in a Wikipedia abstract is in a relatively narrow range (from 29.80% to 50.56%), which motivates dynamically determining the number of features to select based on the number of features in the original entity description and a fixed proportion in automatic approaches.

Some approaches to entity summarization (Sydow et al., 2013; Zhang et al., 2012) rank and select features mainly based on their properties. It is interesting to ask whether Wikipedia abstracts also favor the features with certain properties, and if so, which kinds of properties are preferable.

To measure how likely a property is to be mentioned in a Wikipedia abstract, we define the mention rate of a property as the ratio of the number of Wikipedia abstracts in which a feature mentioned is with this property to the number of entity descriptions where this property is used. The result is in the range 
                           
                              [
                              0
                              ,
                              1
                              ]
                           
                        . The latter number of the ratio is called the support of this mention rate. To obtain statistically significant results, we limit our analysis to those having a support of 200 or larger.

If no preference is given to any properties, the mention rates of all properties will be similar, or in other words, they will have a small standard deviation. However, as Table 2
                         shows, the standard deviations observed on different classes are consistently and considerably large, indicating that properties are favored by Wikipedia abstracts to very different degrees. In the following, we will explore four potential causes of such differences, by analyzing four different dimensions of a property: frequency, popularity, variety, and complexity.

The first dimension we explore is the frequency of observing a property throughout the dataset. Intuitively, a widely used property is likely to be important, and becomes preferable when summarizing an entity description.

We define the frequency of a property as the number of entity descriptions in the dataset where it is used. As shown in Fig. 4
                           , on all the ten classes, the top-10% properties ranked by mention rate have relatively higher frequency (5.94–8.59 times as high as the average of all). Besides, Table 3
                            gives the Pearson’s correlation coefficients (called Pearson’s 
                              
                                 ρ
                              
                           ) between the mention rates of properties and their frequency. Pearson’s 
                              
                                 ρ
                              
                            values are positive on all the classes, and in particular, medium positive (0.3–0.5) on four classes and small positive (0.1–0.3) on the others. Based on these observations, we conclude that properties widely used in the dataset are considerably preferable.

The second dimension we explore is the popularity of a property, which is similar to frequency but considers a larger scope beyond DBpedia, namely the Web. We are interested in whether a property popular on the Web is likely to be preferable when summarizing an entity description.

We define the popularity of a property as the number of search results returned by a Web search engine (Bing in our experiments) against the query comprising the name of the property, i.e. the number of Web documents where the property occurs. As shown in Fig. 5
                           , the top-10% properties ranked by mention rate have relatively higher popularity than average on eight out of the ten classes, but in general, the differences are less notable than those in Fig. 4. Pearson’s 
                              
                                 ρ
                              
                            values given in Table 3 provide a similar indication. Specifically, a medium positive value (0.3–0.5) is only observed on one class, whereas on all the other classes, merely small or basically no correlation (−0.3 to 0.3) are observed. To conclude, Web-based popularity is not as strong an indicator of preference as frequency.

The third dimension we explore is the variety of a property. Intuitively, if a property can take more different values throughout the dataset, a feature with such a property will be more distinguishing, and thus may be preferable when summarizing an entity description.

We define the variety of a property as the number of its distinct values throughout the dataset. As shown in Fig. 6
                           , on all the ten classes, the top-10% properties ranked by mention rate have relatively higher variety (8.17–11.24 times as high as the average of all). Referring to the Pearson’s 
                              
                                 ρ
                              
                            values in Table 3, positive values are observed on all the classes, and in particular, medium positive (0.3–0.5) on two classes and small positive (0.1–0.3) on the others. Therefore, we conclude that properties having a wide variety of values are considerably preferable.

Interestingly, by comparing Figs. 4 and 6, and comparing the corresponding Pearson’s 
                              
                                 ρ
                              
                            values in Table 3, we find that the results obtained on variety are very similar to the results obtained on frequency. We partially attribute it to that, in DBpedia, a property used in a large number of entity descriptions usually takes different values, and thus also has a wide variety of values. As a result, these two dimensions show similar characteristics in our analysis.

The fourth dimension we explore is the complexity of a property. The intuition here is that, since a summary is supposed to be a compact description, a property having a complex (i.e. long) name may not be preferable when summarizing an entity description.

We define the complexity of a property as the number of words in its name. As shown in Fig. 7
                           , the top-10% properties ranked by mention rate have relatively lower complexity than average on nine out of the ten classes, but in general, the differences are not so very significant. Referring to the Pearson’s 
                              
                                 ρ
                              
                            values in Table 3, although strong or medium negative values (−1.0 to −0.3) are observed on three classes, positive values are also observed on three classes. To conclude, the complexity of a property is not a reliable indicator of preference.

When a property takes multiple values in an entity description, even if it is preferable, approaches like DIVERSUM (Sydow et al., 2013) will not select all these features into a summary but rather, aim to diversify an entity summary by choosing as many different properties as possible. It is interesting to ask whether and to what extent Wikipedia abstracts also tend to cover diverse properties.

Let F be the set of features mentioned in a Wikipedia abstract. We define the diversity of F as:
                           
                              (2)
                              
                                 diversity
                                 =
                                 
                                    
                                       #
                                       
                                       of
                                       
                                       distinct
                                       
                                       properties
                                       
                                       in
                                       
                                       F
                                    
                                    
                                       |
                                       F
                                       |
                                    
                                 
                                 .
                              
                           
                        The result is in the range 
                           
                              [
                              0
                              ,
                              1
                              ]
                           
                        . However, when the features mentioned in the Wikipedia abstract have covered sufficiently many properties used in the original entity description, even if they share some common properties, we will not reduce their diversity in consideration of their coverage. Hence, we refine the definition of diversity as:
                           
                              (3)
                              
                                 refined
                                 -
                                 diversity
                                 =
                                 
                                    max
                                 
                                 (
                                 diversity
                                 ,
                                 coverage
                                 )
                                 ,
                              
                           
                        where coverage is given by:
                           
                              (4)
                              
                                 coverage
                                 =
                                 
                                    
                                       #
                                       
                                       of
                                       
                                       distinct
                                       
                                       properties
                                       
                                       in
                                       
                                       F
                                    
                                    
                                       #
                                       of
                                       
                                       distinct
                                       
                                       properties
                                       
                                       used
                                       
                                       in
                                       
                                       the
                                       
                                       original
                                       
                                       entity
                                       
                                       description
                                    
                                 
                                 .
                              
                           
                        The result is still in the range 
                           
                              [
                              0
                              ,
                              1
                              ]
                           
                        . In particular, a fully diversified set of features, which have a diversity of 1.0, either cover all the properties used in the original entity description (though possibly sharing common properties), or share no common property (though possibly missing some properties used in the original entity description).

It is worth noting that, if an entity description contains no multi-valued property, the diversity of the features mentioned in the corresponding Wikipedia abstract will trivially be 1.0. Such entity descriptions and their corresponding Wikipedia abstracts have been excluded from the following analysis.


                        Fig. 8
                         presents the average refined-diversity of the features mentioned in a Wikipedia abstract on different classes. The results are above 0.8 on all the classes, and in particular, above 0.9 on two classes. Therefore, we conclude that Wikipedia abstracts tend to cover diverse properties.

Besides separately considering different properties, some approaches to entity summarization (e.g. Bai et al., 2008) also pay attention to the correlation between properties. For instance, for some pairs of properties such as latitude and longitude, either both or none of them are expected to be selected into a summary in consideration of information integrity. In this subsection, we will investigate whether such correlation can be observed in Wikipedia abstracts, and if so, which kinds of property pairs are preferable.

To measure how likely a property pair is to be mentioned in a Wikipedia abstract, we define the mention rate of a property pair as the ratio of the number of Wikipedia abstracts in which two features mentioned are with these two properties to the number of entity descriptions where these two properties are both used. The result is in the range 
                           
                              [
                              0
                              ,
                              1
                              ]
                           
                        . The latter number of the ratio is called the support of this mention rate. To obtain statistically significant results, we limit our analysis to those having a support of 200 or larger.

As Table 4
                         shows, the mention rates of property pairs observed on different classes have consistently and considerably large standard deviations, which indicates that property pairs are favored by Wikipedia abstracts to very different degrees. In the following, to explore potential causes of such differences, we will analyze three different dimensions of a property pair: co-occurrence frequency, co-occurrence popularity, and string similarity.

The first dimension we explore is the frequency of co-occurrence throughout the dataset. Intuitively, properties frequently used together to describe an entity are also likely to correlate with each other when summarizing an entity description.

We define the co-occurrence frequency of a property pair as the number of entity descriptions in the dataset where these two properties are both used. As shown in Fig. 9
                           , on all the ten classes, the top-10% property pairs ranked by mention rate have relatively higher co-occurrence frequency (1.51–5.90 times as high as the average of all). Besides, Table 5
                            gives the Pearson’s 
                              
                                 ρ
                              
                            values between the mention rates of property pairs and their co-occurrence frequency. Positive values are observed on all the classes, and in particular, medium positive (0.3–0.5) on two classes and small positive (0.1–0.3) on three classes. Therefore, we conclude that properties frequently used together to describe an entity significantly correlate with each other in summarization.

The second dimension we explore is the co-occurrence popularity of a property pair, which is similar to co-occurrence frequency but is measured based on not DBpedia but the Web. We are interested in whether properties frequently mentioned together in a Web document are also likely to correlate with each other when summarizing an entity description.

Let 
                              
                                 hits
                                 (
                                 p
                                 )
                              
                            be the number of search results returned by a Web search engine (Bing in our experiments) against the query comprising the name of the property p, and let 
                              
                                 hits
                                 (
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       p
                                    
                                    
                                       j
                                    
                                 
                                 )
                              
                            be the number of search results returned against the query comprising the names of two properties 
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       p
                                    
                                    
                                       j
                                    
                                 
                              
                           . Inspired by the widely-used Søensen-Dice coefficient, we define the co-occurrence popularity of a pair of properties 
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       p
                                    
                                    
                                       j
                                    
                                 
                              
                            as:
                              
                                 (5)
                                 
                                    co
                                    -
                                    occurrence
                                    -
                                    popularity
                                    (
                                    
                                       
                                          p
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          p
                                       
                                       
                                          j
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          2
                                          ·
                                          hits
                                          (
                                          
                                             
                                                p
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          
                                             
                                                p
                                             
                                             
                                                j
                                             
                                          
                                          )
                                       
                                       
                                          hits
                                          (
                                          
                                             
                                                p
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          +
                                          hits
                                          (
                                          
                                             
                                                p
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                    
                                    .
                                 
                              
                           The result is in the range 
                              
                                 [
                                 0
                                 ,
                                 1
                                 ]
                              
                           .

As shown in Fig. 10
                           , the top-10% property pairs ranked by mention rate have relatively higher co-occurrence popularity than average on nine out of the ten classes, but in general, the differences are less notable than those in Fig. 9. Pearson’s 
                              
                                 ρ
                              
                            values given in Table 5 provide a similar indication. Specifically, small positive values (0.1–0.3) are observed on five classes, whereas on the others, basically no correlation (−0.1 to 0.1) are observed, even including two negative values. To conclude, Web-based co-occurrence is not as strong an indicator of correlation as dataset-specific co-occurrence.

The third dimension we explore is the string similarity between a pair of properties. Intuitively, similar properties are likely to describe closely related topics and thus correlate with each other when summarizing an entity description.

We employ a state-of-the-art string metric (Stoilos, Stamou, & Kollias, 2005) to measure the similarity between the names of two properties. The result is in the range 
                              
                                 [
                                 -
                                 1
                                 ,
                                 1
                                 ]
                              
                           , which is then linearly transformed into the range 
                              
                                 [
                                 0
                                 ,
                                 1
                                 ]
                              
                            as the string similarity between two properties. As shown in Fig. 11
                           , the top-10% property pairs ranked by mention rate have relatively higher similarity only on six out of the ten classes. Table 5 presents the Pearson’s 
                              
                                 ρ
                              
                            values: only small positive (0.1–0.3) on two classes, and basically no correlation (−0.1 to 0.1) on the others. Therefore, we conclude that string similarity is not an effective indicator of correlation.

When ranking and selecting features into an entity summary, the value of a feature also deserves attention, in particular when the property takes multiple values such that the tie is to be broken by the different values. Intuitively, a feature shared by fewer entities is preferable because it reflects a more distinguishing characteristic of the entity, as discussed in Cheng et al. (2011). In this subsection, we will investigate whether Wikipedia abstracts favor such features.

Based on information theory, we define the informativeness of a feature as the amount of self-information contained in the probabilistic event of observing this feature in an entity description:
                           
                              (6)
                              
                                 informativeness
                                 =
                                 -
                                 log
                                 
                                    
                                       #
                                       
                                       of
                                       
                                       entity
                                       
                                       descriptions
                                       
                                       it
                                       
                                       belongs
                                       
                                       to
                                    
                                    
                                       #
                                       
                                       of
                                       
                                       all
                                       
                                       entity
                                       
                                       descriptions
                                    
                                 
                                 .
                              
                           
                        A features that is rarely seen is with a larger amount of self-information and thus with higher informativeness.

As shown in Fig. 12
                        , the informativeness of the features mentioned in Wikipedia abstracts on different classes is consistently and considerably larger than the average of all. The differences are in the range 
                           
                              [
                              1.02
                              ,
                              4.48
                              ]
                           
                        , which are actually very large due to the logarithm in Eq. (6). Therefore, we conclude that features that are rarely seen are considerably preferable.

To sum up, the empirical findings reported in the previous section have the following implications for automatic entity summarization.

Firstly, when configuring an approach to automatic entity summarization, if there is no strict limit on the number of features to select, the default length of an entity summary to generate can be fixed to a proportion of the features in the original entity description, such as some value within 30–50%. However, the specific value depends on the specific application.

Secondly, when ranking and selecting features, one heuristic to recommend is to choose those having a property that is widely used in the dataset and has a wide variety of values, and not to pay particular attention to the complexity of its name. Meanwhile, emphasis can be placed on improving the diversity of an entity summary by choosing as many different properties as possible, and in particular, preference can be given to the properties that are frequently used together to describe an entity. Besides, property values that are rarely seen deserve to be prioritized. However, effectively integrating these heuristics may be a challenge.

Thirdly, it is worth noting that different heuristics may suit entities from different classes to different degrees. Those heuristics not generally recommended (e.g. Web-based popularity, complexity) can still be effective on certain classes. Therefore, the best way to combine various heuristics is to be explored in specific applications.

Last but not least, the reader is reminded that the heuristics recommended here are only derived from an empirical analysis of DBpedia and Wikipedia. Though they are large and at the center of the Web, the extent to which our findings are generalizable to other datasets still needs further examination in future work.

@&#CONCLUSIONS AND FUTURE WORK@&#

We have collected more than one million entity descriptions from DBpedia covering a wide range of topics, and have identified the features in these descriptions that are mentioned in the corresponding Wikipedia abstracts. By analyzing these features in a statistical and comparative manner, we have characterized Wikipedians’ preferences in summarizing entity descriptions along several different dimensions. From the empirical findings, we have derived a number of implications for developing approaches to automatic entity summarization. Despite the limitations of our study, we believe that the heuristics recommended deserve to be considered in future research, when a few of them have been successfully incorporated into existing efforts. In future work, on the one hand, we will continue to investigate more dimensions to draw more implications, and on the other hand, we plan to develop a novel approach to entity summarization that will integrate these effective heuristics in a principled way.

Prior to our analysis, to identify which features are mentioned in a Wikipedia abstract, we have implemented and evaluated a broad spectrum of strategies. The best-performing approach has achieved a significantly high accuracy, which has guaranteed the quality of the subsequent empirical findings and implications. However, there is still room for improvement, e.g. to disambiguate words when using WordNet, and to allow for variations when processing numerical values. In fact, we are interested in accurately capturing Wikipedians’ preferences, and based on this, establishing an extensive benchmark of entity summaries for evaluating approaches to entity summarization. We will also explore how to run our analysis on other datasets, where a major challenge is potentially a lack of available summaries, regardless of textual or structured.

@&#ACKNOWLEDGEMENTS@&#

The authors would like to thank Qingxia Liu and Yanan Zhang for their contributions to the experiments. This work was supported in part by the NSFC under Grant 61100040, 61223003, and 61170068, in part by the JSNSF under Grant BK2012723, and in part by the SSFC under Grant 11AZD121.

@&#REFERENCES@&#

