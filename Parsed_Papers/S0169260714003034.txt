@&#MAIN-TITLE@&#Automatic information timeliness assessment of diabetes web sites by evidence based medicine

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The proposed method predicts the time period that a given content belongs to.


                        
                        
                           
                           The archives of high quality diabetes web sites between 2006 and 2013 were utilized.


                        
                        
                           
                           The method's accuracy is 65% in detecting the timeliness according to years.


                        
                        
                           
                           The accuracy increases when detecting timeliness according to time intervals.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Content-based retrieval

Diabetes

Medical information systems

Timeliness analysis

@&#ABSTRACT@&#


               
               
                  Studies on health domain have shown that health websites provide imperfect information and give recommendations which are not up to date with the recent literature even when their last modified dates are quite recent. In this paper, we propose a framework which assesses the timeliness of the content of health websites automatically by evidence based medicine. Our aim is to assess the accordance of website contents with the current literature and information timeliness disregarding the update time stated on the websites.
                  The proposed method is based on automatic term recognition, relevance feedback and information retrieval techniques in order to generate time-aware structured queries. We tested the framework on diabetes health web sites which were archived between 2006 and 2013 by Archive-it using American Diabetes Association's (ADA) guidelines. The results showed that the proposed framework achieves 65% and 77% accuracy in detecting the timeliness of the web content according to years and pre-determined time intervals respectively. Information seekers and web site owners may benefit from the proposed framework in finding relevant and up-to-date diabetes web sites.
               
            

@&#INTRODUCTION@&#

Since online content is used as an indispensable/essential source of information on health issues, the accuracy, completeness and timeliness of websites have become more of an issue for information seekers. The studies have shown that health recommendations on web sites vary in quality and many users fail to access reliable and accurate information on world wide web [1,2]. Information seekers are generally suggested to check the last update dates and the presence of any broken links in order to gain insight about the currency of a web page [1]. On the other hand, although many web sites have accurate information, recent update dates and no broken links, they may still provide outdated information [2].

In this paper, we propose a framework which automatically assesses the content of health web sites and predicts the time period that a given content belongs to. This system can serve both information seekers and website owners. Information seekers can evaluate the timeliness of health web pages easily and they can eliminate those that are not up-to-date. Website owners can utilize the proposed system to assess the currency of their web page contents automatically according to the latest research without reviewing the literature. The framework can also aid web site owners and information seekers to identify the specifics of content that are not in line with the recent literature.

@&#RELATED WORK@&#

Information timeliness refers to information which is sufficiently up-to-date at the time of publication and it is studied under the data freshness quality dimension [3–5]. From a user's point of view, it has two sub-dimensions: currency and timeliness. Currency is estimated as the difference between the data extraction time and the data delivery time and commonly used in data warehousing systems [6]. Timeliness measures the extent to which the age of data is appropriate for the corresponding task [7].

Temporal information retrieval combines temporal relevance with document relevance and aims to return temporally relevant documents. In a well-known study, Alonso et al. [8,9] aim to extract temporal information from the documents and cluster them along a timeline supporting multiple time granularities using named-entity extraction.

A time-aware document ranking methodology relying on time-aware query suggestions is proposed by Miyanishi and Sakai [10]. Their study makes suggestions along a timeline and helps users access relevant web pages. There are also studies which date a document based on temporal language model [11]. In this approach, time partition of a document is found based on the overlapping term usage in the documents. In the study of Lin et al. [12], it is aimed to construct patient's clinical timeline from text.

There are limited number of studies which have worked on different timeliness aspects of health web sites. Post et al. manually assessed the accuracy of nutrition information on the Internet for Type 2 diabetes and concluded that the website update dates are not correlated with the accuracy of the provided information [2].

In this paper, we propose a methodology to assess the timeliness of a web site according to evidence based medicine (EBM) automatically. EBM is the conscientious, explicit and judicious use of the current best evidence in making decisions about individual patients’ care by gathering the best available external clinical evidence from systematic research [13]. It aims to ensure that medical decisions are evidence based integrating both individual clinical expertise and the best external evidence. ADA guideline is the well-known gold standard for EBM on diabetes which is therefore utilized in this study.

The contributions of the paper are as follows:
                        
                           1.
                           The recent update time of a website does not necessarily indicate that its content has also been updated accordingly. The proposed methodology estimates to what time interval the content really belongs to according to a given reference guideline automatically.

This study is different from other existing studies in temporal information retrieval domain which focus on extraction of time related information (temporal entities) from content to predict the exact time the web page belongs to [8,14]. Such approaches are not suitable in this domain since although many dates are updated in the web pages, content may not reflect all up-to-date information in health domain. Consequently, rather than using temporal expressions, we utilize the entire document content to assess timeliness.

This study is different from existing studies based on temporal language models which are generally utilized for mapping events to a timeline such as [11]. Events occur at certain periods of time which has certain start and end dates. However, here, we focus on evidences about diabetes all of which have been introduced to the literature at a certain date and they become valid for a long time (majority has no end date). As a consequence, the term statistics in the corpus will be insufficient to give optimal term distribution over the time as done in [11].

This study is different from existing studies which aim to automate the process of determining whether a Web site is of high or low quality [15,16]. These studies do not focus on predicting the timeliness of a web site and do not take into account any temporal aspects in quality scoring.

The proposed methodology makes use of ADA guidelines which are gold standards for EBM in diabetes domain in order to measure the timeliness of diabetes web sites.

As there is no standard data set to test the proposed method in the literature, the data set was constructed manually by the authors. To measure the currency of web sites, ADA guidelines which were published between 2008 and 2013 are selected and retrieved from their web sites [17]. ADA guidelines consist of clinical practice recommendations specific to diabetes and are published each year based on a complete review of the relevant literature about diabetes. Several subtopics of diabetes are handled in the guidelines such as bariatric surgery, detection and diagnosis of GDM or foot care. Revisions are made each year and a brief summary of new and revisited sections is given.

The web sites to be utilized in training and testing phases are selected by querying specific search terms “diabetes” and “diabetes mellitus” in HON-search which returns the websites subscribed to the Code of Conduct (HONCode) principles [18] to ensure information quality. The HONCode is the oldest and the best-known quality label on the Web developed by a non-profit organization Health On the Net (HON). The websites which display the logo of the organization indicate that they follow the principles such as authoritativeness, statement of the purpose, confidentiality, reference section, justification of claims, website content details, disclosure of funding resources, and advertising policy.

In the data collection step, the initial HON review and subsequent monitoring dates were paid attention in order to select the corresponding archive copy of the web sites from Archive-it [19]. Archive-it is a subscription web archiving service from the Internet Archive [20] that helps organizations to harvest, build, and preserve collections of digital content. Since Archive-it does not always archive all the websites regularly, those that are missing could not be retrieved.

This study is mainly focused on the timeliness analysis of high quality websites. However, low quality web sites given at the last three rows of Table 2 were also crawled since the proposed method requires low quality web sites in the training phase.

The low quality web sites were selected from the study of Seidman et al. [21]. Seidman et al. collected the websites by querying a specific search term (i.e., “diabetes”) in Direct Hit search engine and the medical experts scored them from 0 to 100 based on several evaluation criteria. The websites that were given scores lower than 60 were accepted as low quality in line with that study.

The reference document (ADA guidelines) and the input web pages are pre-processed before conducting the analyses. They are tokenized and the stop words are filtered. Although ADA guidelines are published each year, there may be no significant differences between some of its sections in subsequent years. For example, the same nutrition recommendations appeared both in 2007 and 2008. In 2007, Referral for Diabetes Management section was published as it was in 2006. On the other hand, major revisions took place in some sections of the document such as Detection of GDM in 2007, Hypoglycemia in 2008 and Pharmacologic and Overall Approaches to Treatment in 2013. Due to this fact, rather than aiming to estimate the timeliness of the web site content in years, this study also targets to predict the timeliness of the web site content in non-overlapping consecutive time intervals (i.e. 2006–2008, 2008–2010). To determine the time intervals, the content variations in the guidelines are analyzed in detail manually and the findings are confirmed by cluster analysis results. There are 35 different sections in the guidelines published so far since 2006.

The variation in the guidelines suggests three time intervals when they are manually inspected. To confirm this finding, documents are also clustered using k-means algorithm. In k-means clustering, each object is assigned to precisely one of a set of clusters and the similarity between objects is calculated using the squared Euclidian distance between them in a maximum of 10 runs. The clustering results also showed that there are 3 clusters which is also confirmed by cluster validity techniques. The clusters produced by k-means algorithm comprise the guidelines published in successive years (Cluster 1: 2005–2008, Cluster 2: 2009–2011, and Cluster 3: 2012–2013). The results suggest that the document revisions between 2008–2009 and 2011–2012 are substantial but the revisions in consecutive years between 2005–2008, 2009–2011 and 2012–2013 are insignificant.

In addition, the ADA document should not be considered as a single entity to estimate the timeliness of a given document. ADA guideline's section descriptions vary in length significantly although each section has equal importance. While applying text processing methods in the whole document, the impact of the longer sections might be higher compared to the shorter sections. Thus, each section of the ADA document is treated separately in this study.

In this study, for data mining tasks such as k-means clustering, RapidMiner is used [22]. The information retrieval tasks such as query generation are conducted using Terrier Information Retrieval Platform 3.5 [23]. We also use the Porter Stemmer algorithm [24], a commonly known and widely used Stemmer for English language words [25] in RapidMiner.

@&#PROPOSED METHOD@&#

The method comprises three main steps: term recognition, query generation and web site scoring.


                        Input: Term recognition is carried out on ADA guidelines.


                        Method: The terms are extracted from each ADA section to obtain candidate lists which resulted in 35 term lists using automatic term recognition (ATR) techniques. In ATR, the aim is to extract words and multi-word expressions that are significant for a given domain [26]. ATR methods in the literature are generally used for keyword extraction and ontology enrichment [26]. In this study, this approach is used to extract candidate terms from each section of the ADA guidelines. Although Glossex which has been reported as a superior method in term extraction in the literature [27] and it was selected as the base method in this step, other well-known ATR models covering Weirdness, Likelihood Ratio and C-value, are also applied to compare their effects in capturing significant terms in the experiment section.


                        Output: The list of the terms and their ATR scores for Glossex, Likelihood and C-value methods are generated for comparison purposes. Candidate lists (CL) are created based on the Glossex scores. Section specific CL's are generated for each guideline. For example, CL of the Retinopathy Screening and Treatment section of the selected RG in 2009 comprise terms such as efr, macroalbuminuria, nondiabetic, bariatric_surgery and microalbumin, in 2010 optometrics, nondiabetic, fundus photographs, retinopathy and dpn autonomic neuropathy, in 2011 nondiabetic, nondiabetic neuropathies, insulin, biopsy, and prediabetes, in 2012 prediabetes, comorbidities, hypoglycemia ketosis, neuropathy and finally in 2013 hypoglycemia, hypoglycemia unawareness, dyslipidemia, urinary albumin excretion, and gastroparesis just to name a few.


                        Input: CL generated in Step 1 and training dataset comprising both high quality and timely websites, D
                        
                           H
                        , and low quality and outdated websites D
                        
                           L
                         are inputs of this step.


                        Method: The extracted CL are used to generate section specific queries to be utilized in modeling of the evolution of each ADA section within the last six years. The queries are produced using relevance feedback which is an automatic process for query reformulation [16]. Term frequency distributions in D
                        
                           H
                         and D
                        
                           L
                         are compared and are used to generate a complex query consisting of weighted words and phrases. The approach is based on the assumption that terms in relevance query occur frequently in relevant text but rarely otherwise. The terms that appear in both D
                        
                           H
                         and D
                        
                           L
                         are given low weights whereas the ones that appear only in D
                        
                           H
                         are assigned higher weights. The weights in the queries are computed using the Robertson–Sparck–Jones (RSJ) formula as follows:
                           
                              (1)
                              
                                 
                                    F
                                    =
                                    log
                                    
                                       
                                          (
                                          (
                                          r
                                          +
                                          0.5
                                          )
                                          /
                                          (
                                          R
                                          −
                                          r
                                          +
                                          0.5
                                          )
                                          )
                                       
                                       
                                          (
                                          (
                                          n
                                          −
                                          r
                                          +
                                          0.5
                                          )
                                          /
                                          (
                                          N
                                          −
                                          n
                                          −
                                          R
                                          +
                                          r
                                          +
                                          0.5
                                          )
                                          )
                                       
                                    
                                 
                              
                           
                        where r is the number of relevant documents that contain the term, R is the number of relevant high quality and timely documents D
                        
                           H
                        ; N is the number of documents in the collection made up of D
                        
                           H
                         and D
                        
                           L
                        , and finally n is the number of documents that contain the term. This formula is computed for each query term 
                           
                              
                                 q
                                 
                                    i
                                    k
                                 
                                 t
                              
                              (
                              y
                              )
                           
                         (kth query term in CL of the ith section of the selected RG) in year y.
                     

RSJ assigns high scores to the terms that appear mostly in highly relevant web pages [28]. The terms are ranked in descending order and the terms which have a rank above a certain threshold th
                        
                           iy
                         (a cut off point for section i and year y) are chosen. These selected terms are denoted as 
                           
                              
                                 q
                                 
                                    i
                                    k
                                 
                                 t
                              
                              
                                 
                                    (
                                    y
                                    )
                                 
                                 ′
                              
                           
                        .

Note that the same D
                        
                           L
                         are utilized in Eq. (1) for all years since our focus is to measure the timeliness aspects of D
                        
                           H
                        
                        .
                     


                        Output: Query generation phase results in several queries which comprise terms 
                           
                              
                                 q
                                 
                                    i
                                    k
                                 
                                 t
                              
                              
                                 
                                    (
                                    y
                                    )
                                 
                                 ′
                              
                           
                        . For example under “Nephropathy Screening and Treatment” section, while in 2013 the generated section specific queries include terms and phrases such as diabetes care, type diabetes, blood glucose, screening, therapy, diet, urine albumin excretion, and chronic kidney disease, in 2008 the generated section specific queries comprise terms and phrases such as diabetes care, type diabetes, glucose control, kidney, complications and etiology.


                        Input: The test dataset D′ which includes high quality websites with actual update time labels and the queries generated in Step 2 are the inputs of this phase.


                        Method: The generated queries in Step 2 are used by the text retrieval system to compute scores for D′ and the score of each website is computed by calculating the average score of its web pages [16]. Okapi BM25 is utilized for the retrieval task [29]. In this formula, tf (term frequency) and idf (inverse document frequency) values and the relative length of a document are taken into account. Okapi BM25 computes the score for each term 
                           
                              t
                              ∈
                              
                                 q
                                 
                                    i
                                    k
                                 
                                 t
                              
                              
                                 
                                    (
                                    y
                                    )
                                 
                                 ′
                              
                           
                         in D′ in the query as follows:
                           
                              (2)
                              
                                 
                                    
                                       w
                                       
                                          t
                                          ,
                                          D
                                       
                                       
                                          B
                                          M
                                          25
                                       
                                    
                                    =
                                    
                                       Q
                                       
                                          w
                                          t
                                       
                                    
                                    ∗
                                    t
                                    
                                       f
                                       
                                          t
                                          ,
                                          D
                                       
                                    
                                    *
                                    
                                       
                                          log
                                          (
                                          (
                                          N
                                          −
                                          
                                             n
                                             t
                                          
                                          +
                                          0.5
                                          )
                                          /
                                          (
                                          
                                             n
                                             t
                                          
                                          +
                                          0.5
                                          )
                                          )
                                       
                                       
                                          2
                                          ∗
                                          (
                                          0.25
                                          +
                                          0.75
                                          ∗
                                          (
                                          (
                                          l
                                          d
                                          )
                                          /
                                          (
                                          a
                                          v
                                          d
                                          l
                                          )
                                          )
                                          )
                                          +
                                          t
                                          
                                             f
                                             
                                                t
                                                ,
                                                D
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where Q
                        
                           wt
                         is the weight attached to the term t. N is the total number of documents in D′. In the corpus, n
                        
                           t
                         is the number of documents containing the term t, and log((N
                        −
                        n
                        
                           t
                        
                        +0.5)/(n
                        
                           t
                        
                        +0.5)) is the idf value of the term t. ld implies the length of the documents and avdl stands for the average length of documents in the corpus. Note that this process is undertaken for all years and all sections of RG.
                           
                              (3)
                              
                                 
                                    
                                       S
                                       
                                          i
                                          y
                                       
                                       w
                                    
                                    (
                                    D
                                    ,
                                    Q
                                    )
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                t
                                                ∈
                                                Q
                                             
                                          
                                          
                                             
                                                w
                                                
                                                   t
                                                   ,
                                                   D
                                                
                                             
                                          
                                       
                                       
                                          N
                                          
                                             P
                                             w
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The score of a website 
                           
                              
                                 S
                                 
                                    i
                                    y
                                 
                                 w
                              
                           
                         is computed by calculating the average score of its web pages where i, y, p and w imply section, year, page and website in D′ respectively. NP
                        
                           w
                         is the number of pages of website w.

For timeliness analysis, a website is scored against each RG section (in this case ADA guideline's sections) for each year between 2008 and 2013. Section specific queries 
                           
                              
                                 q
                                 
                                    i
                                    k
                                 
                                 t
                              
                              
                                 
                                    (
                                    y
                                    )
                                 
                                 ′
                              
                           
                         generated in Step 2 are run and the query which returns the highest score 
                           
                              
                                 S
                                 
                                    i
                                    y
                                 
                                 w
                              
                           
                         is used to determine the update time of the website. This process is repeated for each section i and the update time of the website is predicted as the most common year assigned by all sections. The formulas are as given below. For the following equations, w is the website, y is the year that the context belongs to, and 
                           
                              
                                 S
                                 
                                    i
                                    y
                                 
                                 w
                              
                           
                         is the score obtained for the year y and section i.
                           
                              (4)
                              
                                 
                                    U
                                    p
                                    d
                                    a
                                    t
                                    e
                                     
                                    t
                                    i
                                    m
                                    
                                       e
                                       i
                                       w
                                    
                                    =
                                    
                                       argma
                                    
                                    
                                       
                                          x
                                       
                                       y
                                    
                                    (
                                    
                                       S
                                       
                                          i
                                          y
                                       
                                       w
                                    
                                    )
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    
                                       Update
                                    
                                     
                                    
                                       tim
                                    
                                    
                                       
                                          e
                                       
                                       w
                                    
                                    =
                                    mode
                                    (
                                    
                                       Update
                                    
                                     
                                    
                                       tim
                                    
                                    
                                       
                                          e
                                       
                                       i
                                       w
                                    
                                    )
                                 
                              
                           
                        
                     


                        Output: The predicted update time of a given web site 
                           
                              U
                              p
                              d
                              a
                              t
                              e
                               
                              t
                              i
                              m
                              
                                 e
                                 w
                              
                           
                        .

@&#EXPERIMENTS AND RESULTS@&#

The experiments comprises 3 main steps; automatic term recognition, timeliness prediction according to years and timeliness prediction according to time intervals. Term recognition experiments are accomplished during training phase and timeliness predictions are undertaken under the testing phase.

Although Glossex method has been reported as a superior method in term extraction in the literature [27] and it is utilized as the base method in this paper, to confirm its advantage, an experiment is also conducted in order to measure whether it is able to capture the relevant and significant terms from the guideline. For the methods that require background knowledge, the Open American National Corpus (OANC) is used as a general corpus which includes 14.6 million words [30].

As an evaluation metric, the precision of the methods are reported at 3 points (cuts); first 20 highly ranked terms, first 200 and first 2000 terms which are widely used in the literature for the evaluation of ATR methods [26]. The precision is defined as:
                           
                              (6)
                              
                                 
                                    
                                       Precision
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                0
                                             
                                             
                                                
                                                   |Recognized|
                                                
                                             
                                          
                                          
                                             |
                                             
                                                t
                                                i
                                             
                                             ∈
                                             
                                                Reference
                                             
                                             |
                                          
                                       
                                       
                                          
                                             |Recognized|
                                          
                                       
                                    
                                 
                              
                           
                        where Recognized is a set of highly ranked terms extracted by the method and |t
                        
                           i
                        
                        ∈
                        Reference| is 1 if term t
                        
                           i
                         is in the Reference set containing the list of correct terms. Otherwise it is set to 0. In this experiment, two Reference sets are manually generated to be able to evaluate the metrics: the first list comprises the terms that are added to the guidelines for the first time and the second list includes all the terms in the guidelines. The first list is important since for timeliness analysis, the newly added terms are the most critical elements. For example all the diabetes web pages are expected to include diabetes term but not a term such as VEGF therapy which was first introduced in the literature in 2013. The experiment confirms the findings in the literature for ADA guidelines [26]. Table 1
                         shows the precision results of Weirdness, Glossex, Likelihood Ratio and C-value.

The best results are obtained using Glossex method which ranked first in performance in 3 out of 4 cases. Weirdness and Glossex produce similar results as Glossex is the extension of Weirdness.

The queries in Step 2 of the training phase are generated using the training data set in Table 2
                        .

The training data set comprises 2 high quality web sites’ (D
                        
                           H
                        ) archives and 3 low quality (D
                        
                           L
                        ) web sites’ archives. For low quality websites archives, the ones with capture dates closer to 2006 are preferred since the earliest web site archives will only include base domain specific keywords. The captures of the first two web sites (D
                        
                           H
                        ) belong to 2006 and 2007 since for some web sites, 2006 version was not archived by Archive-it. For high quality websites of each year, their corresponding archives in Archive-it are used.

The experiments are conducted using the web sites in Table 3
                         and the results are given in Table 4
                        . On average, the proposed methodology correctly classifies 65% of the websites into the appropriate years. When the misclassified captures are inspected, it is observed that the proposed framework tends to assign dates which are earlier than their publication dates. This could be due to the fact that even though the contents of the Web sites are checked frequently, their contents are not up to date with the latest research as stated in [4]. The most common failure is labeling the 2013 websites as 2012 and 2011 websites as 2010 all of which appear in the same clusters.

Post and Mainous argue that update times displayed at the websites do not guarantee the accuracy of the information and the recent findings are covered in the websites with a significant delay [2]. In addition, there may be insignificant changes between the consequent years. Consequently, predictions according to predetermined intervals are also undertaken in this study. The same test dataset in Experiment 2 is utilized. The cluster indices corresponding to the predicted years are compared with the cluster indices corresponding to the actual years of the web sites. As can be seen from Table 5
                        , the accuracy is increased to 77%.

When the misclassified captures are inspected, it is observed that the proposed framework still tends to assign dates which are earlier than their publication dates. The websites belonging to cluster 3 are predicted as to belong to cluster 2. This could be due to the fact that we consider many topics regarding diabetes in prediction. As a result, if the authors of a website updates the content of a website for a specific topic only and ignore the others, it causes the framework to miscalculate the update time even according to intervals.

@&#DISCUSSION@&#

The proposed methodology estimates the timeliness of diabetes web sites according to EBM with an average accuracy of 77%. The method is first of its kind in the literature, to our knowledge. Conventional indicators such as the last update time or presence of broken links have shown to be inadequate to express timeliness according to EBM since these indicators do not consider a reference guideline to assess the time interval the content belongs to. In addition, the proposed methodology exceeds the performance of the current temporal information retrieval techniques since the dates given in the text explicitly or implicitly does not guarantee content to reflect up-to-date information. The methodology takes the quality assessment techniques a step forward by analyzing the high quality contents in means of another important quality dimension timeliness which is disregarded in current automated frameworks.

The method is scalable to other health topics by using periodically published EBM guidelines for the relevant topic. These reference guidelines are expected to include specialized terms varying in different time intervals:
                        
                           •
                           To generate timeliness queries, the use of an EBM guideline is essential. Such guidelines are available for some health topics such as depression, arthritis, breast cancer or cardiovascular disease at Cochrane Library [31]. The Cochrane Collaboration provides reviews for several health conditions based on a comprehensive and expert analysis of the available literature. For those that are not addressed by Cochrane library, EBM Online [32] or Database of Abstracts of Reviews of Effectiveness (DARE) [33] can be used to construct “timeliness” queries. These sources contain details of systematic reviews that evaluate the effects of healthcare interventions.

The extraction of specialized terms plays an important role in evaluating timeliness. If there is no significant update or terminology shift in the referenced guidelines, the method may not able to work efficiently. However, many health topics advance day-by-day and new terminology (such as new treatments or drugs) emerge and they are widely used for a period of time.

The precision of the query generation phase strongly depends on the term distribution on training websites. If the websites included in the training set have a delay in publishing the latest research, the generated timeliness queries will fail to meet the expectations and to discriminate the up-to-date contents from the out-dated ones. However, the success of any classifier models in machine learning domain relies heavily on training data.

This study did not make use of any explicit normalization. Okapi 25 uses a term frequency normalization by document length but it is known in the literature that it is sensitive to long documents which can be scored unfairly due to this deficiency [34]. However, web pages are generally short documents due to usability issues and web masters split the content into multiple pages which are linked if the content is comprehensive. As a result, this deficiency does not pose a problem in this study.

In non-health areas, the quality could be harder to define. There may be no clear notion of evidence-based quality. For such areas, it will not be possible to generalize the framework. In addition, this method is scalable to specialized web sites such as diabetes web sites. For general purpose health web sites which cover several health topics, the scalability will be limited.

@&#CONCLUSION@&#

With the exponential growth of health related information presented on the web, the need for tools to query up-to-date, relevant and high quality information has become paramount. In health domain, although timeliness is one of the most important dimensions of information quality, current query systems are far from ensuring timeliness. This paper provides a research effort to extend common information retrieval techniques to assess timeliness in diabetes domain. It is demonstrated that a framework based on automatic term recognition, relevance feedback and information retrieval can be a valid indicator for content timeliness assessment of diabetes websites according to the current EBM recommendations.

Diabetes is a common health condition from which at least 171 million people suffer in the world. The proposed method might be of benefit to a wider community including web masters, doctors, and other information seekers in diabetes domain. However, a further effort is required to make the proposed framework to be fully automated and consumer-friendly form in the future. The validity of the proposed method in other domains can be investigated which is out of scope in this paper.

@&#FUTURE WORK@&#

For general purpose websites, the proposed model can be applied to each health topic separately and all the timeliness predictions can be combined to produce an overall score. However, it may be costly to obtain both evidence based medicine guidelines, low and high quality websites and their archives to build the framework. The method can be improved so as to work with generic web sites.

Another open issue that needs further analysis relates to the weights assigned to the sections while scoring the websites. The method can give weight to each section in the reference guideline differently so as to be in proportion with its variation. By this way, the sections which have changed significantly compared to the other sections in consequent years will have a higher impact in scoring of a web site whereas sections which have not changed at all will have small or no impact.

In this paper, Glossex method has been preferred as it is simple and fast approach to use. Although successful results are obtained using Glossex, it does not always guarantee to extract terms that are solely related to medicine. As a consequence, the use of existing tools to extract medical concepts such as metaMap [35] or cTakes [36] can be considered in the future which take into account semantics in the text.

An alternative semi-automatic approach can be developed in the future to achieve better and more trustworthy results where the automated part provides filtered initial results to the users.

The authors declare that there are no conflicts of interest.

@&#REFERENCES@&#

