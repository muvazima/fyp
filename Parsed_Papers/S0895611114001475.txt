@&#MAIN-TITLE@&#Vision-based endoscope tracking for 3D ultrasound image-guided surgical navigation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A camera tracking method by combining 3D ultrasonography with endoscopy is proposed.


                        
                        
                           
                           This self-contained framework does not require any external tracking devices.


                        
                        
                           
                           It addresses potential issues in conventional visual tracking for fetoscopy.


                        
                        
                           
                           Endoscope tracking ability was demonstrated in phantom and ex vivo placenta imaging.


                        
                        
                           
                           Potential contribution may extend to other minimally invasive procedures.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Vision-based tracking

Surgical navigation

Ultrasound imaging

Endoscopy

Minimally invasive fetal surgery

@&#ABSTRACT@&#


               
               
                  This work introduces a self-contained framework for endoscopic camera tracking by combining 3D ultrasonography with endoscopy. The approach can be readily incorporated into surgical workflows without installing external tracking devices. By fusing the ultrasound-constructed scene geometry with endoscopic vision, this integrated approach addresses issues related to initialization, scale ambiguity, and interest point inadequacy that may be faced by conventional vision-based approaches when applied to fetoscopic procedures. Vision-based pose estimations were demonstrated by phantom and ex vivo monkey placenta imaging. The potential contribution of this method may extend beyond fetoscopic procedures to include general augmented reality applications in minimally invasive procedures.
               
            

@&#INTRODUCTION@&#

This work is motivated by the clinical need for surgical navigation in minimally invasive fetal surgery, as well as the operational limitations of existing techniques. One example of minimally invasive fetal surgery is the treatment of twin-to-twin transfusion syndrome (TTTS) [1]. This condition is life-threatening and must be treated before birth, typically between 17 and 26 weeks of gestation [2,3]. To minimize medical complications from a large uterine incision, minimally invasive fetal endoscopic surgeries have been introduced [4,5]. However, minimal access to the surgical site results in limited visibility and dexterity.

Therefore, it is important to equip surgeons with surgical navigation technologies that ease the operational constraints of minimally invasive fetoscopic surgery. Positional information and spatial awareness are important prerequisites for most, if not all, surgical navigation applications. To address this need, a self-contained method that combines 3D ultrasonography and endoscopy for fetoscopic camera tracking is proposed in this work.

A wide range of research and development efforts have focused on instrument tracking in minimally invasive procedures. However, applying existing state-of-the-art techniques to fetoscopic procedures and endoscopic tissue imaging remains challenging. In this section, we will discuss some popular options and their limitations for fetoscopic procedures.

A common approach for instrument localization is to use optical tracking systems [6–8]. These external localization systems usually require an appropriately equipped operating theater and a team of specialized technical personnel to support the operation. In addition, they are associated with certain operational constraints. For example, lines-of-sight between the optical tracker and markers must be cautiously maintained by the surgical team to avoid disrupting the tracking process [9]. Previous studies [10–12] on surgical navigation applications in fetal surgery have used optical tracking systems for instrument localization that require the attachment of markers to surgical instruments and registration between multiple coordinate systems.

While electromagnetic trackers do not require strict maintenance of lines-of-sight, they are prone to noise, especially in a modern operating room that is filled with electronic devices. To address unstable performances due to noise, multisensor data fusion using an electromagnetic tracking device and an inertia measurement unit (IMU) attached to the surgical tool being tracked has been proposed [13]. The use of an IMU has also been commonly applied to the sub-task of orientation tracking [14]. This approach is an attractive option for the development of low-cost tracking applications. However, it remains challenging in practice because of issues related to tracking initialization, drift errors, and accuracy. In addition, an instrument can only be tracked when it is equipped with the tracking sensors and properly calibrated.

Apart from tracker-based approaches, vision-based tracking for surgical navigation has also been studied extensively [15]. The relevant options for our application include those that exploit the simultaneous localization and mapping (SLAM) framework to solve tracking as a Structure-from-Motion (SfM) problem [16–18]. Shape-from-Shading methods [19] are not suitable for fetoscopic imaging due to the unpredictable characteristics of the amniotic medium and the need to minimize illumination, which may be harmful to the fetus's eye. Although this method offers an elegant solution that is free from most of the operational limitations associated with external sensors, vision-based camera tracking alone is inadequate for our application. In addition to the issue of initialization, the absence of rich texture in fetoscopic placental imaging also makes it extremely challenging to acquire sufficient feature points for accurate pose estimation. This problem is further exacerbated by the close proximity of the camera and scenes during fetoscopic procedures.

A self-contained tracking framework that exploits information from ultrasound and endoscopic imaging is proposed in this study. The designed approach is based on ultrasound image-based localization to register the initial camera position with its scene geometry, followed by subsequent vision-based tracking. This development is relevant to the application of surgical navigation in ultrasound image-guided fetoscopic procedures.

@&#METHODS@&#

@&#OVERVIEW@&#

In this section, the details of the proposed method are presented thematically, in the order of the workflow. The proposed framework integrates 3D ultrasound image- and endoscopic vision-based tracking. It was designed with the intention of being incorporated into surgical workflows with minimal procedural disruptions, as it is a tracker-less approach that only uses information from imaging instruments that are already in place for minimally invasive fetoscopic surgeries. By exploiting prior knowledge of the scene geometry obtained from 3D ultrasonography and a pose estimation algorithm that solves a Perspective-n-Point (PnP) problem, free-hand endoscopic motion can be tracked, even under poor imaging conditions that yield sparse interest points. Fig. 1
                         illustrates an overview of the workflow.

First, registration of the initial camera position with its scene geometry is done via ultrasound image-based localization. Subsequently, the matching feature points from adjacent views are extracted. These corresponding feature points are labeled and assigned as interest points to facilitate 3D pose estimation. The 3D coordinates of these interest points are recovered from a depth map, based on the known initialized camera position. Their 2D projections in successive frames are then inferred sequentially via inter-frame feature matching of the fetoscopic views. With a given set of 2D and 3D interest points in each frame, the pose of the camera can be recovered. Ultrasound image-based initializations can also be programmed at coarse intervals to relocalize the fetoscope's positions, hence correcting cumulative errors. For easy reference to the processes and variables that are explained in detail in the next four subsections, a flowchart is presented in Fig. 2
                        .

As mentioned, our pose estimation framework is a recursive process that computes the present position based on the known camera position in the previous frame, starting from the ultrasound image-based localization of the instrument in the surgical site. While ultrasound image-based localization enables instrument localization, accurate real-time tracking of freehand endoscope trajectories, as is reported in previous studies [20,21], is not trivial for intraoperative applications. Nevertheless, this option is ideal for initialization. Mung et al. [20] demonstrated a sub-mm accuracy of 0.36±0.16mm over a 3D ultrasound field-of-view with an active sensor attached to the tool tip during interventional procedures. While the reported technique was an attractive option, it requires a special active sensor at the tool tip for accuracy and robustness. Stoll et al. [21] proposed a stereotactic passive marker design and localization method for surgical instrument tracking and achieved a position accuracy of 1.7mm in an ex vivo experiment. However, this method requires the fabrication of a stereotactic marker to be attached to the tool tip. Tool calibration for tracking is also required. Both existing methods introduce external tracking systems.

In this study, we termed the process of registering the initial camera position to the scene geometry in the world coordinate system ultrasound image-based initialization. Although this method was introduced in our earlier work on instrument localization [22] and image mapping applications [23,24], its concept and role in this newly proposed vision-based workflow is briefly explained to ensure a self-contained discussion.

First, we acquire the scene geometry by 3D ultrasound imaging of the placenta and construct a 3D surface model from the 3D ultrasound volume data that comprises 60 slices of 440 pixels by 240 pixels, acquired in 0.5s scan. A region of interest covering the placenta is manually selected, followed by thresholding with an isovalue that is predefined before imaging to produce a meshed surface model, typically consisting of 50,000 vertices, using the Marching Cubes Algorithm [25]. The boundary between the placenta model and the fluid medium creates a distinctive intensity change, allowing for segmentation with a single isovalue thresholding operation. Subsequently, the initial camera position is acquired by localizing an 8cm long, 0.3cm diameter cylindrical fiducial attached at the distal end of the endoscopic shaft, as shown in Fig. 3a. It can be observed in Fig. 3b that the geometrical eccentricity of the shaft is prominent with 3D ultrasound imaging. An iterative Principal Component Analysis (iPCA) approach, which is robust against artifacts, is used to compute the position and orientation of the fiducial. The iPCA approach removes outlier data points obtained in the segmentation with an isovalue by exploiting prior knowledge of the endoscope geometry as a constraint. In each of the iterations, the orientation of the principal component axes are updated with the data points simultaneously projected to the first principal axis for further removal of outliers with a radial distance greater than 150% the shaft radius (50% margin). This process continues until orientation of the principal axes converges. Typically, less than three iterations are required. Details of the iPCA approach can also be found in our previous work [22–24].

Using the fixed transformation between the camera center and the fiducial, which is known from a fiducial–camera calibration, the spatial relationship (u
                        T
                        c,0) between the initial camera position and its scene is established, as illustrated in Fig. 3c. This initialization phase is referred to as frame 0. Subsequent vision-based processing frames are denoted by k. While 3D ultrasound image-based localization provides absolute positioning, independent of errors from previous estimations, the practical viability is limited by the acquisition rate and the need for multiple sampling for robustness. Hence, this step is incorporated only for initialization and camera relocalization. The average localization error is approximately 1.32mm and 1.6°. Interested readers may refer to our previous studies [22,23] for a more detailed performance evaluation of this technique.

While a detailed discussion of underwater camera calibration is beyond the scope of this paper, it is worth noting that calibration in the fluid medium, in which the specimens are immersed, is required because differences in the optical properties of the medium result in different intrinsic parameters. Several calibration techniques [26–28] have been reported. One technique based on intraoperative calibration of the endoscopic camera [29] has been demonstrated in a study on mosaicing of endoscopic placenta images [30]. For administrative convenience, we used the same fluid medium (saline solution; 9g/l NaCl) throughout this study and pre-calibrated the camera in this solution. A standard camera calibration process using the toolbox developed in Matlab™ by Bouget [31] was used to perform underwater calibration for computing the intrinsic parameters of the camera for the optical properties of the medium used in the experiments.

All images were corrected for radial and tangential lens distortions prior to feature matching according to the Brown–Conrady model [32], which distorts pixel coordinates 
                           
                              
                                 
                                    
                                       
                                          X
                                       
                                       ¯
                                    
                                 
                                 
                                    d
                                    i
                                    s
                                    t
                                    o
                                    r
                                    t
                                 
                              
                           
                         according to the expression,
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             
                                                X
                                             
                                             ¯
                                          
                                       
                                       
                                          d
                                          i
                                          s
                                          t
                                          o
                                          r
                                          t
                                       
                                    
                                    =
                                    (
                                    1
                                    +
                                    
                                       k
                                       1
                                    
                                    
                                       r
                                       2
                                    
                                    +
                                    
                                       k
                                       2
                                    
                                    
                                       r
                                       4
                                    
                                    )
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                      ¯
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      y
                                                      ¯
                                                   
                                                
                                             
                                          
                                       
                                    
                                    +
                                    (
                                    
                                       
                                          
                                             
                                                
                                                   l
                                                   1
                                                
                                             
                                          
                                          
                                             
                                                
                                                   l
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    )
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      2
                                                      
                                                         x
                                                         ¯
                                                      
                                                      
                                                         y
                                                         ¯
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         r
                                                         2
                                                      
                                                      +
                                                      2
                                                      
                                                         
                                                            y
                                                            ¯
                                                         
                                                         2
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         r
                                                         2
                                                      
                                                      +
                                                      2
                                                      
                                                         
                                                            x
                                                            ¯
                                                         
                                                         2
                                                      
                                                   
                                                
                                                
                                                   
                                                      2
                                                      
                                                         x
                                                         ¯
                                                      
                                                      
                                                         y
                                                         ¯
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                      ¯
                                                   
                                                
                                                
                                                   
                                                      y
                                                      ¯
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 T
                              
                           
                         is the normalized image projection 
                           
                              
                                 K
                              
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                         /
                                                         z
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         y
                                                         /
                                                         z
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 T
                              
                           
                        , and r represents 
                           
                              norm
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            x
                                                            ¯
                                                         
                                                      
                                                      
                                                         
                                                            y
                                                            ¯
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       T
                                    
                                 
                              
                           
                        . The radial and tangential distortion coefficients are denoted by k
                        1–2 and l
                        1–2, respectively. The underwater calibration also solved the three-by-three intrinsic camera matrix,
                           
                              (2)
                              
                                 
                                    
                                       K
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         f
                                                         x
                                                      
                                                   
                                                
                                                
                                                   a
                                                
                                                
                                                   
                                                      
                                                         i
                                                         o
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   
                                                      
                                                         f
                                                         y
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         j
                                                         o
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   0
                                                
                                                
                                                   0
                                                
                                                
                                                   1
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where (f
                        
                           x
                        
                        , f
                        
                           y
                        )T, (i
                        
                           c
                        
                        , j
                        
                           c
                        ) T, and a are the focal length, principal point, and skew coefficient, respectively.

After initialization, the next step in the workflow is to establish a one-to-one relationship H
                        match: 
                           k
                        (i
                        
                           p
                        
                        , j
                        
                           p
                        )↦
                           k-1(i
                        
                           p
                        
                        , j
                        
                           p
                        ) by matching features in adjacent views. The Speed-Up-Robust-Features (SURF) algorithm, which is a commonly used scale-invariant feature detection method proposed by Bay et al. [33], was adopted for this procedure. Briefly, blob-like features are extracted using the FAST-Hessian detector as a score for features and then labeled with the SURF descriptor of 64 elements, representing a distribution of the Haar-wavelet responses of the blob neighborhood. These features are scale and rotation invariant, as shown in Fig. 4a, which depicts severely rotated and scaled feature matches from phantom placental views. The SURF descriptors are also able to obtain reasonably sufficient correspondence during placental imaging, even in a scene with poor texture, as illustrated by the example in Fig. 4b. This result is an important attribute for tissue imaging. By comparing the Sum of Squared Differences (SSD) of the feature pairs against a threshold (equal to 0.01 in this study), matched pairs denoted by 
                           k−1
                        I
                        =
                        
                           k−1(i
                        
                           p
                        
                        , j
                        
                           p
                        ) and 
                           k
                        
                        I
                        
                        =
                        
                        
                           k
                        (i
                        
                           p
                        
                        , j
                        
                           p
                        ) can be identified.

Because the imaging conditions and scene texture associated with fetoscopic imaging are poor, feature matching is not ideal compared to other general imaging procedures. This limitation is one of the rationales for adopting an integrated framework that combines geometrical information of the scene from ultrasonography. Typically, approximately 20 reliable feature matches are required for pose estimation, which will be explained in detail later. This requirement was met for imaging of ex vivo placental specimens, as verified by visual confirmation of the matching results. Fig. 5
                         shows representative matching outcomes of various inlier features. In general, 20–30 reliable matches (Fig. 5 (a–b)) could be readily extracted, and at least 10 pairs could be obtained, even with poor texture conditions (Fig. 5(c)). It should be noted that ex vivo placenta has poorer texture compared to the in vivo condition with blood circulation.

We further removed outliers using the RANSAC algorithm and enforced a projective transformation relationship between adjacent frames. For N randomly selected samples, 
                           k−1
                        I and 
                           k
                        
                        I are removed if the algebraic distance
                           
                              (3)
                              
                                 
                                    norm
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            I
                                                         
                                                         
                                                         
                                                         
                                                         
                                                         
                                                            k
                                                            −
                                                            1
                                                         
                                                      
                                                      ,
                                                   
                                                
                                                
                                                   
                                                      ∏
                                                      
                                                         
                                                            
                                                               
                                                                  I
                                                               
                                                               
                                                               
                                                               
                                                               
                                                               k
                                                            
                                                            :
                                                            
                                                               
                                                                  H
                                                               
                                                               
                                                                  proj
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                (
                                                
                                                   x
                                                   
                                                   
                                                   
                                                   
                                                   k
                                                
                                                −
                                                
                                                   z
                                                   
                                                   
                                                   
                                                   
                                                   k
                                                
                                                ⋅
                                                
                                                   i
                                                   
                                                   
                                                   
                                                   
                                                   
                                                      k
                                                      −
                                                      1
                                                   
                                                
                                                )
                                             
                                             2
                                          
                                          −
                                          
                                             
                                                (
                                                
                                                   y
                                                   
                                                   
                                                   
                                                   
                                                   k
                                                
                                                −
                                                
                                                   z
                                                   
                                                   
                                                   
                                                   
                                                   k
                                                
                                                ⋅
                                                
                                                   j
                                                   
                                                   
                                                   
                                                   
                                                   
                                                      k
                                                      −
                                                      1
                                                   
                                                
                                                )
                                             
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        is greater than the threshold 
                           τ
                        . Here, П denotes the projection of 
                           k
                        
                        I via an estimated three-by-three homography matrix H
                        proj between image coordinates 
                           k
                        (i, j)T and 
                           k+1(i, j)
                           T
                         that is defined as
                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           
                                                                              x
                                                                           
                                                                           
                                                                              y
                                                                           
                                                                           
                                                                              z
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                            T
                                                         
                                                         
                                                         
                                                         
                                                         
                                                         k
                                                      
                                                      =
                                                      
                                                         
                                                            H
                                                         
                                                         
                                                            proj
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           
                                                                              
                                                                                 
                                                                                    i
                                                                                 
                                                                                 
                                                                                    j
                                                                                 
                                                                                 
                                                                                    1
                                                                                 
                                                                              
                                                                           
                                                                        
                                                                     
                                                                  
                                                                  T
                                                               
                                                               
                                                               
                                                               
                                                               
                                                               k
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           
                                                                              i
                                                                           
                                                                           
                                                                              j
                                                                           
                                                                           
                                                                              1
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                            T
                                                         
                                                         
                                                         
                                                         
                                                         
                                                         
                                                            k
                                                            −
                                                            1
                                                         
                                                      
                                                      =
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           
                                                                              
                                                                                 
                                                                                    x
                                                                                 
                                                                                 
                                                                                    y
                                                                                 
                                                                                 
                                                                                    z
                                                                                 
                                                                              
                                                                           
                                                                        
                                                                     
                                                                  
                                                                  T
                                                               
                                                               
                                                               
                                                               
                                                               
                                                               k
                                                            
                                                         
                                                         /
                                                         
                                                            
                                                               z
                                                               
                                                               
                                                               
                                                               
                                                               k
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The value of N is updated dynamically based on the proportion of identified inliers, according to the original RANSAC algorithm [34]. By iteratively updating the estimated projective matrix H
                        proj until the distance metric of n points,
                           
                              (5)
                              
                                 
                                    
                                       S
                                    
                                    =
                                    
                                       ∑
                                        
                                       n
                                    
                                    
                                       threshold
                                       
                                          
                                             norm
                                             
                                                
                                                   
                                                      
                                                         I
                                                      
                                                      m
                                                      
                                                      
                                                      
                                                      
                                                         k
                                                         −
                                                         1
                                                      
                                                   
                                                   ,
                                                   ∏
                                                   
                                                      
                                                         
                                                            
                                                               I
                                                            
                                                            m
                                                            
                                                            
                                                            
                                                            k
                                                         
                                                         :
                                                         
                                                            
                                                               H
                                                            
                                                            
                                                               proj
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             ,
                                             τ
                                          
                                       
                                    
                                 
                              
                           
                        converges, an optimal homography producing the maximum number of inliers is obtained. For our method, approximately 20 inliers are typically sufficient for interest point assignment, which is explained in the next section.

A prerequisite for solving pose estimation as a PnP problem is 2D–3D point correspondence. This process relates 3D interest points to their 2D projections on a camera plane of an unknown position. In this work, we solved this point correspondence problem by mapping the 3D coordinates of interest points to their 2D projection on the camera plane in frame k, given the camera position in frame k−1.

First, matched features 
                           k−1(i
                        
                           p
                        
                        , j
                        
                           p
                        ) and 
                           k
                        (i
                        
                           p
                        
                        , j
                        
                           p
                        ) on two adjacent views are selected as interest points p
                        
                           l
                        
                        ={p1
                        .p
                           L
                        }, which can be expressed in the ultrasound imaging reference frame {u} as u
                        p
                        
                           l
                        , where l denotes the label in the set of L matching feature inliers. The problem can then be formulated as one that aims to calculate the 3D position of the interest points, given a known camera position (i.e., solve for u
                        p
                        
                           l
                         given uTc, 
                           k−1). It should be noted that the 3D interest points are passive features and not fixed throughout tracking. Unlike most PnP cases, in which fixed interest points on known structures are usually used, in our application, the interest points are updated every frame according to the matching features extracted from two adjacent images. Approaches that introduce active interest points are not feasible due to the nature of fetoscopic imaging conditions and the environment, which limit the introduction of artificial markers.

Because there is no closed-form solution that maps the image coordinates (i
                        
                           p
                        
                        , j
                        
                           p
                        ) to the corresponding 3D coordinates (x
                        p, y
                        p, z
                        p) on the parameterized surface patches of the scene, we proposed a method to construct sub-pixel depth maps for recovering the 3D coordinates. In this method, the 3D vertices of the ultrasound image model, denoted by u
                        x
                        =
                        u(x y z)T, are projected to the camera plane to obtain their image coordinates 
                           k−1
                        I according to Eq. (6).
                           
                              (6)
                              
                                 
                                    
                                       
                                          z
                                       
                                       
                                       
                                       
                                       
                                       
                                          k
                                          −
                                          1
                                       
                                    
                                    
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  i
                                                               
                                                               
                                                                  j
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                                T
                                             
                                             
                                             
                                             
                                             
                                             
                                                k
                                                −
                                                1
                                             
                                          
                                       
                                    
                                    =
                                    
                                       K
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            R
                                                         
                                                         u
                                                         
                                                         
                                                         
                                                         
                                                            k
                                                            −
                                                            1
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  t
                                                               
                                                               u
                                                               
                                                               
                                                               
                                                               
                                                                  k
                                                                  −
                                                                  1
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            y
                                                         
                                                         
                                                            z
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          T
                                       
                                       
                                       
                                       
                                       
                                       u
                                    
                                 
                              
                           
                        
                     

Here, K and (
                           k−1
                        R
                        u|
                           k−1 
                        t
                        u) denote the intrinsic and extrinsic camera matrices, respectively. 
                           k−1
                        R
                        u and 
                           k−1 
                        t
                        u are the rotation matrix and translation vector, respectively, from the camera's viewpoint at frame k−1.

As illustrated in Fig. 6
                        , the depth information 
                           k−1
                        z bounded by the camera region of interest (ROI) is interpolated with class C1 continuous functions by the Delaunay triangulation of scattered data points (i, j, 
                           k−1
                        z) to form a dense depth map that can be written in Monge's form as
                           
                              (7)
                              
                                 
                                    
                                       Z
                                    
                                    
                                       
                                          i
                                          ,
                                          j
                                       
                                    
                                    =
                                    Ψ
                                    
                                       
                                          i
                                          ,
                                          j
                                          ,
                                          
                                             z
                                             
                                             
                                             
                                             
                                             
                                                k
                                                −
                                                1
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

This interpolation allows for queries at any sub-pixel location (i, j) and smoothes the data based on neighboring conditions. The depth Z(i
                        
                           p
                        , j
                        
                           p
                        ) associated with the pixel coordinates 
                           k−1(i
                        
                           p
                        
                        , j
                        
                           p
                        ) of an interest point p
                           l
                         from the camera reference system in frame k−1 can consequently be recovered. Using a pin-hole camera projection model, the 3D camera-centric coordinates of the interest points can be express as
                           
                              (8)
                              
                                 
                                    
                                       
                                          p
                                       
                                       l
                                       
                                       
                                       
                                       
                                          k
                                          −
                                          1
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     i
                                                                     p
                                                                  
                                                                  −
                                                                  
                                                                     i
                                                                     o
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                         
                                                         
                                                         
                                                         
                                                            k
                                                            −
                                                            1
                                                         
                                                      
                                                      ⋅
                                                      
                                                         
                                                            
                                                               Z
                                                               
                                                               
                                                               
                                                               
                                                               
                                                                  k
                                                                  −
                                                                  1
                                                               
                                                            
                                                            
                                                               
                                                                  
                                                                     i
                                                                     p
                                                                  
                                                                  ,
                                                                  
                                                                     j
                                                                     p
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               f
                                                               x
                                                            
                                                         
                                                      
                                                      ,
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     j
                                                                     p
                                                                  
                                                                  −
                                                                  
                                                                     j
                                                                     o
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                         
                                                         
                                                         
                                                         
                                                            k
                                                            −
                                                            1
                                                         
                                                      
                                                      ⋅
                                                      
                                                         
                                                            
                                                               Z
                                                               
                                                               
                                                               
                                                               
                                                               
                                                                  k
                                                                  −
                                                                  1
                                                               
                                                            
                                                            
                                                               
                                                                  
                                                                     i
                                                                     p
                                                                  
                                                                  ,
                                                                  
                                                                     j
                                                                     p
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               f
                                                               y
                                                            
                                                         
                                                      
                                                      ,
                                                   
                                                
                                                
                                                   
                                                      
                                                         Z
                                                         
                                                         
                                                         
                                                         
                                                         
                                                            k
                                                            −
                                                            1
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               i
                                                               p
                                                            
                                                            ,
                                                            
                                                               j
                                                               p
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where (i
                        
                           o
                        , j
                        
                           o
                        ) and (f
                        
                           x
                        , f
                        
                           y
                        ) are the principal point and focal length from the intrinsic parameter matrix K. As the camera position at frame k−1 in the ultrasound image reference frame is known, u
                        p
                        
                           l
                         can also be calculated.

Because a one-to-one map Hmatch: 
                           k
                        (i
                        
                           p
                        
                        , j
                        
                           p
                        )↦
                           k−1(i
                        
                           p
                        
                        , j
                        
                           p
                        ) has already been established in the feature matching process and we have established another relationship H
                           k−1: 
                           k−1(i
                        
                           p
                        
                        , j
                        
                           p
                        )↦p
                        
                           l
                         using Eq. (8), H
                           k
                        : 
                           k
                        (i
                        
                           p
                        
                        , j
                        
                           p
                        )↦p
                        
                           l
                         can be obtained, thereby solving the 2D–3D point correspondence problem for frame k.

Given a set of 2D–3D point correspondences, a wealth of methods are available for pose estimation. One suitable approach is to formulate pose estimation as a PnP problem. This approach usually results in better accuracy and stability compared to the Direct Linear Transformation (DLT) approach, which solves the camera matrix directly without considering prior knowledge of the intrinsic parameters. The latter approach also requires a large number of interest points, which is a limitation for tissue imaging, especially for the case of placenta.

In this work, we used the EPnP algorithm [35] for pose estimations. This method is a non-iterative approach that centers on the concept of solving the coordinates of M virtual control points q
                        ={q
                        
                           1…
                         
                        q
                        
                           m
                        
                        … q
                        
                           M
                        }, with their weighted sum representing the 3D points. To solve for the camera pose at frame k, we therefore represent q in camera-centric coordinates and rewrite (4) as
                           
                              (9)
                              
                                 
                                    
                                       
                                          z
                                       
                                       l
                                       
                                       
                                       
                                       k
                                    
                                    
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        i
                                                                        l
                                                                     
                                                                  
                                                               
                                                               
                                                                  
                                                                     
                                                                        j
                                                                        l
                                                                     
                                                                  
                                                               
                                                               
                                                                  1
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                                T
                                             
                                             
                                             
                                             
                                             
                                             k
                                          
                                       
                                    
                                    =
                                    
                                       K
                                    
                                    
                                       ∑
                                       m
                                       M
                                    
                                    
                                       
                                          λ
                                          
                                             l
                                             m
                                          
                                       
                                    
                                    
                                       q
                                       m
                                       
                                       
                                       
                                       c
                                    
                                 
                              
                           
                        where the weights λ
                        
                           lm
                         are homogeneous barycentric coordinates that sum up to one, and M
                        =4(or 3) for general (or planar) cases. The selected control points q consist of the centroid of p and another 3 (or 2) points that align close to the principal direction of p, as recommended [35]. Subsequently, we solve for the 12 (or 9) coordinates and the depth values 
                           k
                        
                        z
                        
                           l
                        
                        
                           =
                        
                        
                           k
                        {z1 … zL}.

Unlike most PnP methods, which result in exponentially increasing computational cost with increasing numbers of interest points, the computational load in the EPnP algorithm increases linearly. It is, therefore, an ideal candidate for our video tracking approach. In addition, our framework uses passive interest points that are not fixed but instead are assigned based on the non-deterministic feature matching outcomes. The EPnP method is known to perform well, even with noisy non-fixed interest points [36]. Fig. 7
                         illustrates the EPnP implementation in our framework. The pink surface represents the placental scene geometry, while the textured patch depicts the camera views projected onto the constructed surface model. Arrows with solid and dotted lines represent the pose estimation by EPnP and the initial ultrasound image-based localization, respectively.

A diagnostic ultrasound imaging system, ProSound α10 with a 3D tilt-scanning convex sector transducer (AUS-1010; Hitachi Aloka Medical, Ltd.), was used to perform 3D ultrasound scanning while a 5.4mm diameter rigid endoscope with a viewing angle of 80 degrees (LS501D; Shinko Optical Co. Ltd.) illuminated by a Xenon light source (CL-75X-250; Shinko Optical Co. Ltd.) acquired video images of the placenta surface at 30 frames per second via a video capture card (Matrox Morphis eQuad; Matrox Electronic Systems Ltd.).

In all our experiments, the 3D ultrasound probe was mounted to a rigid bracket to minimize the influence of motion artifacts. However, it is theoretically feasible to move the probe to any arbitrary position because we are acquiring the initial camera position with respect to the scene and subsequently relying solely on vision-based tracking. The rationale for fixing the ultrasound transducer in the experiment was to control the introduction of motion artifacts by the 3D tilt-scanning transducer [12], as the amount of motion artifacts depends on the temporal resolution and the magnitude of motion.

We investigated both static position estimation and dynamic motion tracking. The former was carried out by comparing various estimations of fixed positions to baseline measurements acquired by a commercial optical tracking system (POLARIS Vicra, NDI, Ontario, Canada), while the latter was evaluated based on robotic trajectories executed by a mechanism designed to replicate the kinematic constraints associated with minimally invasive procedures, as illustrated in the schematic diagram shown in Fig. 8
                        . The passive manipulator was free to position the endoscope in configurations constrained by a pivotal spherical port attached to the distal end of the flexible linkage. With the flexible linkage locked in a particular position, the camera can roll (±180o), pitch (±30o), yaw (±30o) and translate (<80mm) at the pivotal port above the phantom placenta (the longest dimension is ∼18cm).

A phantom model and a robotic device were used for the phantom-based analytical study. The rationale was to conduct repeatable investigations under controlled conditions to analyze the proposed method. The ability to accurately measure physical landmarks on the phantom model and controlled trajectory also provides reliable baseline data for comparison to evaluate the conceptual soundness of the method. Nevertheless, imaging with a freehand endoscopic camera trajectory was also carried out and evaluated in the phantom-based analysis. The camera positions for endoscopic imaging during the analysis were approximately 3cm from the placenta surface, which is in accordance with conditions used in studies for designing enhanced imaging techniques for photocoagulation [37].

For the analysis of static position estimation, the camera was fixed at a specific position by locking the passive manipulator. During the evaluation of motion accuracy with controlled trajectories, the passive manipulator and flexible linkage were also locked, while the active joints were actuated by an x–y translation stage (SGSP 26-50, Sigma Koki Co. Ltd., Japan) with a linear position resolution of 4×10−6
                        m/pulse and a precision of 1×10−6
                        m. Fig. 9
                         depicts the experimental setup in a water tank test rig equipped with the robotic mechanism.

Validation of the static position analysis was also carried out on ex vivo monkey placenta (the longest dimension was ∼8cm) immersed in saline solution (9g/l NaCl), as illustrated in Fig. 10
                        .

The interior of the enclosure was a free hanging membrane that bags the placenta. Due to the close camera-scene proximity, the chorionic plate (fetus-facing side) that was imaged appeared regionally flat in adjacent endoscopic views. This condition may be a useful assumption for imposing a homography relationship between adjacent views for outlier removal. However, it is important to note that the 3D tracking method is not limited to planar scene geometry.

@&#RESULTS@&#

Each of the static estimations obtained by our vision-based method was compared with a mean measurement of 150 data points on that position obtained using a commercial optical tracking system. Table 1
                         summarizes the absolute mean differences between our estimations and the mean measurement of the tracking system. This analysis is based on 100 position estimations of five different static camera configurations. For each static camera position, 20 estimations were computed with video segments associated with their respective static positions using the vision-based method. By setting the tracker-based nominal measurement as the baseline for comparison, the reported differences can be considered as the positioning errors of our vision-based method. All coordinates are referenced from the ultrasound imaging coordinate system. The standard deviations of the optical tracker-based measurements in Cartesian space were (0.11, 0.07, 0.11) mm for position and (0.026, 0.016, 0.020) degrees for orientation. Because the absolute mean error in Table 1 is comparable to the uncertainty associated with the measurement device (represented by standard deviation of optical tracker), the error was considered insignificant.


                        Fig. 11
                         shows the absolute mean discrepancy of the five configurations for each of the 20 estimations. This analysis provides a more detailed illustration of the accuracy of the vision-based estimations over the 20 sequentially processed frames. It can be observed that error accumulates and is especially obvious in the case of orientation estimations. However, the errors were all within 0.2mm and 1o throughout the 20 frames.


                        Fig. 12
                         depicts 100 position estimations transformed to the camera-centric perspective for their respective initialized frames. This visual representation illustrates the consistency of the proposed vision-based estimations, isolated from the effects of initialization errors. The purpose of this analysis was to investigate the innate uncertainty associated with the vision-based localization method, which is different from the accuracy test presented in Table 1 and Fig. 11. Assuming ideal experimental control in maintaining the static positions, all estimations should coincide with their initialized values, and distances from the origin represent the uncertainty associated with the estimations. It should be noted that Fig. 12 shows the top-views of the 3D data because depth discrepancy was insignificant (<0.05mm in the static position analysis).

Errors in motion estimations were larger compared to those observed for static position estimation. Significant cumulative errors were observed. As the estimations were performed in a sequential manner by inferring the positions from previously matched features, errors are expected to accumulate. To study the effect of error accumulations in the video segments, bundle adjustments and filtering techniques were not applied in this study. Instead, ultrasound image-based relocalization was used to rectify the cumulative error, which is discussed in the next section. The error profiles of five trajectories are presented in Fig. 13
                           . The average error in the 20th frame was 2.2mm. It should be noted that each processing frame was at an interval of 10 acquisition frames. Hence, segments were evaluated approximately every 7s. These trajectories had a displacement of approximately 15–25mm at a camera-scene distance of 30mm. This displacement range is comparable to related studies on tracking stereoscopic cameras using the SLAM framework [17,18]. Both of these previous studies manipulated the camera over a range of 25mm. Noonan et al. [18] reported an absolute error of (1.94, 0.7, 1.7) mm i.e., a Euclidean distance of 2.67mm. While an average error of the study by Mountney et al. was not concluded explicitly in the paper [17], the data presented indicate that the uncertainty of the Z-axis was approximately 0.2cm, while the uncertainties of the other two axes appeared to be insignificant.


                           Fig. 14
                            shows the paths of the five trajectories in the 3D ultrasound image reference frame. In this experiment, the effect of velocity cannot be interpreted explicitly because the vision-based tracking accuracy is dependent not only on camera kinematics but also the scene geometry and imaging conditions, including the illumination and texture. Therefore, we are unable to make any quantitative conclusions regarding the effect of velocity on the tracking accuracy until we are able to adequately control the mentioned parameters. However, interested readers may refer to the discussion on homography estimations with synthesized camera trajectories in an ideally controlled phantom model-based simulation [24,38].

It is important to note that while the results from the motion controlled experiment provide a basis for understanding the influences of camera motion on the proposed framework, actual freehand endoscopic images may be less predictable. Hence, the results of freehand trajectory endoscopic imaging are presented to demonstrate the utility of the proposed framework for tracking a freehand camera trajectory.

The freehand trajectory exhibited an error trend that was similar to that of the controlled trajectory, except that the errors were larger in magnitude. In the trajectory of approximately 30mm, the mean absolute error was 2.69mm in the 30 processed frames (300 acquired frames in 10s). Fig. 15
                            illustrates the discrepancy between estimations from a commercial optical tracking system and our proposed method.

An obvious explanation for the increased error compared to the controlled trajectory experiment is the inevitable jerky motion produced by freehand manipulation. It can be observed in Fig. 15 that a sudden change in direction and acceleration (indicated by sharp expansion of the spatial intervals between frames) can result in increased discrepancies between the optical tracking and vision-based results. This error is likely due to the undesirable influence of motion artifacts on the feature matching results.

In addition, the basis for error comparison is not perfect. Unlike the controlled trajectory experiment, which had an ideal known trajectory for comparison, the freehand experiment assumed that the optical tracker-based reading was perfect as a known baseline. In reality, marker-camera calibration and non-rigidity in distal marker attachment can results in significant measurement uncertainty that amplifies throughout the workspace. Without information based on a robot trajectory, the optical tracker-based motion tracking is easily associated with an uncertainty magnitude on the scale of 2–3mm, which substantially affects the evaluation. This degree of uncertainty, which is workspace dependent, is one of the rationales for designing an integrated framework that is trackerless.

Similar to the phantom-based study, static estimations were compared with measurements made with a commercial optical tracking system using ex vivo monkey placenta to validate the previous analysis with a more realistic placental texture. Table 2
                         summarizes the absolute mean differences between our estimations and the mean measurements obtained with the tracking system for ex vivo placenta imaging. Again, this comparison is based on an analysis of 100 estimations (5 positions×20 estimations) from ex vivo placenta imaging video segments.

The estimation differences were larger in the case of monkey placenta imaging, as shown in Fig. 16
                        . This difference can be attributed to several factors in the entire tracking pipeline, with the most significant being the quality of the ultrasound images, which influences the initialization. The geometrical information acquired from the ultrasound image-based initialization degraded under the ex vivo conditions used for the monkey placenta imaging. Nevertheless, the mean absolute error remained less than one mm and one degree, as reported in Table 2.

The 100 position estimations were also transformed to the camera-centric perspective of their respective initialized frames, as was done in the phantom-based analysis. The estimations are shown in Fig. 17
                        .

An important contribution of this integrated ultrasonography–endoscopy design concept is that it addresses the underlying problem of cumulative error faced in pure vision-based approaches, yet achieves a self-contained navigation system that is independent of external navigation devices. To demonstrate the effectiveness of ultrasound image-based relocalization for addressing the inevitable cumulative error derived from vision-based tracking, we applied the method at the 200th acquired frame of a 400-frame video segment, associated with the trajectory indicated by ‘+’ in Fig. 18
                        . This trajectory, which was tracked using an optical tracker, was assigned as a baseline for comparison. Vision-based pose estimation was done at intervals of 10 frames i.e., ultrasound image-based relocalization was triggered at the 20th processed frame (the same length as in previous evaluations). It can be observed in Fig. 18 that the continuous vision-based tracked position indicated by ‘o’ eventually led to large cumulative errors, whereas ultrasound image-based relocalization, indicated by ‘*’, rectified the cumulative error through reinitialization of the camera pose. The final positional error in the trajectory was reduced from 11.35mm to 4.61mm over a total displacement of 45mm when reinitialization was performed at the turning point after traveling 22.5mm linearly.

The computation time for our current system, implemented on a workstation with an Intel® Core™ i 7-2600 3.4GHz processor, is presented in Fig. 19
                        . The timing profile serves as a conservative gauge for the practical viability of our proposed framework using a tracking rate of at least two frames per second (a higher rate can be achieved with more lenient parameters, but at the cost of lower accuracy). As the system specifications have yet to be finalized because they depend on the actual accuracy requirements and users’ priorities, designing for high speed performance was beyond the scope of our current study.

@&#DISCUSSION@&#

A self-contained tracking framework was designed by combining information from ultrasound and endoscopic imaging. This method has potential for application in an ultrasound image-guided fetoscopic surgery. We propose using ultrasound image-based localization to register the initial camera position with its scene geometry, followed by subsequent vision-based tracking. This approach does not introduce additional procedures, as the two imaging modalities are already used during ultrasound image-guided fetoscopic surgeries. While our method is essentially vision-based, it is augmented with scene geometry information acquired by ultrasound imaging. By using information of the scene geometry from ultrasound images to construct a depth map, our method enables camera pose estimation, even with sparse interest points. Moreover, identifying known anatomical landmarks to solve for the scale ambiguity in conventional pure vision-based techniques [39] is non-trivial because the vascular pattern in every placenta is unique and no distinctive anatomical landmark can be reliably extracted for scale estimation. Incorporating ultrasound image-based localization, which is scale-specific, naturally resolves this issue. The proposed framework also enables relocalization to correct for cumulative errors or tracking failures when necessary.

While the experiments in this study demonstrated feasibility of the proposed framework for endoscopic camera tracking, a more extensive analytical study factoring in the relevant imaging conditions will be conducted in future work to further validate its appropriateness for fetoscopic procedures. We plan to investigate tracking performance for various controlled camera kinematics, scene geometries, and possibly illumination conditions that are closer to the conditions in fetoscopic imaging. This will provide a more comprehensive insight for design considerations and improvement of the current prototypical framework to suit the specific requirements of placenta imaging during photocoagulation for TTTS treatment. The results will consequently provide better justification for validation under in vivo conditions. Limitations in the quality of the endoscopic images acquired in the cloudy amniotic medium can also be address when the method is complemented with imaging enhancement technologies for fetoscopic imaging. Some examples include the use of fluorescence endoscope [40,41], ultra-high sensitive endoscopic camera [42], and a developing technique for hyperspectral imaging of the placental vasculature [43].

Apart from the vision-based tracking phase, ultrasound image-based localization is also crucial for the overall performance. While detailed evaluations have been conducted in previous studies [22–24], issues associated with the robustness of image-based localization under various imaging conditions remain, as observed in this current study. Undesirable ultrasound image artifacts in the monkey placental imaging resulted in lower accuracy, suggesting adverse effects associated with poor ultrasound imaging conditions. Thus, this conceptually straightforward approach may be subjected to measurement uncertainties in clinical settings due to various forms of image artifacts. Fortunately, its role as an initialization method allows for rigorous measurement optimization or a large sampling consensus, making it administratively viable (i.e., initialization only ends when measurement confidence is statistically acceptable). Optimization methods beyond the scope of this discussion, such as the port constraint approach discussed in our previous study [22], have been proposed and will be continuously investigated in alignment with our research roadmap.

The proposed self-tracking approach facilitates image registration and the integration of dynamic field-of-view expansion with other 3D imaging modalities such as medical ultrasound imaging, as demonstrated in this study. While the proposed framework was designed based on considerations in the clinical requirements of minimally invasive fetoscopic procedures, it could potentially be used for a wide range of applications in diagnostic and medical interventions. For instance, this framework could potentially be applied to endoscopic camera tracking in the gastrointestinal or vascular tract for ultrasound guided procedures by facilitating the construction of a 3D expanded field-of-view, similar to our previous work in image mapping of endoscopic view mosaics [24]. However, our previous work was not equipped with timely tracking information and, hence, was limited to an open and continuous surface geometry, such as the placenta. With the development of this self-tracking framework, a dynamic field-of-view can be mapped onto irregular 3D surface patches, such as those of the gastrointestinal tract, extending the concept to more general applications.

@&#CONCLUSION@&#

This work contributes towards existing clinical practices by introducing a streamlined endoscopic camera tracking method that is non-disruptive to the surgical workflow. Specifically, it proposes a framework for surgical navigation by fusing 3D ultrasound imaging and endoscopy to bridge existing gaps in the application of state-of-the art techniques to placental imaging.

The tracking method was validated by phantom model and ex vivo monkey placenta imaging. The maximum errors in static position estimations of the Euclidean distance were less than 0.5mm compared to a commercial tracking system, which was considered a baseline, suggesting the technical soundness of the proposed tracking method. A controlled study on motion estimation also yielded results comparable to related studies.

Through the introduction of a self-contained integrated framework for endoscopic tracking in ultrasound image-guided procedures, we hope that this work can contribute to narrowing the gap in the translation of surgical navigation techniques for the treatment of rare conditions such as TTTS during fetoscopic procedures.

There is no relationship with any party that inappropriately influences the results of this work.

@&#ACKNOWLEDGMENT@&#

This work was supported by JSPS KAKENHI (Grant number 20345268) and Grant for Translational Systems Biology and Medicine Initiative (TSBMI).

@&#REFERENCES@&#

