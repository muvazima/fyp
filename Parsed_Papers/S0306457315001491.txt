@&#MAIN-TITLE@&#Query-focused multi-document summarization using hypergraph-based ranking

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a novel hybrid method to capture group relation of sentences.


                        
                        
                           
                           We cluster sentences with a KL-divergence based on word-topic distribution.


                        
                        
                           
                           We proposed a vertex reinforcement random walk process in a hypergraph model.


                        
                        
                           
                           The process simultaneously consider the query similarity, the centrality and the diversity of sentences.


                        
                        
                           
                           We implement our framework and verify improvement over appropriate baselines.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multi-document summarization

Hypergraph-based ranking

HDP

@&#ABSTRACT@&#


               
               
                  General graph random walk has been successfully applied in multi-document summarization, but it has some limitations to process documents by this way. In this paper, we propose a novel hypergraph based vertex-reinforced random walk framework for multi-document summarization. The framework first exploits the Hierarchical Dirichlet Process (HDP) topic model to learn a word-topic probability distribution in sentences. Then the hypergraph is used to capture both cluster relationship based on the word-topic probability distribution and pairwise similarity among sentences. Finally, a time-variant random walk algorithm for hypergraphs is developed to rank sentences which ensures sentence diversity by vertex-reinforcement in summaries. Experimental results on the public available dataset demonstrate the effectiveness of our framework.
               
            

@&#INTRODUCTION@&#

The task of query-focused multi-document summarization is to create a summary for a document set, which aims to provide an answer for a given query. Since the task has been initiated in DUC (Document Understanding Conferences), it has attracted more and more attention.

The main method for query-focused multi-document summarization is based on sentence selection, and the selected sentences both summarize the documents and answer the query. In general, there are three steps to select sentences. First, the document is divided into sentences. Second, an abstractive or extractive summarizer is used to get some representative sentences for the query. The final step is to select the most informative ones as a summary. In this paper, we concentrate on extractive multi-document summarization.

For extractive summarization, the main strategies fall into several categories. Feature-based approaches use features like term frequency, sentence position, length, etc., to rank sentences (Ouyang, Li, Lu, & Zhang, 2010). Graph-based approaches rank sentences by a random walk which explores the relations between sentences. The main graph-based ranking approaches used in summarization include LexRank (Erkan & Radev, 2004b), Manifold-Ranking (Wan, Yang, & Xiao, 2007), Hyperlink-Induced Topic Search (HITS) (Kleinberg, Kumar, Raghavan, Rajagopalan, & Tomkins, 1999) and DivRank (Mei, Guo, & Radev, 2010).

For graph-based approaches, it has been demonstrated that a cluster-based method can effectively improve the quality of extractive summarization (Cai & Li, 2012; Li & Li, 2012; Wan & Yang, 2008; Zhang, Ge, & He, 2012). The method generally focuses on sentence-level or cluster-level similarity, as well as the cluster-level relationship of sentences. For example, Wang, Li, Li, Li, and Wei (2013) and Wang, Wei, Li, and Li (2009) used hypergraphs to model both pairwise similarity and sentence clusters simultaneously, and they employed a hypergraph-based significance score propagation process to rank sentences. A good query-focused summary is expected to meet two factors: (1) high query relevance, (2) high diversity or minimum redundancy. High query relevance indicates the summary accounts for the given query. High diversity or minimum redundancy indicates the summary is able to present information without any convoluted. For these two aspects, existing graph-based methods have two limitations: (1) they cluster sentences simply based on co-occurrence lexical similarity, it may put semantically similar sentences into different topics, if they share few common words. For example:

                        
                           •
                           S1: A forest is a large area where trees grow close together.

S2: Woodland is land with a lot of trees.

Since S1 and S2 share very few words, the methods based on “word co-occurrence” may result in a decision that S1 and S2 are discussing different topics. However, it is clear for a human interpreter that S1 and S2 are both talking about “forest”. (2) The highest ranked sentences may be those ones with higher similarity to each other. In other words, it must provide an extra algorithm to remove redundancy.

In order to address the above limitations, we attempt to integrate probabilistic topic cluster and lexical similarity of sentences and develop a sentence ranking approach for achieving both diversity and centrality on the hypergraph model. Recently, Yin, Pei, Zhang, and Huang (2012) and Hennig and Labor (2009) exploited Probabilistic Latent Semantic Analysis (PLSA) to represent documents as a mixture of topics and clustered sentences by their topic distribution. Mei et al. (2010) proposed a reinforced random walk algorithm in an information network. Inspired by these work, we propose a Hypergraph-based vertex-rEinforced Ranking Framework (denoted as HERF) for query-focused summarization. First, we get a word-topic distribution by the HDP model and then cluster sentences by a hybrid clustering method in which it measures similarity based on word-topic distribution. Second, we build a hypergraph to capture both topical cluster relationship and pairwise cosine similarity of sentences. Then a sentence ranking approach, which balances the centrality and the diversity of the sentences by a vertex-reinforced strategy, is developed for scoring sentences. In practice, enforcing diversity in summarization can effectively reduce redundancy among the sentences. i.e., two sentences providing similar information should not be both present in the summary. Finally, a naive sentence selection and redundancy removal strategy is used to generate a summary. The experiments showed that our HERF framework performs better than other baseline summarizers on widely used benchmark datasets.

Two basic issues addressed in this paper are: (1) how to cluster semantically similar sentences into one topic even if they share few common words and (2) how to integrate redundancy removing policy into a hypergraph-based sentence ranking algorithm. Then, the main contributions of our work are summarized as follows.

                        
                           •
                           We proposed the HERF framework in which an adaptive vertex reinforcement random walk process is used to model the query similarity, the centrality and the diversity of sentences in hypergraph based model.

We propose a hybrid method to construct a hypergraph to integrate topic distribution and word co-occurrence of sentences.

To verify the effectiveness of our framework, we implement and evaluate our proposed framework HERF over widely used benchmark datasets, empirically verifying improvement over similar methods and systems.

The remainder of this paper is organized as follows. In Section 2, we describe our proposed summarization framework and details of constructing hypergraph, ranking sentences and generating summary. Section 3 gives the experiments and results. Section 4 briefly reviews the related work on graph/hypergraph based summarization. Finally, Section 5 concludes the paper.

In this section, we discuss our summarization framework which consists of four crucial components, as shown in Fig. 1
                     . In HERF, we first cluster sentences using a HDP-based approach based on sentence topic similarity. The topic similarity is calculated by the transformed radius (TR) based on the KL divergence, and KL divergence is based on word-topic probability distribution learned by the HDP topic model. We then construct a hypergraph based on sentence clusters as well as pairwise relationship between sentences. Then, we score sentences with a vertex-reinforced ranking approach which considers both the topic sensitivity and the diversity of sentences. Finally, the summary generation follows a greedy approach for selecting ranked sentences.

In state-of-the-art methods, one usually estimates the topic distribution of sentence and cluster sentences into the topic which has the highest probability among its topic distribution. In our method, we use the whole topic distribution but not the main topic of sentence as a similarity measure to group sentences. And we argue that only using topic-based similarity is not enough for selecting important sentences. So, we simultaneously compute similarity of sentences by their cosine distance and integrate both of them into hypergraph. In our work, the topic model HDP (Teh, Jordan, Beal, & Blei, 2006) is employed to represent document with a mixture of topics. We choose HDP but not LDA (Blei, Ng, & Jordan, 2003) because HDP could automatically determine the number of topics and LDA requires choosing the number of topics with cross validation or held-out likelihood. As shown in Eq. (1), a global random probability measure G
                        0 is distributed according to a Dirichlet process (DP) with γ the concentration parameter and H the base probability measure. A probability measure Gd
                         in Eq. (2) is a set of random measures for each document group d, one for each group j. It is drawn from another DP with the concentration parameter α
                        0 and the base probability measure G
                        0.

                           
                              (1)
                              
                                 
                                    
                                       G
                                       0
                                    
                                    
                                       |
                                       γ
                                       ,
                                       H
                                       ∼
                                       D
                                       P
                                       
                                          (
                                          γ
                                          ,
                                          H
                                          )
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       G
                                       d
                                    
                                    
                                       |
                                    
                                    
                                       α
                                       0
                                    
                                    ,
                                    
                                       G
                                       0
                                    
                                    ∼
                                    D
                                    P
                                    
                                       (
                                       
                                          α
                                          0
                                       
                                       ,
                                       
                                          G
                                          0
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

In the HDP model, the observed words in each document are considered as a product of a document-specific mixture of latent topics in corpus wide. We define a word vector 
                           
                              W
                              =
                              
                                 w
                                 1
                              
                              ,
                              
                                 w
                                 2
                              
                              ,
                              
                                 w
                                 3
                              
                              ,
                              …
                              ,
                              
                                 w
                                 l
                              
                              ,
                           
                         
                        l is the length of corpus. The vector 
                           
                              z
                              =
                              
                                 z
                                 1
                              
                              ,
                              
                                 z
                                 2
                              
                              ,
                              …
                              ,
                              
                                 z
                                 l
                              
                           
                         defines the hidden topic of each word in corpus and the HDP automatically decides the total number of topics. After being processed by the HDP model, each word is assigned a topic tag, but this topic tag could not be used to cluster sentences because each sentence is associated with more than one topic (word). Therefore, in our framework, each sentence is represented as a vector 
                           
                              s
                              =
                              
                                 z
                                 
                                    w
                                    1
                                 
                              
                              ,
                              
                                 z
                                 
                                    w
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 z
                                 
                                    w
                                    n
                                 
                              
                           
                         on a latent topic set z, where n is the length of the sentence. Then, the probability that the sentence belongs to the topic zi
                         is represented as 
                           
                              p
                              =
                              
                                 m
                                 i
                              
                              /
                              n
                              ,
                           
                         where mi
                         is the number of words in topic zi
                        . If there are two sentences x and y, we denote sentence-topic probability distribution as vectors 
                           
                              
                                 P
                                 x
                              
                              =
                              
                                 p
                                 x
                              
                              
                                 (
                                 
                                    z
                                    1
                                 
                                 )
                              
                              ,
                              
                                 p
                                 x
                              
                              
                                 (
                                 
                                    z
                                    2
                                 
                                 )
                              
                              ,
                              …
                              ,
                              
                                 p
                                 x
                              
                              
                                 (
                                 
                                    z
                                    k
                                 
                                 )
                              
                           
                         and 
                           
                              
                                 P
                                 y
                              
                              =
                              
                                 p
                                 y
                              
                              
                                 (
                                 
                                    z
                                    1
                                 
                                 )
                              
                              ,
                              
                                 p
                                 y
                              
                              
                                 (
                                 
                                    z
                                    2
                                 
                                 )
                              
                              ,
                              …
                              ,
                              
                                 p
                                 y
                              
                              
                                 (
                                 
                                    z
                                    k
                                 
                                 )
                              
                              ,
                           
                         where k is the number of the topics. The Kullback–Liebler (KL) divergence is a better method to assess the similarity between two distributions, but KL divergence is not symmetric. So, like Yin et al. (2012), we use the transformed radius (TR) based on the KL divergence to measure the sentence similarity between Px
                         and Py
                        :

                           
                              (3)
                              
                                 
                                    T
                                    R
                                    
                                       (
                                       
                                          P
                                          x
                                       
                                       ,
                                       
                                          P
                                          y
                                       
                                       )
                                    
                                    =
                                    
                                       D
                                       
                                          K
                                          L
                                       
                                    
                                    
                                       (
                                       
                                          P
                                          x
                                       
                                       
                                          ∥
                                       
                                       
                                          
                                             
                                                P
                                                x
                                             
                                             +
                                             
                                                P
                                                y
                                             
                                          
                                          2
                                       
                                       )
                                    
                                    +
                                    
                                       D
                                       
                                          K
                                          L
                                       
                                    
                                    
                                       (
                                       
                                          P
                                          y
                                       
                                       
                                          ∥
                                       
                                       
                                          
                                             
                                                P
                                                x
                                             
                                             +
                                             
                                                P
                                                y
                                             
                                          
                                          2
                                       
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              
                                 D
                                 
                                    K
                                    L
                                 
                              
                              
                                 (
                                 P
                                 ∥
                                 Q
                                 )
                              
                              =
                              
                                 ∑
                                 i
                              
                              
                                 l
                                 o
                                 g
                                 (
                                 P
                                 (
                                 i
                                 )
                                 /
                                 Q
                                 (
                                 i
                                 )
                                 )
                                 P
                                 (
                                 i
                                 )
                              
                           
                        . Then the similarity is:

                           
                              (4)
                              
                                 
                                    S
                                    i
                                    m
                                    
                                       (
                                       
                                          P
                                          x
                                       
                                       ,
                                       
                                          P
                                          y
                                       
                                       )
                                    
                                    =
                                    
                                       10
                                       
                                          −
                                          T
                                          R
                                          (
                                          
                                             P
                                             x
                                          
                                          ,
                                          
                                             P
                                             y
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        
                     

Over the similarity, we cluster sentences using a modified DBSCAN (Density Based Spatial Clustering of Applications with Noise) algorithm proposed by Wang et al. (2013). The DBSCAN (Ester, Kriegel, Sander, & Xu, 1996) automatically determines the cluster number and filters noise nodes, this is the reason why we choose DBSCAN in our framework. There are two parameters, Eps defines the search radius and MinPts defines the least nodes number in a cluster. In our experiments, we empirically set MinPts as 3 and Eps as 0.0005. As shown in Algorithm 1, the modified DBSCAN automatically tunes the Eps value to get a reasonable cluster set.
                     

A hypergraph (Berge, 1984) is a generalization of a simple graph where the hyperedges (called edges in simple graph) could contain any number of vertices. This property makes it represent a set of vertices in a hyperedge when all the members of the set have a group relationship. Let G(V, E) be a hypergraph where V denotes the vertex set and E denotes the hyperedge set. A hyperedge e is a subset of V where 
                           
                              
                                 ⋃
                                 
                                    e
                                    ∈
                                    E
                                 
                              
                              =
                              V
                           
                        . A weighted hypergraph is denoted by G(V, E, w) where w(e) called the weight of hyperedge e. A hyperedge e is said to be incident on v when v ∈ e. The incidence matrix of hypergraph is represented by a |V| × |E| matrix H with entries as follows:

                           
                              (5)
                              
                                 
                                    h
                                    
                                       (
                                       v
                                       ,
                                       e
                                       )
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                1
                                             
                                             
                                                
                                                   if
                                                   v
                                                   ∈
                                                   e
                                                
                                             
                                          
                                          
                                             
                                                0
                                             
                                             
                                                
                                                   if
                                                   v
                                                   ∉
                                                   e
                                                   .
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        The degree of vertex and hyperedge are defined as follows:

                           
                              (6)
                              
                                 
                                    d
                                    
                                       (
                                       v
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          e
                                          ∈
                                          E
                                       
                                    
                                    w
                                    
                                       (
                                       e
                                       )
                                    
                                    h
                                    
                                       (
                                       v
                                       ,
                                       e
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    δ
                                    
                                       (
                                       e
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          v
                                          ∈
                                          V
                                       
                                    
                                    h
                                    
                                       (
                                       v
                                       ,
                                       e
                                       )
                                    
                                    =
                                    
                                       |
                                       e
                                       |
                                    
                                 
                              
                           
                        Let De and Dv denote the diagonal matrices which contain the degree of vertex and hyperedge respectively, and W represents the diagonal matrix with the hyperedge weights.

We construct hypergraph using a new way which can put semantically similar sentences into the same group. Our method is different from Wang et al. (2013) and Wang et al. (2009). Comparing with the two work, our work has two main contributions: (1) A new hypergraph construction method. We cluster sentences based on the word-topic distribution while they cluster sentences based on the cosine similarity among sentences. Our method can group semantically similar sentences into one cluster if they share few common words. (2) When selecting sentences, we provide a vertex reinforcement random walk process to rank sentences while they use a scoring function to rank sentences. Our method is concentrated on both centrality and diversity of sentences simultaneous by using a modified time-variant random walk process while they focused on centrality of sentences (as discussed in Section 2.3). In Section 3, we have compared our method with them by experiment and the results show the efficient of our method.

In our framework, we cluster sentences based on the word-topic distribution. After getting the clusters, the thing left to do is construct a sentence hypergraph. In our hypergraph model, each vertex represents a sentence, and the hyperedge captures two types of relationships among vertices which present sentences: (1) pairwise similarity; (2) topic cluster among some vertices. We still use cosine similarity between two sentences to denote the weight of hyperedge in the first type. For the second type, the hyperedge contains all the vertices in the same cluster and the weight of the hyperedge is the cosine similarity between a virtual document 
                           
                              C
                              =
                              
                                 
                                    s
                                    1
                                 
                                 ,
                                 
                                    s
                                    2
                                 
                                 ,
                                 …
                                 ,
                                 
                                    s
                                    i
                                 
                              
                           
                         and the document D multiplying by a factor a which tunes the weight of a cluster-based hyperedge, where si
                         denotes the ith sentence in the cluster. The process of hypergraph construction is summarized as Algorithm 2
                        .

Most ranking algorithms are based on random walk on graph. A random walk is a transition process in which the walker moves from one vertex to another connected vertex after each discrete time step t. The process is assumed to be a Markov chain M over a state set 
                           
                              s
                              =
                              
                                 
                                    s
                                    1
                                 
                                 ,
                                 
                                    s
                                    2
                                 
                                 ,
                                 …
                                 ,
                                 
                                    s
                                    n
                                 
                              
                           
                        . Each state si
                         corresponds to a vertex v in the graph G, and the transition probability is defined as p(u, v). As shown in Zhou, Huang, and Schölkopf (2007), after choosing a hyperedge e which incidents with the current vertex u, the surfer has to pick a vertex v ∈ e uniformly at random because hyperedges involve more than two vertices in hypergraph. For example, a specific path is shown with curved arrows in Fig. 1. For the beginning vertex SN
                        , the surfer chooses cluster-based path hyperedge2 which connected three vertices SN, S
                        4 and S
                        5, and then the surfer picks S
                        4. For the second vertex S
                        4, the surfer chooses a normal pairwise hyperedge between S
                        4 and S
                        3, so the surfer moves to S
                        3, and so on. The final path is { 
                           
                              
                                 S
                                 N
                              
                              −
                              h
                              y
                              p
                              e
                              r
                              e
                              d
                              g
                              e
                              2
                              −
                              
                                 S
                                 4
                              
                              −
                              e
                              
                                 (
                                 
                                    S
                                    4
                                 
                                 ,
                                 
                                    S
                                    3
                                 
                                 )
                              
                              −
                              
                                 S
                                 3
                              
                              −
                              h
                              y
                              p
                              e
                              r
                              e
                              d
                              g
                              e
                              1
                              −
                              
                                 S
                                 1
                              
                              −
                              e
                              
                                 (
                                 
                                    S
                                    1
                                 
                                 ,
                                 
                                    S
                                    6
                                 
                                 )
                              
                              −
                              
                                 S
                                 6
                              
                              −
                              e
                              
                                 (
                                 
                                    S
                                    6
                                 
                                 ,
                                 
                                    S
                                    2
                                 
                                 )
                              
                              −
                              
                                 S
                                 2
                              
                              −
                              e
                              
                                 (
                                 
                                    S
                                    2
                                 
                                 ,
                                 
                                    S
                                    7
                                 
                                 )
                              
                              −
                              
                                 S
                                 7
                              
                              −
                              e
                              
                                 (
                                 
                                    S
                                    7
                                 
                                 ,
                                 
                                    S
                                    5
                                 
                                 )
                              
                              −
                              
                                 S
                                 5
                              
                           
                         }. The transition probability is the key to the random walk algorithm. On hypergraph, the common transition probability is

                           
                              (8)
                              
                                 
                                    p
                                    
                                       (
                                       u
                                       ,
                                       v
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          
                                             e
                                             ∈
                                             
                                                ɛ
                                             
                                             (
                                             u
                                             )
                                          
                                       
                                    
                                    w
                                    
                                       (
                                       e
                                       )
                                    
                                    
                                       
                                          h
                                          (
                                          u
                                          ,
                                          e
                                          )
                                          h
                                          (
                                          v
                                          ,
                                          e
                                          )
                                       
                                       
                                          d
                                          (
                                          u
                                          )
                                          δ
                                          (
                                          e
                                          )
                                       
                                    
                                 
                              
                           
                        where ε(u) is the set of hyperedge incident to u. Then, the transition probability matrix is denoted by 
                           
                              P
                              =
                              
                                 D
                                 v
                                 
                                    −
                                    1
                                 
                              
                              H
                              W
                              
                                 D
                                 e
                                 
                                    −
                                    1
                                 
                              
                              
                                 H
                                 T
                              
                           
                        . Under a standard random walk process, the probability that the surfer at vertex u at time T is defined as

                           
                              (9)
                              
                                 
                                    
                                       Q
                                       T
                                    
                                    
                                       (
                                       v
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          (
                                          u
                                          ,
                                          v
                                          )
                                          ∈
                                          E
                                       
                                    
                                    p
                                    
                                       (
                                       u
                                       ,
                                       v
                                       )
                                    
                                    
                                       Q
                                       
                                          T
                                          −
                                          1
                                       
                                    
                                    
                                       (
                                       u
                                       )
                                    
                                 
                              
                           
                        where E is all the hyperedges that contain the vertex v. The probability QT
                        (v) stops changing after n steps when the random walk process reach the steady state. We use π(v) to denote the stationary distribution which measures the importance of vertices.

In most classic random walk algorithms, the corresponding Markov chain is time-homogeneous, the transition probability does not vary over the time. The importance of vertex, which is calculated using those algorithms, is concentrated on centrality but ignores diversity. For example, if we use those algorithms to rank sentences for summarization task, the top n sentences might focus on a few topics but not a well overview of the document. For achieving diversity in a random walk, Mei et al. (2010) proposed a time-variant random walk process DivRank. It balances prestige and diversity simultaneously by vertex-reinforcement. The iteration process of DivRank is described as follows:

                           
                              (10)
                              
                                 
                                    
                                       Q
                                       T
                                    
                                    
                                       (
                                       v
                                       )
                                    
                                    =
                                    
                                       (
                                       1
                                       −
                                       λ
                                       )
                                    
                                    
                                       p
                                       *
                                    
                                    
                                       (
                                       v
                                       )
                                    
                                    +
                                    λ
                                    
                                       ∑
                                       
                                          u
                                          ∈
                                          V
                                       
                                    
                                    
                                       
                                          p
                                          
                                             (
                                             u
                                             ,
                                             v
                                             )
                                          
                                          ·
                                          
                                             N
                                             T
                                          
                                          
                                             (
                                             v
                                             )
                                          
                                       
                                       
                                          
                                             D
                                             T
                                          
                                          
                                             (
                                             u
                                             )
                                          
                                       
                                    
                                    
                                       Q
                                       
                                          T
                                          −
                                          1
                                       
                                    
                                    
                                       (
                                       u
                                       )
                                    
                                 
                              
                           
                        where QT
                        (v) denotes the DivRank score of vertex v at time step T and p*(v) is the prior value of vertex v. p(u, v) is the transition probability from vertex u to v and NT
                        (v) represents the number of times the surfer has visited v up to time T and 
                           
                              
                                 D
                                 T
                              
                              
                                 (
                                 u
                                 )
                              
                              =
                              
                                 ∑
                                 
                                    v
                                    ∈
                                    V
                                 
                              
                              p
                              
                                 (
                                 u
                                 ,
                                 v
                                 )
                              
                              
                                 N
                                 T
                              
                              
                                 (
                                 v
                                 )
                              
                           
                        .

In our hypergraph model, we propose a modified algorithm to derive the importance of sentences. In Section 3, we compared our algorithm with the original DivRank in Mei et al. (2010), the experiment results verify the performance of our method. Specifically, our algorithm firstly realizes prior value as the query similarity which naturally fits the query-focused summarization task. Secondly, we use 
                           
                              S
                              c
                              o
                              r
                              
                                 e
                                 
                                    T
                                    −
                                    1
                                 
                              
                              
                                 (
                                 
                                    s
                                    j
                                 
                                 )
                              
                              ,
                           
                         which is the ranking score of sentence si
                         up to time T, as the reinforced coefficient but not visited times. The motivation is, in our model, those sentences which have a high similarity with query may get a larger ranking score although they have a less visited times. So our iteration process for sentences ranking is defined as Eq. (11),

                           
                              (11)
                              
                                 
                                    S
                                    c
                                    o
                                    r
                                    
                                       e
                                       T
                                    
                                    
                                       (
                                       
                                          s
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       (
                                       1
                                       −
                                       λ
                                       )
                                    
                                    S
                                    i
                                    m
                                    
                                       (
                                       q
                                       u
                                       e
                                       r
                                       y
                                       ,
                                       
                                          s
                                          i
                                       
                                       )
                                    
                                    +
                                    λ
                                    
                                       ∑
                                       
                                          j
                                          :
                                          j
                                          ≠
                                          i
                                       
                                    
                                    
                                       
                                          
                                             p
                                             0
                                          
                                          
                                             (
                                             
                                                s
                                                j
                                             
                                             ,
                                             
                                                s
                                                i
                                             
                                             )
                                          
                                          ·
                                          S
                                          c
                                          o
                                          r
                                          
                                             e
                                             
                                                T
                                                −
                                                1
                                             
                                          
                                          
                                             (
                                             
                                                s
                                                i
                                             
                                             )
                                          
                                       
                                       
                                          
                                             D
                                             T
                                          
                                          
                                             (
                                             
                                                s
                                                j
                                             
                                             )
                                          
                                       
                                    
                                    S
                                    c
                                    o
                                    r
                                    
                                       e
                                       
                                          T
                                          −
                                          1
                                       
                                    
                                    
                                       (
                                       
                                          s
                                          j
                                       
                                       )
                                    
                                 
                              
                           
                        where ScoreT
                        (si
                        ) is the score of sentence si
                         until time T, and Sim(query, si
                        ) is the cosine similarity between query and sentence si
                        . The parameter λ is used to control the ratios of query similarity, and we will discuss its effect in Section 3.3. p
                        0(sj, si
                        ) is the probability from sentence sj
                         to si
                         which is calculated as follows:

                           
                              (12)
                              
                                 
                                    
                                       p
                                       0
                                    
                                    
                                       (
                                       u
                                       ,
                                       v
                                       )
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   μ
                                                   ·
                                                   p
                                                   (
                                                   u
                                                   ,
                                                   v
                                                   )
                                                
                                             
                                             
                                                
                                                   if
                                                   u
                                                   ≠
                                                   v
                                                
                                             
                                          
                                          
                                             
                                                
                                                   1
                                                   −
                                                   μ
                                                
                                             
                                             
                                                
                                                   if
                                                   u
                                                   =
                                                   v
                                                   .
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        The transition factor μ is a probability that the surfer moves to another vertex. So 
                           
                              1
                              −
                              μ
                           
                         is the probability of staying in the current vertex and the current vertex is reinforced. And

                           
                              (13)
                              
                                 
                                    
                                       D
                                       T
                                    
                                    
                                       (
                                       
                                          s
                                          j
                                       
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          
                                             s
                                             i
                                          
                                          ∈
                                          V
                                       
                                    
                                    
                                       p
                                       0
                                    
                                    
                                       (
                                       u
                                       ,
                                       v
                                       )
                                    
                                    S
                                    c
                                    o
                                    r
                                    
                                       e
                                       
                                          T
                                          −
                                          1
                                       
                                    
                                    
                                       (
                                       
                                          s
                                          i
                                       
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              S
                              c
                              o
                              r
                              
                                 e
                                 
                                    T
                                    −
                                    1
                                 
                              
                              
                                 (
                                 
                                    s
                                    i
                                 
                                 )
                              
                           
                         is the approximate value of NT
                        (si
                        ), the theoretical analysis refers to Mei et al. (2010) for details.

In our method, we use the query similarity as the reinforcement for each vertex when random walking. As shown in Eq. (11), the second term is used to ensure the diversity, it denotes the weighted value transferred from other adjacent sentences and we use the score at last time as the weight coefficient. When some sentences express similar information, they will have much the same adjacent sentences in hypergraph as well as transferred value. This term embodied the thought that rich nodes get richer over time and ”absorb” the scores of its neighbors. At the beginning of random walk, some sentences have ranking scores with little difference. When the random walk process has converged to a stationary distribution, these sentences will have different ranking scores. Therefore, only the sentence with the highest score can be present in the summary.

After our ranking process, the importance score of a sentence automatically balances diversity and centrality. However, there are some paragraphs and/or sentences in different documents may describe the same contents. So, there is a requirement that the similarity of any two sentences in summary does not exceed a threshold k, and we set k as 0.3 in our experiments. At last, the summary generation algorithm is shown in Algorithm 3
                        .

@&#EXPERIMENTS@&#

We validate our model using DUC 2007
                           1
                        
                        
                           1
                           
                              http://www-nlpir.nist.gov/projects/duc/data/2007_data.html
                           
                         and TAC 2008
                           2
                        
                        
                           2
                           
                              http://www.nist.gov/tac/2008/summarization/
                           
                         data set. The former contains 45 document clusters and every cluster contains 25 documents, the latter contains 48 document clusters and every cluster contains 10 documents. For every cluster, it gave a query sentence which is a question or topic description. For each cluster, the summarization system should provide an answer to the corresponding query. The answer is strictly limited to 250 and 100 words respectively. We use a rule-based method to perform sentence segmentation. The Porter stemmer
                           3
                        
                        
                           3
                           
                              http://tartarus.org/martin/PorterStemmer/
                           
                         is used to stem word in sentence after removing stop words.

As for automatic evaluation summarization, we use the ROUGE toolkit
                           4
                        
                        
                           4
                           
                              http://www.isi.edu/licensed-sw/see/rouge/
                           
                        , which is officially adopted by DUC. Here we report the average F-measure scores of ROUGE-1, ROUGE-2 and ROUGE-SU4 and their corresponding 95% confidential intervals.

We consider following settings to evaluate our framework.

                           
                              •
                              Comparing HERF framework with four baselines which are graph-based methods.

Comparing HERF framework with HyperSum (Wang et al., 2013) which is a hypergraph-based method and switch off one component at a time in HERF.

Comparing HERF with the participating systems in DUC 2007 and TAC 2008.

In our framework, a is a factor to tune the weight of a cluster-based hyperedge. We set 
                           
                              a
                              =
                              1.5
                           
                         the same as in Wang et al. (2013). λ is adopted as the penalty factor of query sensitive (Eq. 11), the transition factor μ is utilized to adjust the probability of random walk of leaving the current vertex (Eq. 12). To get the final query-focused summary using our approach, we also need to optimize the parameters for HDP topic model (Dirichlet hyperparameters γ and α) and determine parameters λ and μ.

For HDP topic model, we randomly select 25 clusters for training and other 20 clusters for testing on DUC 2007 data set. We test sensible combinations in ranges 
                           
                              γ
                              =
                              1
                              …
                              10
                           
                         and 
                           
                              α
                              =
                              0.5
                              …
                              5
                           
                        . We repeat this experiment 10 times. The highest performance of topic cluster for summary was found when 
                           
                              γ
                              =
                              8
                           
                         and 
                           
                              α
                              =
                              1
                           
                        . We use the same parameters on TAC 2008 data set.

With a gradient search strategy, we first fix μ to an assigned value. Then the performance of separate λ values is evaluated. Fig. 2
                         demonstrates the influence of λ when 
                           
                              μ
                              =
                              0.9
                           
                        . Fig. 2a and b shows the ROUGE-2 and ROUGE-SU4 score when λ is changing from 0 to 0.9 with an interval of 0.1 respectively. When λ is set as the value of 0, the score of a sentence is only determined by query similarity (Eq. 11). That is, one sentence is likely to be selected into summary, only when it has a large similarity with the query. When λ is set to a larger value, penalty on the query similarity is more heavily considered. We can see the ROUGE score is also small when λ is set to a small value. The ROUGE score reaches its peak when λ varies at around 0.7. The experimental results explain that the centrality and the diversity of sentences should be considered more than query relevance.

Next, we fix the penalty factor λ at 0.7. Fig. 3
                         illustrates the performance of HERF on the DUC 2007 data set when μ ranges from 0.1 to 0.99. Similar to the process of tuning λ, we first use different values of μ from 0.1 to 0.9 with an interval of 0.1. We find the curve is monotonously increasing and gets peak performance located in 0.9 as showing in Fig. 3a. So, we continue experiments for μ in the range from 0.9 to 0.99 with an interval of 0.01. Fig. 3b shows the change curve of ROUGE-2 score. It is observed the best performance achieved when μ set to 0.98, and the result suggests the probability of leaving the current vertex should be higher.

In order to verify the effectiveness of our framework, in this section, we compare our framework with several baseline methods and system on DUC 2007 and TAC 2008 datasets. The results of all of the baseline approaches are from the in-house re-implementation by us. One kind of baseline methods is graph-based approach. We provide four state-of-the-art methods:

                           
                              •
                              
                                 DivRank. A time-variant random walk approach (Mei et al., 2010), which has been defined in Eq. (6). It focuses on achieving diversity in sentence ranking for summary. The difference between our HERF and DivRank is that HERF employs a topic cluster as hyperedge in hypergraph and ranks sentences based on sentence topic similarity, whereas DivRank is based on graph in which an edge denoted by sentence lexical similarity.


                                 HyperSum. A hypergraph based semi-supervised sentence ranking approach (Wang et al., 2013), which is the first hypergraph-based approach for multi-document summarization.


                                 ManiFold. A manifold-ranking based approach used by Wan et al. (2007). This method ranks sentences based on the relationships among sentences and the relationships between the topic and the sentences by manifold-ranking process.


                                 QLexRank. A modified random walk based on LexRank with prior belief (Otterbacher, Erkan, & Radev, 2009). QLexRank does not take into account the diversity when ranking sentences.


                        Tables 1
                         and 2
                         show the comparison results with graph-based method and our HERF framework is better than the three baselines on DUC 2007 and TAC 2008 datasets. To see whether the difference of ROUGE scores between HERF and the three baselines is significant, we conduct the paired t-tests. In our experiments, we perform HERF 8 times to get 8 results which are regarded as sampled observations. Tables 3
                         and 4
                         list the p-Values and their significance (under sign) for each pair of HERF and baseline. In following tables, **, * and ∼ correspond to the p-Value < 0.001, p-Value < 0.05 and p-Value > 0.05, which means that the difference are, respectively, strong significant, significant or not significant.

We can see that HERF performs significantly better than other baselines. Since both HERF and HyperSum have considered the group relationship among sentences, they get a better performance than others. HERF is concentrated on both centrality and diversity of sentences simultaneous HyperSum focused on centrality of sentences. Therefore HERF achieves a better performance than HyperSum.

For further evaluating the effect of each component in the hypergraph-based method (i.e. HyperSum and our HERF), we design an addition experiments (Table 5
                        ). Firstly, we list all of the components used in Wang et al. (2013) and our framework:

                           
                              (1)
                              
                                 CS: A clustering method using DBSCAN algorithm with cosine similarity;


                                 SP: A score propagation algorithm proposed by Wang et al. (2013);


                                 HDPC: A clustering method using the DBSCAN algorithm with topic similarity (Eq. 4) based on a topic distribution learned by HDP;


                                 ER: The vertex-reinforcement ranking algorithm of us.

Then, our HERF framework can be considered as the combination of HDPC and ER. And there are three baseline methods combined by those components.

                           
                              •
                              
                                 HyperSum. A hypergraph based semi-supervised sentence ranking approach (Wang et al., 2013), which is combined by CS and SP.


                                 HDPC+SP. A combination of HDPC and SP.


                                 CS+ER. We construct hypergraph based on cosine similarity but not topic similarity based on HDP and keep other component unchanged in HERF.

From Table 5, we can see that the HDPC+SP method performs better than HyperSum (CS+SP) attributed to our HDP-based clustering approach. CS+ER performs comparable result as HyperSum (CS+SP) but HERF (HDPC+ER) is better than HyperSum (CS+SP), the results show that ER, the vertex-reinforced ranking algorithm, could more effectively rank sentences than the score propagation algorithm CS in the hypergraph model in which group relationship is based on topic-based cluster.

And then, we compare our result with the participating systems in DUC 2007 and TAC 2008. For reference, we present the following representative ROUGE results: (1) the top three participating systems (ordered by their ROUGE-2 scores) among all the unsupervised extractive systems; (2) The best system in all participating systems (denoted by SYS BEST); (3) The NIST baseline which returns all the first sentences from the documents; (4) The average ROUGE score of all the participating systems (denoted by SYS AVG). Tables 6
                         and 7
                         show that our framework performs better than the unsupervised extractive systems according to ROUGE-2 score, and outperforms baseline as well as average score.

@&#RELATED WORK@&#

As discussed in Section 1, There are two main approaches to the task of summarization–extraction and abstraction. Extraction consists of concatenating extracts taken from the corpus into a summary, whereas abstraction involves generating novel sentences from information extracted from the corpus. In our work, we concentrate on graph-based summarization which belongs to the extraction one.

Many research efforts have been devoted to graph-based summarization. As an example, LexRank (Erkan & Radev, 2004b) is a graph-based summarization approach which selects the central sentences as a summary from a sentence graph. They constructed an undirected graph in which a vertex represents a sentence and an edge represents the similarity between two sentences. Then, they used a PageRank-like algorithm to calculate the centrality in the similarity graph. Other work used a biased LexRank algorithm for query-focused summarization and passage retrieval for question answering (Erkan & Radev, 2004a; Otterbacher, Erkan, & Radev, 2005, 2009). TextRank (Mihalcea & Tarau, 2004) is another random-walk-based graph model for single-document summarization. A general graph-based framework has three stages for the summarization: graph construction, sentence ranking and summary generation. We review existing work from these three aspects as follows.


                     Graph construction: Several approaches (Shen & Li, 2010; Wan, 2013; Wan & Xiao, 2009; Wan et al., 2007) built an undirected graph using the same approach as LexRank (Erkan & Radev, 2004b). Canhasi and Kononenko (2014); Wei, Li, Lu, and He (2010); Yin et al. (2012); Zhang et al. (2012) employed vertices and edges to represent various nodes and group relationship. Cai and Li (2012); Li and Li (2012); Shen, Wang, and Li (2010); Wan and Yang (2008) constructed a two-level graph or bipartite graph to model the group relationship of sentences. In our work, we use hypergraph to capture semantic relationship and word co-occurrence relationship among sentences.


                     Sentence ranking: Once the graph is constructed, a ranking algorithm will be employed. Canhasi and Kononenko (2014) calculated the significance of each archetype (sentence) using an archetypal analysis approach. Shen and Li (2010) converted summarization to the set cover problem of the graph domination and exploits an approximation algorithm for summarization. Cai and Li (2012); Wan (2013); Wan and Xiao (2009); Wan et al. (2007) used a manifold-ranking based approach to estimate the biased information richness score of each sentence. Zhang et al. (2012) ranked sentences according to the mutual effect among terms, sentences and clusters. Yin et al. (2012) used an iterative algorithm which integrated with the prior to determining the importance of sentences. Wan and Yang (2008) proposed a cluster-based Conditional Markov Random Walk Model and a cluster-based HITS model to rank sentences. Li and Li (2012); Shen et al. (2010) employed a graph based score propagation algorithm to score sentences. Du, Guo, and Cheng (2011) proposed a supervised lazy-random-walk-based approach which combines the rich features of sentences with the intrinsic sentence graph structure. In our work, we provide a vertex reinforcement random walk process to rank sentences.


                     Summary generation The main goal is to select sentences into the summary in this stage. Removing redundancy is a necessary process which ensures the novelty of the summary, where novelty is a measure of the difference between the sentences in a summary. A widely used approach is the Maximal Marginal Relevance (MMR). Carbonell and Goldstein (1998) used a combined criterion of query relevance and novelty of information to select sentences. For example, the MMR was used by Li and Li (2012) in their graph-based summarization approach, a greedy algorithm was used to remove redundancy (Cai & Li, 2012; Canhasi & Kononenko, 2014) and another greedy algorithm is used to decrease the overall ranking score of less informative sentences (Wan, 2013; Wan & Xiao, 2009; Wan & Yang, 2008; Wan et al., 2007; Yin et al., 2012). In our method, it has integrated redundancy removing mechanism in sentence ranking. Therefore, we need only to provide a threshold to control the similarity of any two sentences in summary.

There are some other representative work. Celikyilmaz and Hakkani-Tur (2010) incorporated a hierarchical topic model into a regression model to form a summary. For achieving diversity in a random walk, Mei et al. (2010) proposed a time-variant random walk process DivRank. Baralis, Cagliero, Mahoto, and Fiori (2013) built a correlation graph based on a frequent item set and then exploited a variation of the PageRank algorithm to rank and select sentences. Zhao, Wu, and Huang (2009) derived a query expansion approach for query-focused multi-document summarization. They all use both sentence-to-sentence relations and sentence-to-word relations to measure sentence importance.

@&#CONCLUSIONS AND FUTURE WORK@&#

In this study, we propose a Hypergraph-based vertex rEinforcement Ranking Framework (HERF) for multi-document summarization, which tries to integrate multiple important factors for sentence ranking. To do that, we propose a hypergraph construction method using HDP-based topic relatedness, and adapt a vertex reinforcement random walk process to model the query similarity, the centrality and the diversity of sentences when ranking sentences. Compared with other topic model, HDP can estimate the number of topics automatically. Experiments are conducted to verify the effectiveness of our framework.

Under the assumption that a given document set usually covers several topic aspects, each sentence may describe one or more topics, in this paper we mainly consider the topic distribution relationship as well as the word co-occurrence relationship. If relaxing this assumption by considering more relations among sentences, we can achieve better performance. Our HDP-based sentences topic clustering only consider the topic distribution relationship among sentences. When more relations among sentences are discovered, it must provide a new clustering method to incorporate other relations. In our framework, we employ a naive sentence selection strategy to generate a summary. A sophisticated generation method based on our ranked sentences may be an effective complement. Our HDP-based sentences topic clustering is a semantical grouping, another semantic representation method, distributed representation, may be an alternative method for sentences clustering. In the future, we will attempt to do some work in the above aspects.

@&#ACKNOWLEDGMENTS@&#

We thank all reviewers for the insightful comments. This work is supported by the State Key Program of National Natural Science Foundation of China (No. 61133012), the National Natural Science Foundation of China (No. 61373108), the National Philosophy Social Science Major Bidding Project of China (No. 11&ZD189).

@&#REFERENCES@&#

