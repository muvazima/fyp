@&#MAIN-TITLE@&#Hierarchical transfer learning for online recognition of compound actions


@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel online action recognition method for fast detection of compound actions.


                        
                        
                           
                           A key contribution is a transfer learning strategy from simple to complex datasets.


                        
                        
                           
                           Another key contribution is an automatically configured hierarchical body model.


                        
                        
                           
                           Experimental results show an improvement in action recognition performance of 16%.


                        
                        
                           
                           The proposed algorithm is real-time with an average latency of just 2 frames.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Online action recognition

Online interaction recognition

Hierarchical

Transfer learning

@&#ABSTRACT@&#


               
               
                  Recognising human actions in real-time can provide users with a natural user interface (NUI) enabling a range of innovative and immersive applications. A NUI application should not restrict users’ movements; it should allow users to transition between actions in quick succession, which we term as compound actions. However, the majority of action recognition researchers have focused on individual actions, so their approaches are limited to recognising single actions or multiple actions that are temporally separated.
                  This paper proposes a novel online action recognition method for fast detection of compound actions. A key contribution is our hierarchical body model that can be automatically configured to detect actions based on the low level body parts that are the most discriminative for a particular action. Another key contribution is a transfer learning strategy to allow the tasks of action segmentation and whole body modelling to be performed on a related but simpler dataset, combined with automatic hierarchical body model adaption on a more complex target dataset.
                  Experimental results on a challenging and realistic dataset show an improvement in action recognition performance of 16% due to the introduction of our hierarchical transfer learning. The proposed algorithm is fast with an average latency of just 2 frames (66 ms) and outperforms state of the art action recognition algorithms that are capable of fast online action recognition.
               
            

@&#INTRODUCTION@&#

The research field of human action recognition has rapidly expanded in recent years with many innovative applications in a range of sectors including healthcare, education and entertainment. In healthcare, action recognition enables touch-free browsing of medical images in operating rooms, physical therapy at home and in clinics and for patient monitoring. In education, action recognition can increase the engagement of users by providing realistic and immersive training simulations. In entertainment, action recognition enables touch-free interaction with Smart TVs and games consoles for more intuitive and natural interaction. A key requirement of these interactive applications is the ability to robustly detect actions in real-time so the system can provide an appropriate response to the user with no apparent delay.

Historically, action recognition research has focused on increasing accuracy on datasets in highly controlled environments. These datasets normally contained a single person that was instructed to perform a single action clearly (see Fig. 1
                     ). Recognition was performed offline after viewing a complete sequence and algorithms were evaluated by the number of correctly classified sequences. A recent survey [1] showed perfect or near perfect action recognition accuracy on simple datasets with a small number of actions.

The traditional offline approach led to simplification of the problem, overinflated accuracy and lack of applicability to real world situations. Recent research toward more realistic action recognition has changed to online action recognition where different actions are detected in real-time whilst they are being observed. However, the focus has been on recognising actions which are temporally well separated and easy to segment. In contrast, this work considers multiple actions performed in quick succession, which are critical for robust action detection in natural user interface (NUI) applications. When multiple actions are performed in quick succession movements from different actions may temporally overlap resulting in complex poses, which we term as compound actions. For example, in a full body fighting game a player may throw punches in quick succession, one arm may still be finishing the previous punch whilst the other arm is performing the next punch or a player may leave one arm in the defend position and punch with the other arm (as shown in Fig. 2
                     ). Detecting multiple actions in quick succession is a more complex problem than recognising actions which are temporally well separated.

Existing work on recognising more complex actions has to date only been researched in an offline context. To evaluate the performance of action recognition algorithms on more realistic actions several datasets have been extracted from TV and film (YouTube Action Dataset [4], Hollywood Human Actions Dataset [5], UCF sports action dataset [6]). In these datasets the actions are performed in real-world scenarios with diverse and cluttered backgrounds as well as significant changes in viewpoint. The individual actions are realistic but the major limitation of these datasets is that they have been segmented into sequences containing a single action suitable for offline action recognition. The diversity and complexity of real-world datasets makes accurate labelling difficult and time consuming. To overcome this problem Ma et al. [7] employed transfer learning to transfer knowledge from a simpler domain (e.g. KTH [2]) to a more complex target domain (e.g. YouTube Action Dataset) but their approach was limited to offline action recognition. An area that has not been considered before is the potential for transfer learning to improve online action recognition.

Several NUI datasets with multiple actions in each sequence have been captured (MSRC-12 [8], G3D [9], G3Di [3]) and action points [10] provided, as temporal anchors to enable evaluation of online action recognition algorithms. Good performance has been achieved on the datasets where the actions were recorded under controlled circumstances (MSRC-12, G3D) but performance dramatically decreased when the same algorithm [3] was applied to a real-world scenario of a full body fighting game (G3Di). All three datasets contain multiple actions but the difference is that the MSRC-12 and G3D datasets contain actions that are temporally well separated whereas the G3Di dataset, contains transitions between actions and even multiple actions at the same time. Temporal merging of a user's actions results in compound actions comprising of movements from different actions, which have not been adequately addressed by existing approaches.

In this work we propose a novel hierarchical transfer learning algorithm for online action recognition of compound actions. Specifically, transfer learning is employed to allow the tasks of action segmentation and modelling to be performed on a related but simpler dataset, combined with model adaptation to improve performance on a more complex dataset. Furthermore, we represent actions hierarchically to provide the flexibility to recognise poses that are not in the source dataset by introducing independence between limbs. Evaluation on a realistic and challenging public action dataset confirms the effectiveness of our approach.

@&#LITERATURE REVIEW@&#

A key requirement of many real-world applications is the ability to recognise actions online. However, recent surveys [11,12] show that the majority of existing action recognition algorithms are offline and rely on observing a pre-segmented action sequence before classification of a single action. A common adaptation of existing approaches is to use a sliding window and classify the current frame based on the recent temporal history. This enables continuous recognition of multiple actions in real world scenarios such as monitoring elderly patients at home [13]. However, there is an additional requirement in NUI applications to detect actions with low latency so the system can provide an appropriate response to the user with no apparent delay. For example, increasing the volume on a Smart TV by raising a hand should be detected with low latency to provide natural interaction.

Existing work has demonstrated that action points [10], temporal anchors within the course of the action are important for evaluating the latency of the detection. An action point is a single pose that can be clearly and easily identified as a representative of an action. Several, sliding window approaches for online action recognition have been validated using action points [8,14,15]. Fothergill et al. [8] used fixed size sliding windows on the streaming data and performed the classification by a Random Forest. Similarily, Bloom et al. [14] used a fixed size sliding window and perform the classification by AdaBoost. However, the fixed size of the sliding window in both approaches is a source of classification error due to execution rate variations. To address this Zhao et al. [15] optimise the size of the segment during their feature extraction using a DTW variant for subsequence matching. However, as these methods were tested with temporally separated actions their ability to robustly detect compound actions is unclear. Especially as AdaBoost which achieved good performance on relatively simple actions [16] but when applied to more complex actions performance dramatically decreased [3].

Manual labelling of action points is possible in complex datasets as they represent the most significant part of the action, however subsequently automatically selecting a sequence of training examples around the point leads to inconsistencies. Firstly, as some actions have long duration such as defending (see Fig. 2), later samples of the current action will be incorrectly selected as negative samples. Secondly, samples from another action class may be incorrectly selected due to the close proximity of neighbouring actions (see Fig. 2). The first problem has been overcome by action segments [3] which incorporate the duration of the most significant part of the action. The second problem has not yet been adequately addressed but could be alleviated by reducing the need for labelling.

Transfer learning [17] has been beneficial to many machine learning research areas, including classification, regression and clustering problems to reduce the need to collect and label training data. However, transfer learning applied to action recognition is a relatively new topic with limited research in the computer vision community. Transfer learning has been used for cross-view action recognition [18,19] to recognise human actions from different views. In both cases the methods were tested offline on a multi-view dataset (IXMAS) [20], which comprised of simple actions with simple backgrounds so it has limited applicability to real world scenarios.
                     
                  

More significantly transfer learning has been used cross-dataset [7,35] to harness lab datasets to facilitate real-world action recognition. The aim is to generalise action models built from a source dataset to a target dataset, to alleviate the problem of labelling complex sequences. The source dataset typically has a clean background and each video clip may involve only one type of action and a single person, which describes most lab collected datasets. In contrast, in the target dataset the background may be cluttered and there may be multiple people and multiple actions which may overlap temporally. Cross-dataset learning aims to adapt the existing classifier from a source dataset to a new target dataset, while requiring only a small or even no labelled samples in the target dataset. Ma et al. [7] built a model within a multi-task framework so the actions of one domain are associated with its own features. The general Schatten p-norm was applied to mine the shared components between the lab data and the real world data. The main advantage of their approach is the ability to share knowledge between the two datasets even if they have different action categories. However, the method was tested offline with sequences containing just a single action. Cao et al. [21] combine model adaption and action detection into a Maximum a Posterior (MAP) estimation framework for action detection. The advantage of this approach over the previous method is that it can perform spatial-temporal detection of the action within a sequence. However, as a search for the optimal 3D sub-volume is performed across all frames in the target sequence this approach is also offline.

The approaches described so far are limited to single actions or multiple actions that are temporally separated. However, in NUI applications the user may wish to perform multiple actions in quick. This temporal merging of different actions results in complex poses comprising of movements from multiple actions. Hierarchical models have been successfully applied to pose estimation [22–26] to recover novel poses not present in the training dataset. Hierarchical models have also been applied to improve action recognition performance [27]. Following the popular bag-of-words approach several efforts constructed a hierarchical representation of local feature descriptors but as the temporal order is ignored they are not suited to many real-world problems. To overcome this Song et al. [28] propose hierarchical sequence summarisation to capture discriminative information at various temporal resolutions. However, as the testing was performed at the sequence level this approach is limited to offline action recognition.

We propose a novel hierarchical transfer learning algorithm for online detection of compound actions for robust action recognition in natural user interface (NUI) applications. Specifically, transfer learning is employed to allow the tasks of action segmentation and modelling to be performed on a related but simpler dataset, combined with model adaptation to improve performance on a complex NUI dataset. We represent actions using a hierarchical human body model to allow independence between low-level body parts. Our novelty is to automatically weight each low-level body part based on their discriminative ability to detect specific actions. We propose hierarchical peak poses for low latency detection which provide the flexibility to recognise poses that are not in the source dataset. Hierarchical template matching is performed with Dynamic Time Warping (DTW) to ensure execution rate invariance and we use a sliding window approach for online recognition. Evaluation on a public dataset with complex, realistic actions demonstrates that our approach outperforms existing methods in terms of accuracy and latency.

@&#METHODOLOGY@&#

The proposed method for online action recognition consists of two phases: an offline training phase and an online testing phase as illustrated in Fig. 3.
                  

We propose a novel hierarchical transfer learning algorithm for online detection of compound actions for fast and robust action recognition in natural user interface (NUI) applications. Our method is based on skeleton data, specifically joint angles which are viewpoint and anthropometric invariant and can be generated in real-time with a pose estimation method [29]. A key contribution is our hierarchical body model that can be automatically configured to detect actions based on the low level body parts that are the most discriminative for a particular action. Another key contribution is a transfer learning strategy to allow the tasks of action segmentation and whole body modelling to be performed on a related but simpler source dataset, combined with automatic hierarchical body model adaption on a more complex target dataset (as shown in Fig. 3).

The training phase is based on our existing approach for online action detection [16] that achieved high accuracy and low latency for multiple actions that were separated temporally (see Fig. 4). Our contribution is to adapt these action templates to detect compound actions by representing and detecting actions hierarchically. The two key stages in training, as published in our previous work [16] are dimensionality reduction and key pose generation. Dimensionality reduction of the skeleton data produces spatio-temporal manifolds which removes individual style whilst maintaining the temporal ordering of the poses. Clustering the manifolds and projecting the cluster centres back to the high dimensional space creates key poses. An individual key pose represents a generic pose from an action at a specific point in time and the sequence of these key poses represent the entire action (as illustrated in Fig. 5
                        ). A major benefit of the clustering is that the number of key poses is significantly less than the original number of training poses which dramatically reduces the computation time and enables our approach to scale efficiently to much larger datasets.

The two stages are explained in detail below:

Stylistic variations are removed by learning a clustered spatio-temporal manifold (CSTM) for each action [16]. Given a set of training poses from the source dataset 
                              
                                 X
                                 =
                                 
                                 
                                    {
                                    
                                       x
                                       i
                                    
                                    }
                                 
                                 
                                    
                                    
                                       (
                                       
                                          i
                                          =
                                          1
                                          …
                                          n
                                       
                                       )
                                    
                                 
                              
                           , 
                              
                                 
                                    x
                                    i
                                 
                                 ∈
                                 
                                 
                                    R
                                    D
                                 
                              
                           , distributed in a high dimensional space, Temporal Laplacian Eignemaps (TLE) [30] discovers their low dimensional representation 
                              
                                 
                                    X
                                    ′
                                 
                                 =
                                 
                                 
                                    {
                                    
                                       x
                                       
                                          
                                          i
                                          ′
                                       
                                    
                                    }
                                 
                                 
                                    
                                    
                                       (
                                       
                                          i
                                          =
                                          1
                                          …
                                          n
                                       
                                       )
                                    
                                 
                              
                           , 
                              
                                 x
                                 
                                    
                                    i
                                    ′
                                 
                                 
                                 ∈
                                 
                                 
                                    R
                                    d
                                 
                              
                            where d ≪ D by combining two neighbourhood graphs. Temporal neighbours are the closest points in the sequential order and spatial neighbours are the geometrically similar neighbours. These neighbour relations are used in the construction of two graphs where any two vertices are connected when a neighbour relationship exists between these points. Neighbourhood connections defined in the Laplacian graphs place neighbours from the high dimensional space nearby in the embedded space. Consequently, the temporal neighbours preserve the temporal structure and the spatial neighbours reduce style variability by aligning the time series in the embedded space (see Fig. 6
                           ).

Clustering is then performed on the embedded space to reduce computation time by removing redundant poses. k-means [31] is applied to cluster the n low dimensional points X′ into m clusters 
                              
                                 C
                                 =
                                 
                                    
                                       (
                                       
                                          c
                                          j
                                       
                                       )
                                    
                                    
                                       (
                                       
                                          j
                                          =
                                          1
                                          …
                                          m
                                       
                                       )
                                    
                                 
                              
                           , 
                              
                                 
                                    c
                                    k
                                 
                                 
                                 ∈
                                 
                                 
                                    R
                                    d
                                 
                              
                           , where m ≪ n.

Key poses remove redundant information to improve classification accuracy and reduce the computational latency of action detection [13,16]. To generate key poses we follow the method proposed in [30] that uses the training set 
                              
                                 M
                                 =
                                 
                                    
                                       {
                                       
                                          
                                             x
                                             i
                                          
                                          ,
                                          x
                                          
                                             
                                             i
                                             ′
                                          
                                       
                                       }
                                    
                                    
                                       (
                                       
                                          i
                                          =
                                          1
                                          …
                                          n
                                       
                                       )
                                    
                                 
                                 
                              
                           to learn a Radial Basis Function Network (RBFN) that represents the mapping between the embedded and the high dimensional space [30]. Then using the RBFN mappings the cluster centres are projected into the high dimensional space to generate new poses that are a direct representation of the average poses. The implicit temporal order in the low dimensional space can be extracted from the training data to order the corresponding key poses 
                              
                                 K
                                 =
                                 
                                    
                                       {
                                       
                                          k
                                          j
                                       
                                       }
                                    
                                    
                                       (
                                       
                                          j
                                          =
                                          1
                                          …
                                          m
                                       
                                       )
                                    
                                 
                              
                           to create action templates (K) for each action as illustrated in Fig. 5. Action templates are the high dimensional representations of the clustered spatio-temporal models and inherit their advantages, including style invariance and compactness.

To detect compound actions such as those performed in NUI applications we propose a hierarchical template matching algorithm (see Fig. 7
                        ). Representing actions using a hierarchical model of human body allows independence between the low-level body parts 
                           
                              B
                              =
                              
                                 
                                    (
                                    
                                       b
                                       l
                                    
                                    )
                                 
                                 
                                    (
                                    
                                       l
                                       =
                                       1
                                       …
                                       L
                                    
                                    )
                                 
                              
                           
                         (as illustrated in Fig. 8
                        ). Each low-level body part is represented by joint angles. Our contribution is to automatically weigh each low-level body part based on their discriminative ability to detect specific actions. Weighting the individual low-level body parts, creates flexible body part configurations at different levels of a normal body hierarchy e.g. whole body, upper body or right arm and atypical combinations such as right arm and left leg.

The action peak is a fundamental concept of the proposed approach which we define as the segment in time when the goal of the action is being satisfied. For example, in a boxing game the aim of the punch is to hit the opponent which is being fulfilled when the arm is maximally extended as shown in Fig. 9
                        . The peak poses in the training data of the target dataset are manually labelled with an action label, there must be at least one frame labelled as the peak pose for each action instance. If the action peak has duration, as in the case of the defense action there will be multiple sequential labelled frames.

There are three main steps to adapt the action templates learnt from the source dataset for hierarchical template matching: learning the most discriminative body part combinations, detecting the most representative hierarchical peak key pose and optimising the peak segment threshold.

All three steps use exemplar matching between the peak poses in the target dataset training poses and the action templates to find the optimum matching parameters. To incorporate the temporal history of the action and increase the robustness of the matching process sequences of poses are matched rather than single poses. To extract a fragment F from a sequence of poses 
                           
                              S
                              
                              =
                              
                              (
                              
                                 
                                    s
                                    1
                                 
                                 ,
                                 
                                    s
                                    2
                                 
                                 ,
                                 …
                                 
                                    s
                                    G
                                 
                              
                              )
                           
                         
                        Eq. (1) is used:

                           
                              (1)
                              
                                 
                                    F
                                    
                                       (
                                       
                                          S
                                          ,
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       (
                                       
                                          
                                             s
                                             
                                                i
                                                −
                                                s
                                             
                                          
                                          ,
                                          
                                             s
                                             
                                                i
                                                −
                                                s
                                                +
                                                1
                                             
                                          
                                          ,
                                          …
                                          
                                             s
                                             i
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        where, i is the pose index, s is the number of poses in the fragment and G is the number of poses in sequence S and the conditions i > s and i ≤ G are satisfied.

DTW [32] is a well-known algorithm for determining the similarity of time-series data that allows “elastic” transformation to gain execution rate invariance. The similarity of two series of poses, the query sequence 
                           
                              Q
                              =
                              
                              (
                              
                                 
                                    q
                                    1
                                 
                                 ,
                                 
                                    q
                                    2
                                 
                                 ,
                                 
                                 …
                                 
                                 
                                    q
                                    U
                                 
                              
                              )
                           
                         and the reference sequence 
                           
                              R
                              
                              =
                              
                              (
                              
                                 
                                    r
                                    1
                                 
                                 ,
                                 
                                    r
                                    2
                                 
                                 ,
                                 
                                 …
                                 
                                 
                                    r
                                    V
                                 
                              
                              )
                           
                         can be computed using the standard DTW distance metric using Eq. (2).

                           
                              (2)
                              
                                 
                                    D
                                    T
                                    W
                                    
                                       (
                                       
                                          Q
                                          ,
                                          R
                                       
                                       )
                                    
                                    =
                                    min
                                    
                                       {
                                       
                                          
                                             c
                                             p
                                          
                                          
                                             (
                                             
                                                Q
                                                ,
                                                R
                                             
                                             )
                                          
                                          ,
                                          p
                                          ∈
                                          
                                             P
                                             
                                                U
                                                ×
                                                V
                                             
                                          
                                       
                                       }
                                    
                                 
                              
                           
                        
                     

Where cp
                         is the global cost function associated with a warping path 
                           
                              p
                              =
                              (
                              
                                 
                                    p
                                    1
                                 
                                 ,
                                 …
                                 ,
                                 
                                    p
                                    H
                                 
                              
                              )
                           
                         and c is the local cost function, which is the Euclidean distance between two poses, which will be small if the poses are similar to each other:

                           
                              (3)
                              
                                 
                                    
                                       c
                                       p
                                    
                                    
                                       (
                                       
                                          Q
                                          ,
                                          R
                                       
                                       )
                                    
                                    =
                                    
                                    
                                       ∑
                                       
                                          h
                                          =
                                          1
                                       
                                       H
                                    
                                    c
                                    
                                       (
                                       
                                          
                                             q
                                             
                                                u
                                                h
                                             
                                          
                                          ,
                                          
                                             r
                                             
                                                v
                                                h
                                             
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

In our previous approach [16] the DTW distance was computed for the whole body. To increase flexibility we propose a hierarchical DTW distance measurement (HDTW):

                           
                              (4)
                              
                                 
                                    H
                                    D
                                    T
                                    W
                                    
                                       (
                                       
                                          Q
                                          ,
                                          R
                                          ,
                                          W
                                       
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          l
                                          =
                                          1
                                       
                                       L
                                    
                                    D
                                    T
                                    W
                                    
                                       (
                                       
                                          
                                             Q
                                             l
                                          
                                          ,
                                          
                                             R
                                             l
                                          
                                       
                                       )
                                    
                                    
                                       W
                                       l
                                    
                                 
                              
                           
                        
                     

For two series of poses, the query sequence Q and the reference sequence R, the similarity of low level body parts l is computed independently using the standard DTW distance metric. A weighted combination 
                           
                              W
                              =
                              
                                 
                                    (
                                    
                                       w
                                       l
                                    
                                    )
                                 
                                 
                                    (
                                    
                                       l
                                       =
                                       1
                                       …
                                       L
                                    
                                    )
                                 
                              
                              ,
                              
                              
                                 w
                                 l
                              
                              
                              ∈
                              
                              
                                 (
                                 
                                    0
                                    ,
                                    1
                                 
                                 )
                              
                           
                         of the low level body part distances provides a discriminative distance metric for compound actions.

The most discriminative body part combinations for each action are discovered by maximising the ratio of intra-class matches between the labelled peak poses in the target dataset training data and the action templates. This procedure is repeated for all body part combinations, so for computational efficiency we selected binary weights,  wl
                            ∈ (0, 1) for each of the low level body parts which results in 2
                              L
                            permutations. For each permutation ɛ, the intra-class ratio ρ is computed by the number of intra-class matches μ over the number of total training instances in the target dataset ny
                           . The intra-class matches are counted for each action by exemplar matching between the peak poses from the target dataset training data and the key poses from all the action templates. For each action a, if the closest matching action template is the same action this is counted as an intra-class match. The maximum intra-class ratio represents the most discriminative body part combination for each action, as illustrated in Fig. 10
                            and summarised in Algorithm 1
                           .

In our previous work on simple actions, peak key poses were proposed as the generic representation of peak poses in the training data and were automatically selected from the key poses by exemplar matching with the whole body [16]. To increase robustness on compound actions we propose hierarchical peak key poses. Hierarchical peak key poses are also automatically selected from the key poses but the exemplar matching is performed using the most discriminative body parts rather than the whole body. The hierarchical peak key poses are selected as follows: for each action and for each peak pose in the target dataset training data, the best matching key pose is found (as shown in Fig. 11
                           ). A hierarchical peak key pose can be represented by its index ja
                            in the action template. The best matching index j* is found by minimising the distance between the peak pose fragments F
                           Y and the key pose fragments F
                           K using the most discriminative body part combination for each action. The hierarchical peak key pose for the action is the key pose that has the maximum number of matches, as summarised in Algorithm 2
                           .

Some existing methods for online action recognition detect the action as a single point in time [8,16] whereas others incorporate the duration of the action [13,33]. The duration of the action is important for subsequently detecting interactions between multiple players in a sports game [3] and illustrated by Fig. 12.
                           
                        

Peak key poses [13] were limited to detecting a single temporal point so we introduce a threshold τ to incorporate the duration of the peak. Similar to [13,33] we introduce a threshold τ for action detection but instead of specifically learning a threshold for each action we learn a single threshold for all actions. Confining, the threshold to a single parameter reduces the time taken to adapt the model and this time will not increase even if more actions are considered, providing scalability to larger datasets.

The threshold τ and fragment size s are learnt on the training part of the target dataset by optimising the action point metric F1 [10] with our hierarchical template matching algorithm (summarised in Algorithm 3
                           ) but using the training data from the target dataset rather than the testing data.

We propose a hierarchical template matching algorithm with a temporal sliding window for online action recognition (summarised in Algorithm 3). For each new frame the sliding window buffer is updated and compared with learnt exemplars. The minimum hierarchical DTW distance to the nearest neighbour is used to detect the action (see Fig. 13
                        ).

The hierarchical matching process is performed using DTW to ensure execution rate invariance. The normalised hierarchical DTW distances d*, are recorded for each frame as illustrated in Fig. 14
                        . To detect actions in real-time we compare the lowest hierarchical DTW distance at each frame with a threshold τ. τ discriminates which pose fragments are most similar to the peak key pose fragment. Therefore, whilst pose fragments are similar to the peak key pose fragment (d* ≤ τ) the action is at its peak, as shown by the coloured segments on Fig. 14. Before and after the peak, the pose fragments will be less similar (d* > τ) and therefore the action is not considered at its peak.

One of the advantages of using clustering to identify peak poses is that the computational time is independent on the size of the training dataset, although it is linearly dependent on the number of actions. In case of many actions, a parallel implementation, i.e. one thread per action, would achieve real-time performance.

@&#EXPERIMENTS@&#

In this section we present experiments to evaluate the ability of our online action recognition method to improve accuracy at low latency in complex scenarios.

The performance of our algorithm is evaluated using publicly available datasets designed specifically for real time action recognition: G3D [9], MSRC-12 [8] and G3Di [3]. All datasets contain multiple actions in each sequence in a controlled indoor environment with a fixed camera, a typical setup for NUI applications. Both datasets provide sequences of skeleton data captured using the Kinect pose estimation pipeline at 30fps. However, G3D contains scripted actions which are temporally well separated whereas G3Di was captured using a gamesourcing approach where the users were recorded whilst playing computer games and consequently contains more complex actions which overlapping temporally. The G3Di also contains noisier skeleton data than G3D as there was interference from multiple Kinects during the recording, making it more realistic of a home scenario where there may be interference from the sunlight.

The G3D dataset contains 10 subjects performing 20 gaming actions grouped into seven categories. The fighting category was selected as it has the same actions as the G3Di boxing category although there are substantial variations in execution rate as well as personal style between these two datasets due to the different recording environments. The G3D fighting category contains five gaming actions: right punch, left punch, right kick, left kick and defend.

The MSRC-12 dataset comprises of 30 people performing 12 gestures. These gestures are categorized into two categories: iconic and metaphoric gestures. The iconic gestures directly correspond to real world actions and represent first person shooter (FPS) gaming actions. There are six FPS gaming actions: crouch, shoot, throw, night goggles, change weapon and kick. Whereas metaphoric actions represent abstract concepts for manipulating a music player e.g. raise volume of the music. The dataset was obtained using different instruction modalities and the modality that produced the most accurate results was video + text so we will use this particular subset of the dataset.

The G3Di dataset contains 12 people split into 6 pairs. Each pair interacted through a gaming interface showcasing six sports: boxing, volleyball, football, table tennis, sprint and hurdles. Boxing is a competitive sport and the interactions can be decomposed by an action and counter action. The boxing actions were right punch, left punch and defend and the interactions between the players are shown in Table 1
                        . The total number of action and interaction instances used for our experiments is shown in Table 2.
                        
                     

Joint angles are viewpoint and anthropometric invariant and can be generated in real-time with a pose estimation method [29]. More specifically, the skeleton poses are first normalised and then the three angles defining each joint position are computed and represented by a 4-D quaternion. The skeleton is parameterised as a high dimensional feature vector by concatenating quaternions for all joints. For each pose 13 quaternions are calculated so each feature vector has 52-dimensions (see [13] for more details).

The following is a brief introduction of the comparison algorithms in our experiments:

                           
                              •
                              AdaBoost: AdaBoost has shown high accuracy and low latency for online action recognition [4,16]. AdaBoost was trained on the source dataset and the parameters: the number of training frames around each peak pose the sliding smoothing window size were optimised on the training part of the target dataset and the method was evaluated on to the target testing data.

Clustered Spatio-Temporial Manifolds (CSTM): is a state-of the art approach for low latency online action recognition [16]. CSTM was trained on the source dataset and the parameters: the template size and the stream size and the peak pose detector were optimised on the training part of the target dataset and the method was evaluated on to the target testing data.

Hierarchical Transfer Segments (HiTS): The proposed method in this paper, a version of CSTM extended for transfer learning, allowing knowledge to be transferred from simple actions in a source dataset to complex actions in a target dataset by adapting the learnt models with a hierarchical pose representation. The parameters: peak segment matching threshold (τ = 0.22) and fragment size (
                                    
                                       s
                                       =
                                       7
                                    
                                 ) were optimised on the training part of the target dataset and the method was evaluated on to the target testing data.

For all the above experiments we performed leave one-person out cross validation on the target dataset; each cross validation fold was trained on 11 subjects and tested on the remaining subject.

Evaluating of action recognition algorithms has previously been done in isolation, focusing historically on high accuracy and more recently also on low latency. However, in reality most actions form part of an interaction where the duration of the action is important. To test our proposed algorithm in a realistic context we employ the interaction detection and evaluation framework [3] and the action point metric [10] which is the most commonly used metric for online action recognition.

For evaluation we use an existing latency-aware performance metric for based on temporal anchors known as action points [10]. For a specified amount of latency (Δms) the action point F1-score determines whether a detection made at time tp
                            for action a is correct in relation to a ground truth action point at time tg
                            by using the following formula:

                              
                                 (5)
                                 
                                    
                                       Φ
                                       
                                          (
                                          
                                             
                                                t
                                                p
                                             
                                             ,
                                             
                                             
                                                t
                                                g
                                             
                                             ,
                                             
                                                
                                                Δ
                                             
                                          
                                          )
                                       
                                       =
                                       
                                       
                                          {
                                          
                                             
                                                
                                                   
                                                      1
                                                      
                                                      if
                                                      
                                                      
                                                         (
                                                         
                                                            
                                                               |
                                                               
                                                                  
                                                                  
                                                                     t
                                                                     g
                                                                  
                                                                  
                                                                  −
                                                                  
                                                                  
                                                                     t
                                                                     p
                                                                  
                                                               
                                                               |
                                                            
                                                            
                                                            ≤
                                                            
                                                            Δ
                                                         
                                                         )
                                                      
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      0
                                                      
                                                      otherwise
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

For a specified amount of latency (Δms) the precision and recall are measured for each action and combined to calculate a single F-score.

                              
                                 (6)
                                 
                                    
                                       F
                                       1
                                       −
                                       score
                                       
                                          (
                                          
                                             a
                                             ,
                                             Δ
                                          
                                          )
                                       
                                       
                                       =
                                       
                                       2
                                       
                                          
                                             pre
                                             
                                                c
                                                a
                                             
                                             
                                                (
                                                Δ
                                                )
                                             
                                             
                                                
                                                re
                                             
                                             
                                                c
                                                a
                                             
                                             
                                                (
                                                Δ
                                                )
                                             
                                          
                                          
                                             pre
                                             
                                                c
                                                a
                                             
                                             
                                                (
                                                Δ
                                                )
                                             
                                             
                                             +
                                             
                                                
                                                re
                                             
                                             
                                                c
                                                a
                                             
                                             
                                                (
                                                Δ
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

As online action recognition algorithms need to detect multiple actions, the mean F-score over all actions is used, defined as:

                              
                                 (7)
                                 
                                    
                                       
                                          Average
                                          
                                          F
                                       
                                       1
                                       −
                                       score
                                       
                                          (
                                          
                                             A
                                             ,
                                             Δ
                                          
                                          )
                                       
                                       =
                                       
                                       
                                          1
                                          
                                             |
                                             A
                                             |
                                          
                                       
                                       
                                       
                                          ∑
                                          
                                             a
                                             ∈
                                             A
                                          
                                       
                                       F
                                       1
                                       −
                                       score
                                       
                                          (
                                          
                                             a
                                             ,
                                             Δ
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        

The interaction detection framework [3] enables online interaction recognition between multiple people by detecting their individual actions independently and combining them by a set of interaction rules to infer the interaction. This modular approach is applicable for NUI and enables interaction between people that are not in the same physical location. Actions from different people are detected independently. At each frame, these detections are combined to infer the current interaction. The interaction rules include the valid combinations of actions (as depicted in Table 1) together with timing constraints. The action (a) and counter action (ca), are checked at each frame together with a timing constraint (f) to detect interactions in real time using Eq. 8. The timing constraint depends on the scenario, for example all the interactions in boxing are instant (f = 0), the action and counter action co-occur.

                              
                                 (8)
                                 
                                    
                                       ψ
                                       
                                       
                                          (
                                          
                                             
                                                a
                                                s
                                             
                                             ,
                                             
                                             
                                                a
                                                e
                                             
                                             ,
                                             c
                                             
                                                a
                                                s
                                             
                                             ,
                                             
                                             c
                                             
                                                a
                                                e
                                             
                                          
                                          )
                                       
                                       
                                       =
                                       
                                       
                                          {
                                          
                                             
                                                
                                                   
                                                      1
                                                      
                                                      if
                                                      
                                                      
                                                         (
                                                         
                                                            
                                                               a
                                                               s
                                                            
                                                            +
                                                            f
                                                            
                                                            ≤
                                                            
                                                            c
                                                            
                                                               a
                                                               e
                                                            
                                                         
                                                         )
                                                      
                                                      
                                                      and
                                                      
                                                      
                                                         (
                                                         
                                                            c
                                                            
                                                               a
                                                               s
                                                            
                                                            
                                                            ≤
                                                            
                                                            
                                                               a
                                                               e
                                                            
                                                            +
                                                            f
                                                         
                                                         )
                                                      
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      0
                                                      
                                                      otherwise
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Where s and e represent the start and end of the action segment respectively and s ≤ e.

@&#RESULTS@&#

Our method (HiTS) outperforms existing state of the art approaches for fast online action and interaction recognition, as shown in Fig. 15
                        . Both AdaBoost and CSTM show a significant drop in accurately detecting actions on the G3Di (Fighting) dataset in comparison with previously published results [16] on the G3D (Boxing) dataset. This is significant especially as the G3Di (Fighting) actions are a subset of the G3D (Boxing) actions but confirms our hypothesis that compound actions are more difficult to detect than multiple actions that are temporally well separated.

Additionally, we highlight the recognition accuracy for each category of action and interaction for a more detailed analysis of each method, as shown in Fig. 16
                        . A significant outcome is that even though CSTM [16] can detect all of the action categories it is unable to detect any interactions which are comprised of actions with duration, specifically the block interaction. In addition to showing the limitation of this approach it also highlights a weakness of the action point metric [10] which does not incorporate the duration of the action peak. Interaction detection is improved by our baseline method Peak Segment Matching (PSM) which instead of a binary decision for matching a peak key pose introduces a threshold which can detect the duration of the peak. The key contributions of this paper are the hierarchical body model (HSM) and a transfer learning strategy (TSM). Individually, applied to our baseline method these contributions actually decrease the action and interaction recognition but together (HiTS) they form a powerful combination that significantly increases the action and interaction recognition, as shown in Fig. 12. Intuitively, our hierarchical representation is only useful if adapted to the target dataset.
                     

In this paper we are exclusively interested in action recognition approaches that are suitable for NUI applications. Research has shown that a delay of 100 ms is not perceivable by the user [34]. Therefore, in this section we have only compared our method against online action recognition methods that are capable of fulfilling this requirement. Table 3
                         shows that all the methods we evaluated are capable of detecting actions with a low average latency of approx. 2 frames, which is equivalent to 66 ms. We did not evaluate online action recognition methods with high latency (830–1500 ms [15], 2000 ms [13]) as they are better suited to other applications.


                        Fig. 17 illustrates a typical failure case caused by noisy skeleton data at the action level resulting in an incorrect interaction to be inferred. The main limitation of our approach is that we only utilise the skeleton modality which is subject to interference from sunlight.

The dependency of the proposed transfer learning methodology on the amount of training data used from the target dataset is investigated. Specifically, Fig. 18
                         demonstrates the action and interaction recognition performance (F1) for varying number of training subjects. The proposed method may achieve similar results to other competitor methods, i.e. around 0.6 and 0.4 F1 score for action and interaction recognition respectively (see Fig. 15) with almost half the training data from the target dataset, i.e. 6 subjects.

Regarding the template size s in theory it is possible to use different values in the matching process. However, in practice it was not computationally feasible to test all of these combinations so in our experiments we actually used a single parameter s which was learnt on the training part of the target dataset. Fig. 19
                         shows how this parameter affects performance. This parameter does not model the duration of the action as the graph shows that even 3 frames (100 ms) can accurately detect the action peak and overall performance is fairly consistent for higher values.

@&#CONCLUSION@&#

In this work we presented a novel hierarchical transfer learning algorithm for fast online action recognition. It overcomes the limitations of existing approaches by representing the human body hierarchically and learning the most discriminative body parts to detect compound actions. A transfer learning strategy was introduced to allow the tasks of action segmentation and whole body modelling to be performed on a related but simpler dataset. Combined with hierarchical model adaptation on a more complex dataset to introduce independence between limbs and provide the flexibility to recognise poses that are not in the source dataset. Evaluation on a public target dataset that is more challenging and realistic than the source dataset shows our hierarchical transfer learning algorithm significantly increases performance at low latency. As the target dataset was recorded whilst users were actually playing a game the actions are more natural than subjects that are given instructions or restrictions and demonstrates the viability of our algorithm for use in real-world applications.

The limitation of our approach is that we only utilise the skeleton modality which is subject to interference from sunlight. Our future work is improve the robustness of our algorithm by fusing features from the depth or colour with our hierarchical skeleton features and evaluate its effectiveness using the G3Di multi-modal dataset.

@&#REFERENCES@&#

