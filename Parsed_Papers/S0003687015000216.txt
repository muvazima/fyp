@&#MAIN-TITLE@&#Sonification of in-vehicle interface reduces gaze movements under dual-task condition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We studied the impact of using a sonified or silent interface while performing a visual primary task.


                        
                        
                           
                           We measured participants gaze and performance to both primary and secondary tasks.


                        
                        
                           
                           Participants used the sonified interface nearly exclusively be ear while performing the primary task.


                        
                        
                           
                           The reaction times in the primary task were increased in both sonified and silent conditions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sonification

Dual-task

Eye-tracking

@&#ABSTRACT@&#


               
               
                  In-car infotainment systems (ICIS) often degrade driving performances since they divert the driver's gaze from the driving scene. Sonification of hierarchical menus (such as those found in most ICIS) is examined in this paper as one possible solution to reduce gaze movements towards the visual display. In a dual-task experiment in the laboratory, 46 participants were requested to prioritize a primary task (a continuous target detection task) and to simultaneously navigate in a realistic mock-up of an ICIS, either sonified or not. Results indicated that sonification significantly increased the time spent looking at the primary task, and significantly decreased the number and the duration of gaze saccades towards the ICIS. In other words, the sonified ICIS could be used nearly exclusively by ear. On the other hand, the reaction times in the primary task were increased in both silent and sonified conditions. This study suggests that sonification of secondary tasks while driving could improve the driver's visual attention of the driving scene.
               
            

@&#INTRODUCTION@&#

According to recent surveys (http://www.drive-safely.net/top-ten-driving-distractions.html), the use of external devices such as a telephone, a music player, or the radio is responsible for most driving distractions. Sending a text message or having a phone conversation while driving is undoubtedly unsafe and can have fatal consequences (Victoria et al., 2013). In this paper we focus on the use of in-car infotainment systems (ICIS), i.e., a device installed in the car to provide services such as navigation systems, music player or telephone. To minimize its impact on driver's visual attention, the ICIS is designed to have a visual display located in the middle of the dashboard, and a control device close to the gear stick. In the US, the National Highway Traffic Safety Administration (NHTSA) has recently published a series of guidelines for designing in-car devices with a limited impact on driver's visual attention (NHTSA, 2013). Nevertheless, the use of such technologies in cars is still responsible for frequent off-road glances: Sodhi et al. (2002) found that tuning the radio while driving resulted in 42% of off-road glances (total task time of 21.1 s on average), Young et al. (2012) showed that using a portable music player not only resulted in increased off-road glances but also reduced a driver's ability to maintain a constant line position. Kujala et al. (2013) found that most tasks performed on a touch screen while driving increased the number of rapid steering wheel movements. New solutions to further reduce even more the number and the duration of off-road glances are required.

Car driving is primarily a visual task (Sivak, 1996), whereas using an ICIS can rely on visual, auditory or both modalities. Visual-only ICIS has been largely addressed in the field of HMI ergonomics that provided design guidelines for improved information presentation (Singleton, 1971; Bastien and Scapin, 1992; Scapin and Bastien, 1997; Ziefle, 2010). Recently, Mitsopoulos-Rubens et al. (2011) demonstrated that list scrolling while driving significantly impaired driving performance (mean lane deviation and percentage of correct lane changes), but no gaze data were provided. Using an eye-tracker, Rydström et al. (2012) showed that both a touch screen and a rotary knob affected lateral control performance while doing alphanumeric input or list scrolling. Based on the occlusion technique, Baumann et al. (2004) indicated that a task performed with in-car navigation system must be portionable in 1–2 s chunks and the total task time is insufficient to evaluate the impact on driving performance. In other words, even if an ICIS has been optimized to be less visually demanding, it is still responsible for off-road glances and thus could impair the driving performances.

Sonification, as a way to display information using the only auditory modality, appears to be a strong candidate to reduce eye movements while driving. Introduced by Kramer (1992), sonification is based on at least three fundamental concepts: (1) earcon,
                        1
                     
                     
                        1
                        The name “earcon” comes from the term icon as an icon for the ear (Sumikawa, 1985).
                      i.e., an abstract sound for which the sound/meaning relationship is arbitrary and must therefore be learned by the user (Blattner et al., 1989). Earcons are usually based on acoustic and musical characteristics: pitch, rhythm, duration, etc. (2) auditory icon (Gaver, 1986), i.e., a sound which establishes a direct link with the object or concept that it represents, by referring to an easily recognizable sound from our daily environment, and is accordingly almost directly comprehensible by the user. This approach is preferred in the context of driving since it reduces the time needed by the drivers to learn the sounds. (3) spearcon (speech-earcon), i.e., accelerated text to speech synthesis employed to facilitate fast scrolling in long menus (Walker et al., 2006, 2013).

An ICIS is generally comprised of a small display presenting different menus and sub-menus organized in a large hierarchical structure. Sonification of hierarchical menus must address two issues: (1) sound representation of the hierarchical position of each item and (2) sound representation of the semantic content of each item. Earcon based sonifications are easily learned but not very well adapted to complex hierarchies (Brewster et al., 1995, 1998; Brewster, 1998). Leplâtre and Brewster (2000) and Leplâtre (2002) developed a new implementation of hierarchical earcons more closely linked to the hierarchical position of the items but too confusing in terms of semantic representation. On the other hand and as expected, auditory icons provide less arbitrary sonification as illustrated in Barrass (1998) and Conversy (1998). Auditory icons were also used in Gaver's Sonic Finder (Gaver, 1986), a computer auditory interface in which actions were mapped to everyday sound events (e.g., selecting a file mapped to the sound of an object being hit), added to another layer of information (e.g., file size with object size). In a previous study, we developed an original approach based on a combination of earcons and auditory icons (Langlois et al., 2010; Misdariis et al., 2011) and preference tests were performed in a driving simulator. The result was an improvement of the model achieved by adding synthesized speech at lower levels of the menu. A detailed presentation of this sonification is given in Section 2.5. The main limitation, the absence of gaze data, is addressed in the present paper.

Using a dual task paradigm, Jeon et al. (2009) measured the benefits of sonification on the performance to a visual primary task (a ball catching game) while navigating in a list of items. Results showed that sonification significantly improved both reaction times in the primary task and search times in the secondary task. However, no gaze data were collected to confirm that sonification reduced the visual demands of the secondary task. Sodnik et al. (2008) studied the navigation inside mobile phone menus (sonified or not) while driving in a simulator. The sonification was based on earcons mixed with synthesized speech and virtually placed in the space using a surround sound system. Results showed that sonification decreased the unsafe driving behaviors (e.g. unexpected deceleration) but did not improve menu navigation.

This review of related works reveals a lack of gaze data supporting the claim that sonification can efficiently reduce off-road glances while navigating in the menus of an ICIS. Our study addressed this issue with an experiment in which participants' gaze was measured while they performed two simultaneous tasks: (1) a visual primary task on a computer screen that simulates the sustained visual attention required when driving, and (2) a secondary navigation task in which participants navigated inside the menus of a realistic ICIS mock up (either sonified or not). The remainder of this article describes the experiment in Section 2, an analysis of the results in Section 3 and a discussion in Section 4.

@&#METHOD@&#

46 Participants (39 female, mean age 25.3, SD = 5.2) were recruited and paid for this study and gave consent prior to the experiment. Participants reported to have normal or corrected-to-normal vision and normal hearing. All the participants were right handed and had their driving license. Half of the participants performed the experiment with the silent interface (no sonification) and the other half with the sonified interface.


                        Fig. 1
                         presents the experimental apparatus. The participant was seated in front of the 17″ monitor of a Tobii T120 eye-tracker that presented both the primary and the secondary tasks. All sounds were listened through a pair of Sennheiser HD380 Pro headphones. For optimal gaze tracking, participants were requested to keep their head positioned in front of the screen center at an approximate distance of 65 cm.

The two tasks were implemented on a desktop PC and displayed on a 34 cm × 27 cm monitor with a resolution of 1280 × 1024 pixels and a refresh rate of 60 Hz. According to the Renault engineering department, the dimensions of both tasks (see Fig. 1) proportionally reproduced what is usually found in a car:
                           
                              -
                              The angles between the driving scene and the visual display of an ICIS located in the middle of the dashboard of a small car (21° horizontal and a 10° vertical);

The angular size of an ICIS with a 7″ visual display;

The angular size of the area in which drivers' gaze is mostly located (+6° horizontally and −6° vertically).

Car driving is a very complex task in terms of visual attention, working memory, as well as physical activity. According to Waard (1996), most of the driving-related tasks are automated and the required attention is mainly based on visual resources. In addition, one common driving context is the situation that requires sustained attention to events occurring in the central vision area and for which immediate reaction is needed (e.g. reacting to other cars or traffic lights). This driving context has been employed in previous studies to measure the impact of using mobile phones while driving (Lamble et al., 1999; Strayer et al., 2003). As mentioned by Harvey et al. (2011), “A driver's ability to detect and respond to events and hazards in the driving environment can be used as a measure of the interference from secondary tasks”. The primary task developed in this paper aimed at mimicking the sustained visual attention to sudden events that is required while driving.

We designed a visual selective attention task that needed sustained attention from the participant who had to constantly adapt his/her behavior to his/her environment. This type of primary task has been chosen in previous studies as an appropriate simulation of the visual attention needed while driving. Strayer and Johnston (2001) used a tracking task in which participants had to maneuver a cursor with a joystick to keep it as close as possible to a visual target on the screen. In Suied et al. (2008), participants used the mouse to keep a circular dot in a constantly moving circular boundary.

The task consisted of pressing a key on the keyboard as fast as possible when a target appeared randomly in the visual scene. The target was a picture of a round yellow smiling face (in the web version) (see Fig. 2
                        ). Some of the targets were randomly replaced by a distractor: a yellow (in the web version) sad face (see Fig. 2). Participants were asked to ignore the distractors. This task was continuous since its unfolding was not dependent on participant's answer. The task did not stop even if the participant did not answer. No feedback was given on his/her response.

The stimulus (target or distractor) remained on the screen for 800 ms, and then disappeared during an inter onset time interval that varied randomly from 1000 ms to 2250 ms by steps of 250 ms. This time scale was chosen according to previous studies showing that a secondary task must be portionable in 1–2 s chunks (Wierwille, 1993; Baumann et al., 2004). Each stimulus had a random position inside the primary task window.

The background color of both the monitor and the primary task window was set to a gray with a luminance of 160 cd/m2. This task was programmed using Processing,
                           2
                        
                        
                           2
                           
                              http://processing.org/.
                         also used to collect the gaze data
                           3
                        
                        
                           3
                           Thanks to the PEEP library for Processing http://text20.net/node/14.
                         and the secondary task data via UDP.
                           4
                        
                        
                           4
                           Universal Datagram Protocol.
                         The gaze data was sampled at a rate of 50 Hz.

The secondary task was a navigation inside a mock up of a real ICIS. The structure of the menu, as well as the visual elements was identical to the menus available in the ICIS (see Figs. 3 and 4
                        
                        ).

Participant navigated inside the menu using the dedicated keys on the keyboard (see Fig. 1): scroll up/down, scroll right/left, select an item, go back to the previous menu and replay the instructions. Apart from the Home screen and the “New destination” sub-menu that were scrollable both horizontally and vertically, all the sub-menus were made of vertical lists sorted alphabetically (vertical scrolling only). This task was programmed using Max/MSP v.5.1.7
                           5
                        
                        
                           5
                           
                              http://cycling74.com.
                         also used to control the experiment unfolding.

Implemented by Andrea Cera, composer and sound designer associated to the related studies (Langlois et al., 2010; Misdariis et al., 2011), the sonification had three functions (see Fig. 4 and Table 1
                        ):
                           
                              1.
                              Indicate to which menu an item belongs to,

Indicate the level of depth of the item,

Indicate the scrolling inside lists of items.

A video example of the sonified menu is available at http://petra.univ-tlse2.fr/spip.php?article105.

@&#EXPERIMENTAL DESIGN@&#

The experimental plan was a mixed plan with one between subject factor (sound condition of the secondary task: sonified or silent) and two within subject factors: (1) item length (short, medium or long, see Table 2
                        ) and (2) absence or presence of the secondary task (single versus dual task condition).

The dependent variables collected follow:
                           
                              -
                              Primary task: reaction time to detect the visual target;

Secondary task:
                                    
                                       •
                                       Time to reach the requested item by the participant;

Number of browsed items before the participant found the requested item;

Gaze data:
                                    
                                       •
                                       Time spent in each area of the two areas of interest T1 and T2 (see Fig. 1), respectively the primary task window and the secondary task window;

Number of gaze saccades, i.e., number of times the eyes moved to the secondary task window.

@&#PROCEDURE@&#

This was a dual task experiment in which the participant was first asked to do the primary task alone (target detection, see Section 2.3), and after 1 min to find an item in the ICIS (secondary task, see Section 2.4) while doing the primary task simultaneously (see Fig. 5
                           ). The participant was instructed to give maximum priority to the primary task (complete instructions provided in Appendix). The experiment was made of three blocks of four trials: three items to search in the ICIS and one city name to type. The three blocks were presented in a balanced random order between participants.

The items to be searched by the participants were chosen to get different path lengths inside the structure of the hierarchical menu, i.e., the minimum number of browsed items before arriving at the searched item. The nine chosen items were grouped in short, medium or long categories in accordance with their path length (see Table 2).

In each block, participants were also asked to type a city name in the New Destination sub-menu. The 3 cities (Lyon, Genève, Paris) were different in path length: 22, 31 and 39 keystrokes, respectively.

The training phase was performed in two parts: (1) a complete presentation of the ICIS was given to the participant, followed by a few minutes of free use of the ICIS mock up (sonified or not, depending on the group), and (2) the participant ran one training block, with the same structure as an experimental block but with different items. This training block was repeated until the participant was comfortable with the dual task condition and could use the keyboard without looking at his/her fingers (controlled in real time with the eye-tracker). This training phase, added to the setting of the eye-tracker lasted between 15 and 20 min. The total duration of the experiment was approximately 1 h.

@&#RESULTS@&#

The primary task data showed that participants made very few errors of detection in both silent and sonified conditions. No analysis of errors could be performed. The reaction times as a function of the task condition (single versus dual task) are provided in Fig. 6
                        . Data show an increase of the reaction time when the participant is in dual task condition compared to the single task condition. Moreover, this increase is similar in both sound conditions: 629–729 ms in silent condition, 646–772 ms in sonified condition. The data were analyzed using a mixed linear model
                           6
                        
                        
                           6
                           Using the lme function of nlme package in R version 3.1.1.
                         and demonstrated a significant effect of the dual versus single task (F(1,8290) = 1101.32, p < 0.001), but no significant effect of the sound condition (F(1,44) = 3.79, p = 0.0578). In addition, no interaction between sound condition and task was revealed in this analysis: F(1,8290) = 9.551, p = 0.002.

The time to target is the duration from the onset of the first navigation keystroke once the instructions have been played to the time when the participant successfully selected the requested item. As expected, Fig. 7
                            shows an increase of the time to target with item length in the silent condition. The same result is observed in the sonified condition. The data were analyzed using a mixed linear model and showed a significant effect of the item length, F(2,436) = 49.41, p < 0.001. The sound condition is not significant F(1,38) = 3.70 (p = 0.06), and no interaction is apparent (F(2,436) = 0.96, p = 0.38).

Path length is calculated as the number of items that have been browsed by the participant before he/she reached the requested item. It is comparative with the number of keystrokes used in previous works to measure the navigation efficiency (e.g., Jeon et al., 2009). Results are provided in Fig. 8
                           . Just as seen with the results observed for the time to target, the path length increases with item length in both the silent and the sonified conditions. For example in silent condition, an average of 95 items were browsed to reach long items versus 14 in average for short items. The data were analyzed using a generalized linear model
                              7
                           
                           
                              7
                              Using the glm function of nlme package in R version 3.1.1.
                            since path length is not a continuous variable. The analysis revealed a significant effect of the item length, p < 0.001, as well as the sound condition, p < 0.001. The interaction between item length and sound condition is also significant, p < 0.001.

Due to tracking problems that occurred with 6 participants, the analysis of gaze data was performed on 40 participants, 20 participants in each sound condition (silent and sonified).

The two areas of interest analyzed in this section correspond to the window of the primary task (T1) and the window the secondary task (T2). Fig. 9
                            shows the time spent in each area as a function of item length and sound condition. These results demonstrate that for short, medium and long items, the time spent in the primary task window (T1) increases with the sonification as the time spent in the secondary task window decreases. The proportion between the time spent in T1 and the time spent in T2 has been averaged across item length. In the silent condition, participants spent 28% of the time in T2 and 72% in T1, whereas in the silent condition they spent 4% of the time in T2 and 96% in T1.

The data were analyzed using a mixed linear model and showed significant effect of the sound condition on the time spent in T1 (F(1,38) = 12.77, p < 0.001) and the time spent in T2 (F(1,38) = 78.75, p < 0.001). The item length had also a significant effect on the time spent in T1 ((2,910) = 39.51, p < 0.001) and T2 (F(2,910) = 14.35, p < 0.001). No interaction between sound condition and item length was revealed in the analysis.

The number of gaze saccades is calculated as the number of times participant's gaze moved to the secondary task window during the dual task condition. Fig. 10
                            shows the number of gaze saccades as a function of the item length and the sound condition. As expected from the previous result, the number of gaze saccades decreases with sonification for all item lengths. As for the path length, the saccades data were analyzed using a generalized linear model. The analysis showed a significant effect of sound condition (p < 0.001), as well as item length (p < 0.001), but no interaction (p = 0.007).

@&#DISCUSSION@&#

This paper presents a laboratory study that examined the gaze movements of 40 participants requested to perform navigation tasks in a hierarchical menu (secondary task), while prioritizing their attention on a visual detection task (primary task). This primary task was designed to produce a sustained visual attention to sudden visual events, which is one component of the visual attention required when driving. The secondary task was a realistic mock-up of an ICIS available in Renault cars. Two conditions were compared: the ICIS was either silent (with only a visual interface) or sonified (with a visual and auditory interface) according to principles developed in a previous study (Langlois et al., 2010; Misdariis et al., 2011).

The gaze data revealed a very significant impact of the sonification on the time spent in either the primary task or the secondary task. Indeed, in dual task condition, the proportion of time spent looking at the primary task increased from 72% to 96% between the silent and the sonified conditions. Moreover, the number of gaze saccades was also significantly reduced: for example with long items, the analysis revealed 16.6 saccades on average without sonification versus 2.6 saccades on average with sonification. According to recommendations made by Zwahlen et al. (1988), there must be less than four saccades of less than 1.2 s to ensure a sufficient level of safety. Our findings show that the ICIS does not satisfy this requirement when the interface is not sonified, whereas it is fully acceptable when the interface is sonified. In the silent condition, our results agree with Sodhi et al. (2002) in which participants took 21.1 s in average to perform a radio tuning task while driving, with 9 s of off road glance: in our data, medium items were reached in 21.9 s with 9.3 s of off road glance. In other words, we showed that sonification helped participants to keep their eyes focused on the primary task while doing the menu navigation almost exclusively by ear. This result supports the findings of previous works on sonification in dual task situations (Jeon et al., 2009; Sodnik et al., 2008) in which participants were able to perform both tasks efficiently, but nothing was reported on participants' visual behavior. With the support of gaze data, our study demonstrated that sonification helped participants to keep their eyes on the primary task most of the time. Sonification also made the ICIS fully acceptable regarding safety recommendations. Further dual task experiments are required to adapt these results in a more ecologically valid driving environment such as driving simulator or real driving.

On the other hand, both time to target and number of browsed items in the secondary task were not significantly improved by the sonification (see Figs. 7 and 8). This is coherent with our previous study in which the navigation duration was increased with the sonification (Langlois et al., 2010; Misdariis et al., 2011). This is also consistent with findings of Leplâtre and Brewster (2000) in which navigation performances in sonified mobile phone menus were not significantly different from the navigation without sonification. However, this contrasts the benefit of sonification for menu navigation revealed in many works as noted in the introduction. This can be explained by the fact that most of these works did not use a dual-task paradigm, so our results suggest that sonification benefits depend on the presence of another task performed simultaneously. In Jeon et al. (2009) though, sonification improved participants' performance to menu navigation while doing a primary task. The work of Lee and Spence (2008) also revealed sonification benefits when using a phone while driving. However, the instructions given to the participant prior to the experiment were different from the ones we gave in the present work: in both Jeon et al. (2009) and Lee and Spence (2008), the participants were instructed to perform both tasks as fast as possible, whereas we explicitly asked the participants to perform the secondary task while remaining as fast as possible to the primary task. This suggests that a dual task experiment in a driving simulator should consider what task must be prioritized: the driving or the secondary task.

As regards the reaction times to the primary task, results indicated that in single task condition participants took an average of 629 ms to detect the target (see Fig. 6), versus an average of 729 ms in dual task condition when the interface was not sonified. The 629 ms baseline reaction time is consistent with a previous study by Thorpe et al. (1996) who observed mean reaction times between 385 ms and 567 ms in a go/no-go detection task. The slower reaction times we observed could be a result of the random positions of the targets and distractors. In addition, the increased reaction time induced by the secondary task is also similar to previous findings on drivers' attention (Lamble et al., 1999; Strayer et al., 2003; Jeon et al., 2009). According to gaze data (Figs. 9 and 10), in silent condition this increase is due to the time spent looking at the ICIS. However, in the sonified condition the reaction times were also increased (from 646 ms to 772 ms on average, see Fig. 6), but the participants used the ICIS almost exclusively by ear as evidenced by the gaze data. In other words, even if participants were staring at the primary task, reaction times in the primary task were still impaired due to the secondary task that was almost exclusively auditory. This finding is discussed below.

On one hand, this finding contrasts with two of the aforementioned research (namely Jeon et al., 2009; Sodnik et al., 2008) in which the performance to the primary task was improved when the secondary task was sonified. However, our study differs from those in several crucial ways. In Sodnik et al. (2008), the driving performance was measured as the number of unsafe driving behaviors. This differs from our study in which performance was measured with reaction times to sudden events. In Jeon et al. (2009), the primary task (a ball-catching game) did not imply sustained attention as in our primary task since it was possible to anticipate the act of catching. Furthermore, they used a secondary task (finding an item in a long list) that was less complex and less realistic than the interface navigation task employed in this research. As mentioned previously, these findings indicate the importance of future work in a more realistic driving context to more accurately evaluate the benefits of a sonified secondary task depending on the complexity of the driving situation and the secondary task.

On the other hand, this finding is consistent with previous studies in which a focused attention on a phone conversation impaired drivers' performances (Strayer et al., 2003; Strayer and Johnston, 2001). In Strayer et al. (2003), gaze data illustrated that phone conversation reduced drivers' attention to visual signs even if they were visually fixated. This finding is likely related with the inattentional blindness (Neisser and Becklen, 1975; Mack and Rock, 1998; Simons and Chabris, 1999; Most et al., 2005), referring to failures to detect obvious elements of the visual scene due to attentional engagement in another visual task. This phenomenon also exists in the auditory modality as revealed by Dalton and Fraenkel (2012). Our findings suggest that this phenomenon could be extended to a cross modal effect, i.e., a failure to detect unexpected visual stimuli when engaged in an auditory task, even though our study showed a decreased speed in detecting rather than a failure. This could be further investigated by increasing the difficulty in the primary task to observe more detection failures. Future research could also include a real time measurement of workload (using pupil dilatation for example) that could reveal significant differences as suggested in Jeon et al. (2009) as well as Langlois et al. (2010) and Misdariis et al. (2011) in which the driving comfort perceived by the participants in a driving simulator was increased with sonification.

@&#CONCLUSION@&#

This work examined the use of sonification as a way to reduce gaze movements towards the visual display of an in-car infotainment system (ICIS) in order to ensure a greater level of safety. A laboratory experiment was performed in which participants were requested to prioritize a primary task while performing a secondary task simultaneously. The primary task was a visual target detection task on the computer involving a sustained visual attention to sudden events, and the secondary task was a mock up of an ICIS which was either sonified or not. The sonification used a combination of earcons, auditory icons and text to speech. Eye-tracking measurements demonstrated that participants used the sonified interface almost exclusively by ear, while staring at the prioritized primary task (96% of the time on average). Comparatively, when the interface was not sonified, participants spent 72% of the time on average looking at the primary task. On the other hand, reaction times in the primary task as well as menu navigation in the secondary task were not significantly impacted by the sonification. This study has implications in the design of auditory interfaces for dual-task contexts, and suggests the requirements of further investigations to evaluate how these results are contrasted in more realistic driving contexts such as a driving simulator or real driving.

@&#ACKNOWLEDGMENTS@&#

This study was part of the “Findability” research project supported by Renault at Cognitive Ergonomics & HMI department. The authors wish to thank Matt Coler (INCAS3) for his editorial assistance, as well as two anonymous reviewers for their constructive comments on earlier versions of this article.

The quoted text below is an example of the instructions given to the participants before each block, it was presented visually on the screen, the participant had to hit a key to start the block. The text was presented in French and is translated to English by the author.
                           “Your task is to keep your attention on the primary task and to hit the space bar as soon as you see the [picture of the smiley (
                              
                                 Fig. 2
                               
                              left)]. Try to be as fast as you can.
                        
                        
                           After a certain amount of time, a spoken message will indicate the menu item you'll have to find. Go to the requested item while remaining as good as possible in the primary task.
                        
                        
                           Below are the four items that you'll have to find, in random order (click an item to listen to the corresponding spoken message):
                        
                        
                           - Go and find Aurélie Savary in the Address Book
                        
                        
                           - Go and find the restaurant Georges
                        
                        
                           - Go and find the Artist Vanessa Paradis and then the song Divine Idylle
                        
                        
                           - Type in the city LYON in the menu New Destination
                        
                        
                           Hit any key to start.”
                        
                     

@&#REFERENCES@&#

