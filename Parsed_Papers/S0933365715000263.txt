@&#MAIN-TITLE@&#An improved I-FAST system for the diagnosis of Alzheimer's disease from unprocessed electroencephalograms by using robust invariant features

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           New version of I_FAST for blind classification of electroencephalogram tracing.


                        
                        
                           
                           Application to mild cognitive impairment, early Alzheimer's disease and controls.


                        
                        
                           
                           Avoidance of pre-processing and filtering procedure of EEG data.


                        
                        
                           
                           Accuracies in distinguishing between two classes around 98%.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Implicit function as squashing time

Training with input selection and testing

Multi scale ranked organizing maps

Electroencephalogram

Alzheimer's disease

Mild cognitive impairment

@&#ABSTRACT@&#


               
               
                  Objective
                  This paper proposes a new, complex algorithm for the blind classification of the original electroencephalogram (EEG) tracing of each subject, without any preliminary pre-processing. The medical need in this field is to reach an early differential diagnosis between subjects affected by mild cognitive impairment (MCI), early Alzheimer's disease (AD) and the healthy elderly (CTR) using only the recording and the analysis of few minutes of their EEG.
               
               
                  Methods and material
                  This study analyzed the EEGs of 272 subjects, recorded at Rome's Neurology Unit of the Policlinico Campus Bio-Medico. The EEG recordings were performed using 19 electrodes, in a 0.3–70Hz bandpass, positioned according to the International 10–20 System. Many powerful learning machines and algorithms have been proposed during the last 20 years to effectively resolve this complex problem, resulting in different and interesting outcomes. Among these algorithms, a new artificial adaptive system, named implicit function as squashing time (I-FAST), is able to diagnose, with high accuracy, a few minutes of the subject's EEG track; whether it manifests an AD, MCI or CTR condition. An updating of this system, carried out by adding a new algorithm, named multi scale ranked organizing maps (MS-ROM), to the I-FAST system, is presented, in order to classify with greater accuracy the unprocessed EEG's of AD, MCI and control subjects.
               
               
                  Results
                  The proposed system has been measured on three independent pattern recognition tasks from unprocessed EEG tracks of a sample of AD subjects, MCI subjects and CTR: (a) AD compared with CTR; (b) AD compared with MCI; (c) CTR compared with MCI. While the values of accuracy of the previous system in distinguishing between AD and MCI were around 92%, the new proposed system reaches values between 94% and 98%. Similarly, the overall accuracy with best artificial neural networks (ANNs) is 98.25% for the distinguishing between CTR and MCI.
               
               
                  Conclusions
                  This new version of I-FAST makes different steps forward: (a) avoidance of pre-processing phase and filtering procedure of EEG data, being the algorithm able to directly process an unprocessed EEG; (b) noise elimination, through the use of a training variant with input selection and testing system, based on naïve Bayes classifier; (c) a more robust classification phase, showing the stability of results on nine well known learning machine algorithms; (d) extraction of spatial invariants of an EEG signal using, in addition to the unsupervised ANN, the principal component analysis and the multi scale entropy, together with the MS-ROM; a more accurate performance in this specific task.
               
            

@&#INTRODUCTION@&#

It is well known that Alzheimer's disease (AD) is a progressive, neurodegenerative disease of the elderly, characterized by memory loss as well by as additional cognitive and behavioral abnormalities [1]. Early AD is associated with pathological changes in the basal forebrain cholinergic system, in the thalamo-cortical system, in the associative parietal-temporal areas, and in the complex of inter-related brain regions formed by entorhinal cortex, hippocampus and amygdala [2]. One of AD's frightening aspects is that the cohort of initial symptoms heralding this condition is preceded for a long time by a pre-symptomatic stage during which the disease hallmarks are already active in destroying synapses and connections. The duration of those pre-symptomatic stages is quite variable, depending on the individual defense properties including the amount of silent synapses for neuroplastic mechanisms and the amount of an adequate “neural reserve” [3].

From this perspective an existing condition of paramount importance is represented by the mild cognitive impairment (MCI) [4,5], which is considered as being a prodromal stage of the aging-related brain degeneration, characterized by a measurable memory impairment (in this case it is defined as amnestic MCI), or associated with deficits in other cognitive domains, but is not overt dementia [4–6]. For instance, the MCI subject is slightly cognitively impaired, but such impairment does not yet impact daily life skills; the overall condition does not fulfill the diagnosis of dementia. The MCI condition is widely represented in the general, aged population, and it evolves into clinical AD among 50–60% of the cases within 5–6 years. Indeed, the rate of AD progression, among the MCI subjects, is several tens of times higher than among the age/sex matched non-MCI elderly population. In fact, after approximately 6 years, 80% of MCI cohort has been documented to progress to dementia [7]. Another condition is the prodromal stage of AD in which the final diagnosis - even in presence of little clinical deficits - is proposed when neuropsychological tests are associated to instrumental signs of neurodegeneration as revealed by cerebrospinal fluid (CSF), blood/flow brain measurements (PET/SPECT) and volumetric magnetic resonance imaging (MRI) markers combined with independent genetic risk factors [8–10].

The literature notes a predictive accuracy of 90%, considering the basic electroencephalogram (EEG) characteristics, and the probability of future decline among the MCI subjects [11–15]. EEG markers have identified AD-prone MCI subjects about 1 year before clinical conversion [16]. Decreased coherence of high-frequency gamma rhythms has been identified as being a highly specific predictive sign of conversion from MCI to initial AD. Moreover, both EEG coherence of fronto-parietal connections and the source strength of the δ (temporal), θ parietal, occipital and temporal) and α
                     1 (central, parietal, occipital, temporal and limbic) activity are significantly more evident in MCI subjects who will convert to AD during the follow-up than in those who will not [16]. More recently, two synchronization measures were found to be particularly helpful in distinguishing MCI subjects from age-matched controls and from AD patients, i.e. the Granger causality (in particular, full-frequency directed transfer function) and the stochastic event synchrony. These measures yielded a leave-one-out classification rate of 83%. The Authors posit that this classification performance could be further improved by adding complementary features from the EEG [17].

The capability to extract useful information from an unprocessed EEG track, by using only mathematical algorithms, is a challenging task. There is an existing medical need to achieve an early, accurate, differential diagnosis between subjects affected by MCI, early AD subjects, and healthy elderly subjects control subjects (CTR), only by recording and analyzing a few minutes of their EEG. Many powerful learning machines and algorithms have been proposed to face this complex problem during the last twenty years, resulting in a range of different outcomes [17].

We have recently created and presented a new artificial adaptive system, implicit function as squashing time (I-FAST),
                        1
                     
                     
                        1
                        Implicit function as squashing time (I-FAST) is an International Patent: Applicant Semeion Research Centre; Inventor M. Buscema; International Patent: Application n. PCT/EP2007/055646 deposited 06-08-2007; European Patent: Application n. 06115223.7 deposited 06-09-2006
                      which classifies a few minutes of an AD, MCI or control subject's EEG track with high accuracy [18–21]. I-FAST system tries to approximate the spatial implicit function of the values of the 19 EEG channels along time. For this reason the name I-FAST is the abbreviation of “Implicit Function as Squashing Time”. We have used EEG tracks in previous research which were filtered by frequency, according to experts’ knowledge, cleaning the source signal from noise by using the free software LORETA [19]. This paper proposes an updating of I-FAST technique, capable of making a blind classification of the original EEG tracing of each subject, without applying any preliminary pre-processing other than I-FAST.

Inclusion and exclusion criteria for a-MCI subjects were based on previous seminal reports [4,5,7]. Summarizing, the inclusion criteria were: (i) objective memory impairment in the neuropsychological evaluation probing cognitive performance in the domains of memory, language, executive function/attention, etc; (ii) normal daily life activities as documented by the history and evidence of independent living; and (iii) clinical dementia rating (CDR; [22]) score of 0.5. The exclusion criteria were: (i) mild dementia of the AD type, as diagnosed by standard protocols including NINCDS-ADRDA [23] and according to the diagnostic statistical manual IV (DSM-IV); (ii) evidence (including MRI procedures) of concomitant cerebral impairment such as fronto-temporal degeneration, cerebrovascular disease as indicated by Hachinski ischemic score (HIS) [24], MRI vascular lesions, and reversible cognitive impairment (including pseudo-depressive dementia); (iii) marked fluctuations in cognitive performance compatible with Lewy body dementia and/or features of mixed cognitive impairment including cerebrovascular disease; (iv) evidence of concomitant extra-pyramidal symptoms; (v) clinical and indirect evidence of depression as revealed by the geriatric depression scale (GDS) [25] scores>14; (vi) other psychiatric diseases, epilepsy, drug addiction, alcohol dependence (as diagnosed by a psychiatric interview) and use of psychoactive drugs including acetylcholinesterase inhibitors or other drugs enhancing brain cognitive functions; and (vii) current or previous uncontrolled or complicated systemic diseases (including diabetes mellitus) or traumatic brain injuries.

Probable AD was diagnosed according to the criteria of NINCDS-ADRDA [23] and the DSM-IV. The recruited AD patients underwent general medical, neurological, neuropsychological, and psychiatric assessments. Patients were rated with a number of standardized diagnostic and severity instruments including mini mental state evaluation (MMSE) [26], clinical dementia rating scale (CDR) [22], GDS, HIS [24], and the instrumental activities of daily living scale (IADL) [27]. Neuroimaging diagnostic procedures (MRI) and complete laboratory analyses were carried out to exclude other causes of progressive or reversible dementias, in order to have a clinically homogenous mild AD group. Exclusion criteria were any evidence of: (i) fronto-temporal dementia [28]; (ii) vascular dementia, diagnosed according to NINDS-AIREN criteria [29]; (iii) extra-pyramidal syndromes; (iv) reversible dementias (including pseudodementia of depression); and (v) Lewy body dementia, according to the criteria in McKeith [30].

Finally, the CTR subjects were recruited mostly from non-consanguineous relatives of AD patients. All CTR subjects underwent physical and neurological examinations as well as cognitive screening (including MMSE). For control subjects the exclusion criteria were: (i) presence of chronic systemic illness (i.e. diabetes mellitus or organ failure); (ii) use of psychoactive drugs; and (iii) history of neurological or psychiatric disease. All CTR subjects had a GDS score lower than 14 (no depression).

EEG data were recorded at Rome's Neurology Unit of Policlinico Campus Bio-Medico from the recruited subjects at resting state (eyes-closed and eyes-open). The EEG recordings were performed (0.3–70 Hz bandpass) from 19 electrodes positioned according to the International 10–20 System (i.e. Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2). The horizontal and vertical electrooculogram (0.3–70 Hz bandpass) was also collected to monitor eye movements. All data were digitized in continuous recording mode (few minutes of EEG; 256 Hz sampling rate). The EEG recordings were performed during the late morning. In order maintain a constant level of vigilance, an experimenter controlled on-line the subject and the EEG traces to avoid sleep onset.

Each EEG was recorded and processed without any filter which eliminated noise and/or other artifacts (see Fig. 1a
                        ).

The structure of I-FAST is composed of different phases (see Fig. 1
                        ):
                           
                              a.
                              
                                 squashing phase: the transformation of the 19 EEG channels of each subject into a vector of features;


                                 noise elimination phase: the dynamic elimination of the noisy features from the vector representing each subject;


                                 classification phase: the intelligent classification, with the support of Machines Learning, of the features of each subject.

The squashing phase is designed to discover the spatial invariants into the EEG time flow. In technical terms: this phase needs to transform a two dimensional matrix of each EEG, where the columns are the channels’ values and the row is the EEG discrete time flow, into a one-dimensional vector. This task can be completed using different algorithms. In this paper, we compare some of the “classic” algorithmic techniques with the new ones, in order to evaluate their different effectiveness.

Among the classic algorithms able to extract invariant features from EEG, we have considered the following:
                              
                                 •
                                 
                                    Principal component analysis (PCA): PCA is a classic unsupervised multivariate analysis [31]. This paper proposes the innovation of using the PCA “loadings” (eigenvectors) among the channels as a feature's vector of the invariants of each EEG channel. If we have N EEG channels, PCA will generate N
                                    2 features.


                                    Multi scale entropy (MSE): MSE is a recent methodology that aims to distinguish the different degrees of complexity and/or information of a biological signal [32–34].The features vector generated by MSE from an EEG track is a series of measures of its autocorrelation at different time scales. The assumption is that a random signal has, more or less, the same autocorrelation level at every time scale. A deterministic signal presents a decreasing of autocorrelation when increasing the time scale, while a biological signal manifests an increasing autocorrelation when increasing the time scales. If we have N EEG channels and if we scale the source sample P times, then MSE will generate N
                                    ·
                                    P features.


                                    New recirculation neural networks (NRC): NRC is an unsupervised ANN that was proposed in a previous paper about I-FAST [18]. During its learning phase, NRC is able to project the invariant features of each EEG tracing in its weights matrix. This ANN, with other multi layers auto associative perceptrons, has shown excellent performances on pre-filtered and pre-processed EEG signals [18–21]. The NRC will generate N
                                    ·
                                    H
                                    +
                                    N
                                    +
                                    H features (weights and output and hidden bias). When there are N channels and the NRC is trained with H hidden units.

The noise elimination phase operates to select the minimal number of the more informative features for EEG automatic classification. The dynamic input selection TWIST algorithm was used for this phase. It was previously presented, theoretically, in another paper [35], and is often applied in the medical field with excellent results [36–42]. TWIST also splits the whole sample of the EEG vector invariants into two subsamples with a similar probability density function. We have used the two subsamples generated by TWIST both as a training sample and as testing sample (reverse technique) for two independent blind classification tasks. Fig. 2
                           , with reference to a generic dataset, shows the application of TWIST five times, each with a reverse. The following sections provide a short summary and other details about this algorithm.

The classification phase is executed through different types of Learning Machines. We have considered only the most well known algorithms, implemented in many freeware academic software packages,
                              2
                           
                           
                              2
                              We have used WEKA 3.6, 2012 [43] and MatLab 7.0, 2005 [44] implementation of the listed learning machines.
                            in order to allow an easy replication of our research [43,44]. For the reader's convenience, their basic principles are briefly noted below, with different levels of detail, depending on how well they are used and known and their relevance to the specific application, referring to cited literature for further details.
                              
                                 •
                                 
                                    Naïve Bayes 
                                    [45,46]: The Bayesian algorithms, based on Bayes’ theorem, states that the posterior probability P(C
                                    
                                       i
                                    
                                    ∣
                                    F
                                    1, F
                                    2, ⋯, F
                                    
                                       N
                                    ) of a class C
                                    
                                       i
                                    , of a pattern with N features F
                                    
                                       i
                                    , can be calculated on the basis of its prior probability P(C
                                    
                                       i
                                    ), its likelihood P(F
                                    1, F
                                    2, ⋯, F
                                    
                                       N
                                    
                                    ∣
                                    C
                                    
                                       i
                                    ) to have specific features and the general evidence P(F
                                    1, F
                                    2, ⋯, F
                                    
                                       N
                                    ) of features. In particular, the relation is given by:
                                       
                                          (1)
                                          
                                             P
                                             (
                                             
                                                C
                                                i
                                             
                                             ∣
                                             
                                                F
                                                1
                                             
                                             ,
                                             
                                                F
                                                2
                                             
                                             ,
                                             ⋯
                                             ,
                                             
                                                F
                                                N
                                             
                                             )
                                             =
                                             
                                                
                                                   P
                                                   (
                                                   
                                                      F
                                                      1
                                                   
                                                   ,
                                                   
                                                      F
                                                      2
                                                   
                                                   ,
                                                   ⋯
                                                   ,
                                                   
                                                      F
                                                      N
                                                   
                                                   ∣
                                                   
                                                      C
                                                      i
                                                   
                                                   )
                                                   P
                                                   (
                                                   
                                                      C
                                                      i
                                                   
                                                   )
                                                
                                                
                                                   P
                                                   (
                                                   
                                                      F
                                                      1
                                                   
                                                   ,
                                                   
                                                      F
                                                      2
                                                   
                                                   ,
                                                   ⋯
                                                   ,
                                                   
                                                      F
                                                      N
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    With the naïve assumption of statistical independence of features F
                                    
                                       j
                                    , the prior probability can be reduced to:
                                       
                                          (2)
                                          
                                             P
                                             (
                                             
                                                C
                                                i
                                             
                                             ∣
                                             
                                                F
                                                1
                                             
                                             ,
                                             
                                                F
                                                2
                                             
                                             ,
                                             ⋯
                                             ,
                                             
                                                F
                                                N
                                             
                                             )
                                             =
                                             
                                                
                                                   P
                                                   (
                                                   
                                                      C
                                                      i
                                                   
                                                   )
                                                
                                                
                                                   P
                                                   (
                                                   
                                                      F
                                                      1
                                                   
                                                   ,
                                                   
                                                      F
                                                      2
                                                   
                                                   ,
                                                   ⋯
                                                   ,
                                                   
                                                      F
                                                      N
                                                   
                                                   )
                                                
                                             
                                             ·
                                             
                                                ∏
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                N
                                             
                                             P
                                             (
                                             
                                                F
                                                i
                                             
                                             ∣
                                             
                                                C
                                                i
                                             
                                             )
                                          
                                       
                                    
                                 


                                    K nearest neighbor (kNN) 
                                    [47]: kNN is one of the simplest classification algorithms in the literature and is based on the similarity between objects in the dataset. Substantially, a hyper-dimensional space defined by variables and records from the dataset is tessellated into a series of small areas of influence, which surround the hyper-points defined by records. Each new hyper-point added to the hyper-surface falls into a specific area of influence, which is the presumed class to which it belongs. Starting with a training dataset, each element of that test dataset is compared with those in the training dataset. The frequency of the classes of K elements determines the most similar class of membership. The parameter K is generally determined by the characteristics of the datasets, with the greater error due to the noise abatement at the expense of the class correctness criterion. In a strongly linear dataset and, therefore possessing little noise, a K value equal to 1 can be the correct choice. In fact, in a space with a well-defined tessellation, one needs only to assign the classification of the nearest point to the new situation. Increasing the value of K, may result in a classification error if, near the point of correct classification, there were more points belonging to other classes. In this case, the algorithm could create an incorrect classification to the new instance, based on the principle of Major Vote. There is a different course of action for highly non-linear datasets. In this case, the space would be tessellated ending with a series of very small areas of influence and distributed in a manner similar to that of Leopard spots. A new instance might be positioned on the border between two areas, thus making it difficult to assign it to a class with a value of K equal to 1. Increasing the value of K increases the areas of influence involved in the calculation of the attribution of the class, thus allowing the algorithm to consider the high fragmentation of the distribution of records.


                                    Multi layer perceptron (MLP) 
                                    [48]: MLP is an artificial neural network based on the back propagation learning law. It is assumed for each node an 
                                       
                                          x
                                          j
                                          
                                             [
                                             s
                                             ]
                                          
                                       
                                    , in its [s] layer, a transfer function given by:


                                    
                                       
                                          
                                             (3)
                                             
                                                
                                                   x
                                                   j
                                                   
                                                      [
                                                      s
                                                      ]
                                                   
                                                
                                                =
                                                f
                                                
                                                   
                                                      
                                                         
                                                            Net
                                                            j
                                                            
                                                               [
                                                               s
                                                               ]
                                                            
                                                         
                                                      
                                                   
                                                
                                                =
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         f
                                                         
                                                            
                                                               
                                                                  
                                                                     ∑
                                                                     i
                                                                     N
                                                                  
                                                                  
                                                                     w
                                                                     ji
                                                                     
                                                                        [
                                                                        s
                                                                        ]
                                                                     
                                                                  
                                                                  ·
                                                                  
                                                                     x
                                                                     i
                                                                     
                                                                        [
                                                                        s
                                                                        −
                                                                        1
                                                                        ]
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         =
                                                      
                                                   
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         
                                                            1
                                                            
                                                               1
                                                               +
                                                               
                                                                  e
                                                                  
                                                                     −
                                                                     α
                                                                     ·
                                                                     
                                                                        ∑
                                                                        i
                                                                        N
                                                                     
                                                                     
                                                                        w
                                                                        ji
                                                                        
                                                                           [
                                                                           s
                                                                           ]
                                                                        
                                                                     
                                                                     ·
                                                                     
                                                                        x
                                                                        i
                                                                        
                                                                           [
                                                                           s
                                                                           −
                                                                           1
                                                                           ]
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

For each input patter p, the following global error function is evaluated on the output layer, comparing outputs 
                                       
                                          x
                                          
                                             p
                                             ,
                                             i
                                          
                                          
                                             [
                                             out
                                             ]
                                          
                                       
                                     with target values t
                                    
                                       p,i
                                    :


                                    
                                       
                                          (4)
                                          
                                             E
                                             =
                                             
                                                1
                                                2
                                             
                                             ·
                                             
                                                ∑
                                                p
                                                M
                                             
                                             
                                                ∑
                                                i
                                                N
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            t
                                                            
                                                               p
                                                               ,
                                                               i
                                                            
                                                         
                                                         −
                                                         
                                                            x
                                                            
                                                               p
                                                               ,
                                                               i
                                                            
                                                            
                                                               [
                                                               out
                                                               ]
                                                            
                                                         
                                                      
                                                   
                                                
                                                2
                                             
                                          
                                       
                                    
                                 

The gradient descent principle of optimization is applied to reduce the error:


                                    
                                       
                                          (5)
                                          
                                             Δ
                                             
                                                w
                                                ji
                                                
                                                   [
                                                   s
                                                   ]
                                                
                                             
                                             =
                                             −
                                             lcoef
                                             ·
                                             
                                                
                                                   ∂
                                                   E
                                                
                                                
                                                   ∂
                                                   
                                                      w
                                                      ji
                                                      
                                                         [
                                                         s
                                                         ]
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

Defining a local error 
                                       
                                          e
                                          j
                                          
                                             [
                                             s
                                             ]
                                          
                                       
                                     for each node according to:


                                    
                                       
                                          (6)
                                          
                                             
                                                e
                                                j
                                                
                                                   [
                                                   s
                                                   ]
                                                
                                             
                                             =
                                             −
                                             
                                                
                                                   ∂
                                                   E
                                                
                                                
                                                   ∂
                                                   
                                                      Net
                                                      j
                                                      
                                                         [
                                                         s
                                                         ]
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

we have:


                                    
                                       
                                          
                                             (7)
                                             
                                                
                                                   
                                                      ∂
                                                      E
                                                   
                                                   
                                                      ∂
                                                      
                                                         w
                                                         ji
                                                         
                                                            [
                                                            s
                                                            ]
                                                         
                                                      
                                                   
                                                
                                                =
                                                
                                                   
                                                      ∂
                                                      E
                                                   
                                                   
                                                      ∂
                                                      
                                                         Net
                                                         j
                                                         
                                                            [
                                                            s
                                                            ]
                                                         
                                                      
                                                   
                                                
                                                ·
                                                
                                                   
                                                      ∂
                                                      
                                                         Net
                                                         j
                                                         
                                                            [
                                                            s
                                                            ]
                                                         
                                                      
                                                   
                                                   
                                                      ∂
                                                      
                                                         w
                                                         ji
                                                         
                                                            [
                                                            s
                                                            ]
                                                         
                                                      
                                                   
                                                
                                                =
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         −
                                                         
                                                            e
                                                            j
                                                            
                                                               [
                                                               s
                                                               ]
                                                            
                                                         
                                                         ·
                                                         
                                                            ∂
                                                            
                                                               ∂
                                                               
                                                                  w
                                                                  ji
                                                                  
                                                                     [
                                                                     s
                                                                     ]
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  
                                                                     ∑
                                                                     k
                                                                  
                                                                  
                                                                     w
                                                                     jk
                                                                     
                                                                        [
                                                                        s
                                                                        ]
                                                                     
                                                                  
                                                                  ·
                                                                  
                                                                     x
                                                                     k
                                                                     
                                                                        [
                                                                        s
                                                                        −
                                                                        1
                                                                        ]
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         =
                                                      
                                                   
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         −
                                                         
                                                            e
                                                            j
                                                            
                                                               [
                                                               s
                                                               ]
                                                            
                                                         
                                                         ·
                                                         
                                                            x
                                                            i
                                                            
                                                               [
                                                               s
                                                               −
                                                               1
                                                               ]
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

and then:


                                    
                                       
                                          (8)
                                          
                                             Δ
                                             
                                                w
                                                ji
                                                
                                                   [
                                                   s
                                                   ]
                                                
                                             
                                             =
                                             lcoef
                                             ·
                                             
                                                e
                                                j
                                                
                                                   [
                                                   s
                                                   ]
                                                
                                             
                                             ·
                                             
                                                x
                                                i
                                                
                                                   [
                                                   s
                                                   −
                                                   1
                                                   ]
                                                
                                             
                                          
                                       
                                    
                                 

In the output layer:


                                    
                                       
                                          
                                             (9)
                                             
                                                
                                                   e
                                                   j
                                                   
                                                      [
                                                      out
                                                      ]
                                                   
                                                
                                                =
                                                −
                                                
                                                   
                                                      ∂
                                                      E
                                                   
                                                   
                                                      ∂
                                                      
                                                         Net
                                                         j
                                                         
                                                            [
                                                            out
                                                            ]
                                                         
                                                      
                                                   
                                                
                                                =
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         −
                                                         
                                                            
                                                               ∂
                                                               E
                                                            
                                                            
                                                               ∂
                                                               
                                                                  x
                                                                  j
                                                                  
                                                                     [
                                                                     out
                                                                     ]
                                                                  
                                                               
                                                            
                                                         
                                                         ·
                                                         
                                                            
                                                               ∂
                                                               
                                                                  x
                                                                  j
                                                                  
                                                                     [
                                                                     out
                                                                     ]
                                                                  
                                                               
                                                            
                                                            
                                                               ∂
                                                               
                                                                  Net
                                                                  j
                                                                  
                                                                     [
                                                                     out
                                                                     ]
                                                                  
                                                               
                                                            
                                                         
                                                         =
                                                      
                                                   
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         −
                                                         
                                                            f
                                                            ′
                                                         
                                                         
                                                            
                                                               
                                                                  
                                                                     Net
                                                                     j
                                                                     
                                                                        [
                                                                        out
                                                                        ]
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         ·
                                                         
                                                            
                                                               
                                                                  
                                                                     t
                                                                     j
                                                                  
                                                                  −
                                                                  
                                                                     x
                                                                     j
                                                                     
                                                                        [
                                                                        out
                                                                        ]
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         =
                                                      
                                                   
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         α
                                                         ·
                                                         
                                                            x
                                                            j
                                                            
                                                               [
                                                               out
                                                               ]
                                                            
                                                         
                                                         ·
                                                         
                                                            
                                                               
                                                                  1
                                                                  −
                                                                  
                                                                     x
                                                                     j
                                                                     
                                                                        [
                                                                        out
                                                                        ]
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         ·
                                                         
                                                            
                                                               
                                                                  
                                                                     t
                                                                     j
                                                                  
                                                                  −
                                                                  
                                                                     x
                                                                     j
                                                                     
                                                                        [
                                                                        out
                                                                        ]
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

In the hidden layers:


                                    
                                       
                                          
                                             (10)
                                             
                                                
                                                   e
                                                   j
                                                   
                                                      [
                                                      s
                                                      ]
                                                   
                                                
                                                =
                                                −
                                                
                                                   
                                                      ∂
                                                      E
                                                   
                                                   
                                                      ∂
                                                      
                                                         Net
                                                         j
                                                         
                                                            [
                                                            s
                                                            ]
                                                         
                                                      
                                                   
                                                
                                                =
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         −
                                                         
                                                            
                                                               ∂
                                                               E
                                                            
                                                            
                                                               ∂
                                                               
                                                                  x
                                                                  j
                                                                  
                                                                     [
                                                                     s
                                                                     ]
                                                                  
                                                               
                                                            
                                                         
                                                         ·
                                                         
                                                            
                                                               ∂
                                                               
                                                                  x
                                                                  j
                                                                  
                                                                     [
                                                                     s
                                                                     ]
                                                                  
                                                               
                                                            
                                                            
                                                               ∂
                                                               
                                                                  Net
                                                                  j
                                                                  
                                                                     [
                                                                     s
                                                                     ]
                                                                  
                                                               
                                                            
                                                         
                                                         =
                                                      
                                                   
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         −
                                                         
                                                            f
                                                            ′
                                                         
                                                         
                                                            
                                                               
                                                                  
                                                                     Net
                                                                     j
                                                                     
                                                                        [
                                                                        s
                                                                        ]
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         ·
                                                         
                                                            ∑
                                                            k
                                                         
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        ∂
                                                                        E
                                                                     
                                                                     
                                                                        ∂
                                                                        
                                                                           Net
                                                                           j
                                                                           
                                                                              [
                                                                              s
                                                                              +
                                                                              1
                                                                              ]
                                                                           
                                                                        
                                                                     
                                                                  
                                                                  ·
                                                                  
                                                                     
                                                                        ∂
                                                                        
                                                                           Net
                                                                           j
                                                                           
                                                                              [
                                                                              s
                                                                              +
                                                                              1
                                                                              ]
                                                                           
                                                                        
                                                                     
                                                                     
                                                                        ∂
                                                                        
                                                                           x
                                                                           j
                                                                           
                                                                              [
                                                                              s
                                                                              ]
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         =
                                                      
                                                   
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         −
                                                         
                                                            f
                                                            ′
                                                         
                                                         
                                                            
                                                               
                                                                  
                                                                     Net
                                                                     j
                                                                     
                                                                        [
                                                                        s
                                                                        ]
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         ·
                                                         
                                                            ∑
                                                            k
                                                         
                                                         
                                                            
                                                               
                                                                  
                                                                     e
                                                                     k
                                                                     
                                                                        [
                                                                        s
                                                                        +
                                                                        1
                                                                        ]
                                                                     
                                                                  
                                                                  ·
                                                                  
                                                                     w
                                                                     kj
                                                                     
                                                                        [
                                                                        s
                                                                        +
                                                                        1
                                                                        ]
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         =
                                                      
                                                   
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         α
                                                         ·
                                                         
                                                            x
                                                            j
                                                            
                                                               [
                                                               s
                                                               ]
                                                            
                                                         
                                                         ·
                                                         
                                                            
                                                               
                                                                  1
                                                                  −
                                                                  
                                                                     x
                                                                     j
                                                                     
                                                                        [
                                                                        s
                                                                        ]
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         ·
                                                         
                                                            ∑
                                                            k
                                                         
                                                         
                                                            
                                                               
                                                                  
                                                                     e
                                                                     k
                                                                     
                                                                        [
                                                                        s
                                                                        +
                                                                        1
                                                                        ]
                                                                     
                                                                  
                                                                  ·
                                                                  
                                                                     w
                                                                     kj
                                                                     
                                                                        [
                                                                        s
                                                                        +
                                                                        1
                                                                        ]
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

Reading input patterns enough times and applying these adjustments iteratively on each node, the network tends to converge, learning to output the expected target.


                                    Logistic function 
                                    [49]: logistic regression models the relation between a two-values categorical variable y and independent variables X
                                    =(x
                                    1, x
                                    2, ⋯, x
                                    
                                       n
                                    ) through the logistic function. Considering π
                                    =
                                    P(y
                                    =
                                    C
                                    1|X) the probability of outcome of interest for y, conditioned to X, the odds 
                                       O
                                     of y and the logit 
                                       L
                                    , that is natural logarithm of odds, the relation is given by:


                                    
                                       
                                          
                                             (11)
                                             
                                                L
                                                =
                                                ln
                                                
                                                   
                                                      O
                                                   
                                                
                                                =
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         ln
                                                         
                                                            
                                                               
                                                                  
                                                                     π
                                                                     
                                                                        1
                                                                        −
                                                                        π
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         =
                                                      
                                                   
                                                   
                                                      
                                                      
                                                         =
                                                      
                                                      
                                                         α
                                                         +
                                                         
                                                            β
                                                            1
                                                         
                                                         
                                                            x
                                                            1
                                                         
                                                         +
                                                         
                                                            β
                                                            2
                                                         
                                                         
                                                            x
                                                            2
                                                         
                                                         +
                                                         ⋯
                                                         +
                                                         
                                                            β
                                                            n
                                                         
                                                         
                                                            x
                                                            n
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                     more convenient, but equivalent to:
                                       
                                          (12)
                                          
                                             π
                                             =
                                             
                                                
                                                   
                                                      e
                                                      
                                                         α
                                                         +
                                                         
                                                            β
                                                            1
                                                         
                                                         
                                                            x
                                                            1
                                                         
                                                         +
                                                         
                                                            β
                                                            2
                                                         
                                                         
                                                            x
                                                            2
                                                         
                                                         +
                                                         ⋯
                                                         +
                                                         
                                                            β
                                                            n
                                                         
                                                         
                                                            x
                                                            n
                                                         
                                                      
                                                   
                                                
                                                
                                                   1
                                                   +
                                                   
                                                      e
                                                      
                                                         α
                                                         +
                                                         
                                                            β
                                                            1
                                                         
                                                         
                                                            x
                                                            1
                                                         
                                                         +
                                                         
                                                            β
                                                            2
                                                         
                                                         
                                                            x
                                                            2
                                                         
                                                         +
                                                         ⋯
                                                         +
                                                         
                                                            β
                                                            n
                                                         
                                                         
                                                            x
                                                            n
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 


                                    Support vector machine (SVM) 
                                    [50]: A support vector machine (SVM) is a binary classifier that defines the optimum hyperplane separating two different classes by maximizing the distance between the closest training examples and minimizing the error. The hyperplane separates nonlinear transformations of the original X
                                    =(x
                                    1, x
                                    2, ⋯, x
                                    
                                       n
                                    ), through a kernel function K(X
                                    
                                       i
                                    , X
                                    
                                       j
                                    ). Defining the independent variable y
                                    ∈{−1, 1}, on a dataset of N points the problem may be reduced to the following equation:


                                    
                                       
                                          (13)
                                          
                                             
                                                max
                                                α
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                α
                                                i
                                             
                                             −
                                             
                                                1
                                                2
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                y
                                                i
                                             
                                             
                                                y
                                                j
                                             
                                             K
                                             
                                                
                                                   
                                                      
                                                         X
                                                         i
                                                      
                                                      ,
                                                      
                                                         X
                                                         j
                                                      
                                                   
                                                
                                             
                                             
                                                α
                                                i
                                             
                                             
                                                α
                                                j
                                             
                                          
                                       
                                    
                                 

in which


                                    
                                       
                                          (14)
                                          
                                             0
                                             ≤
                                             
                                                α
                                                i
                                             
                                             ≤
                                             C
                                             ;
                                             i
                                             =
                                             1
                                             ;
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                y
                                                i
                                             
                                             
                                                α
                                                i
                                             
                                             =
                                             0
                                          
                                       
                                    
                                 

and where C is constant, K(X
                                    
                                       i
                                    , X
                                    
                                       j
                                    ) is the kernel function, and α
                                    
                                       i
                                     represent Lagrange multipliers solving the optimization problem.


                                    Sequential minimal optimization (SMO) 
                                    [51,52]: With the sequential minimal optimization (SMO) the optimization problem posed by the SVM is solved by an iterative algorithm, through a decomposition into a series of sub-problems.


                                    Parzen classifier 
                                    [53]: Parzen classifier considers for each class C
                                    
                                       i
                                     its priori probability P(C
                                    
                                       i
                                    ) and its own related likelihood estimation P(X)=
                                    P(x
                                    1, x
                                    2, ⋯, x
                                    
                                       n
                                    
                                    ∣
                                    C
                                    
                                       i
                                    ), where X
                                    =(x
                                    1, x
                                    2, ⋯, x
                                    
                                       n
                                    ) is the input pattern. Having these quantities, the input pattern is classified into class C
                                    
                                       i
                                     if:


                                    
                                       
                                          (15)
                                          
                                             P
                                             
                                                
                                                   
                                                      
                                                         C
                                                         i
                                                      
                                                   
                                                
                                             
                                             ·
                                             P
                                             
                                                
                                                   
                                                      X
                                                      ∣
                                                      
                                                         C
                                                         i
                                                      
                                                   
                                                
                                             
                                             >
                                             P
                                             
                                                
                                                   
                                                      
                                                         C
                                                         j
                                                      
                                                   
                                                
                                             
                                             ·
                                             P
                                             
                                                
                                                   
                                                      X
                                                      ∣
                                                      
                                                         C
                                                         j
                                                      
                                                   
                                                
                                             
                                             ,
                                             
                                             ∀
                                             j
                                             ≠
                                             i
                                          
                                       
                                    
                                 

With the Parzen approach, the likelihood estimation is based on the integration of the class probability density function p(X
                                    ∣
                                    C
                                    
                                       i
                                    ) on a volume V around X, which is identified as hypercube of side h (Parzen window):


                                    
                                       
                                          (16)
                                          
                                             P
                                             
                                                
                                                   
                                                      X
                                                      ∣
                                                      
                                                         C
                                                         i
                                                      
                                                   
                                                
                                             
                                             =
                                             
                                                
                                                   
                                                      k
                                                      i
                                                   
                                                   
                                                      
                                                         X
                                                      
                                                   
                                                   /
                                                   
                                                      n
                                                      i
                                                   
                                                   
                                                      
                                                         X
                                                      
                                                   
                                                
                                                V
                                             
                                          
                                       
                                    
                                 

where k
                                    
                                       i
                                    (X) is the number of training samples of class C
                                    
                                       i
                                     falling in the hypercube of volume V around X, whereas n
                                    
                                       i
                                    (X) is the total number of training samples of the same class.

The fraction in Eq. (16) is evaluated as the average amount of the m
                                    
                                       i
                                     kernel functions 
                                       Φ
                                       (
                                       (
                                       X
                                       −
                                       
                                          T
                                          
                                             
                                                i
                                                r
                                             
                                          
                                       
                                       )
                                       /
                                       h
                                       )
                                     centered on the training samples:


                                    
                                       
                                          (17)
                                          
                                             P
                                             
                                                
                                                   
                                                      X
                                                      ∣
                                                      
                                                         C
                                                         i
                                                      
                                                   
                                                
                                             
                                             =
                                             
                                                1
                                                
                                                   
                                                      m
                                                      i
                                                   
                                                
                                             
                                             
                                                ∑
                                                
                                                   r
                                                   =
                                                   1
                                                
                                                
                                                   
                                                      m
                                                      i
                                                   
                                                
                                             
                                             Φ
                                             
                                                
                                                   
                                                      
                                                         
                                                            X
                                                            −
                                                            
                                                               T
                                                               
                                                                  
                                                                     i
                                                                     r
                                                                  
                                                               
                                                            
                                                         
                                                         h
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

where m
                                    
                                       i
                                     is the number of training patterns of class C
                                    
                                       i
                                     and 
                                       
                                          T
                                          
                                             
                                                i
                                                r
                                             
                                          
                                       
                                     the generic r
                                    
                                       th
                                     input pattern of the same class.

The performance of Parzen classifier depends on the chosen kernel function and on the value of the Parzen window width, also called kernel width.


                                    Quadratic discriminant classifier (QDC) [54].


                                    Linear discriminant classifier (LDC) [54].

The similarity of accuracy, obtained by using different classification algorithms, supports the robustness of I-FAST methodology.

This section introduces a new algorithm to detect the invariants from the channels of an EEG track. This algorithm is named multi scale ranked organizing map (MS-ROM), because it is based on the self organizing map (SOM) neural network [55]. MS-ROM is a processing system composed of three steps:
                           
                              a.
                              
                                 Sampling: Each EEG track is sampled many times at different scales, according to the MSE methodology [34], as we have previously demonstrated.


                                 Projection: Each subsample generated through MSE is projected into a two dimensional grid (R rows × C columns), using a SOM, with the same random weights as starting point. After the training phase, each cell of the grid is transformed in a codebook including all the points similar from the point of view of the 19 EEG channels. A decision to produce P scaling processing will result in P subsamples and P trained SOM grids.


                                 Ranking: Each grid generated by SOM is ranked according to its cell frequency, also called point-frequency; the X and Y coordinates of each ranked grid represent the invariant features of this algorithm.

The final length of the input vector for each EEG track depends on the number of scaling processing P of each EEG, and on the number of the cells of each SOM grid (see Fig. 3
                        ). Despite its simplicity, this algorithm has been demonstrated to be very efficient in detecting the complex similarities among channels in an EEG track. The key to its effectiveness is the ranking process of each trained grid which are able to transform a geometrical projection into a topological vector. We outline this difference for many reasons: SOM ANNs are able to design a tessellation of the input space into a two dimensional grid. When two independent SOM, with the same number of rows and columns, are trained with two different samples, starting with the same random weights, each SOM will generate a map of a codebook influenced by the number of records and by the input vector variance. When we rank the cells of each of the two SOM grids according to their frequency, we ignore the specific metric of each SOM and we enhance their topological interrelationship. The Spearman rank correlation coefficient works in a similar way [56].

Each EEG in 5 multi scale samples (see previous description of MSE technique) was divided and trained for 100 epochs with a SOM, using a 10×10 grid in this research. At the end of the training phase each EEG was thus represented by a vector of 1000 features: (100 x-coordinates + 100 y-coordinates)×5 samples. Appendix A provides a practical application of MS-ROM system on a small dataset.

TWIST is an evolutionary algorithm [35] capable of generating two subsets of data with a very similar probability density of distribution and with the minimal number of effective variables for pattern recognition.


                        Fig. 4
                         shows the structure of the TWIST algorithm. The flow chart of its implementation and its evolution, according to the genetic algorithm dynamics [57], is shown below:
                           
                              1
                              
                                 Design and coding of hypotheses and cost function: every individual of the sample population, in the TWIST algorithm, will be defined by two vectors of different length:
                                    
                                       a.
                                       the first one, shows which records (N) have to be stored into the subset A and which ones have to be stored into the subset B;

the second vector shows which inputs (M) have to be used in the two subsets and which ones have to be deleted.


                                 Creation of a population of hypotheses (Individual): all of the genetic population's individuals are randomly initialized with boolean values {0,1}.


                                 Generation Loop: evaluation of fitness of each individual:
                                    
                                       a.
                                       the hypotheses of each individual are first decoded to decide which records have to be added in subset A and which ones in subset B, and which inputs have to be saved and which have to be deleted;

then a classification algorithm (kNN or naïve Bayes) is trained on the subset A and tested on the subset B;

the same classification algorithm is trained on the subset B and tested on the subset A;

the minimal classification rate of the two classification tasks is saved as the fitness of that individual.

Definition of crossover among individuals and generation of offspring.

Other optimizations (random mutation, etc.).

Definition of the new population.

Loop until not converged, else exit.

TWIST algorithm is capable of removing most of the noisy features coded by the squashing phase. This is the main reason enabling a direct analysis of the unprocessed EEG track without any previous preprocessing and cleaning phase. The following results show the consistency of this assumption.

We have shown in previous studies [35–42] that the results obtained with TWIST algorithm surpass those of other splitting strategies (i.e. random distribution and K-Fold Cross Validation) when applied to real medical data and to classic datasets available from the UCI machine learning repository [58].

The reverse strategy used in this algorithm tends to generate two subsets with the same probability density function, which is exactly the gold standard of every random distribution criterion [35]. In addition, when the reverse strategy is applied, two fitness indicators are generated: the pattern recognition accuracy on subset B after the training on subset A, and the pattern recognition accuracy on subset A after the training on subset B. But only the lower accuracy of the two is saved as the best fitness of each individual of the population, instead of an average of the two or the higher of the two. This criterion increases the statistical probability that the two sub-samples are equally balanced during the evolution because of the quasi-logarithmic increase of the optimization process. We have also demonstrated experimentally [35] that when there is no information in a dataset, the behaviors of the TWIST algorithm, the training and testing random splitting and the K-fold cross validation are absolutely equivalent. Therefore, TWIST does not code noise to reach optimal results [40].

Our previous results have demonstrated that the TWIST algorithm is superior to current methods: pairs of subsets with similar probability density functions are generated, without coding noise, according to an optimal strategy that extracts the minimal number of features and the most useful information for pattern classification. TWIST may use any kind of supervised learning machine for the Training & Testing phase. This paper shares the implementation of two very fast machines learning: naïve Bayes and kNN. The software implementing TWIST algorithm with naïve Bayes and kNN is available free for academic research [59].

272 subjects were enrolled in this research. They were divided into three diagnostic groups (see Tables 1
                         and 2
                        ).

We have generated 322 EEG tracks in 272 subjects, because the EEG was recorded with both closed and open eyes in 50 subjects (about 18%), in order to complicate the pattern recognition task (see Table 3
                        ).

We decided to select the first continuous 30,000 points of each EEG for the analysis, without using any filter to remove artifacts and enhance frequencies, because we wanted to face with the so-called unprocessed EEG track.

An automatic classification system (I-FAST) was implemented, capable of performing three independent pattern recognition tasks from an unprocessed EEG track of a sample of AD subjects, MCI subjects and CTR:
                           
                              •
                              AD compared with CTR;

AD compared with MCI;

CTR compared with MCI.

The data analysis was organized in three steps:
                           
                              a.
                              
                                 selection of the best features extraction technique (BFE), using the same EEG data and the same I-FAST methodology;


                                 application of the BFE technique using the reverse protocol;


                                 comparison of the pattern recognition performances of different learning machines, with datasets generated from the BFE, after training on first and prediction on second dataset, with reverse technique.

The classification results were measured using the following cost functions:
                           
                              •
                              Accuracy =(∑ True positive +∑ True negative )/∑ Total population;

Sensitivity =∑ True positive/(∑ True positive +∑ False Negative);

Specificity =∑ True negative/(∑ True negative +∑ False Positive).

The different techniques were initially compared to extract the invariant features from the EEG tracks for the squashing phase (see Section 2.3.1):
                              
                                 •
                                 
                                    the principal component analysis (PCA): the matrix of the eigenvectors represents the features extracted. Considering the 19 EEG channels, PCA generates 361 features;


                                    the multi scale entropy (MSE): 10 scaling processes on each EEG, consequently MSE presents 190 input features (19 channels x 10 subsamples) for each EEG;


                                    the new recirculation neural network (NRC): training of all the NRCs with 19 hidden units, 100 training epochs and a learning coefficient of 0.1 for each EEG. The features extracted then consist of 399 inputs (19 channels × 19 hidden units + 19 output bias + 19 hidden bias);


                                    the multi scale and ranked organizing maps (MS-ROM): using a 10×10 SOM and 5 scaling processes for each EEG, so the features extracted consist of a vector of 1000 components (10 rows × 10 columns × 5 subsamples × 2 coordinates).

In this step:
                              
                                 i.
                                 the methods used for noise elimination and intelligent classification was initially set, in order to compare the performances of the different extraction algorithms in a controlled setting;

the TWIST algorithm was subsequently applied to detect the minimal number of useful features for each technique using naïve Bayes as the classifier;

compared the different datasets generated by TWIST, using naïve Bayes again as the final classifier, and;

lastly, selected the best features extraction (BFE) algorithm for these EEG sample.

In this step, the best features extraction (BFE) algorithm is applied on the EEG tracks and datasets are generated which are suitable for the next classification step.

It was important to understand whether or not the paper's proposed method was capable of correctly classifying two different EEGs of the same subject, with eyes-open and with eyes-closed, in different and independent classification tasks (that is, when the classifier has to distinguish between AD and CTR, or AD and MC, or CTR and MCI).

The classification accuracy of the different learning machines in the three pattern recognition tasks: AD compared with CTR, AD compared with MCI, and CTR compared with MCI were compared using:
                              
                                 a.
                                 the BFE algorithm, that is MS-ROM, as a common dataset generator from EEG data;

two different TWIST implementations with reverse protocol: one with a naïve Bayes classifier and one with a standard kNN classifier (k
                                    =1). The best of the two TWIST implementations for each learning machine was selected.

@&#RESULTS@&#

The results, in terms of the three steps noted above, relate to:
                        
                           a.
                           selection of the best features extraction technique (BFE);

application of the BFE technique using the reverse protocol;

comparison of the pattern recognition performances of different learning machines.

We have used naïve Bayes as general classifier in this phase because of its speed and probabilistic robustness. Tables 4a–4c
                        
                        
                         show the results of the comparison of the different features’ extraction techniques, in which naïve Bayes was the classifier used for this first step. The results in these tables show how the pattern recognition capability of the MS-ROM clearly surpasses the other extraction algorithms outcomes: a 93% accuracy distinguishing AD subjects from CTR subjects; 94% accuracy recognizing the difference between AD subjects and MCI subjects; and 97% accurately distinguishing between CTR subjects and MCI subjects. The MS-ROM algorithm is also the system demonstrating the largest number of effective features extracted from each EEG, both during the squashing phase and in the noise elimination phase.

MS-ROM, the best features extraction (BFE) algorithm, was applied on the EEG tracks, transforming each of them into a vector of 1000 features, according to what has been previously explained (Section 2.4) and exemplified later in Appendix A.

The naïve Bayes classifier, using the features generated by MS-ROM, delineated with reasonable good accuracy different EEGs of the same subject with eyes-open and with eyes-colsed. In all of the experimentations using MS-ROM as feature detector and naïve Bayes as final classifier, we have correctly classified many repetitions of the same subject's EEG with eyes-open and eyes-closed as shown in Table 5
                        , reporting the degree of this pattern recognition task.

The pattern recognition has been performed using nine learning machines: quadratic discriminant classifier (QDC), naïve Bayes (NVB), K-nearest neighbor (KNN), multi layer perceptron (MLP), Parzen classifier (PZC), logistic (LOG), sequential minimal optimization (SMO), support vector machine (SVM), linear discriminant classifier (LDC). The results have been compared and are shown in Tables 6a–6c
                        
                        
                        :


                        Tables 6a–6c merit consideration from the standpoint of pattern recognition:
                           
                              a.
                              Only kNN algorithm obtained its best performances using a TWIST algorithm implemented with a kNN cost function. This is not surprising, given that it is known that lazy classifiers present different mathematical features from the others:
                                    
                                       i.
                                       MLP, logistic, SMO and most of the others follow the theory of function approximation, using hyperplanes to properly segment the input space;

naïve Bayes and Parzen classifiers also represent a probabilistic approximation function of the input space. They are guided by probability theory and, in the case of naïve Bayes, by a surprising assumption of mutual independence of the input variables. The averages and variances of each input variable in each target class represent the results of its classification rule, learned from the training set;

kNN works in a completely different way; kNN does not present a real training phase. It executes a direct comparison of each testing record with all training set records. kNN is an implicit Voronoi tessellation of the input space according to the records of the training set.

kNN, QDC and naïve Bayes achieved the best results in all of the classification tasks. Considering the mathematical differences between these three algorithms, their performances support the robustness of the MS-ROM algorithm, as a general system to extract invariant features from a many signals time flow process. In addition, the other learning machines demonstrated interesting performances. MLP, in fact, presents the best fourth performance: 86% in AD compared with CTR, 87% in AD compared with MCI, and 96% in CTR compared with MCI.

A linear discriminant classifier is shown to always be inadequate for this task: the dataset generated by MS-ROM is not suitable for a linear analysis.

SVM and SMO show oscillating performances. This is linked to the fact that a probabilistic optimization of the global sample into two subsamples (TWIST implemented with naïve Bayes) is not always a good optimization from the kernel trick based approach viewpoint, especially when the sample is not a large one.

Logistic function was demonstrated to be a robust classifier, with good performances in the three tasks. But Logistic function cannot overcome its structural limits.

We have used a multi-dimensional scaling algorithm [60,61] to project each EEG into a two-dimensional map, according to the estimation of the nine algorithms listed in Tables 6a–6c, in order to better highlight this classification ability. Fig. 6a
                         respectively show the maps of AD compared with CTR, AD compared with MCI, and CTR compared with MCI.


                        Tables 4a–4c and Tables 6a–6c document new learning machine-based medical diagnostic tools which merit consideration. The distinction between AD and MCI subject presents the same differential diagnostic difficulties as when distinguishing between AD and CTR subjects. The easiest diagnostic distinction made is between CTR and MCI subjects. Table 7
                         shows the average of accuracies of the three best classifiers, working on data extracted by MS-ROM, where kNN, naïve Bayes, QDC have been used for the three steps.


                        Table 7 shows that the distance between AD EEGs and CTR EEGs is similar to the distance between AD EEGs and MCI EEGs. Moreover: the CTR EEGs are more distant from MCI EEGs than from AD EEGs. If these data are quite representative of the AD and MCI universe, these results pose a new medical problem: why do AD and MCI subjects, who are very connected from a clinical point of view, present such different EEGs from the mathematical viewpoint?

@&#DISCUSSION AND CONCLUSIONS@&#

In the original paper published by our group suggesting using artificial intelligence in medicine [18] – advocating the use of artificial neural networks for an automatic classification of cognitive disability – we introduced, for the first time, the concept that very useful information can be obtained by extracting spatial information content of the resting EEG voltage by ANNs. The spatial content of the EEG voltage was extracted by a novel step-wise procedure named I-FAST. The core of the procedure was that the ANNs did not classify individuals using EEG data as an input. Rather, the data inputs for the classification were the weights of the connections within an ANN trained to generate the recorded EEG data. The results obtained were at that time superior to those obtained with the more advanced currently available nonlinear techniques.

This paper advances I-FAST's mathematical optimization of the complex information embedded in the EEG. These optimization steps are summarized as follows. (a) Bypassing the EEG's data pre-processing phase and filtering procedure. We have shown that the new algorithms do not require the pre-processing of the EEG before applying artificial neural networks. On the contrary, they use the native information, i.e. the unprocessed EEG, very efficiently achieving the final diagnostic objective. The problem of the registration with open or closed eyes is overcome with this improvement. This represents an important practical simplification of the procedure. (b) Noise elimination. This fundamental task has been improved using a variant of the TWIST system, which has successfully used in many studies in the medical field. It is based on a very speedy and robust classifier (Naïve Bayes). (c) Its classification phase is more robust. This paper analyzes the testing of nine different types of learning machines based on a vast array of specific mathematics. It documents the robustness of I-FAST; its consistent very high accuracy rate. This consistency strengthens the generalization of these findings in the real world. kNN, QDC and Naïve Bayes obtained the best results in all of the classification tasks. (d) Extraction of spatial invariants of EEG signal. In addition to NRC, the unsupervised ANN proposed in the original paper, we have also tested two other known algorithms: PCA and MSE. We have, in addition, introduced for the first time a new algorithm, the MS-ROM, and have shown that it currently is the best technique available for this diagnostic task. Improvement in the unsupervised classification performances of PCA has also been noted to emphasis the quality of information generated by MS-ROM.

There is a need to consider learning-machine diagnoses in terms of clinical perspectives. From a clinical point of view, the most remarkable result is the overall improvement of the diagnostic accuracy produced by the above-described methodological innovation. In fact, in comparison with the original paper, the global differential diagnostic accuracy between AD and MCI, with the best ANN, increased from 92% to 98%. This is an impressive jump both from mathematical and clinical points of view. The same is true for differentiating between the controls and MCI groups (overall accuracy with best ANN = 98.25%). These results confirm, again, that the EEG can be considered to be a very important source of diagnostic information. It is both inexpensive and extremely efficient when analyzed with the advanced tools provided by artificial intelligence.

The counterintuitive finding that the differentiation between AD and control subjects is more difficult than both differentiating between AD and MCI and between controls and MCI deserves further theoretical effort. One possible explanation is that the EEG signal could be affected by the oxidative stress status of the brain. This assumption has an experimental basis [62]. A vast body of literature during the last twenty years has documented that the MCI condition is uniquely characterized by a very high oxidative stress state, which stabilizes down when AD develops. This is only an hypothesis, currently, which is open to ongoing debate and research. The method being proposed is quite general; starting from EEG, it could be applied to the analysis of other diseases, such as Parkinson and ALS.

A simple example of MS-ROM's application is provided to allow for an easy replication of our results. We understand that this machine learning processing system could appear to be quite complicated.

We have randomly selected, from our dataset, 10 EEGs of AD subjects and 10 EEGs of MCI subjects. Only the first 1000 points of each EEG were selected, in order to make the example easy to understand for the reader and difficult to process for the MS-ROM algorithm. Fig. A.3 displays the 10 selected EEGs of the AD subjects. Fig. A.4 presents the 10 selected EEGs of MCI subjects.

We set up a SOM with a 3×3 grid and 3 multi scale samples for each EEG. Each SOM was trained for 100 epochs on each subsample. 54 features were saved at the end of the training phase for each EEG: (9 X-coordinates + 9 Y-coordinates)×3 times. The multi scale technique (MSE) has been shown to be very effective to detect the EEG autocorrelation at different time scales. Table A.1
                      shows the output of MS-ROM for the EEG of one AD subject, in terms of X and Y cell grid coordinates and the frequency of points into cells (PF: Points Frequency):

At this point, the ranked values of the SOM coordinate are taken as features of each EEG, each one with 54 features (see Table A.2
                     ).

The dataset generated in Table A.2 is ready to be learned by different types of learning machines, splitting the dataset itself into two random halves (training set and testing set). It becomes evident when applying an exploratory unsupervised technique as PCA to the total dataset, that the classification of AD and MCI subjects, diagnosed by MS-ROM, is quite accurate. Fig. A.1
                      shows the first two components of an unsupervised PCA analysis applied to the MS-ROM dataset, while Fig. A.2
                      shows the Pareto histogram for the first 10 components. It appears that the first two components (about 23% of the explained variance) are sufficient to differentially diagnose AD subjects from MCI subjects.


                     Fig. A.3
                      shows the 10 EEGs of the randomly selected AD subjects, containing only the first 1000 points.

Analogously, Fig. A.4
                      shows the 10 EEGs of the randomly selected MCI subjects.

@&#REFERENCES@&#

