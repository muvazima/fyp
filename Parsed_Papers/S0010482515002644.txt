@&#MAIN-TITLE@&#Improving point correspondence in cephalograms by using a two-stage rectified point transform

@&#HIGHLIGHTS@&#


               Highlight
               
                  
                     
                        
                           
                           A two-stage rectified point correspondence for cephalograms is proposed.


                        
                        
                           
                           Correspondence is improved by constraining the variance of point transformations.


                        
                        
                           
                           Dental landmarks are classified into three categories to improve correspondence.


                        
                        
                           
                           Users can flexibly add and remove landmarks without intensive image pretraining.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Cephalometric landmarking

Cephalogram correspondence

Point transform correspondence

SIFT

Cephalometry

@&#ABSTRACT@&#


               
               
                  Background
                  An improved point correspondence method was developed for automatically detecting two-dimensional cephalometric landmarks. The proposed method uses a two-stage rectified point transform: the global correspondence of interest points between two images and the local correspondence of landmarks.
               
               
                  Method
                  In the first stage, point-to-point matching pairs were established using local corner point features. The matched points on an input image were treated as a set of transformations, with varying directions and magnitudes, from the template image. Similarity of the transformation vectors was achieved through rectification to exclude vectors that deviated widely from the statistical mean. Rectification attempted to remove noise and irrelevant matched points. In the second stage, the point correspondences were fine-tuned within the regional centers of the landmarks, which were classified into three categories—corners, edges, and structural points—and each category was fine-tuned using a different strategy. Correspondence was performed by evaluating the shortest Euclidean distance between the point descriptors of the template and test images.
               
               
                  Results
                  The correspondence results of 20 orthodontic landmarks were compared with those identified by dental professionals on 80 digital cephalograms collected from a dental clinic. The proposed method detected both hard and soft tissue landmarks with mean error distances of 1.63mm, compared with the 2-mm standard reported by previous studies.
               
               
                  Conclusions
                  This study enhanced the point correspondence technique for cephalometric landmarking. Using the proposed method, users can preferentially and flexibly add and remove landmarks on a template before correspondence without intensive image pretraining.
               
            

@&#INTRODUCTION@&#

Cephalograms are medical X-ray images used in dentistry, showing lateral views of the head, and analysis of the prominent landmarks on cephalograms is termed cephalometric analysis. Developed by Broadbent in 1931 [1], cephalometric analysis provides diagnostic information in orthodontic treatment [2,3]. Manually identifying cephalometric landmarks is challenging because of the superimposition of complex bony structures of the head; their readability depends heavily on image quality. Practitioners must repeatedly adjust brightness and contrast levels for observing obscure details, which complicates clinical practice.

Automating cephalometric landmark detection was first explored in 1984 [4], and numerous automation techniques have since been proposed. These techniques can be classified into four categories: image filtering and knowledge-based landmark search, model-based, soft-computing, and hybrid approaches [5]. The image filtering and knowledge-based approach uses a knowledge base for locating landmarks along lines and edges after applying specific filters [6–10]. The model-based approach uses pretrained models for matching landmark patterns [10–15]. In soft computing, artificial intelligence algorithms predict landmark locations [16–22]. The hybrid approach combines two or more of the aforementioned approaches [23–28]. The high precision required in clinical applications is a challenge for these approaches, and most fail to meet the criterion [29] of a ≦2mm mean error for each automatically detected landmark. Although mean error of a ≦0.81mm has been recommended in orthodontic practice and research, most studies published on automatic landmark location considered a ≦2mm error to be successful and a ≦4mm error acceptable [5]. In this paper, we propose an automated correspondence technique for cephalograms to assist dental practitioners. The proposed system reduces the massive professional effort required in preprocessing and model training. The correspondence errors were measured and compared with the aforementioned criterion.

Previous studies have primarily focused on detecting hard tissue, which is the most prominent structure. However, cephalometric analysis is no longer focused only on hard tissues and has begun to consider the esthetics of soft tissue profiles. In 1981, Ricketts [30] proposed the esthetic line (E-line), a soft tissue profile estimator, which is a line drawn from the nose tip to the lower chin for determining the lip profile status. Numerous studies [31–33] have since used E-line for esthetic measurement. A recent study [34] emphasized that the esthetics of E-line soft tissue profiles differ according to racial groups. In summary, soft tissue measurement is now essential and has been widely used in esthetic treatments [35–37]. However, hard and soft tissues exhibit unequal readability on cephalograms, and optimal enhancement algorithms that simultaneously enhance both prominent and hidden tissues are scant. This study performed point correspondence for soft tissue esthetic landmarks seldom reported by previous studies. Practitioners can manually fine-tune any mislocated landmarks on a user interface with minimal effort.

Point correspondence has improved over the years. In the scale-invariant feature transform (SIFT) proposed by Lowe in 2004 [38], the descriptor appears invariant to image scaling, rotation, and transformation and outperforms related algorithms. SIFT extensions, such as principal component analysis SIFT [39], gradient location orientation histogram [40], speeded up robust features (SURF) [41], and Affine SIFT [42], exhibit enhanced performance, small dimensional features, and high robustness to image variations. They detect corner points for identifying the same object in two images in nonmedical environments. In dental applications, cephalometric correspondence detects human anatomic landmarks. Because structures differ according to the patient, the aforementioned methods can fail in landmark correspondences in X-ray images. Moreover, previous studies have focused on the corner point correspondences in images and ignored noncorner point correspondences for clinical applications. To overcome the drawbacks of previous studies, we propose a method to reduce correspondence errors in dental images. Cephalometric landmarks are classified into three categories and correspondence is performed through a rectification process customized for each category to achieve a similar correspondence pattern, thereby increasing the accuracy of landmark detection substantially.

@&#METHODS@&#

Dental cephalograms record structural appearance, which can be similar in shape but exhibit a certain degree of variation. These variations, as viewed on the images, arise from a combination of linear, affine, projective, and polynomial transform functions, and determining the transforms between two images is complicated and sometimes impossible. However, these transforms must be determined for detecting the structural changes between images. In this study, we assumed that a structure on the input image is a result of a transformation from the structure on the template image. The correspondence points in the input images can be obtained by applying a transformation to the points in the template image.

Orthodontic landmarks are structural points of the human skull present in two-dimensional cephalograms and are commonly used in orthodontic diagnosis and treatment planning. In the experiments, 16 common orthodontic landmarks and four soft tissue landmarks—upper and lower lips, nose, and chin points—were used. 
                        Table 1 defines these landmarks.

Cephalometric landmarks and the soft tissue profile are illustrated in 
                        Fig. 1. The landmarks are grouped into three categories, the first being corner points. ANS and PNS are located at the sharp angle point of the nasal bone and the hard palate. Na and Ar are located at the intersection of different bone structures. The proposed procedure reliably detects these four landmarks as corner points. Therefore, the Harris corner detector [43,44] was employed to detect the corner points relevant to these landmarks. The second category of landmarks contains those located on the edges of the image; in other words, on the smooth outline of the structures. Nose, UL, LL, and Chin are located on the outline of the soft tissue, and Go, Me, Gn, and Pog lie on the lower mandibular border. These edge points are located higher than their neighbors. The Canny edge detector [45] was employed for detecting the edge pixels relevant to these landmarks. The third category contains landmarks, termed “structural points,” that are neither corner points nor edge points. In our system, only landmark S, a virtual point at the center of the hypophysial fossa, belongs to this category. The landmarks S and N are critical in building a Nasion–Sella reference line, corresponded on S, in the cephalometric superimposition for growth evaluation [46,47]. The landmark S is estimated using the nearest corner and edge neighborhoods, as shown in 
                        Fig. 2(a). Fig. 2(b) shows the area around the landmark, and Fig. 2(c) labels the nearest relevant neighborhoods on the real image of the hypophysial fossa. In our experiments, both Harris corner and Canny edge detectors were employed for a fine-tuned search of the nearest relevant corner and edge points around the structural point landmarks to estimate the closest transformation, as shown in Fig. 2.

SIFT is a robust technique for detecting and describing local image features [38]. SIFT features are constructed in four steps: scale-space extrema detection, keypoint localization, orientation assignment, and keypoint description. First, scale-space extrema detection identifies potential interest points invariant to scale and orientation by using a difference-of-Gaussians function. Second, keypoint localization selects stable keypoints by determining the true location of extrema, removing low contrast points, and rejecting points with strong edge response. Third, keypoint features are assigned several orientations according to the local image gradient directions. Finally, the keypoint descriptor is represented relative to the main orientation and is therefore invariant to image rotation. In the final step, a set of 8-bin orientation histograms are generated for 4×4 sample regions computed from the samples’ magnitude and orientation. A descriptor is created for each 16×16 region around the keypoint. Accordingly, this transform is invariant to image transformation, scaling, rotation, and partial illumination changes are robust to local geometric distortion.

The first two SIFT steps are complicated and require excessive computation but solve the scaling problem. Medical images are exempt from these time-intensive procedures because they do not undergo considerable scale changes. In this study, only the final two SIFT steps were applied. Corner points, rather than scale-invariant keypoints, were detected using the Harris corner detector [43]. Image gradients in a 20×20 window with the corner point located at its center were used as local features because they recorded directional changes in pixel intensities. The window size was derived from the image samples, such that the resulting accuracy was optimized. The window was subsequently grouped into 4×4 arrays. The orientations of the features in each array were recorded and quantized into 8-bin histograms, termed “orientation-histograms,” which describe the gradient strength in eight equally spaced directions. Each corner point was described using a 128-byte array and assigned a main orientation. The schema for generating the point descriptor is depicted in 
                        Fig. 3.

Point correspondence between the template and test images was conducted by evaluating the shortest Euclidean distance between the point descriptors. Pairwise distances between the interest points of the images were established. Interest points in the template with the shortest Euclidean distance from unique corresponding points in the test image were recorded, thereby establishing several point-to-point relationships. The interest points that initially failed to match their correspondences were processed in the subsequent process by using the next shortest Euclidean distance. Correspondence continued for all points.

The corresponding Harris corner points yield point-to-point pairs between two images. The correspondence is considered the shifting of a point on the template image to a different location on the test image. The transform is formally defined as follows.

For a given set of landmarks [X
                        1, X
                        2, …, X
                        
                           n
                        ] in the template, each landmark 
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                           =
                           
                              [
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   x
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   y
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              ]
                           
                         is moved to a new position X′
                           i
                         in the input image, according to the following transformation equation:
                           
                              (1)
                              
                                 
                                    
                                       X
                                    
                                    
                                       i
                                    
                                    
                                       ′
                                    
                                 
                                 =
                                 
                                    
                                       R
                                    
                                    
                                       i
                                    
                                 
                                 ⁎
                                 
                                    
                                       X
                                    
                                    
                                       i
                                    
                                 
                                 +
                                 
                                    
                                       D
                                    
                                    
                                       i
                                    
                                 
                                 ,
                              
                           
                        where 
                           ⁎
                         denotes matrix multiplication, 
                           
                              
                                 R
                              
                              
                                 i
                              
                           
                           =
                           
                              [
                              
                                 
                                    
                                       
                                          
                                             cos
                                             
                                             
                                                
                                                   θ
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          
                                             −
                                             
                                             sin
                                             
                                             
                                                
                                                   θ
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             sin
                                             
                                             
                                                
                                                   θ
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          
                                             cos
                                             
                                             
                                                
                                                   θ
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              ]
                           
                         is the rotation matrix, and 
                           
                              
                                 D
                              
                              
                                 i
                              
                           
                           =
                           
                              [
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ρ
                                                
                                                
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   ρ
                                                
                                                
                                                   
                                                      
                                                         y
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                              ]
                           
                         is the displacement matrix of the landmark. Thus, 
                           
                              
                                 ρ
                              
                              
                                 i
                              
                           
                           =
                           
                              
                                 
                                    
                                       ρ
                                    
                                    
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       ρ
                                    
                                    
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       2
                                    
                                 
                              
                           
                         is the displacement magnitude of the ith landmark.

Because human skulls are similar, a collection of transforms T may have similar properties, which can be accomplished by constraining the correspondence pattern to include only the similar transformation set on the images. This paper proposes a method to rectify the transformation vector. In each template and test image pair, the proposed rectification process filters transformations that deviate widely from their statistical mean and removes noise and irrelevant point matching during correspondence, thereby controlling the similarity of the transformation vectors between two images.

The degree of rectification is defined as a rectification factor 
                           φ
                         of the standard deviation of the vector distribution. Thus, the similarity of transformation vectors was controllable by using the following Gaussian function:
                           
                              (2)
                              
                                 
                                    
                                       T
                                    
                                    
                                       k
                                       (
                                       θ
                                       ,
                                       ρ
                                       )
                                    
                                 
                                 =
                                 A
                                 
                                    
                                       e
                                    
                                    
                                       −
                                       
                                          (
                                          
                                             
                                                
                                                   
                                                      
                                                         (
                                                         θ
                                                         −
                                                         
                                                            θ
                                                            
                                                               ¯
                                                            
                                                         
                                                         )
                                                      
                                                      2
                                                   
                                                
                                                
                                                   2
                                                   
                                                      
                                                         (
                                                         φ
                                                         
                                                            
                                                               σ
                                                            
                                                            
                                                               θ
                                                            
                                                         
                                                         )
                                                      
                                                      2
                                                   
                                                
                                             
                                             +
                                             
                                                
                                                   
                                                      
                                                         (
                                                         ρ
                                                         −
                                                         
                                                            ρ
                                                            
                                                               ¯
                                                            
                                                         
                                                         )
                                                      
                                                      2
                                                   
                                                
                                                
                                                   2
                                                   
                                                      
                                                         (
                                                         φ
                                                         
                                                            
                                                               σ
                                                            
                                                            
                                                               ρ
                                                            
                                                         
                                                         )
                                                      
                                                      2
                                                   
                                                
                                             
                                          
                                          )
                                       
                                    
                                 
                                 ,
                              
                           
                        where A is the total number of transformation pairs, σ
                        
                           θ
                         and σ
                        
                           ρ
                         are the standard deviations of vector direction θ and magnitude ρ, respectively, and 
                           φ
                         is the rectification factor. Ahead of the calculation, since the two parameters have different units, θ and ρ were normalized to within 0–100.

A small rectification factor causes a high similarity of the transformation pattern. The samples, with various landmark window sizes, show that a large window contains more corresponding points than a small window. For a large window, which contains more than five hundred corresponding pairs, fewer than 0.5 standard deviations of the pairs are sufficient for favorable results. However, for a small window, which may contain less than 10 corresponding pairs, all of the corresponding pairs (3.0 standard deviations) should be used. The number of corresponding pairs used is closely related to the size of the landmark window. The factor ranged from 0.5 to 3.0 and was inversely proportional to the size of the correspondence window of the transformation set being rectified; 
                           φ
                         is given by
                           
                              (3)
                              
                                 φ
                                 =
                                 a
                                 +
                                 b
                                 
                                    (
                                    
                                       1
                                       −
                                       
                                          
                                             window
                                             
                                             size
                                          
                                          
                                             image
                                             
                                             size
                                          
                                       
                                    
                                    )
                                 
                                 ,
                              
                           
                        where a and b are 0.5 and 2.5, respectively, in this study.

According to Eq. (3), higher transform variations are tolerable when point correspondence is performed in a smaller correspondence window. The rectification factor controls the similarity of transformations, which in turn controls the number of correspondence pairs for a particular size of correspondence window. In a large correspondence window, such as a whole image, the number of correspondence pairs is minimized for reducing the variation caused by irrelevant correspondence. However, the number of correspondence pairs should be adequate for representing valid correspondences. In a small landmark window, more correspondence pairs are processed using a larger rectification factor. In the first stage, global correspondence between the template and test images was performed and the landmark regions in the test image were estimated. Because the window contained the whole image, φ equaled 0.5. In the second stage, detailed landmark correspondence was performed within the estimated regions. The size of the regional window should be large enough to contain sufficient local features of the neighbor points that characterize the landmark. The samples showed that a window with width and height one-tenth that of the original image was favorable for all of the landmarks. Therefore, the same window size, one-hundredth of the image size, was used for all landmarks.

For each landmark in the template image, the average transformation vector of the neighboring corresponded points (i.e., points within a five-pixel distance from the landmark) was applied. In the absence of corresponded points within the neighboring region, the nearest corresponded transformation was applied to the landmark.

To determine the k landmarks, 
                           
                              
                                 X
                              
                              
                                 k
                              
                              
                                 ′
                              
                           
                        , in the test image, practitioners manually marked the pretrained landmarks, X
                        
                           k
                        , in the template image. Subsequently, corner points and their orientation histogram descriptors were employed for detecting the relevant corner points in the test image. The procedure was performed in two stages, as follows:
                           
                              
                                 Stage 1: Corner point correspondence between template and test images
                                 
                                    
                                       
                                          Step 1:
                                       
                                       Detect the Harris corner points in the test image.

Generate local point descriptors using the orientation histogram of the neighborhood features.

Correspond the corner points in the test image by matching local point descriptors on the basis of Euclidean distances.

Compute the point transformations T between the two images and find the correspondence mean 
                                             
                                                V
                                                
                                                   ¯
                                                
                                             
                                           and correspondence variance 
                                             
                                                
                                                   V
                                                
                                                
                                                   σ
                                                
                                             
                                           of the transformations.

Rectify all point transformations T
                                          k.

Locate the local windows of each template landmark in the test image.


                                 Stage 2: Corner and/or edge point conversion to landmark points
                                 
                                    
                                       
                                          Step 7:
                                       
                                       For each landmark, generate the orientation histogram descriptors of the interest points detected inside the landmark window:
                                             
                                                
                                                   Interest
                                                   
                                                   points
                                                   
                                                   for
                                                   
                                                   correspondence
                                                   =
                                                   
                                                      {
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     corner
                                                                     
                                                                     points
                                                                  
                                                               
                                                               
                                                                  
                                                                     if
                                                                     
                                                                     landmark
                                                                     
                                                                     belongs
                                                                     
                                                                     to
                                                                     
                                                                     category
                                                                     
                                                                     1
                                                                  
                                                               
                                                            
                                                            
                                                               
                                                                  
                                                                     edge
                                                                     
                                                                     points
                                                                  
                                                               
                                                               
                                                                  
                                                                     if
                                                                     
                                                                     landmark
                                                                     
                                                                     belongs
                                                                     
                                                                     to
                                                                     
                                                                     category
                                                                     
                                                                     2
                                                                  
                                                               
                                                            
                                                            
                                                               
                                                                  
                                                                     corner
                                                                     
                                                                     and
                                                                     
                                                                     edge
                                                                     
                                                                     points
                                                                  
                                                               
                                                               
                                                                  
                                                                     if
                                                                     
                                                                     landmark
                                                                     
                                                                     belongs
                                                                     
                                                                     to
                                                                     
                                                                     category
                                                                     
                                                                     3
                                                                     .
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       

Correspond the points in the windows by determining the shortest Euclidean distances between the orientation histograms of the points.

Compute the point transformations T between the two corresponding landmark windows and determine the correspondence mean and correspondence variance of the transformations.

Rectify all point transformations 
                                             
                                                
                                                   T
                                                
                                                
                                                   k
                                                
                                             
                                           through similarity checking within the windows.

Assign close neighbor transformations to the landmark.

@&#EXPERIMENTAL RESULTS@&#

Eighty digital cephalograms of orthognathism patients were collected during routine orthodontic treatments. The images in this study were all 800×665 in size. One cephalogram was used as the template image. Two dental professionals identified 20 landmarks on the template (Table 1). The landmark positions from the professionals were averaged and set as the ground truth. Error distances between the predicted landmarks and those identified by professionals were measured in millimeters.

Although SIFTs [38] perform strongly in most applications, dental cephalograms reveal superimposed bony structures with a low signal-to-noise ratio, rendering image features less distinct than those in other applications. Point correspondence performed using SIFT is presented in 
                        Fig. 4. The green lines represent transformation lines linking the corresponded interest points in the left and right images. Because of image similarity, the transformation lines of correct correspondences appear similar in direction and magnitude. The random patterns of the transformation lines reveal the unreliability of these methods in X-ray image point correspondences. Thus, they are inappropriate for medical radiological applications. Moreover, these methods cannot generate sufficient interest points on soft tissues for estimating the esthetic profile along the nose, upper lip, lower lip, and chin, as indicated by arrows in Fig. 4. Consequently, the methods do not facilitate the detection of cephalometric landmarks.

Point-to-point correspondences between two diagonal cephalograms behave as point transformations from one image to another. To rectify the transformations, their vector components (angles and magnitudes) were constrained as closely as possible. The similarity of transformations was directly proportional to the rectification factor.

The global transformation rectifications of the correspondences are shown in 
                        Fig. 5. The correspondence window (red-outlined inset, lower right quarter) and the input image are of the same size. Only corresponded point pairs that satisfy the rectification criteria were included. This global step trained the relative positions between the images and estimated the landmark windows in the test image.

In Stage 1, the variance of transformations is constrained by the rectification factor. Although errors in global correspondence can lead to erroneous detection of landmark regions, these errors were constrained within the rectification limit and therefore were not critical. The erroneous landmark regions could be corrected in the following stage; if errors occur in Stage 1, regional landmark windows of appropriate size are devised in Stage 2 to retrieve additional neighboring features that are helpful for regional correspondence and fine tuning the landmark locations. Because the landmarks were estimated using the average transformation vector of the neighboring correspondence points in Stage 1, the average error distance was 4.27mm. After correspondence rectification in the second stage, the average error distance decreased to 1.63mm. 
                        Fig. 6 is the superimposition of the autodetected (green) and manually marked (blue) landmarks; the discrepancy between the green and blue asterisks for the same landmark reveals the error distance. Even with the low contrast of soft tissues, soft tissue landmarks—Nose, UL, and Chin—were detected.

All landmarks were detected in the 80 cephalograms used in this study. The average error distances after the two-stage rectification are reported in 
                        Table 2. The average errors along the x and y axes are also listed.

@&#DISCUSSION@&#

The proposed method successfully detected 20 landmarks with an average error distance of 1.63mm and corresponded soft-tissue landmarks seldom identified using existing approaches. This study also addressed the clinical need for an automated landmarking application to be feasible for doctors changing the landmark set by adding or removing landmarks in the templates according to their preferences. Because clinical doctors are typically not software engineers and do not have time to resolve software problems, setting up training data and updating template models pose difficulties for them. The proposed method without data training should be more suitable to meet their clinical needs. In our previous study [51], a point correspondence method without data training was employed using 20 cephalogram samples. This study extends the previous study by using 80 cephalograms and classifying dental landmarks into three categories before correspondence to further improve the performance.

The method exhibits an enhanced performance compared with all existing approaches for orthodontic landmark detection, except that by Grau et al. [24], Rosalia et al. [52] and Pei et al. [53] (
                        Table 3). The methods proposed by Grau et al., Vuinić et al. [50], Rosalia et al. and Pei et al. entail pretraining for generating mathematical detection models, and their accuracies depend heavily on the amount of data used for training. By contrast, the proposed method does not require intensive pretraining and is more flexible than the computer-assisted cephalogram diagnosis proposed by Banumathi et al. [55]. Moreover, users can add and remove landmarks from the template and set up their preferred analysis without pretraining.

However, the standard deviation of the proposed method is higher. The standard deviations of landmarks S, Ar, Cd and Or were less than 1.00, whereas those of N, A, B, Me, Pog, U1, L1, ANS, PNS, Po, Nose, UL, LL, and Chin were between 1.00 and 2.00, and those of Go and Gn exceeded 2.00. Several factors may account for such wide variation, the first being poor image quality. Landmarks located in regions of low image quality and superimposition of bony structures render feature identification difficult. We tested various image enhancement techniques aimed at improving image quality before correspondence; however, no remarkable improvement was observed. First, dental cephalograms differ from other medical X-ray images; because the skull structure is not preexposed by radiation to estimate the precise dose in advance, the image may suffer from low signal to noise ratio which is caused by varying bone density. Enhancement of the image simultaneously enhances both the tissue and noise intensity to a similar level. Second, images from individual patients may suffer from varying contrast and brightness; however, no enhancement technique can normalize individual images to a standard quality. Third, the pixel intensities of a cephalogram reflect the accumulation of superimposed skull structures. Because these structures have similar bone density, they have similar pixel intensities. Enhancement filters that enhance all pixel intensities simultaneously cannot assist in distinguishing among superimposed structures. Thus, the detection of the landmarks located at the superimposed structures cannot be improved by applying simple enhancement filters. 
                        Fig. 7(a)–(e) present examples of landmarks in low-quality regions: (a) S, (b) Ar and Cd, (c) A and ANS, (d) Or, and (e) UL and LL. These landmarks (shown in green) are accurately detected in the upper panel images but not in the lower panel images.

Image quality varies regionally in cephalograms, meaning that in most cases, some image regions facilitate accurate correspondence, whereas others do not. Therefore, including only those images with perfect overall quality in which most of the landmarks are clearly identifiable was unfeasible.

Second, local feature alterations caused by adjacent structure interruptions hinder accurate detection, as exemplified in Fig. 7(f) and (g). Because the maxillomandibular and vertebral bones are distinct bones in the human body, their relative positions in the image depend on the head posture. In Fig. 7(f), the vertebrae are part of the local features representing Gonion. The local features of the landmark changes as the posture changes; the alteration of bone positions severely affect feature recognition. In Fig. 7(g), the guide rod is part of the features representing N. As the position of the guide shifts, the local features of N are altered, leading to inappropriate correspondences.

Finally, errors can be induced by individual variations. Fig. 7(h) explains this scenario using PNS as an example. Because maxillary third molars are commonly unerupted or impacted in the posterior palate, they affect the local features PNS, obscuring the landmark. The left-side image of Fig. 7(h) shows an impacted upper third molar in the posterior hard palate; the impaction affected the PNS correspondence. The left-side image in Fig. 7(i) presents U1 and L1, which are accurately detected in the image with clear upper and lower incisal crown outlines. However, the landmarks are indistinct when the anterior teeth are crowded, as is the case in the right-side image in Fig. 7(i). Crowded anterior teeth commonly require orthodontic treatment and hence this problem is unavoidable in dental cephalograms as they are primarily employed in orthodontic treatment. In Fig. 7(j), the left-side image defines the Chin landmark (shown in green), and the right-side image demonstrates the discrepancy between the defined Chin (shown in red) and the detected Chin locations. The discrepancy is typically caused by abnormal lip strains in patients with forcibly corrected lip incompetence, which distorts the chin profile and renders it different from that in the template, leading to the detection discrepancy.

Moreover, some landmarks suffer from a combination of these errors. For example, Ar is composed of two independent bones: the mandible and the occipital bone. If the head is lowered, the local features of Ar are interrupted by the C1 vertebra. Moreover, the landmark region of Ar often exhibits inadequate image quality because of the superimposition of the bilateral mandibular rami and the basilar portion of the occipital bone and sometimes even the C1 vertebra.

Some errors can be solved using image quality improvement, apparatus improvement, and manual fine-tuning, respectively. The following imaging advancements and recommendations can improve the accuracy of automatic landmark detection in cephalograms.

For improving image quality,
                           
                              1.
                              Developed advanced methods for detecting skull bone density and dimension to determine appropriate radiation dosage.

In addition to using ear posts for head stabilization, additional stabilizing points are required to position the head with optimal symmetry.

For reducing local feature alteration,
                           
                              1.
                              Patients must raise their heads when cephalograms are captured so that the mandibular rami appear distinct from the vertebrae.

Radiographically translucent guide rods and ear posts must be used so that they do not appear in the cephalograms.

To account for individual variation,

Because such errors vary among individuals and are unpredictable, we recommend human intervention to fine-tune the result after automatic detection.

The aforementioned error was partially caused by bottlenecks in the point correspondence technique and could not be solved using local point features alone. Global features, such as shape and appearance, in addition to point correspondence can be focused on for future development.

@&#CONCLUSIONS@&#

Anatomic structures in the human skull are primarily composed of skeletal and soft tissues, which are similar in healthy individuals. The proposed correspondence method addresses this similarity. The similarity of structures facilitates point-to-point correspondence between images. Moreover, medical images are less susceptible to scale, rotational, and distortional changes.

We used these uniform characteristics of medical images to improve correspondence by using the proposed two-stage rectified transformation. The point-to-point transformations between images were highly rectified. In the first stage, global information was acquired and the relative locations of the landmarks were predicted. In the second stage, correspondences focused only on a small local region to perform a detailed search. In addition, we overcame the limitations associated with SIFT techniques, which correspond only the corner points, and provided a solution for clinical landmark correspondences.

This study excluded computation that solves severe scale, rotational, and distortional problems, and the detection accuracy may differ in images affected by such problems. Future research efforts will be focused on solving existing problems and improving detection stability.

None declared.

@&#REFERENCES@&#

