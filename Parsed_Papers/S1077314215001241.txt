@&#MAIN-TITLE@&#Color constancy by combining low-mid-high level image cues

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We integrate image statistics, regions and scene characteristics.


                        
                        
                           
                           A Bayesian framework is adopted to combine all cues in a principled way.


                        
                        
                           
                           Results of different cues combination are analyzed.


                        
                        
                           
                           We demonstrate that all cues combined together produces the best result.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Color constancy

Low-level image statistics

Intermediate-level regions

High-level scene detection

Bayesian framework

@&#ABSTRACT@&#


               
               
                  In general, computational methods to estimate the color of the light source are based on single, low-level image cues such as pixel values and edges. Only a few methods are proposed exploiting multiple cues for color constancy by incorporating pixel values, edge information and higher-order image statistics. However, expanding color constancy beyond these low-level image statistics (pixels, edges and n-jets) to include high-level cues and integrate all these cues together into a unified framework has not been explored.
                  In this paper, the color of the light source is estimated using (low-level) image statistics, (intermediate-level) regions, and (high-level) scene characteristics. A Bayesian framework is proposed combining the different cues in a principled way.
                  Our experiments show that the proposed algorithm outperforms the original Bayesian method. The mean error is reduced by 33.3% with respect to the original Bayesian method and the median error is reduced by 37.1% on the re-processed version of the Gehler color constancy dataset. Our method outperforms most of the state-of-the-art color constancy algorithms in mean angular error and obtains the highest accuracy in terms of median angular error.
               
            

@&#INTRODUCTION@&#

Color is an important cue in computer vision and imaging related topics, like human–computer interaction [1], color feature extraction [2] and color appearance models [3]. Differences in illumination cause measurements of object colors to be biased towards the color of the light source. Fortunately, humans tend to perceive object colors more or less constant despite large differences in illumination color. Color feature extraction and color appearance models would benefit from a similar color constancy ability.

Typically, computational methods to estimate the color of the light source are based on single, low-level image cues. For instance, pixel values are used in well-established methods like the Grey-World method [4] and the Gamut mapping [5]. However, using only pixel information ignores a considerable amount of information that is available in pixel derivatives. To this end, edge information is exploited in the context of color constancy such as the Grey-Edge method [6]. Hirakawa et al. [7] decompose an input color image into distinct spatial sub-bands, and then model the color statistics separately in each subband. This approach is similar to modeling edge distributions. Only a few methods are proposed exploiting multiple image cues for color constancy. The generalized Gamut mapping [8] incorporates pixel values, edge information and higher-order statistics. But extensions beyond low-level statistics are cumbersome at best.

For mid-level image cues, segmentation is used to improve the accuracy of color constancy algorithms, e.g. [9]. Image segmentation reduces the effect of large, uniformly colored regions. Furthermore, it reduces the computational complexity as the number of regions is typically a fraction of the number of pixels. The exemplar-based method of [10] exploits region information. First, nearest neighbor surfaces for each region are computed. Then, the illuminant is estimated by integrating the neighboring votes. Learning-based approaches are proposed using multiple types of cues by selecting the most appropriate estimation algorithm based on specific criteria, e.g. image statistics [11], scene geometry information [12] or scene information [13]. However, these methods do not scale easily for multiple cues.

Recent advances in automatic scene segmentation and recognition have now reached a level of accuracy to adequately classify semantic cues (e.g. objects and scenes) in images and videos. Color constancy can largely benefit from semantic cues to determine the possible light sources under which an image is recorded. For example, for images of outdoor scenes (e.g. streets, landscapes and forests), the possible light sources are daylight variants. This argument holds also for other concepts (offices, meeting rooms and shops) where a restricted set of indoor lighting is possible. Based on scene classification, the context of these concepts can be derived and consequently the range of possible light sources can be learned. For example, Bianco et al. [13] show that the image statistics of different concepts will result in different “preferred methods”, e.g. the Shades of Grey method [14] will generally result in a better performance when applied to indoor images, while the second-order Grey-Edge is more suited for outdoor images [13].

In this paper, color constancy is computed by integrating (low-level) image statistics, (intermediate-level) regions and (high-level) scene characteristics. A Bayesian framework is adopted to combine all cues in a principled way. More cues can easily be added and no prior color constancy algorithms are assumed. An overview of the proposed system is shown in Fig. 1
                     . First, the input image is decomposed into several components. Low-level components involve pixel-values, edges and higher-order derivatives. Medium-level components are blobs and super-pixels. High-level semantic cues involve concepts (e.g. indoor/outdoor). Then, all these components are fused by the proposed Bayesian framework.

The remainder of this paper is organized as follows. In Section 2, we give a brief overview of the Bayesian approach to color constancy. In Section 3, we describe a Bayesian framework used for multiple visual cues as well as the visual cues used. In Section 4, experimental results are given followed by our conclusion in Section 5.

Most illuminant estimation algorithms are based on simplifying assumptions. The White-Patch algorithm [15] assumes that the maximum response in RGB channels is caused by a perfect white reflectance. The Grey-World algorithm [4] assumes that the average reflectance in a scene is achromatic. Bayesian approaches [16,17], on the other hand, do not explicitly model these assumptions but rather model the variability of the surface reflectances and illuminants as random variables. An illuminant estimate is obtained from the posterior distribution conditioned on the image data. The advantage of such methods is the adaptability to the data. This approach provides a robust and elegant way to combine multiple cues and learn the light sources.

To be precise, let a pixel in a linear RGB image be denoted by 
                        
                           y
                           ,
                        
                      with three color channels: (yr, yg, yb
                     ). Under a Lambertian surface assumption, the relation between the image pixel 
                        
                           y
                           ,
                        
                      the reflectance 
                        
                           x
                           =
                           (
                           
                              x
                              r
                           
                           ,
                           
                              x
                              g
                           
                           ,
                           
                              x
                              b
                           
                           )
                        
                      and the light source 
                        
                           l
                           =
                           (
                           
                              l
                              r
                           
                           ,
                           
                              l
                              g
                           
                           ,
                           
                              l
                              b
                           
                           )
                        
                      is given by

                        
                           (1)
                           
                              
                                 
                                    y
                                    r
                                 
                                 =
                                 
                                    l
                                    r
                                 
                                 
                                    x
                                    r
                                 
                                 
                                 
                                    y
                                    g
                                 
                                 =
                                 
                                    l
                                    g
                                 
                                 
                                    x
                                    g
                                 
                                 
                                 
                                    y
                                    b
                                 
                                 =
                                 
                                    l
                                    b
                                 
                                 
                                    x
                                    b
                                 
                                 ,
                              
                           
                        
                     which can be written in matrix form as

                        
                           (2)
                           
                              
                                 L
                                 =
                                 diag
                                 (
                                 l
                                 )
                                 
                                 y
                                 =
                                 L
                                 x
                                 .
                              
                           
                        
                     If the observed data are:

                        
                           (3)
                           
                              
                                 Y
                                 =
                                 (
                                 y
                                 (
                                 1
                                 )
                                 ,
                                 …
                                 ,
                                 y
                                 (
                                 n
                                 )
                                 )
                                 ,
                              
                           
                        
                     with unknown reflectances:

                        
                           (4)
                           
                              
                                 X
                                 =
                                 (
                                 x
                                 (
                                 1
                                 )
                                 ,
                                 …
                                 ,
                                 x
                                 (
                                 n
                                 )
                                 )
                                 ,
                              
                           
                        
                     the posterior probability for the illuminant is given by

                        
                           (5)
                           
                              
                                 
                                    
                                       
                                          p
                                          (
                                          l
                                          |
                                          Y
                                          )
                                       
                                    
                                    
                                       ∝
                                    
                                    
                                       
                                          p
                                          (
                                          Y
                                          |
                                          l
                                          )
                                          p
                                          (
                                          l
                                          )
                                       
                                    
                                 
                                 
                                    
                                    
                                       =
                                    
                                    
                                       
                                          
                                             ∫
                                             X
                                          
                                          (
                                          
                                             ∏
                                             i
                                          
                                          p
                                          
                                             (
                                             y
                                             
                                                (
                                                i
                                                )
                                             
                                             |
                                             L
                                             ,
                                             x
                                             
                                                (
                                                i
                                                )
                                             
                                             )
                                          
                                          p
                                          
                                             (
                                             X
                                             )
                                          
                                          d
                                          X
                                       
                                    
                                 
                                 
                                    
                                    
                                       =
                                    
                                    
                                       
                                          
                                             |
                                          
                                          
                                             L
                                             
                                                −
                                                1
                                             
                                          
                                          
                                             
                                                |
                                             
                                             n
                                          
                                          p
                                          
                                             (
                                             X
                                             =
                                             
                                                L
                                                
                                                   −
                                                   1
                                                
                                             
                                             Y
                                             )
                                          
                                          p
                                          
                                             (
                                             l
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                  

In [16], the illuminants and the reflectances are assumed to be independent. The prior distribution of reflectances is estimated by an empirical distribution, which improves over a Gaussian prior. For the illuminant prior, instead of assuming to be uniform over a constrained set [16], Gehler et al. [17] propose to use an empirical distribution which is obtained from the illuminants of the training set. We adopt the approach of Gehler to model the illuminant prior. It has the advantage of steering the algorithm towards frequently occurring illuminants.

The goal of Bayesian color constancy is to compute the color of the light source that minimizes the Bayesian risk. The estimated illuminant is selected by computing the posterior probability of the model under all admissible illuminants for any given image.

In general, image features can be divided into low-level features (e.g. corners and edges), medium-level features (e.g. regions) and high-level features (e.g. objects and scene types). In this section, we will discuss different levels of features for color constancy and show how to incorporate them into the Bayesian framework.

Traditionally, pixel values are used as image cues for illumination estimation [4,5,15]. However, these approaches ignore a considerable amount of information that is available in the image derivatives, i.e. correlations between pixels. It is shown that the n-jet describes the derivative structure of an image [18,19]. The n-jet description encompasses pixel values, edges and higher-order structures. In this paper, we will consider the n-jet up to the second order

                           
                              (6)
                              
                                 
                                    J
                                    =
                                    {
                                    f
                                    ,
                                    
                                       f
                                       x
                                    
                                    ,
                                    
                                       f
                                       y
                                    
                                    ,
                                    
                                       f
                                       
                                          x
                                          x
                                       
                                    
                                    ,
                                    
                                       f
                                       
                                          x
                                          y
                                       
                                    
                                    ,
                                    
                                       f
                                       
                                          y
                                          y
                                       
                                    
                                    }
                                    ,
                                 
                              
                           
                        where 
                           
                              f
                              =
                              
                                 R
                                 ,
                                 G
                                 ,
                                 B
                              
                           
                         and the derivatives are computed by a convolution with a Gaussian at the scale of the derivative filter

                           
                              (7)
                              
                                 
                                    f
                                    ⊗
                                    
                                       ∂
                                       
                                          ∂
                                          x
                                       
                                    
                                    
                                       G
                                       σ
                                    
                                    =
                                    
                                       ∂
                                       
                                          ∂
                                          x
                                       
                                    
                                    
                                       (
                                       f
                                       ⊗
                                       
                                          G
                                          σ
                                       
                                       )
                                    
                                    .
                                 
                              
                           
                        
                     

Note that the diagonal model consists of strictly positive elements, but the first and second-order derivatives can contain negative as well as positive values. Instead of using the derivative features, we compute ∇f for the 1-jet and ∇∇f for the 2-jet as follows:

                           
                              (8)
                              
                                 
                                    
                                       
                                          
                                             ∇
                                             f
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   f
                                                   
                                                      x
                                                   
                                                   2
                                                
                                                +
                                                
                                                   f
                                                   
                                                      y
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∇
                                             ∇
                                             f
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      f
                                                      
                                                         x
                                                         x
                                                      
                                                      2
                                                   
                                                   +
                                                   4
                                                   *
                                                   
                                                      f
                                                      
                                                         x
                                                         y
                                                      
                                                   
                                                   +
                                                   
                                                      f
                                                      
                                                         y
                                                         y
                                                      
                                                      2
                                                   
                                                
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        
                     

We use these higher-order image statistics, in terms of n-jet, for the Bayesian color constancy.

According to the Bayesian framework, the posterior probability for the illuminat using pixel information is given by

                           
                              (9)
                              
                                 
                                    p
                                    
                                       (
                                       l
                                       |
                                       
                                          Y
                                          pixel
                                       
                                       )
                                    
                                    ∝
                                    
                                       
                                          |
                                          
                                             L
                                             
                                                −
                                                1
                                             
                                          
                                          |
                                       
                                       n
                                    
                                    p
                                    
                                       (
                                       
                                          X
                                          pixel
                                       
                                       =
                                       
                                          L
                                          
                                             −
                                             1
                                          
                                       
                                       
                                          Y
                                          pixel
                                       
                                       )
                                    
                                    p
                                    
                                       (
                                       l
                                       )
                                    
                                    ,
                                 
                              
                           
                        where the empirical distributions for pixel values 
                           
                              p
                              (
                              
                                 X
                                 pixel
                              
                              )
                           
                         is learned by computing histograms in RGB space using 32 × 32 × 32 bins similar to [17]. The probability of each reflectance is a function of the number of appearances in the learning set. Similar to the posterior probability using pixel information, the posterior probability using n-jets information is written as follows:

                           
                              (10)
                              
                                 
                                    p
                                    
                                       (
                                       l
                                       |
                                       
                                          Y
                                          
                                             n
                                             -jet
                                          
                                       
                                       )
                                    
                                    ∝
                                    
                                       
                                          |
                                          
                                             L
                                             
                                                −
                                                1
                                             
                                          
                                          |
                                       
                                       n
                                    
                                    p
                                    
                                       (
                                       
                                          X
                                          
                                             n
                                             -
                                             jet
                                          
                                       
                                       =
                                       
                                          L
                                          
                                             −
                                             1
                                          
                                       
                                       
                                          Y
                                          
                                             n
                                             -
                                             jet
                                          
                                       
                                       )
                                    
                                    p
                                    
                                       (
                                       l
                                       )
                                    
                                    ,
                                 
                              
                           
                        Given an image, the n-jet information is computed for each channel, and then normalized to [0, 1]. The empirical distributions for the n-jet information 
                           
                              p
                              (
                              
                                 X
                                 
                                    n
                                    -
                                    jet
                                 
                              
                              )
                           
                         is also learned by computing histograms of 32 × 32 × 32 bins.

Rather than using the entire image, segmentation is used to improve the accuracy of color constancy algorithms, e.g. [9]. This preprocessing step leads to improved results. For example, the gray world approach is sensitive to large, uniformly colored surfaces. The same problem also holds for the Bayesian approach. Large regions may considerably skew the reflectance distributions. Segmenting the image reduces the effects of large, uniformly colored regions. By balancing surfaces to a more equal distribution, segmentation will also reduce the complexity of the algorithms as the number of regions is typically only a fraction of the number of pixels.

Many methods are proposed to segment an image in a bottom-up way such as Normalized Cuts (NCuts) [20], the Felzenszwalb and Huttenlocher (FH) [21] and Mean-Shift [22]. Felzenszwalb and Huttenlocher is typically used in high recall settings to create an oversegmentation of superpixels. Mean Shift and Normalized Cuts provide better precision, but often produce artifacts by breaking large uniform regions (e.g. sky) into chunks.

In this paper, we use a hierarchical segmentation method proposed by van de Sande et al. [23]. Firstly, the image is over-segmented into several initial regions. Starting from the initial regions, a greedy algorithm is used to iteratively group the two most similar regions together and calculates the similarities between this new region and its neighbors. The process stops when the whole image becomes a single region. The method uses size and appearance features to measure the similarity, encouraging small regions to merge early and preventing a single region from gobbling up all others one by one.

Images are segmented based on the RGB color space as well as opponent color space, normalized rgb color space, and the hue channel, generating different hierarchical segmentations for each color space. The hierarchical segmentation method uses a graph-based approach to generate the initial segments. Parameter k denotes the minimum size of the segments. Two initial segmentations are used for 
                           
                              k
                              =
                              100
                              ,
                              200
                           
                        . For each image, we generate eight hierarchical segmentation trees of about 15,000 regions. Then, images are corrected with the given light source colors, transferring to the ones that are taken under the white illuminant. The mean color of these regions is computed on the corrected images and normalized to [0, 1] in RGB color space. The posterior probability for the illuminant using mean region color information is given by

                           
                              (11)
                              
                                 
                                    p
                                    
                                       (
                                       l
                                       |
                                       
                                          Y
                                          region
                                       
                                       )
                                    
                                    ∝
                                    
                                       
                                          |
                                          
                                             L
                                             
                                                −
                                                1
                                             
                                          
                                          |
                                       
                                       n
                                    
                                    p
                                    
                                       (
                                       
                                          X
                                          region
                                       
                                       =
                                       
                                          L
                                          
                                             −
                                             1
                                          
                                       
                                       
                                          Y
                                          region
                                       
                                       )
                                    
                                    p
                                    
                                       (
                                       l
                                       )
                                    
                                    ,
                                 
                              
                           
                        Similar to the low-level features, we learn the empirical distributions 
                           
                              p
                              (
                              
                                 X
                                 region
                              
                              )
                           
                         for region values by computing the histogram with the same size.

Finally, high-level image features are considered. Concepts will enforce different priors on the set of possible illuminants. Moreover, the empirical distribution will differ for various concepts. Bianco et al. [13] show that the statistics of different concepts will result in different preferred methods, e.g. the Shades of Grey method will generally result in a better performance when applied to indoor images, while the second-order Grey-Edge is more suited for outdoor images [13]. Lu et al. [12] show that different 3D geometry models (stages) prefer different color constancy methods. As opposed to previous methods, our approach uses high level visual features to model the priors on the set of possible illuminants and the empirical distributions of different concepts.

For indoor and outdoor images, the images are usually different and they are usually taken under different light sources. Furthermore, the empirical distribution of indoor and outdoor concepts is different [17]. In our algorithm, for each category of concepts (indoor and outdoor), an empirical distribution is modeled based on low-level and medium-level cues. The distributions of different illuminant conditions are also learned.

Given the concept in an image, the posterior probability for the illuminant is defined as follows:

                           
                              (12)
                              
                                 
                                    
                                       
                                          
                                             p
                                             (
                                             l
                                             |
                                             
                                                Y
                                                concept
                                             
                                             )
                                          
                                       
                                       
                                          ∝
                                       
                                       
                                          
                                             
                                                p
                                                
                                                   (
                                                   
                                                      P
                                                      
                                                         1
                                                         …
                                                         N
                                                      
                                                   
                                                   |
                                                   l
                                                   ;
                                                   C
                                                   )
                                                
                                                p
                                                
                                                   (
                                                   l
                                                   ;
                                                   C
                                                   )
                                                
                                             
                                             
                                                p
                                                (
                                                
                                                   P
                                                   
                                                      1
                                                      …
                                                      N
                                                   
                                                
                                                ;
                                                C
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   p
                                                   (
                                                   l
                                                   ;
                                                   C
                                                   )
                                                
                                                
                                                   p
                                                   (
                                                   
                                                      P
                                                      
                                                         1
                                                         …
                                                         N
                                                      
                                                   
                                                   ;
                                                   C
                                                   )
                                                
                                             
                                             
                                                ∏
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             p
                                             
                                                (
                                                
                                                   P
                                                   i
                                                
                                                |
                                                l
                                                ;
                                                C
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   p
                                                   (
                                                   l
                                                   ;
                                                   C
                                                   )
                                                
                                                
                                                   p
                                                   (
                                                   
                                                      P
                                                      
                                                         1
                                                         …
                                                         N
                                                      
                                                   
                                                   ;
                                                   C
                                                   )
                                                
                                             
                                             
                                                ∏
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                [
                                                
                                                   
                                                      p
                                                      
                                                         (
                                                         l
                                                         |
                                                         
                                                            P
                                                            i
                                                         
                                                         ;
                                                         C
                                                         )
                                                      
                                                      p
                                                      
                                                         (
                                                         
                                                            P
                                                            i
                                                         
                                                         ;
                                                         C
                                                         )
                                                      
                                                   
                                                   
                                                      p
                                                      (
                                                      l
                                                      ;
                                                      C
                                                      )
                                                   
                                                
                                                ]
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          ∝
                                       
                                       
                                          
                                             
                                                
                                                   ∏
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                p
                                                
                                                   (
                                                   l
                                                   |
                                                   
                                                      P
                                                      i
                                                   
                                                   ;
                                                   C
                                                   )
                                                
                                             
                                             
                                                p
                                                
                                                   
                                                      (
                                                      l
                                                      ;
                                                      C
                                                      )
                                                   
                                                   
                                                      N
                                                      −
                                                      1
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where P
                        1 …N
                         represent the image cues that may contain empirical distribution information for color constancy, and C represents different concept categories (e.g. indoor and outdoor scenes). Pi
                         represents low-level and mid-level features: P
                        pixel, P
                        1-jet, P
                        2-jet and P
                        region. For different low-level and mid-level features, we assume that they are conditionally independent and contain information about the illuminant distribution. We exploit the posterior probability 
                           
                              p
                              (
                              l
                              |
                              
                                 P
                                 i
                              
                              ;
                              C
                              )
                           
                         for the illuminant using different low-level and mid-level features for different concepts. The indoor and outdoor images are usually taken under different illuminant conditions. Therefore, we also learn the illuminant distribution p(l; C) for different concepts.

Besides classifying images as indoor and outdoor, we also consider the 3D geometry (stages) of images. For each category of stages, the stage information is manually labeled. The empirical distribution and the illuminant conditions are modeled in the same way for indoor and outdoor image categories.

@&#EXPERIMENTS@&#

Gehler et al. [17] provide a data set
                         of 568 images including a wide variety of indoor and outdoor scenes. Images are taken with two high quality DSLR cameras (Canon 5D and Canon 1D) with all settings in auto mode and stored in RAW format. Furthermore, Shi and Funt [24] reprocessed the raw data to obtain linear images with a higher dynamic range. Lynch et al. [25] show that although the renderings provided by Shi and Funt [24] are linear, the pixels are a translation away from the actual responses, causing a “washed-out” effect. They re-render the original dataset and provide 482 images taken on the Canon 5D camera. In our experiments, we analyze and evaluate the proposed method on these two datasets.

In order to evaluate the performance of the color constancy algorithms, the angular error between the illuminant color and the estimated color is taken,

                           
                              (13)
                              
                                 
                                    
                                       e
                                       ang
                                    
                                    =
                                    
                                       cos
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       (
                                       
                                          
                                             e
                                             ^
                                          
                                          1
                                       
                                       ·
                                       
                                          
                                             e
                                             ^
                                          
                                          e
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              
                                 e
                                 ^
                              
                              1
                           
                         is the normalized ground truth of the illuminant, while 
                           
                              
                                 e
                                 ^
                              
                              e
                           
                         is the normalized estimation. Mean, median angular errors and the results of 25% worst are taken as performance indicator in our experiment [8,11,12].

In the Bayesian method, the empirical distribution for different cues is learned by the computation of histograms. Thus, the bin size of the histogram may influence the final results. In this section, we analyze how the results change by varying the bin size on the data set reprocessed by Shi and Funt [24]. Fig. 2 shows the results for low-level features (pixel, 1-jet and 2-jet) and medium-level features (region) for different bin sizes (8, 16 and 32). It can be derived the bin size of 32 provides best results. Therefore, in the following experiments, the bin size is set to 32.

In this section, three experiments are conducted. We train and test our Bayesian model, using threefold cross-validation. The empirical distributions are modeled using two of three folds, and the algorithm is tested on the remaining fold. Also, the prior of the illuminants is learned from two of three folds. The whole procedure is repeated three times, leaving out each fold once for testing.

In the first experiment, we evaluate the results for the Bayesian framework using low-level features (pixel, 1-jet and 2-jet) and medium-level features (region), independently. Firstly, the models are trained on both indoor and outdoor images jointly. Secondly, the models are trained on indoor and outdoor images which are labeled manually (ground-truth). For simplification, we assume that the illuminant distribution p(l; C) is uniform over the training dataset.


                        Table 1 shows the results of our experiments on the data set reprocessed by Shi and Funt [24]. The methods using high-order statistical cues and mid-level cues produce higher accuracy than the method using only low-level pixel values. Using indoor/outdoor classification, all methods are improved. The method based on mid-level cues produces the best result, with a mean and median angular error of 3.5 and 2.6°, respectively.


                        Table 2
                         shows the results of our experiments on the data set reprocessed by Lynch et al. [25]. The methods using 2-jet cue and mid-level cue produce better results than the method using low-level pixel values. Using indoor/outdoor classification, all methods are improved. It is also hold true that the method based on mid-level cue produces the best result, with a mean and median angular error of 3.0 and 2.1° respectively.

In the second experiment, we fuse different features using the Bayesian framework. In this experiment, we assume that the illuminant distribution p(l; C) is uniform over the training dataset. We present different combinations for indoor and outdoor scenes, respectively. Table 3
                        
                         shows the results for different combinations on the data set reprocessed by Shi and Funt [24]. The mid part of Table 3 shows the results for indoor scenes. The method combining all features produces the best results, with a mean angular error of 4.6° and median angular error of 3.6°. All combined methods produce better results than the original Bayesian method. The right part of Table 3 shows the results for outdoor scenes. Also, the method combining all visual features produces the best results, with a mean angular error of 2.5° and median angular error of 1.9°. Again, all the combined methods produce better results than the original Bayesian method. From Table 3, it can be derived that the improvement for indoor images is 29.2%, higher than that for outdoor images (28.6%) in mean angular error. Furthermore, Table 3 shows that when cues are combined with middle level cues (region), their performance will increase. The worst result using middle level cues (region) is 3.6° while the best result using the combination of cues is 3.7°. This implies that the middle level cue provides important information for color constancy.


                        Table 4 shows the results for different combinations on the data set reprocessed by Lynch et al. [25]. The middle part of Table 4 presents the results for indoor scenes. The method combining all features produces the best results. The mean angular error is 3.5° and median angular error is 2.8°. All combined methods produce better results than the original Bayesian method. The right part of Table 4 shows the results for outdoor scenes. Again, the method combining all visual features produces the best results with a mean angular error of 2.6° and median angular error of 1.8°. All the combined methods produce better results than the original Bayesian method in mean angular error. The improvement is 19.4% in mean angular error.
                        
                        
                        
                         Furthermore, the middle level cue is the most important cues for color constancy. The worst result using middle level cues is 3.0° while the best result for the combination of cues is 3.3°.

In the third experiment, we classify images into indoor and outdoor automatically instead of manually labeling. The algorithm proposed by [13] is used for automatic concept classification. For each image, color features are extracted based on the YCbCr color space and RGB color space as well as salient edges and texture information. These features are incorporated into a random forest with 50 random trees for indoor and outdoor classification. Table 5 shows the results of the algorithm proposed by [13] on the data set reprocessed by Shi and Funt [24] with the same threefold configuration. Most of the images are correctly classified. Table 6 shows the best combination results for indoor/outdoor images labeled manually and automatically. We combine all the visual features for all categories images. They produce similar results for indoor/outdoor images labeled manually and automatically.

The same method is applied on the data set reprocessed by Lynch et al. [25] (see Table 7). The result for images labeled manually is better than the result for images which are automatically labeled. However, the result for images labeled automatically is still better than the original Bayesian method with an improvement of 11.1% in mean angular error.

In this section, we will analyze different feature combinations (pixel, 1-jet, 2-jet and region) as well as the integration of concept and stage information on the data set reprocessed by Shi and Funt [24]. In this paper, 15 different stages proposed by [26] are divided into six categories (no bkg, box,tiled bkg, corner, person + bkg and others) because some stages are a bias of the dataset. Fig. 3 shows the example images and the corresponding stage models.

Per stage, the posterior probability for the illuminant is given by

                           
                              (14)
                              
                                 
                                    p
                                    
                                       (
                                       l
                                       |
                                       
                                          Y
                                          stage
                                       
                                       )
                                    
                                    ∝
                                    
                                       
                                          
                                             ∏
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          p
                                          
                                             (
                                             l
                                             |
                                             
                                                P
                                                i
                                             
                                             ;
                                             S
                                             )
                                          
                                       
                                       
                                          p
                                          
                                             
                                                (
                                                l
                                                ;
                                                S
                                                )
                                             
                                             
                                                N
                                                −
                                                1
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        Similar to concepts, Pi
                         represents low-level and mid-level features: P
                        pixel, P
                        1-jet, P
                        2-jet and P
                        region, S represents different stage categories.

Here, we use manual annotation of stage images [27]. Fig. 4 shows the results for different combinations per stage. Table 8
                        
                        
                         shows the best combination of cues for each stage, respectively. We fuse all the best combination of cues for each stage to obtain the final results (see Table 9). The results show that this approach produces similar median angular errors differentiated by indoor and outdoor concepts. The method has a higher mean angular error. Still, the result is better than the original Bayesian method with a mean angular error of 3.6° and a median angular error of 2.6°. Furthermore, we combine the posterior probability using stage information with indoor/outdoor information (see Table 9). The mean angular error using manually labeled concepts is slightly improved from 3.39 to 3.35°. Using automatic classification proposed by [26], the results are worse than those using indoor and outdoor concepts. Still, it is better than the original Bayesian method in both mean and median angular error.

In
                         this section, we compare our method with different state-of-the-art algorithms on the data set reprocessed by Shi and Funt [24]. Five standard methods are selected: Grey-World [4], White-Patch [15], general Grey World [14]
                        
                           
                              (
                              p
                              =
                              6
                              ,
                              σ
                              =
                              0
                              )
                              ,
                           
                         1st order Grey-Edge [6]
                        
                           
                              (
                              p
                              =
                              4
                              ,
                              σ
                              =
                              1
                              )
                           
                         and 2nd order Grey-Edge [6]
                        
                           
                              (
                              p
                              =
                              4
                              ,
                              σ
                              =
                              7
                              )
                           
                        . For gamut-based methods, both pixel-based Gamut [5]
                        
                           
                              (
                              σ
                              =
                              5
                              )
                           
                         and edge-based Gamut [8]
                        
                           
                              (
                              σ
                              =
                              7
                              )
                           
                         are used. The other algorithms are the Bayesian [17] (the baseline); the Natural Image statistics [11]; the high level visual information with the combination of the bottom-up and top-down information [28]; the spatio-spectral statistics [7] and exemplar-based color constancy method [10]. For our method, besides a uniformly illuminant prior, we evaluate a general prior over all illuminants using a Gaussian distribution.


                        Table 10 shows the comparison with state-of-the-art methods on the data set reprocessed by Shi and Funt [24]. The mean error is reduced by 57.9% with respect to the White Patch method [15] (worst result) and 33.3% with respect to the original Bayesian method [17] (our baseline). The median error is reduced by 65.1% with respect to the Gray World method [4] (worst result) and 37.1% with respect to the original Bayesian method [17]. Our result in mean angular error is slightly worse than the best result (the spatio-spectral statistics [7] and exemplar-based color constancy method [10]) with 3.2 to 3.1. Our method (median angular error) is slightly better than state-of-the-art methods (the spatio-spectral statistics [7] and the exemplar-based color constancy method [10]) with 2.2 to 2.3. Fig. 5 and Fig. 6 show a number of visual results for indoor and outdoor images, respectively.

@&#CONCLUSION@&#

In this paper, a method has been proposed to obtain the color of the light source by using (low-level) image statistics, (intermediate-level) regions and (high-level) scenes which are integrated in a Bayesian framework. The proposed algorithm outperforms most of state-of-the-art color constancy algorithms based on the dataset of Shi and Funt [24]. Furthermore, from the experimental results, it can be derived that Bayesian color constancy is improved by fusing different cues on two datasets i.e. Shi and Funt [24] and Lynch et al. [25]. The proposed Bayesian framework indoor/outdoor classification further improves color constancy. For future work, we plan to incorporate other high level information to improve the color constancy results (like objects [29,30]).

@&#REFERENCES@&#

