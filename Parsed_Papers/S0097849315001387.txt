@&#MAIN-TITLE@&#A discriminative approach to perspective shape from shading in uncalibrated illumination

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We estimate shape and reflectance map of an unknown object from a single image.


                        
                        
                           
                           The object is assumed to have uniform diffuse albedo.


                        
                        
                           
                           The model works in uncalibrated illumination with a perspective projection model.


                        
                        
                           
                           We leverage a novel large scale dataset in a discriminative learning approach.


                        
                        
                           
                           Training uses synthetic data rendered given the estimated lighting.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Shape from shading

Discriminative prediction

Silhouette cues

Synthetic training data

@&#ABSTRACT@&#


               
               
                  Estimating surface normals from a single image alone is a challenging problem. Previous work made various simplifications and focused on special cases, such as having directional lighting, known reflectance maps, etc. This is problematic, however, as shape from shading becomes impractical outside the lab. We argue that addressing more realistic settings requires multiple shading cues to be combined as well as generalized to natural illumination. However, this requires coping with an increased complexity of the approach and more parameters to be adjusted. Starting from a novel large-scale dataset for training and analysis, we pursue a discriminative learning approach to shape from shading. Regression forests enable efficient pixel-independent prediction and fast learning. The regression trees are adapted to predicting surface normals by using von Mises–Fisher distributions in the leaves. Spatial regularity of the normals is achieved through a combination of spatial features, including texton as well as novel silhouette features. The proposed silhouette features leverage the occluding contours of the surface and yield scale-invariant context. Their benefits include computational efficiency and good generalization to unseen data. Importantly, they allow estimating the reflectance map robustly, thus addressing the uncalibrated setting. Our method can also be extended to handle perspective projection. Experiments show that our discriminative approach outperforms the state of the art on various synthetic and real-world datasets.
               
            

@&#INTRODUCTION@&#

Shape from shading – the problem of estimating surface normals from just a single image – is a heavily ill-posed problem. For this reason many simplifying assumptions have been made, such as assuming smooth surfaces, uniform albedo, a known reflectance map, or even light coming from a single directional light source in known direction. Such strong assumptions strongly limit the applicability in practice, however. Outside of controlled lab settings, less restrictive assumptions are needed. In this paper, we estimate the surface of a diffuse object with uniform albedo together with its reflectance map in uncontrolled illumination, given only a single image (Fig. 1
                     ). To recover fine surface detail, our goal is to avoid strong spatial regularization. To that end, we generalize shading cues to more realistic lighting, as well as combine them owing to their complementary strengths. While this affects the model and computational complexity, and leads to an increased number of parameters, we show how to address these challenges with a discriminative learning approach to shape from shading.

A key property of our approach is that it allows to combine several shading cues. We consider (1) the color of the pixel itself, which is a strong cue in hued illumination [2], and is often exploited by using a second order approximation of Lambertian shading [3]. Our experiments (Section 9.1) show, however, that the cue becomes less reliable in the presence of correlated color channels (e.g. in near white light) or noise. We aid disambiguation by adding (2) local context 
                     [4], which to date has been limited to the case of directional lighting. We capture the local appearance context using a texton filter bank [5], instead of using the colors in the neighborhood directly. Through cue combination in our learning framework, we achieve automatic adaptation to uncontrolled lighting and reconstruct fine surface detail. Finally, we introduce novel (3) silhouette features. While the use of silhouette information in shape from shading dates back to foundational work by Ikeuchi, Horn, and Koenderink [6,7], previous work has only constrained surface normals at the occluding contour and employed global reasoning to propagate the information to the interior [8]. We show how to generalize the occluding contour constraint to the surface interior, which yields (spatial) contour information at every pixel that is furthermore invariant to the local scale of the object. These silhouette features are applicable to both orthographic and perspective cameras. Moreover, our novel silhouette features also give a coarse estimate of the surface by themselves, which allows us to estimate the unknown reflectance map.

A number of challenges arise in discriminative learning for uncalibrated shape from shading: First, we require a training database of surfaces captured in the same conditions as the object to be reconstructed. It seems infeasible to capture all possible combinations of surfaces and lighting conditions, and inserting known reference objects in the scene [9] is typically impractical. For this reason, some learning approaches [10,11] create databases on-the-fly by rendering synthetic shapes once the lighting condition is known. Our approach adopts this strategy, but relies on a significantly larger database of 3D models than previous work in order to capture the variation of realistic surfaces. Second, and in contrast to [10,11], we cope with unknown illumination at test time. To that end we estimate the reflectance map from our silhouette features, and train the discriminative approach once the reflectance has been estimated. Third, (re-)training for the specific lighting condition at test time requires efficient learning and inference. Enabled by the diverse cues discussed above, we adopt regression forests for efficient pixel-independent surface normal prediction by storing von Mises–Fisher distributions in the leaves. Finally, an optional refinement step enforces integrability of the predicted surface normals. Fig. 2
                      depicts the entire pipeline. Note that this work is based on a previous conference publication [12], which we generalize here to the perspective case. Moreover, we provide additional detail and illustrations.

After introducing our approach, we assess the contribution of the different cues using a statistical evaluation and as components of our pipeline. Moreover, we evaluate our method both qualitatively and quantitatively on synthetic data as well as a novel real-world dataset, where it outperforms several state-of-the art algorithms.

@&#RELATED WORK@&#

As shape from shading has an extensive literature, we only review the most relevant, recent work here and refer the reader to [13,14]. Lambertian shape from shading has historically assumed a single white point light source, presuming this to simplify the problem. Recently, it became apparent [15,2], however, that chromatic illumination not only resembles real-world environments more closely, but also yields additional constraints on shape estimation, thus substantially increasing accuracy. Nevertheless, these methods focus on the case of favorable illumination and do not address nearly monochromatic lighting. Moreover, assuming the illumination to be known limits their practical applicability.

Recent work also aimed to infer material properties or illumination alongside the shape. Oxholm and Nishino [8] exploit orientation cues present in the lighting environment to estimate the object׳s bidirectional reflectance function (BRDF) together with its shape. They require a high-quality environment map to be captured, however. Barron and Malik [16,17] integrate shape estimation into the decomposition of a single image into its intrinsic components. Training and inference in their generative model take significant time; moreover, extending the model with additional cues is not necessarily straightforward. Since their formulation requires strong regularity assumptions, the amount of fine surface detail recovered is quite limited.

Only a comparably small fraction of work addresses the more general perspective projection case [18–20]. Recent methods dealing with more complex lighting [2,15–17] are often limited to the simpler orthographic projection case.

While learning approaches to shape from shading have been investigated and often outperform their hand-tuned counterparts, they have been limited by simple shape priors and the lack of adequate training data. Relying on range images or synthetic data [11,21,22] can be problematic: while the noise of range images is a limiting factor in predicting fine-grained surface variations, synthetic datasets often fail to capture real-world environments with their variability. Khan et al. [11], for example, used synthetic data and a database of laser scans to train a Gaussian mixture model on the isophotes. Barron and Malik [16] trained their shape model on one half of the MIT intrinsic image dataset [23]. Example-based methods [10,24] have also shown reasonable qualitative results, but their quantitative performance remains unclear.

Hertzmann and Seitz [9] used objects of known geometry imaged under the same illumination to perform a photometric stereo reconstruction. Multiple images need to be captured, each of which contains a known reference object. Our approach, in contrast, only requires a single image of an unknown object and uses “example geometry” only to synthesize our training data.

Our approach relates to Geodesic Forests [25], as both use a regression tree-based predictor. Both enable pixel-independent predictions by incorporating spatial information directly into the tree-based approach. Kontschieder et al. [25] address discrete labeling tasks, such as semantic segmentation, have a complex entanglement, and use generalized geodesic distances as spatial features. We instead predict the normal direction, i.e. a two-dimensional continuous variable, employ newly proposed silhouette features, and rely on only a single stage.

@&#OVERVIEW@&#


                     Fig. 2 shows an overview of our discriminative approach. Given a test image of a diffuse object with uniform albedo, taken under orthographic or perspective projection, we begin by extracting color, textons and the proposed silhouette features (Section 6). Our silhouette features additionally enable us to estimate the reflectance map (Section 7), with which we render patches of objects from our database of example geometries in turn (Section 4). The training set is obtained as the surface normal of the central pixel of each synthetic patch; the features are the same as for testing. After training the regression forest (Section 5), it allows predicting surface normals independently for each pixel of the test image from the extracted features. Optionally, we enforce integrability of the normal field. The prediction can be adapted to the perspective projection case by rotating each normal according to its position in the image (Section 8).

High-quality data for training models of surface variation has been scarce. The situation has somewhat improved with the advent of low-cost depth sensors, but range images are typically too noisy to exhibit and allow learning fine-grained structures. Synthetic data have been generated as an alternative, often resembling simple geometric shapes like cylinders or blobs [2,10,21,22]. However, the underlying parametric models often do not capture real-world surface variations, like self-occlusions or fine detail, such as in wrinkles of clothing.

We instead leverage a dataset of shapes from artists [26], which yields the advantages of both range maps and synthetic data: being created by modeling experts, the shapes resemble real-world objects with parts of varying size and complex phenomena, e.g. self-occlusions. Moreover, rendering many 3D models in different orientations allows to obtain very large training sets. The 3D models cover a range of categories, mainly with an organic shape, such as humans and animals (Fig. 3
                     ).

Consisting of 100 objects, our dataset is much larger and more varied than those considered in other learning approaches to shape from shading. For example, only 6 realistic surfaces of the same object class (faces) were used in [11], and 10 objects obtained by taking half of the MIT intrinsic image dataset were used for training (the other half for testing) by [17]. Although the dataset on which we train is qualitatively rather different from any of the test datasets, we obtain state-of-the-art performance across a variety of settings (Section 9).

Building on the success of decision and regression tree-based methods in various applications, including human pose estimation [27], image restoration [28], semantic labeling [25] and others, we here use regression forests for discriminative shape from shading. For now we only outline the basic learning approach; the features that serve as input will be discussed later. Regression forests are very useful for our purposes as both learning and prediction are computationally efficient. This is crucial, since learning and prediction are carried out at test time once the reflectance map has been estimated (Fig. 2). Training the different trees of the forest can proceed in parallel. As the prediction of surface normals of objects is done independently for each pixel, this step is efficient as well as parallelizable. Since the predicted normal field is not necessarily integrable, we optionally enforce integrability in a post-processing step.


                     Basic regression forest model: Regression forests average the output of several regression trees to improve robustness. Each tree yields a prediction of an output variable by traversing a path that depends on the input features [29]. A split criterion at each (non-leaf) node guides the traversal into either the left or right branch, until a leaf node is reached. As is most common, thresholds on the input features are used as split criteria. Each leaf node stores a probability distribution over the output variable, which ultimately enables the prediction.


                     Normal vectors as output: Predicting normal vectors using regression trees incurs some additional challenges. First, the output variable is continuous, which is typically achieved by storing the average output of all training samples that fall in that particular leaf. This is equivalent to storing the mean of a multivariate Gaussian in each leaf, which is a reasonable assumption when the posterior is sufficiently close to a Gaussian distribution in 
                        
                           
                              R
                           
                           
                              d
                           
                        
                     . Second, surface normals are actually distributed on a 3-dimensional unit hemisphere, which means that a Gaussian assumption does not appear appropriate. In our approach, we address this by modeling the output distribution in each leaf as a von Mises–Fisher distribution [30]; we store the mean and dispersion parameters.

The von Mises–Fisher distribution models unit vectors on a d-dimensional hypersphere. For our case of d=3, the probability density function of a normal 
                        n
                        ∈
                        
                           
                              S
                           
                           
                              2
                           
                        
                      is given as
                        
                           (1)
                           
                              p
                              (
                              n
                              ;
                              μ
                              ,
                              κ
                              )
                              =
                              
                                 
                                    κ
                                 
                                 
                                    2
                                    π
                                    (
                                    
                                       
                                          e
                                       
                                       
                                          κ
                                       
                                    
                                    −
                                    
                                       
                                          e
                                       
                                       
                                          −
                                          κ
                                       
                                    
                                    )
                                 
                              
                              exp
                              (
                              κ
                              
                                 
                                    μ
                                 
                                 
                                    T
                                 
                              
                              n
                              )
                              ,
                           
                        
                     where 
                        μ
                        ,
                        ∥
                        μ
                        ∥
                        =
                        1
                      is the mean vector and 
                        κ
                        ∈
                        R
                      the dispersion (analogous to the precision of a Gaussian).


                     Learning: For the most part, learning proceeds as usual for regression forests. To train each tree of the forest, a randomly chosen 90% subset of the training data is used. The split criterion for each node is chosen from a random subset of features. In particular, we choose the feature that minimizes the aggregated entropy of the new child nodes compared to the entropy of their parent node.

We estimate the von Mises–Fisher parameters using the approach of Dhillon and Sra [31]. They show that the maximum likelihood estimate can be approximated well by
                        
                           (2)
                           
                              
                                 
                                    μ
                                 
                                 
                                    ^
                                 
                              
                              =
                              
                                 
                                    r
                                 
                                 
                                    ∥
                                    r
                                    ∥
                                 
                              
                              
                              and
                              
                              
                                 
                                    κ
                                 
                                 
                                    ^
                                 
                              
                              =
                              
                                 
                                    3
                                    
                                       
                                          R
                                       
                                       
                                          ¯
                                       
                                    
                                    −
                                    
                                       
                                          
                                             
                                                R
                                             
                                             
                                                ¯
                                             
                                          
                                       
                                       
                                          3
                                       
                                    
                                 
                                 
                                    1
                                    −
                                    
                                       
                                          
                                             
                                                R
                                             
                                             
                                                ¯
                                             
                                          
                                       
                                       
                                          2
                                       
                                    
                                 
                              
                              .
                           
                        
                     Here, 
                        r
                        =
                        
                           
                              ∑
                           
                           
                              i
                              =
                              1
                           
                           
                              N
                           
                        
                        
                           
                              n
                           
                           
                              i
                           
                        
                      is the resultant vector and 
                        
                           
                              R
                           
                           
                              ¯
                           
                        
                        =
                        
                           
                              ∥
                              r
                              ∥
                           
                           
                              N
                           
                        
                      is the average resultant length.

Most previous learning approaches to shape from shading require re-training the model on each unseen reflectance map [10,11]; our approach does so as well. To carry this out efficiently, we randomly sample 5×5 normal patches from our geometry dataset (Section 4), precompute the silhouette features ahead of time, as they are independent from the lighting condition, and store them alongside the normals. When a new illumination condition occurs, we render training images from the normals, compute the remaining features, and train the forests. Rendering and feature extraction are efficient and take below one second for the entire dataset; training the forest takes approximately 90s, and inference below a second. It is important to note that unlike previous learning approaches [10,11], we do not require the lighting at test time to be known, but instead estimate it as well (Section 7).

For each of the 100 models in our training dataset, we obtain 10 training images by placing an orthographic camera looking at the model center from random positions. We evaluated how many training patches are needed on a validation set (different from the models used at test time) and found that 100–200 samples per image yield the best trade-off between performance and computational effort for training.


                     Integrability: The regression forests independently predict the surface normals of each pixel, without considering neighboring predictions. In the absence of any spatial regularization, the surface predictions are susceptible to image noise; on the other hand penalizing discontinuities usually results in oversmoothed surfaces and a loss of detail. For this reason we fuse pixel-independent predictions only by enforcing integrability, as this is a necessity for obtaining a valid surface. Integrability requires the derivatives of the surface normal to fulfill
                        
                           (3)
                           
                              
                                 
                                    
                                       
                                          ∂
                                       
                                       
                                          2
                                       
                                    
                                    z
                                 
                                 
                                    ∂
                                    u
                                    ∂
                                    v
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          ∂
                                       
                                       
                                          2
                                       
                                    
                                    z
                                 
                                 
                                    ∂
                                    v
                                    ∂
                                    u
                                 
                              
                              ,
                           
                        
                     where the depth z depends on the image coordinates 
                        u
                        ,
                        v
                     . Several approaches have been proposed to penalize violations of Eq. (3), e.g. 
                     [32,33]; we consider and evaluate different choices in Section 9.2.

Shape from shading, like other pixel labeling/prediction problems, benefits from taking the spatial regularity of the output into account, in other words modeling the expected smoothness of the recovered surface. That is, neighboring predictions should account for the fact that their normals are often very similar. Regression forests [29], which we use here, perform pixelwise independent predictions and thus do not necessarily model such regularities well. The recent regression tree fields [28] address this by estimating the parameters of a Gaussian random field instead of the output variables; using maximum a-posteriori (MAP) estimation in the resulting conditional random field yields the final prediction. Unfortunately, the MAP estimation step creates a computational overhead, which also renders training inefficient. Our method is inspired by geodesic forests [25] instead, which include a geodesic distance feature to circumvent explicit modeling of more global dependencies of the output; we introduce spatial features that enable encouraging spatial consistency despite pixel-independent prediction.


                     Basic color feature: Regression trees excel when the desired output strongly correlates with the input features [29], because this allows for splits that reduce the entropy well. It is in turn a fundamental assumption of shape from shading that strong correlations between surface normals and color exist, as their relation can be described by the rendering equation. The common Lambertian case is well described by a second order approximation [3]: 
                        
                           
                              I
                           
                           
                              c
                           
                        
                        =
                        
                           
                              
                                 
                                    n
                                 
                                 
                                    ^
                                 
                              
                           
                           
                              T
                           
                        
                        
                           
                              M
                           
                           
                              c
                           
                        
                        
                           
                              n
                           
                           
                              ^
                           
                        
                      at each point on the surface, where I
                     
                        c
                      is the intensity of color channel c, 
                        
                           
                              n
                           
                           
                              ^
                           
                        
                      is the surface normal in homogeneous coordinates, and 
                        
                           
                              M
                           
                           
                              c
                           
                        
                      is a symmetric 4×4 matrix representing the reflectance map for that color channel. A single input image thus puts three nonlinear constraints onto the two unknowns of the surface normal at each pixel. Under ideal circumstances, the reflectance maps of the individual color channels are independent from each other. In that case, they produce small isophotes (areas with the same luminance), which turns the shape from shading problem into photometric stereo such that a surface can be recovered very well with just the color [2]. However, if the reflectance maps and corresponding constraints are more correlated, e.g. in nearly white light, large isophotes cause many surface patches to explain the same color. Moreover, image noise weakens the correlation significantly. Hence, to avoid making strong assumptions about the type of lighting present, we not only consider the color, but also look for spatial features that depend on a neighborhood of pixels as well as the object contour, and are able to reduce the remaining ambiguity, even in the absence of an explicit spatial model.


                     Texton features: To capture how the local variation of the input image correlates with the output, we first compute features from a texton filter bank [5]. The filter bank contains Gaussians, their derivatives, as well as Laplacians at multiple scales, and has been used in many areas, such as material classification, segmentation, and recognition. While having been used in shape from texture [34], to our knowledge they have not been considered in shape from shading. Before filtering, we convert the image to the L⁎a⁎b⁎ opponent color space. Gaussian filters are computed on all channels, while the remaining filters are applied only to the luminance channel.

As we will see below, texton features provide local context that strongly boosts accuracy compared to using color alone. Embedding them in a discriminative learning framework allows for adaptation to various types of surface discontinuities instead of simply assuming smoothness as has been common in shape from shading. Magnifying the local context by enlarging the filters can lead to better adaptation to various surface types and also faster convergence to an integrable surface later. It, however, requires a much larger dataset to capture fine detail and achieve similar generalization. In our experiments, we used filters that match the normal patches in size (5×5).

Projected onto the image plane, normals are not distributed equally across the object. Most objects are roughly convex, or composed of convex parts. Thus, normals at the center of an object tend to face the viewer and normals at the occlusion boundary are perpendicular to the viewing direction [6,7]. Consequently, the probability of a normal facing a certain direction given its position within the projection is non-uniform. Previous work has exploited this fact only by placing priors on the normals at the occlusion boundary and propagating information to the interior with a smoothness prior [8]. As both priors do not consider scale, balancing them can be challenging. This becomes especially problematic if different scales are present within the same object (e.g. the tail vs. head of the dinosaur in Fig. 4
                        ). Here, we consider a more explicit relation between the silhouette and the normal, which automatically adapts to scale.

To that end, let us first examine the correlation between a point׳s surface orientation and its position within the object׳s projection onto the image plane. Consider the object in Fig. 4. As expected [6,7] and can be seen in the visualization of the out-of-plane component (d) (white – toward the viewer, black – away), normals are orthogonal to the viewing direction starting at the silhouette. Moving inwards, the normals change until they finally face the viewer. If we now look further at the distance of an interior point to the silhouette (b), we can see some apparent correlation. Similarly, we can see apparent correlation between the direction to the nearest point on the silhouette (c) and the image-plane component of the normal (e). We now formalize and analyze this relationship.

We define the absolute distance of an interior point 
                           p
                         to the contour as
                           
                              (4)
                              
                                 
                                    
                                       d
                                    
                                    
                                       abs
                                    
                                 
                                 (
                                 p
                                 )
                                 =
                                 
                                    
                                       
                                          min
                                       
                                       
                                          b
                                          ∈
                                          B
                                       
                                    
                                 
                                 
                                 ∥
                                 p
                                 −
                                 b
                                 ∥
                                 ,
                              
                           
                        where B denotes the set of points on the occlusion boundary. The absolute distance, however, depends on the scale of the object. Normalizing it by the length of the shortest line segment that passes through 
                           p
                         and connects boundary and the medial axis of the object makes it scale-invariant. The medial axis is the set of all points that have two closest points on the boundary. If M denotes the medial axis and 
                           
                              
                                 pb
                              
                              
                                 ¯
                              
                           
                         the (infinite) line that passes through 
                           p
                         and 
                           b
                        , we define the relative distance to the silhouette as
                           
                              (5)
                              
                                 
                                    
                                       d
                                    
                                    
                                       rel
                                    
                                 
                                 (
                                 p
                                 )
                                 =
                                 
                                    
                                       
                                          min
                                       
                                       
                                          b
                                          ∈
                                          B
                                       
                                    
                                 
                                 
                                 
                                    
                                       
                                          min
                                       
                                       
                                          m
                                          ∈
                                          M
                                          ∩
                                          
                                             
                                                pb
                                             
                                             
                                                ¯
                                             
                                          
                                       
                                    
                                 
                                 
                                 
                                    
                                       ∥
                                       p
                                       −
                                       b
                                       ∥
                                    
                                    
                                       ∥
                                       m
                                       −
                                       b
                                       ∥
                                    
                                 
                                 ,
                              
                           
                        
                        i.e. the relative distance is normalized by the minimal line that passes through 
                           p
                         and connects medial axis and contour. In practice, we approximate Eq. (5) using two distance transforms, d
                        
                           B
                         for the contour set and d
                        
                           M
                         for the medial axis. We thus define the scale-invariant boundary distance
                           
                              (6)
                              
                                 
                                    
                                       d
                                    
                                    
                                       rel
                                    
                                    
                                       ′
                                    
                                 
                                 (
                                 p
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             d
                                          
                                          
                                             B
                                          
                                       
                                       (
                                       p
                                       )
                                    
                                    
                                       
                                          
                                             d
                                          
                                          
                                             B
                                          
                                       
                                       (
                                       p
                                       )
                                       +
                                       
                                          
                                             d
                                          
                                          
                                             M
                                          
                                       
                                       (
                                       p
                                       )
                                    
                                 
                                 .
                              
                           
                        
                     

Finally, we define the direction to the contour as
                           
                              (7)
                              
                                 β
                                 (
                                 p
                                 )
                                 =
                                 −
                                 
                                    
                                       ∇
                                       
                                          
                                             d
                                          
                                          
                                             rel
                                          
                                          
                                             ′
                                          
                                       
                                       (
                                       p
                                       )
                                    
                                    
                                       ∥
                                       ∇
                                       
                                          
                                             d
                                          
                                          
                                             rel
                                          
                                          
                                             ′
                                          
                                       
                                       (
                                       p
                                       )
                                       ∥
                                    
                                 
                                 .
                              
                           
                        
                     


                        Statistical analysis: We analyze the correlation of our silhouette features and surface orientation on three different datasets as shown in Fig. 5
                        : synthetic data (“blobby shapes”) [2] in the first column, real world data from the MIT intrinsic image dataset [23] in the second column, and the set of artist-crafted 3D models used for training in the third column. For all datasets we calculated the relative distance and direction to the silhouette and graphed them against the out-of-plane and the image-plane component of the surface normals. We consistently observe a strong correlation between the plotted variables. While the relation between direction to the silhouette and image-plane component is strongly linear, the relative distance relates roughly quadratically to the out-of-plane component. The strong correlation clearly suggests the proposed silhouette features to be helpful for reconstructing a surface from a single image. In Section 9.1 we investigate the importance of our input features for surface prediction.

All observable reflectance values of an object with uniform albedo can be mapped one-to-one onto a hemisphere, assuming distant light sources and no self-reflections or occlusions. Moreover, to approximate a Lambertian reflectance map well, only 9 spherical harmonics coefficients per color channel suffice [3]. Thus, to calibrate against a reflectance map, [2] provided each object of interest with a calibration sphere of the same BRDF. Barron and Malik [16] obviated the sphere and jointly recovered the reflectance map and the surface with a generative model.

Our discriminative approach reconstructs the reflectance map directly from an initial surface estimate that we derive solely from the object silhouette. In particular, we map the input image to a sphere according to our silhouette features (Fig. 6
                     , left). The features define a mapping from a pixel 
                        p
                      to polar coordinates on a unit sphere:
                        
                           (8)
                           
                              σ
                              (
                              p
                              )
                              :
                              Ω
                              →
                              
                                 
                                    S
                                 
                                 
                                    2
                                 
                              
                              ,
                              
                              σ
                              (
                              p
                              )
                              =
                              (
                              
                                 
                                    
                                       cos
                                    
                                    
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       d
                                    
                                    
                                       rel
                                    
                                    
                                       ′
                                    
                                 
                                 (
                                 p
                                 )
                                 ,
                                 β
                                 (
                                 p
                                 )
                              
                              )
                           
                        
                     However, since the mapping from pixels to polar coordinates is many-to-one, we average the colors of points with similar distance and direction to the silhouette. In particular, the color at a polar coordinate (i.e. normal or lighting direction) 
                        s
                        ∈
                        
                           
                              S
                           
                           
                              2
                           
                        
                      is obtained by averaging the colors of those input pixels 
                        p
                     , whose mapping is a k-nearest neighbor of 
                        s
                     :
                        
                           (9)
                           
                              C
                              (
                              s
                              )
                              =
                              
                                 
                                    1
                                 
                                 
                                    k
                                 
                              
                              
                                 
                                    ∑
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    k
                                 
                              
                              I
                              (
                              
                                 
                                    P
                                 
                                 
                                    i
                                 
                              
                              )
                              ,
                              
                              P
                              =
                              {
                              p
                              
                              |
                              
                              σ
                              (
                              p
                              )
                              ∈
                              k
                              −
                              NN
                              (
                              s
                              )
                              }
                              ,
                           
                        
                     where 
                        I
                        (
                        
                           
                              P
                           
                           
                              i
                           
                        
                        )
                      is the observed color. The number of neighbors k considered is adjusted for the size of the object. This acts as a low-pass filter, effectively reducing estimation errors from incorrectly mapped points.

We thus obtain a robust approximation of a calibration sphere without actually having one. While the silhouette features alone yield only a coarse estimate of the surface normals, we only need to recover a small number of spherical harmonics coefficients of the reflectance (Fig. 6, second to right column), which can be done in closed form. We found that adjusting the mean and standard deviation of the reflectance map to match the input image (effectively matching brightness and contrast) improves the final estimate (Fig. 6, right column).

Note that certain objects do not fulfill our assumption of being composed of convex parts. A bowl seen from above will cause problems, for example, but likely also for other algorithms that estimate shape and reflectance. Most objects, however, contain limited concavities whose effect on the estimated reflectance map is generally compensated by other convexities.

To adapt both the estimation of surface normals and the reflectance map to the perspective case, we re-visit the idea of occluding contours: at an occluding contour, surface normals are perpendicular to the direction 
                        d
                      to the camera center. Thus, at an object׳s center, we expect surface normals to be parallel to 
                        d
                     . For orthographic projection, the camera center is thought to be at an infinite distance, resulting in the same vector 
                        d
                        =
                        
                           
                              (
                              0
                              ,
                              0
                              ,
                              1
                              )
                           
                           
                              T
                           
                        
                      for each pixel. For perspective projection, however, the direction to the camera depends on the position 
                        (
                        u
                        (
                        p
                        )
                        ,
                        v
                        (
                        p
                        )
                        )
                      on the image plane and on the focal length f.

To adapt the whole surface estimate to perspective projection, we rotate each normal by a rotation matrix 
                        
                           
                              R
                           
                           
                              d
                           
                        
                      that maps the vector 
                        
                           
                              (
                              0
                              ,
                              0
                              ,
                              1
                              )
                           
                           
                              T
                           
                        
                      to 
                        d
                      and keeps the up-direction. This is equivalent to subsequently rotating around the y-axis by 
                        
                           
                              R
                           
                           
                              ϕ
                           
                        
                        (
                        p
                        )
                      and the x-axis by 
                        
                           
                              R
                           
                           
                              θ
                           
                        
                        (
                        p
                        )
                     , where
                        
                           
                              
                                 
                                    R
                                 
                                 
                                    ϕ
                                 
                              
                              (
                              p
                              )
                              =
                              (
                              
                                 
                                    
                                       
                                          cos
                                          ϕ
                                          (
                                          p
                                          )
                                       
                                       
                                          0
                                       
                                       
                                          sin
                                          ϕ
                                          (
                                          p
                                          )
                                       
                                    
                                    
                                       
                                          0
                                       
                                       
                                          1
                                       
                                       
                                          0
                                       
                                    
                                    
                                       
                                          −
                                          sin
                                          ϕ
                                          (
                                          p
                                          )
                                       
                                       
                                          0
                                       
                                       
                                          cos
                                          ϕ
                                          (
                                          p
                                          )
                                       
                                    
                                 
                              
                              )
                              ,
                              
                                 
                                    R
                                 
                                 
                                    θ
                                 
                              
                              (
                              p
                              )
                              =
                              (
                              
                                 
                                    
                                       
                                          1
                                       
                                       
                                          0
                                       
                                       
                                          0
                                       
                                    
                                    
                                       
                                          0
                                       
                                       
                                          cos
                                          θ
                                          (
                                          p
                                          )
                                       
                                       
                                          sin
                                          θ
                                          (
                                          p
                                          )
                                       
                                    
                                    
                                       
                                          0
                                       
                                       
                                          −
                                          sin
                                          θ
                                          (
                                          p
                                          )
                                       
                                       
                                          cos
                                          θ
                                          (
                                          p
                                          )
                                       
                                    
                                 
                              
                              )
                           
                        
                     and 
                        
                           
                              ϕ
                              (
                              p
                              )
                              =
                              
                                 
                                    tan
                                 
                                 
                                    −
                                    1
                                 
                              
                              
                                 
                                    u
                                    (
                                    p
                                    )
                                 
                                 
                                    f
                                 
                              
                              ,
                              
                              θ
                              (
                              p
                              )
                              =
                              
                                 
                                    tan
                                 
                                 
                                    −
                                    1
                                 
                              
                              
                                 
                                    v
                                    (
                                    p
                                    )
                                 
                                 
                                    f
                                 
                              
                              .
                           
                        
                     
                  

Analogously we adapt the reflectance map estimate. If the object is not centered at the principal point, the calibration sphere we construct from it is neither. Hence, instead of mapping the input image to a hemisphere as in the orthographic case, we map it to a rotated hemisphere. However, instead of rotating each point on the hemisphere differently, we consider a single rotation matrix for the whole hemisphere. We obtain the rotation matrix for the centroid of the object on the image plane, as this averages the pixel positions. From the rotated hemisphere we again recover the spherical harmonics coefficients.

@&#EXPERIMENTS@&#

To gain insight into the importance of our input features, we analyze their qualitative (Fig. 7
                        ) and quantitative (Table 1
                        a) effects on surface normal prediction. We investigate the unary features on the surfaces from the training part of the MIT intrinsic image dataset, rendered under all illuminations from [2]. In contrast to the illuminations used by [16], these are not sampled from a learned prior, but were captured in natural environments and include nearly white illumination. To simulate image noise, we added Gaussian noise (
                           σ
                           =
                           0.001
                        ) to the rendered images and thresholded values below 0 and above 1. The training dataset is the same as described in Section 4. In Table 1a we show the results evaluated using the median angular error (MAE) and the mean-squared error of the normal (nMSE, see [16]).

The basic color feature (RGB) acts as our baseline. The silhouette-based features (+Silh) increase the overall performance. Nonetheless, they excel at round objects or those that are composed of convex parts with a curved surface. Consequently, they boost performance on objects fulfilling these assumptions, but only marginally on planar objects or in presence of self-occlusions. In colorful illuminations (Fig. 7, bottom) the silhouette features can be partially deceptive, but overall clearly improve performance. Adding the texton filters (+Tex) particularly improves estimates under chromatic illumination, indicating that the captured spatial information eliminates many ambiguities; yet even in white illumination they ameliorate. The best overall performance stems from the combination of all features (+Silh+Tex) and is robust w.r.t. the illumination conditions.

We investigate four variants of enforcing the integrability constraint upon the estimated surface in Table 1b. We start with an l
                        2-penalty on violations of Eq. (3). Next, we restrict the method to form surface normals from a convex combination of samples drawn from the leaf distributions at each pixel. We further enforce integrability with an l
                        1-penalty [33], and finally an l
                        2-penalty under perspective projection following [32]. The unary predictions are a reasonable baseline as the surface normals can be reconstructed already with good accuracy without any post-processing. The performance may even decrease after post-processing under synthetic illumination. For real images, which potentially violate the Lambertian assumptions, however, we observed significant improvements. For the objects in the MIT dataset, which were presumably imaged with a long focal length, the benefits of a perspective approach are negligible. Although the convex combination of samples (conv) clearly outperformed all other approaches, we rely on the simple l
                        2-penalty in further experiments due to the much lower run-time.

We quantitatively compare to two state-of-the-art methods on three different datasets, two of which are contributed by the respective methods, and one is recorded by ourselves.

First, we compare to the shape-from-shading component of the SIRFS method of Barron and Malik [16] (termed “Cross scale”). We evaluate using the source code provided by the authors both under unknown and given illumination. In unknown illumination, we additionally record the accuracy of the estimated reflectance map (lMSE, see [16]). In Table 2
                        a we present results on the dataset of [16], a variant of the MIT intrinsic image dataset [23] re-rendered under chromatic illumination.

The method of Xiong et al. [4] (termed “Local context”) is our second baseline. It exploits local shading context to predict shape from shading under known illumination. Xiong et al. provide a dataset of 10 objects captured under white directional illumination and also evaluated the shape-from-shading component from [16]. Thus, we simply restate their results (Table 2b, first column) and run our algorithm on their dataset.

As with all experiments, we use our own separate set of artist-created models (Section 4) as sole training data. To set the hyperparameters (number of trees, maximum tree depth, etc.) of our method, we once used Bayesian optimization [35] on the training split of the MIT intrinsic images and fixed the parameters for all of our experiments.


                        Real-world experiment: The performance on synthetic data can be misleading and may not necessarily translate to realistic settings [13]. To demonstrate the accuracy and robustness of our method quantitatively in a real-world setting, we recorded a new dataset for shape from shading in natural illumination, since no dataset captured under natural illumination exists so far; methods considering natural illumination were instead evaluated only qualitatively or on synthetic data [2,16].

To record highly accurate ground truth in laboratory illumination, photometric stereo methods are well established [4]. They require a carefully designed lighting environment, however, and produce only a normal map. Hence, capturing data in natural illumination would require either synthesizing illumination in the lab, which violates the real-world assumption, or re-building a controlled setup at each scene, which is next to impossible for many realistic scenes.

Instead, we used multi-view stereo [36,37] to reconstruct surface meshes from 
                           ~
                           200
                         images we took for each of four objects. That enabled us to take test images under real illumination in different environments and later align the test images to the meshes by mutual information [38]. For the test images, we painted the objects with a white diffuse paint. To recover the ground truth illumination for each scene, we recorded a calibration sphere of the same BRDF as the objects. Images and ground truth are publicly available on our website.

We give quantitative results on the dataset in the three right-most columns of Table 2b; the reconstructed surfaces are shown in Fig. 8
                        . As before, our method was not specifically adapted to the dataset. The shape prior used by [16] is neither; the shapes used for training (MIT dataset) are still representative (i.e. of similar kind). We show additional results in Fig. 1.


                        Projection experiment: To evaluate the performance of our approach under perspective projection, we recorded three new test images depicting objects from our orthographic experiments. In contrast to the orthographic setting, where we used a telephoto lens to approximate the presumably infinite focal length for all images, we here chose different focal lengths ranging from 35mm to 128mm (full frame 35mm equivalent). Again, the images were taken in different environments featuring natural illumination. We compare our perspective extension to the orthographic version from the experiments above. As baseline we include a coarse normal estimate derived from the silhouette features and its perspective adaption following Section 8. By exploiting the silhouette features as polar coordinates, we directly compute a coarse surface normal 
                           n
                         as
                           
                              (10)
                              
                                 n
                                 (
                                 p
                                 )
                                 =
                                 (
                                 
                                    
                                       
                                          
                                             c
                                             (
                                             p
                                             )
                                             
                                             cos
                                             β
                                             (
                                             p
                                             )
                                          
                                       
                                       
                                          
                                             c
                                             (
                                             p
                                             )
                                             
                                             sin
                                             β
                                             (
                                             p
                                             )
                                          
                                       
                                       
                                          
                                             
                                                
                                                   1
                                                   −
                                                   c
                                                   
                                                      
                                                         (
                                                         p
                                                         )
                                                      
                                                      
                                                         2
                                                      
                                                   
                                                
                                             
                                             ,
                                          
                                       
                                    
                                 
                                 )
                                 ,
                                 
                                 c
                                 (
                                 p
                                 )
                                 =
                                 1
                                 −
                                 
                                    
                                       d
                                    
                                    
                                       rel
                                    
                                 
                                 (
                                 p
                                 )
                              
                           
                        
                     

The focal length was given to the perspective variants.


                        Results: The quantitative and qualitative results show that our method robustly recovers surfaces and reflectance maps in synthetic, laboratory, and natural illumination. We clearly outperform the cross-scale approach [16] in all metrics; the only exception are the real images with known illumination, where we perform about the same. In the more challenging setting of unknown illumination, we perform significantly better, however. As can be seen in the bottom row of Fig. 8, correctly estimating the reflectance map is crucial to the performance of surface reconstruction. The silhouette features allow for a robust estimate, which is leveraged by our discriminative learning approach. The results in Fig. 8 further highlight that our approach is able to recover fine surface detail on real data, since it does not need to rely on strong spatial regularizers.

We also outperform the local context approach of [4]. One point to note is that our approach can deal with images of different scales (the images of Fig. 8, top are approximately twice the size of those in Fig. 8, bottom). This is due to the scale-invariant nature of our silhouette features.

It may seem surprising that the performance of all methods decreases in realistic settings, since the illumination is more colorful. However, this can be explained by observing that the real data exhibits shadows and inter-reflections, which the synthetic datasets do not. Despite these challenges our discriminative approach is able to provide high-quality surface estimates in uncalibrated illumination.

Adapting the algorithm to perspective projection slightly, but consistently improves its performance (Table 3
                        ). As can be seen in Fig. 9
                        , the improvement is noticeable in all conditions, as well as for the estimation of the reflectance map. We observed the improvement to increase with bigger fields of view, consistent with the increasing difference between the projection models. In Fig. 9 the effects are more prominent in the baseline examples as there are no other features that compensate for perspective errors by local or global context.

@&#CONCLUSION@&#

In this paper we demonstrated a discriminative learning approach to the shape-from-shading problem in uncontrolled illumination, assuming only a single image of an unknown diffuse object with uniform albedo is given. To this end, we tailored regression forests to output surface normals. These pixel-independent estimates are processed spatially only by constraining the reconstructed surface to be integrable. We introduced and analyzed suitable input features that capture context on a local and scale-invariant global level. Besides removing the need for explicit spatial regularization, the proposed silhouette features allow for estimating the unknown reflectance map. Both the reflectance map estimation and the surface reconstruction can be easily generalized to perspective projection. Our model needs to be trained for each illumination condition, similar to other learning approaches. Owing to its computational efficiency, our approach can be trained and tested within the time other recent methods need for just testing. We used a novel, large scale dataset to train our model and evaluated it on various challenging datasets, where it outperforms recent approaches from the literature. Finally, we demonstrated its ability to reconstruct fine surface detail outside of the laboratory on a new real-world dataset.

@&#ACKNOWLEDGMENTS@&#

We thank Simon Fuhrmann for assistance with multi-view stereo capturing. SRR was supported by the German Research Foundation (DFG) within the Research Training Group "Cooperative, Adaptive and Responsive Monitoring in Mixed Mode Environments" (GRK 1362). SR was supported in part by the European Research Council under the European Union׳s Seventh Framework Programme (FP/2007-2013)/ERC Grant agreement no. 307942, as well as by the EU FP7 project “Harvest4D” (No. 323567).

@&#REFERENCES@&#

