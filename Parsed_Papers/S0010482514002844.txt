@&#MAIN-TITLE@&#Odorant recognition using biological responses recorded in olfactory bulb of rats

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present an analysis of pattern recognition and dimensionality reduction techniques applied to odor discrimination.


                        
                        
                           
                           The local field potential signals recorded in the olfactory bulb are stimulus specific in normal rats.


                        
                        
                           
                           Artificial olfaction systems discriminate odor information from local field potential recorded in rat olfactory bulb.


                        
                        
                           
                           PCA followed by SVM are able to discriminate odorant stimuli with accuracy of over 95%.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Feature extraction

Pattern recognition

Odorant classification

Local field potential in olfactory bulb

Principal component analysis (PCA)

Fisher Transformation (FT)

Sammon NonLinear Map (NLM)

Wavelet Transform (WT)

Multilayer Perceptron (MLP)

Support Vector Machine (SVM)

@&#ABSTRACT@&#


               
               
                  In this study we applied pattern recognition (PR) techniques to extract odorant information from local field potential (LFP) signals recorded in the olfactory bulb (OB) of rats subjected to different odorant stimuli. We claim that LFP signals registered on the OB, the first stage of olfactory processing, are stimulus specific in animals with normal sensory experience, and that these patterns correspond to the neural substrate likely required for perceptual discrimination. Thus, these signals can be used as input to an artificial odorant classification system with great success. In this paper we have designed and compared the performance of several configurations of artificial olfaction systems (AOS) based on the combination of four feature extraction (FE) methods (Principal Component Analysis (PCA), Fisher Transformation (FT), Sammon NonLinear Map (NLM) and Wavelet Transform (WT)), and three PR techniques (Linear Discriminant Analysis (LDA), Multilayer Perceptron (MLP) and Support Vector Machine (SVM)), when four different stimuli are presented to rats. The best results were reached when PCA extraction followed by SVM as classifier were used, obtaining a classification accuracy of over 95% for all four stimuli.
               
            

@&#INTRODUCTION@&#

Artificial odor recognition is an important and challenging problem in the real world [1,2]. In many industries, health services and military applications, Artificial Olfaction Systems (AOS) are important in identifying compounds present in a substance either for quality control and certification purposes, or in finding pathogenic agents or illegal elements. For example, Electronic Noses, understood as “an instrument comprising an array of specific electronic chemical sensors and a pattern recognition system capable of recognizing odors” [3], have been studied extensively and analyzed. They have become popular for several uses, such as in analyzing the discrimination mechanisms in the mammalian olfactory system [4], in agriculture and forestry [5], in certifying meals and fruits [6,7], and in identification of wines and beers [8–12].

Application of artificial odor recognition based on machines has many advantages such as availability, economical cost, and capacity, but the human olfactory system has better performance since it can distinguish thousands of different odorants in non-controlled environments. In order to reproduce the outstanding performance of the biological olfactory system, artificial systems including algorithms and large sensor arrays, have been developed by several authors [13–15]. It has also been shown that the approach based on bio systems can overcome typical sensor failures [16–18].

Some animals have developed a better olfactory system than humans because they have a larger number of genes coding for receptor molecules and a wider epithelium area [19]. However a comprehensive understanding of how animals encode odors is still under investigation [20,21]. Many efforts have been made to capitalize on the advantages of olfactory sensory neurons such as their wide range, keen sensitivity, and fast response [22]. Some studies present biomimetic designs in which responses of these types of cells are included as part of the sensory system [23] with applications from biomedicine to environmental detection [24]. In the animal olfactory system, neuronal responses to odorants have been characterized extensively using single unit and local field potential (LFP) recordings. In order to extract conclusions from a biological olfactory system, the Olfactory Bulb of rats has been largely investigated. But, a better understanding of odor coding and odor receptors in biological olfaction systems may provide valuable insights for the design of general purpose AOS [25,26].

This work presents the design of an AOS which is able to recognize the odorant stimuli applied to rats based on LFP signals recorded in the olfactory bulb (OB) when they are subjected to different odorant stimuli. LFP signals are the result of extracellular currents generated from spikes and synaptic potentials within a neural network. It has been determined that odorants modulate LFP signals [27]. Therefore it is interesting to ask if those signals, as input to an artificial odorant recognition system, would be able to predict (recognize) the odorant stimulus applied to a rat. We claim that LFP signals registered from the OB, the first stage of its olfactory processing, and produced in response to different odorants presented to rats, are stimulus specific in animals with normal sensory experience, and these patterns reflect the neural substrate that is likely to be necessary for perceptual discrimination.

In the design of the AOS we explored different approaches to extracting the information contained in LFP signals when four different stimuli are applied to rats. As a first stage, the four feature extraction methods (FE); Principal Component Analysis (PCA), Fisher Transformation (FT), Sammon NonLinear Map (NLM) and Wavelet Transform (WT) were used. As a second stage, three pattern recognition techniques were applied; Linear Discriminant Analysis (LDA), Multilayer Perceptron (MLP) and Support Vector Machine (SVM).

It is important to point out that according to the results shown in Section 4, our working hypothesis is completely verified and the best results are obtained when PCA together with SVM techniques are used, reaching a classification accuracy of over 95% for all stimuli. The main contribution of this work is precisely the verification of this hypothesis, in the sense that information contained in LFP signals recorded from the OB of rats is sufficient to determine the odorant stimulus that it produced.

To the best of our knowledge, this is one of the few papers that reports applying advanced FE and PR techniques for extracting stimulus information from LFP signals in a sensory system [28,29]. Several studies have used a similar approach but on the motor cortex of primates to determine the direction of an arm movement, using LFP, multiunit, single unit or EEG signals [30–32].

The paper is organized as follows. In Section 2a brief description of the experimental procedures followed in the study is presented. A summary of the FE and PR techniques used in this work is presented in Section 3. Section 4 presents the main results obtained in this work. Finally, some conclusions are drawn in Section 5.

The information used in this study (LFP recorded on OB of rats) was obtained by the team of ICBM using specific procedures. Surgical and experimental techniques were performed in accordance with institutional guidelines (protocol CBA# 079 FMUCH) and the National Institute of Health Guide for the Care and Use of Laboratory Animals (NIH Publications No. 80-23) revised 1996. The recordings were performed on adult animals and after the experiments the animals were euthanized with a barbiturate overdose.

LFP signals were obtained using 16-channel linear-probe electrodes (CNCT, Michigan, USA) with impedances between 1 and 2MOhms measured at 1kHz. The contact separation was 50μm and all recordings were done using 2 identical probes. The impedance of electrodes between recordings was examined and found to remain fairly stable (1–2MOhms). All penetrations were performed perpendicular to the OB surface and the electrodes were lowered until mitral cell action potentials were in the center of the electrode array. Each of these locations was thereafter named as a recording site with an identifying number. The LFP signals were amplified (10K), band pass filtered (0.1–300Hz) and digitized (2.9kHz/channel) using custom designed PC software. To check that the animal was breathing through both nostrils the respiratory cycle was recorded using a thermocouple.

Stimuli were presented to the rats with a custom made olfactometer. This device consists of computer-controlled solenoid valves, allowing switching between different stimuli. One valve controls clean-humidified air passing through an empty tube, whereas the remaining valves control each one of the odorants used in the experiment (r-carvone, isoamylacetate and hexanal). These are obtained by passing air through a tube with the odorant diluted in mineral oil with total volume of 1000ml. The airflow system consisted of pressurized air from commercially purified tanks passing through the olfactometer connected to an inverted funnel facing the animal׳s nose [69]. The stimulation protocol consisted of the sequential presentation of single odorants: r-carvone, isoamylacetate and hexanal (Sigma-Aldrich, St. Louis, MO) diluted in a 10/1000 ratio in mineral oil. Each trial started with 4s of clean humidified air (PRE) followed by 2s of the odorant stimulus (STIM) and then by 4s of clean air. In every recording set, the animals were exposed to 10s trials for each stimulus. The intertrial interval was 5s. The stimuli were presented in the following sequence: air, r-carvone, isoamylacetate, hexanal. In this way, a stimulus was presented every 13s and the same stimulus was presented once every 60s intercalated with the 3 other stimuli. An example of the LFP signals obtained for recording Site #1, is presented in 
                        Fig. 1, showing the four recordings corresponding to four LFP responses to the four stimuli (air, r-carvone, isoamylacetate and hexanal) in the pre-stimulus stage and during the stimulus presentation.

In this section general information about the signal processing, feature extraction and pattern recognition techniques used in the study are presented. Detailed information about the material presented in the section can be found in the specific references given in each subsection and in reference [63].

LFP signals were recorded on different sites from OBs and from different rats. All those signals were analyzed offline. LFP signals were first normalized by the Euclidean norm and re-sampled at 580Hz. A continuous 6s time epoch was chosen, corresponding to the last 2s before odorant stimulus (PRE), the 2s of stimulation (STIM) and the first 2s after stimulus presentation (POST) for each trial. Thus, the pattern was reduced to a total of one 6s LFP signal.

For the pattern recognition methodology we built a database with the collection of the LFP signals including all the trials at each recording site. Every recording site generated one database consisting of 4 different stimuli, 10 repetitions for each stimulus and LFP signals was registered using 16 electrodes per trial. In summary, we had a database of 640 trials (160 per odorant) per recording site.

Finally, we analyzed a total of 46 recording sites obtained from the OBs of 10 normal rats. For notation purposes we enumerated each recorded site, from 1 to 46. Since every pattern contains 6s of LFP response sampled at 580Hz, each one was represented as a vector of 3480 components.

Feature extraction represents a fundamental step in pattern recognition methodology [33,34] whose function is to choose those elements of the signals that are relevant for recognition tasks and discard those that are not necessary. This occurs because a piece of data may contain relevant information for problem description but a part of the data could also bear irrelevant or redundant information about odorant identification which would not be necessary in the classification process.

Different techniques have been developed for this purpose, some of them “select” some features, meaning that they choose some components of every pattern (composed by “n” features) and other techniques “extract” features, meaning that they originate a new feature vector (composed by “k” new features) as a combination of original components, which represents the most important information of the original pattern.

Feature extraction methods are based on different mathematical or optimal criteria. In this study we applied Principal Component Analysis (PCA), Discrete Wavelet Transform (DWT), Sammon NonLinear Map (NLM) and Fisher Transform (FT) or Fisher Discriminant Analysis (FDA). Next, a brief description of these FE techniques is presented.

Principal Component Analysis (PCA), as a feature extraction technique, provides a new set of attributes which are a linear combination of original features [35]. PCA is used for dimensionality reduction by retaining those components that contribute most to the signal variance contained in the lower-order principal components. To select the number of components we examined the quality of the feature extraction by characterizing the relationship between the number of principal components and the percentage of the total variance (PTV) of the original data. Based on the analysis of the information recorded in the 46 sites, we chose the first 100 principal components, since they represented on an average of about 75% of the total variance of the signals, as shown in 
                           Fig. 2A. Thus, after PCA is applied to original patterns of dimension 3480, a new pattern of dimension 100 will be obtained.

Discrete Wavelet Transform (DWT) is similar to Fourier analysis, but every base function used in the Wavelet transform is associated with a specific frequency band. This transform used as a feature extraction technique provides a new set of attributes based on coefficients of the signal level decomposition as a sum of different frequency signals and base functions (Haar functions in our study). In this decomposition the original pattern 
                              X
                              (
                              t
                              )
                           is written as the weighted sum of the Approximation Functions 
                              
                                 
                                    φ
                                 
                                 
                                    L
                                    ,
                                    j
                                 
                              
                           (of level L) and the Detail Functions 
                              
                                 
                                    ψ
                                 
                                 
                                    m
                                    ,
                                    j
                                 
                              
                           (of level m) [45] in the following form:
                              
                                 
                                    X
                                    (
                                    t
                                    )
                                    =
                                    
                                       ∑
                                       
                                          j
                                          =
                                          −
                                          ∞
                                       
                                       ∞
                                    
                                    
                                       
                                          
                                             α
                                          
                                          
                                             L
                                             ,
                                             j
                                          
                                       
                                       
                                          
                                             φ
                                          
                                          
                                             L
                                             ,
                                             j
                                          
                                       
                                       (
                                       t
                                       )
                                    
                                    +
                                    
                                       ∑
                                       
                                          m
                                          =
                                          L
                                       
                                       1
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             −
                                             ∞
                                          
                                          ∞
                                       
                                       
                                          
                                             
                                                δ
                                             
                                             
                                                m
                                                ,
                                                j
                                             
                                          
                                          
                                             
                                                ψ
                                             
                                             
                                                m
                                                ,
                                                j
                                             
                                          
                                          (
                                          t
                                          )
                                       
                                    
                                 
                              
                           
                        

Functions 
                              
                                 
                                    φ
                                 
                                 
                                    L
                                    ,
                                    j
                                 
                              
                           and 
                              
                                 
                                    ψ
                                 
                                 
                                    m
                                    ,
                                    j
                                 
                              
                           are defined in terms of the mother Haar Wavewlet used in this study. Their definitions, as well as the coefficients 
                              
                                 
                                    α
                                 
                                 
                                    L
                                    ,
                                    j
                                 
                              
                           and 
                              
                                 
                                    δ
                                 
                                 
                                    m
                                    ,
                                    j
                                 
                              
                           can be found in [64]. In our study, only the approximation coefficients were considered since they contain most of the energy signal [65]. Further details on DWT technique can be found in [65,66].

Previous evidence shows that odorants modulate the power of LFP oscillations in the gamma (35–90Hz) [27,36–40] and beta (15–30Hz) [27,36,38,39,41] ranges. Though gamma and beta oscillations have been correlated to odor identification or concentration [41,42], other studies have related oscillations to experience dependence modulations [39,40,43] or to an underlying mechanism for odorant discrimination [44]. In our case, a preliminary signal analysis revealed that frequency range of the signals recorded in the OB was close to the beta range. Therefore, parameters of the DWT will be chosen according to the frequency range (beta range).

Based on these biological arguments it was decided to select a 5th order wavelet decomposition (m=5), which has an associated frequency of 18.13Hz, close to the beta range. His choice will result in k=109 features for the new patterns, since the original pattern has n=3480 components and the number of Haar Wavelet coefficients is defined by the relation k=n/(2
                              m
                           ).

In this study the Matlab wavelet toolbox, that uses Haar Wavelet as the base function, was used in the simulations presented in Section 5 
                           [63].

This algorithm is also known as the Sammon Map, and studies the projection on k dimensions of an n-dimension space (n>k). To accomplish this objective this method minimizes the Sammon stress (Sammon error) [46] defined as
                              
                                 
                                    E
                                    =
                                    
                                       1
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                j
                                             
                                             
                                                
                                                   
                                                      d
                                                   
                                                   
                                                      i
                                                      j
                                                      X
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       N
                                    
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          j
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      [
                                                      
                                                         
                                                            
                                                               d
                                                            
                                                            
                                                               i
                                                               j
                                                               X
                                                            
                                                         
                                                         −
                                                         
                                                            
                                                               d
                                                            
                                                            
                                                               i
                                                               j
                                                               Y
                                                            
                                                         
                                                      
                                                      ]
                                                   
                                                   2
                                                
                                             
                                             
                                                
                                                   
                                                      d
                                                   
                                                   
                                                      i
                                                      j
                                                      X
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 
                                    d
                                 
                                 
                                    i
                                    j
                                    X
                                 
                              
                            represents the distance between X
                           
                              i
                            and X
                           
                              j
                            patterns in the original n-dimensional space and 
                              
                                 
                                    d
                                 
                                 
                                    i
                                    j
                                    Y
                                 
                              
                            denotes the distance between their projections Y
                           
                              i
                            and Y
                           
                              j
                           , respectively, in the k-dimensional space. In other words, the nonlinear map attempts to preserve the distance among X patterns expressed in the original space with respect to the distance of Y patterns (new feature patterns) in the output space. For more details about this algorithm the reader is referred to [46,63,67].

This algorithm was implemented using the Matlab toolbox SOM [47], considering a feature space of dimension k=100, i.e. reducing the original space dimension from 3480 to 100. This number was chosen to make the classification accuracy comparable with the obtained with PCA (k=100) and DWT (k=109).

Fisher Transformation (FT) [11,48] is a supervised method. This algorithm has a fixed number of features for the feature space of k=C−1, with C the number of classes or stimuli contained in the databases. In our study C=4, (air, r-carvone, isoamylacetate and hexanal).

Since this problem includes C=4 classes, then the Fisher Transformation has only 3 dimensions (k=C−1=3). Thus this technique allowed the visualization of the spatial distribution of the different patterns of the same database (corresponding to one OB recording site) associated with the four stimuli.

The Fisher transformation was implemented using a simultaneous diagonalization procedure described in [49] for those cases where the number of samples was less than the dimensionality of the samples (as it is in this problem).

A classifier is the component of the system (or the step in the classifications process) that assigns every pattern (also called example) to a class [50–52]. In mathematical terms, the classifier is a function that maps the feature space onto a decision space. In this work we have analyzed three different supervised classifiers; Linear Discriminant Analysis (LDA), Multilayer Perceptron (MLP) and Support Vector Machine (SVM). It is interesting to point out that LDA can be used either as a dimensionality reduction technique or as a classifier. In our case we will use it as a classifier as it is used in [53]. These classifiers are briefly described in what follows.

The simplest statistical classifier LDA [54], was used to compare the effects of the feature extraction stage. LDA consists of applying the maximum a posteriori rule assuming normal data distribution and the same covariance matrix for each class. Using these last two assumptions, this is simplified to assigning pattern X to class w
                           
                              i
                            if and only if 
                              
                                 
                                    C
                                 
                                 
                                    i
                                 
                              
                              ≥
                              
                                 
                                    C
                                 
                                 
                                    j
                                 
                              
                              ∀
                              i
                              ≠
                              j
                            where
                              
                                 
                                    
                                       
                                          C
                                       
                                       
                                          k
                                       
                                    
                                    =
                                    2
                                    
                                       
                                          X
                                       
                                       T
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       
                                          μ
                                       
                                       
                                          k
                                       
                                    
                                    +
                                    
                                       
                                          μ
                                       
                                       
                                          k
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    
                                       
                                          μ
                                       
                                       
                                          k
                                       
                                    
                                    −
                                    2
                                    
                                    log
                                    (
                                    P
                                    (
                                    
                                       
                                          w
                                       
                                       
                                          k
                                       
                                    
                                    )
                                    )
                                 
                              
                           
                        

Σ corresponds to the data covariance matrix, 
                              
                                 
                                    μ
                                 
                                 
                                    k
                                 
                              
                           is the mean of class w
                           
                              k
                            and 
                              P
                              (
                              
                                 
                                    w
                                 
                                 
                                    k
                                 
                              
                              )
                           is the a priori probability of class w
                           
                              k
                           . The structure of the linear classification implies that decision boundaries are hyper-planes. Thus, this algorithm can classify correctly only problems where data is linearly separable.

Considering the methodology used here to obtain the information from OB we can chose the classifier with equally distributed a priori probability 
                              P
                              (
                              
                                 
                                    w
                                 
                                 
                                    k
                                 
                              
                              )
                              =
                              
                                 
                                    1
                                    4
                                 
                              
                              
                              ∀
                              
                                 
                                    w
                                 
                                 
                                    k
                                 
                              
                              =
                              1
                              ,
                              2
                              ,
                              3
                              ,
                              4
                           .

A neural network (NN) can be defined as a parallel distributed processor [55], whose neurons are distributed in layers. A multilayer perceptron (MLP) is an artificial neural network in which the output of one layer is the input of the following layer. This supervised methodology is based on neural network learning (back-propagation), by modifying the synaptic weights between layers.

For practical implementation we used the Neural Network toolbox from Matlab. Network architecture was chosen to be [k-20-10-2], where k means the number of features of the input pattern. This number k depends on the feature extraction methodology used (k=100 for PCA, k=109 for DWT, k=100 for NLM, and k=3 for FT). A hyperbolic tangent was used as the activation function with a momentum rate of 0.2 and learning rate of 0.01. Finally, as stopping criterion we selected 10,000 iterations.

Since this problem is a four class problem and we had 2 neurons in the output layer of the MLP, we applied a classifier for a multi-class problem directly to the output of MLP, by defining every class as a binary number of two digits ([−1, −1] for air; [−1, +1] for r-carvone, [+1, −1] for isoamylacetate and [+1, +1] for hexanal).

Support Vector Machine (SVM) is a supervised methodology based on statistical learning [56]. This methodology was used to allocate every single trial LFP (pattern) to a particular class or stimulus [50–52]. The Support Vector Machine [57] selects the optimal hyper-plane that separates the different data groups, by choosing an appropriate kernel function to map the feature space into a high dimensional space where the linear separability can be obtained. Although SVM can directly deal with multiclass classification problems [68], in this study we used SVM in a four-class problem adding a one versus one by a max-wing voting multi-class algorithm implemented in SVM toolbox for Matlab LIBSVM [58]. Different kernel functions; linear, polynomial, and radial basis function were analyzed, resulting in the one using polynomial kernel being the most appropriate for this problem. In general, the degree of the polynomial kernel required for the classification indicates how “non-linear” the transformation is from the feature space to the “decision space”. It also indicates the need for a high dimensional space to produce linear separability among classes (odorants). We tested a linear polynomial with degrees from 1 to 10, and the best performance was obtained with the 6th degree. Thus, all the results shown below were obtained using SVM with a 6th degree polynomial kernel.

@&#RESULTS@&#

As a general methodology for the classification process, the whole data set was divided into three disjoint subsets (keeping balance among classes); one for training purposes, another for validations purposes and the third one for testing. The training subset was used for training different structures for the classifiers. The validation subset was used to select the best structure (metaparameters) of the classifiers according to the criterion function. Finally, the test subset was used to evaluate the performance of the classification system. The partitions used in this work were chosen randomly as follows: 60% of the samples for the training subset (384 OB signals, 96 for each class), 20% of the samples for the validation subset (128 OB signals, 32 for each class) and 20% of the samples for the test subset (128 OB signals, 32 for each class).

We first examined the results of expressing the original patterns (n=3480 components) into the four feature spaces (PCA space with k=100, DWT with k=109, NLS space with k=100 and Fisher space with k=3).

In the PCA feature extraction we used the relationship between the number of principal components and the percentage of total variance (PTV) contained in the principal component vectors as a quality measure of the methodology. Although the percentage of total variance retained in principal components varies from one site to another, the PTV contained in the first 100 principal components was on average of about 75% considering all 46 sites. Fig. 2A shows the PTV contained in the 100 low-order principal component features for all 46 sites used in this study. The average PTV was 74.4±15.5%.

To perform the feature extraction with wavelet decomposition, we analyze the signal decomposition for different approximation and detail levels, as explained in Section 3.2.2). As an example, Fig. 2B show the decomposition of a LFP signal recorded on Site #1 using different wavelet decomposition levels (approximation level L=6, and detail levels m=6, 5 and 4). In this study, the feature extraction stage used the coefficients associated with level m=5, which corresponds to k=109 coefficients, as stated in Section 3.2.2).

In the non-linear map projection, we used the minimization of the corresponding Sammon stress function as a quality measure of the methodology. The error obtained in every recording site is presented in Fig. 2C. In all 46 sites Sammon error was less than 0.1 and on average was 0.0441±0.0221. From the analysis given in Section 3.2.3) we choose a fixed number of 100 components for all sites (the same feature number as in PCA extraction and similar to DWT).

Once the feature extraction procedure using PCA, DWT and NLM was applied on LFP signals recorded on the 46 OB sites, the resulting patterns were analyzed carefully, observing no significant differences to specify their class membership, i.e. the membership could not be determined by simple examination of the patterns.

In 
                        Fig. 3, the 640 patterns from Site #1 are shown in the 3-dimesional Fisher space, where it can be observed different spatial distributions for different stimuli. It is important to note that this FE technique is a supervised algorithm that facilitates stimuli separation on Fisher space. Besides, the information in Fig. 3 corresponds to only one site and for other sites it was found that this “separation” was not as evident as in the Site # 1.

From the previous analysis, we conclude the necessity of introducing a classification (pattern recognition) stage.

To evaluate the performance of each classification system (the combination of a feature extraction algorithm and a pattern recognition technique) a confusion matrix for each method was built. In this matrix, the (i,j)-element corresponds to the number of times the i-th odorant was classified as j-th odorant, divided by the total number of patterns of i-th class. This represents the probability that an element of class i-th will be classified as belonging to class j-th. As an example, the performance classifier on the test set for Site 1, is presented in 
                        Table 1 using PCA combined with each of the three PR methods for one simulation.

The correct classifications are shown on the diagonal elements and the incorrect classifications are represented by the off-diagonal elements. The results of the classifiers׳ performance is expressed in terms of probabilities with values ranging between 0 and 1. From Table 1 we can observe that the best performance is obtained when SVM is used.

The results obtained for all combinations of FE and PR are shown in 
                        Table 2, which shows the classification accuracy including the information from all 46 sites, considering the test set. The values (mean±SD) were computed over 10 simulations and in each simulation the training, validation and test subsets were chosen randomly according to the criteria given at the beginning of Section 4. Since no significant differences were found among stimuli classification, we used the mean over all stimuli classification as global performance index of each FE/PR combination.

In 
                        Fig. 4 is shown the classification accuracy for each combination of the four FE techniques and the three PR methodologies. Large differences were found depending on the FE technique used (PCA got the best performance and NLM got the worst performance) whereas small differences in the classification performance were found among the PR techniques (for a given FE methodology).

When PCA, DWT and NLM are used, the best classifier performance is obtained for SVM, whereas when FT is used, the LDA classifier has the best performance. In all methodologies studied the worst classifier performance was obtained when using NLM as FE method. Apparently this is due to the fact that NLM tries to preserve the distance between the patterns in original as well as in the transformed spaces, and no efforts are explicitly made to separate the classes.

To reduce the number of features, we explored four feature extraction methods. One challenging step was to select the number of features for pattern representation, considering that original patterns had 3480 components. For PCA we selected the first 100 principal components, which contained approximately 75% of the total variance of the signals. Based on classifier performance (see Table 2) we concluded that the remaining 25% of the variance would contribute very little to stimulus classification and prediction.

For DWT we chose the detail Haar Wavelet level associated to beta range (15–30Hz). Since the classification performance based on DWT is worse than classification performance based on PCA (see Table 2), we could guess that this wavelet level does not contain as much information as it is contained in PCA features for classification purposes.

Fisher Transform FT also resulted in a good performance in the test set (see Table 2). This method has the advantage of visualizing the separation of the different clusters corresponding to the different olfactory stimuli. All patterns were projected into the Fisher space, taking advantage of Fisher space dimensionality (3 in this case, since we had only 4 classes).

From Table 2, it is interesting to note that NLM classification accuracy (with all classifiers) was quite poor (25.1%, 21.6% and 24.8% for SVM, LDA, and MLP respectively), corresponding to an almost random classification. As said before, apparently this is due to the fact that NLM tries to preserve the distance between the patterns in original as well as in the transformed spaces, and no efforts are explicitly made to separate the classes.

An examination of all FE/PR combinations (see Table 2) indicates that most of the differences are found among feature extraction methodologies. SVM had good performance (close to 96% in the majority of the sites) when PCA was applied, and had better performance than using DWT, NLM or FT. Only with FT did the Bayesian classifier LDA have the best performance, which could be attributed to the fact that discrimination in the FT space requires a linear hyper-plane. In contrast, the worst classifier performance was obtained when using NLM, suggesting that an acute adjustment of parameters is required to obtain better accuracy.

@&#CONCLUSIONS@&#

In this paper the successful study of odorant stimuli classification using biological responses recorded in the olfactory bulb of rats has been reported. It was shown that LFP signals recorded from different sites on the OB contain enough information to accurately determine the applied odorant using a methodology based on a combination of FE and PR techniques.

A key problem in this methodological approach is the selection of the suitable features. This should include a good representation of the original data, keeping only relevant information for predicting class membership, while keeping the number of features low. Sometimes the classifier׳s generalization error increases as the number of features increases [59].

From Table 1 it can be seen that PCA with any of the three classifiers gave results over 90%, indicating a reliable stimulus prediction for a single trial (Site #1). Moreover, the accuracy did not vary among the four stimulation conditions (air and 3 odorants) indicating that for these conditions, the chemical nature of the stimulus did not affect the performance of the classification. This was also verified for all Sites, as shown in Table 2. This fact could indicate that the LFP signals from different OB regions contain information about stimulus identity, supporting the notion of a distributed representation of the olfactory signals in the OB [60–62].

Finally, we conclude that the application of Principal Component Analysis followed by a Support Vector Machine classifier exhibited the best performance (see Table 2). Classification accuracy for this combination PCA-SVM was over 95% for all stimuli (Means=95.7±11.1, 95.6±10.8, 96.5±10.5 and 96.1±10.3 for air, r-carvone, isoamylacetate and hexanal, respectively). When all sites and odorants were combined the classification accuracy was on average of about 96.0+10.7%.

The authors declare that they do not have conflict of interests of any kind regarding the research contained in the paper.

@&#ACKNOWLEDGMENTS@&#

The results reported here were supported by CONICYT-Chile through fellowship 22070244-2007 (M.A.V), CONICYT through grant FONDECYT-Chile 
                  1061170 (M.A.D) and Iniciativa Científica Milenio ICM 
                  P04-068-F (M.L.A).

@&#REFERENCES@&#

