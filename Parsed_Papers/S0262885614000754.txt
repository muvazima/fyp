@&#MAIN-TITLE@&#Coupled person orientation estimation and appearance modeling using spherical harmonics

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Estimation of a person's orientation, 3D shape and texture from overlapping cameras


                        
                        
                           
                           Spherical harmonics are used as low-dimensional 3D shape- and texture-representation.


                        
                        
                           
                           Spherical harmonics properties allow to cope with orientation estimation elegantly.


                        
                        
                           
                           Outperformance of orientation estimation methods that use a fixed 3D shape model


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Person appearance modeling

Orientation estimation

Spherical harmonics

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

A person's overall body orientation (i.e. rotation around 3D torso major axis, facing direction) conveys important information about the person's current activity and focus of attention. In this paper, we focus on the estimation of overall person body orientation from few, overlapping cameras. To obtain a more accurate orientation estimate, we jointly estimate it with a 3D shape and texture representation of a person's torso-head, under a rigidity assumption. By projection onto a basis of Spherical Harmonics (SH), a low dimensional appearance model is created that can cope with object deformations while retaining the spatial information captured in a textured 3D representation. By comparing the texture model of the person at consecutive points in time, the relative body orientation can be estimated elegantly by using the properties of the SH. This in turn allows to update the 3D shape and texture model of a person.

Apart from facilitating an accurate orientation estimate, the proposed 3D shape and texture model furthermore has the potential to offer improved track disambiguation in a multiple person scenario, compared to less descriptive 2D view-based models. The 3D representation could also provide an initialization for applications aiming at full articulated 3D pose recovery.

The remainder of this paper is organized as follows. In Section 2, we discuss the related work. Section 3 starts with an overview of the proposed approach and the basic properties of spherical harmonics. Thereafter, it describes the estimation of texture, orientation, and shape in alternating fashion. In Section 4, we present experimental results on both artificial and real-world datasets. We conclude and suggest directions for future work in Section 5.

@&#RELATED WORK@&#

Extensive research has been performed in the areas of person appearance modeling [1–5], 3D body shape modeling [6–9] and pose estimation [8–16]. This section focuses on the work that we consider to be most closely related to our paper.

Modeling person appearance is most commonly done based on single view information. Color histograms representing the full extent of a person's appearance are often used [1,17], while more sophisticated methods split a single person's appearance up into several layers, incorporating some spatial information in the descriptor [18,19]. More recent methods make use of ensembles of different features to be more robust and cover different aspects of persons' appearance like color and texture information [2,3].

Viewpoint invariance is addressed by Gray et al. [4], where AdaBoost is used to learn a good set of spatial and color features for representing persons. Some approaches combine histograms created at multiple viewpoints (e.g. Liem and Gavrila [18]). Others try estimating the full body texture of a person by projecting the person's appearance onto a 3D structure like a cylinder (e.g. Gandhi and Trivedi [5]). These multi-view types of methods provide robustness to perception from different angles, while the use of full body textures also maintains the spatial properties of the appearance per view.

Since the human body shape is largely non-rigid, modeling body texture is not straightforward. As we will see in the experiments, mapping the texture of a non-rigid shape onto a rigid object as done by Gandhi and Trivedi [5] may result in instable textures over time, introducing artifacts into learned texture models and making it hard to accurately compare body textures over time. Using a more accurate estimate of the 3D object shape could help in creating more discriminative appearance models.

Some research has specifically aimed to generate a more accurate 3D reconstruction of the visual hull of objects, computed using shape-from-silhouette methods. Kutulakos and Seitz [20] compute an improved voxel model of the visual hull by coloring all voxels using the camera views and checking color consistency among cameras for each voxel, eliminating inconsistent voxels. Cheung et al. [21] present a method for aligning rigid objects in multiple time steps and reconstructing an object using information from multiple time instances. Translation and rotation are estimated by matching and aligning colored surface points over time. By transforming the camera viewpoints according to the estimated transformation, new virtual viewpoints are created extending the number of viewpoints usable for shape-from-silhouette methods.

Mitzel and Leibe [7] learn a 3D shape model of a person over time based on stereo depth data. Using the Iterative-Closest-Point (ICP) algorithm, the model is aligned with the stereo data at each time step enabling person tracking and updating the model.

An alternative approach is to estimate fully articulated 3D body pose, either by lifting 2D part-based body models [10,16] or by using 3D models directly [11,8,9]. This approach can in principle provide accurate 3D body shape and texture models, but is computationally expensive and is hard to apply in uncontrolled, complex environments (e.g. dynamic background, multiple persons).

Some work has been done on estimating the body facing direction of persons without estimating the full articulated pose. Several 2D single frame approaches combine orientation-specific person detectors [12–14], while work by Chen and Odobez [15] estimates body and head pose in batch mode, coupling the output of underlying classifiers. These methods offer an absolute orientation estimate with respect to some global reference frame. In Gandhi and Trivedi [5], texture sampled from a cylinder surrounding a person is shifted along the rotational axis in a generate-and-test fashion to find the best matching orientation.


                     SH have been used in order to perform face recognition under varying lighting conditions (e.g. Yue et al. [22]). Representing 3D objects by projecting them onto an SH basis has been researched mainly with respect to exemplar based 3D object retrieval. A collection of rotationally invariant 3D object descriptors has been compared by Bustos et al. [23]. Recently, an SH decomposition of a 3D extension of the HOG descriptor was presented by Liu et al. [24]. Makadia et al. [25] present a method for the registration of 3D point clouds that uses an SH representation of Extended Gaussian Images (EGI) to estimate the difference in orientation between point clouds. Several rotation invariant 3D surface shape descriptors have been compared by Huang et al. [6], for the purpose of finding matching poses in sequences with high quality 3D data.

@&#OVERVIEW@&#


                        Fig. 1
                         gives a schematic overview of the proposed procedure for person appearance modeling and orientation estimation. Appearance is modeled as the combination of a 3D shape model and a texture model. We take an intermediate approach between modeling a person using a fixed body shape and doing full articulated pose estimation. We estimate the largest rigid partition (core) of the body over time by regarding all non-rigid elements of the human body (arms, legs) as ‘noise’ around the rigid body core (torso-head).

The preprocessing step uses multi-camera data to compute a volumetric 3D reconstruction of the scene. A person detection and tracking system localizes the person of interest, and the volume corresponding to this person is segmented from the volume space. In the next step, an SH based shape measurement is created by using the segmented volume (Section 3.3.2). Furthermore, an SH based texture measurement is computed by using the learned shape model and Kalman filtered orientation estimate from the previous time step t-1 (Section 3.3.1). Using an EM-like optimization scheme, an optimized orientation measurement is computed by iteratively comparing the texture measurement to the learned texture model from t-1, and adjusting the shape model's orientation to compute an improved texture measurement (Section 3.3.3). When the orientation measurement has converged, it is used to correct the Kalman filtered orientation estimate from t-1, and the SH shape and texture models from t-1 are updated making use of the shape and texture measurements and the estimated orientation.

Our main contribution is a method for estimating a person's relative body orientation while simultaneously generating a basic model of the person's shape and texture. The estimate is made on a per-frame basis using an on-line learned appearance model consisting of low dimensional SH shape and texture representations. By using SH as a basis, orientation estimation can be performed elegantly, without the need of explicitly testing for different orientations and without a constraint on the maximum angular difference at successive image frames. The taking advantage of texture information and the SH formulation differentiates our approach from [7,25]. This paper is based on our earlier work [26].

In order to simplify the estimation of a person's orientation at time t based on a shape model 
                           
                              S
                              t
                           
                         and texture model 
                           
                              T
                              t
                           
                        , we represent both in the linear space of Spherical Harmonics (SH). SH are the equivalent of Fourier transformations on a 3D sphere; they decompose a spherical function f(u, v) (a function defined on the surface of a sphere) into a linear combination of orthonormal spherical basis functions. An SH subspace consists of L
                        +1 bands each characterized by l, 0≤
                        l
                        ≤
                        L, and M components (spherical basis functions) per band. For a certain band l there are M
                        =2l
                        +1 components m in the range [−
                        l, l]. If a 3D shape can be represented as a spherical function f
                        
                           S
                         by extruding a sphere along vertices (u, v), the SH decomposition of f
                        
                           S
                         can be seen as a linear combination of canonical shapes of which Fig. 2(a) shows the first 3 bands (9 components). Analogous, the SH decomposition of a texture represented by fT
                         as gray values at vertices (u, v) can be seen as a linear combination of textured spheres seen in Fig. 2(b).

The SH decomposition of a spherical function f computes SH parameters A, where A
                        
                           l,m
                         is the weight of component m in band l. SH components with m
                        <0 are rotated 90/m degrees around the vertical axis compared to their positive counterparts labeled by |m|, as can be seen in Fig. 2. It is therefore common practice to merge the positive and negative order harmonics A
                        
                           l,m
                         and A
                        
                           l,−
                           m
                         into a complex item 
                           
                              A
                              
                                 l
                                 ,
                                 m
                              
                           
                         as follows:
                           
                              (1)
                              
                                 
                                    
                                       A
                                       
                                          l
                                          ,
                                          m
                                       
                                    
                                    =
                                    
                                       A
                                       
                                          l
                                          ,
                                          m
                                       
                                    
                                    +
                                    i
                                    
                                       A
                                       
                                          l
                                          ,
                                          −
                                          m
                                       
                                    
                                    ,
                                 
                              
                           
                        where i is the imaginary unit. For m
                        =0, the imaginary part is 0. Since here 0≤
                        m
                        ≤
                        l, this leaves l
                        +1 parameters per band. A reconstruction 
                           
                              S
                              
                                 F
                                 A
                              
                           
                         of the original function f can be computed from the parameters 
                           A
                         as a weighted sum of the SH basis functions (much like the spectral components of a Fourier decomposition are used in the Fourier reconstruction). In practice we use a limited number of bands such that 
                           
                              S
                              
                                 F
                                 A
                              
                           
                         is a band limited approximation of f. More details on SH and how to decompose a spherical function into SH components can be found in [27].

An interesting property of SH is that they are rotationally covariant. This means that rotating a 3D object and then projecting it onto an SH subspace gives exactly the same result as if the object was first projected onto an SH subspace and then rotated. Furthermore, the properties of the spherical basis functions and the rules of orthogonality make SH rotation a linear operation in which components between bands do not interact. The change in 
                           
                              A
                              
                                 l
                                 ,
                                 m
                              
                           
                         is especially simple for rotations on the vertical axis that interest us the most. As suggested in [28], the vertical axis rotation over ϕ of a spherical function represented by SH can be represented simply as
                           
                              (2)
                              
                                 
                                    
                                       B
                                       
                                          l
                                          ,
                                          m
                                       
                                    
                                    =
                                    
                                       A
                                       
                                          l
                                          ,
                                          m
                                       
                                    
                                    
                                       e
                                       
                                          −
                                          imϕ
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Here 
                           
                              A
                              
                                 l
                                 ,
                                 m
                              
                           
                         is the weight of component m in band l before rotation, i is the imaginary unit and 
                           B
                        
                        
                           l,m
                         is the weight of component m in band l after rotation over angle ϕ. The non-complex SH parameters B
                        
                           l,m
                         and B
                        
                           l,−
                           m
                         can be computed by taking the real part and the imaginary part of the complex SH parameters, respectively. For notational simplicity, we will sometimes leave out the superscript l, m when describing the rotation of a complete SH object and use 
                           
                              A
                              ϕ
                           
                         to denote 
                           B
                        . In those cases, the SH rotation operator is assumed to be applied to all components l, m of 
                           A
                        .

In case an SH parametrization 
                           A
                         and 
                           B
                         of same object is given, the rotation ϕ between them can be found by minimizing the following sum squared error (SSE) for ϕ using a quasi-Newton approach like BFGS [28]:
                           
                              (3)
                              
                                 
                                    
                                       
                                          ∑
                                          
                                             1
                                             ≤
                                             l
                                             ≤
                                             L
                                          
                                       
                                       
                                    
                                    
                                    
                                       
                                          ∑
                                          
                                             1
                                             ≤
                                             m
                                             ≤
                                             l
                                          
                                       
                                       
                                    
                                    
                                       
                                          
                                             
                                                A
                                                
                                                   l
                                                   ,
                                                   m
                                                
                                             
                                             
                                                e
                                                
                                                   −
                                                   imϕ
                                                
                                             
                                             −
                                             
                                                B
                                                
                                                   l
                                                   ,
                                                   m
                                                
                                             
                                          
                                       
                                       2
                                    
                                    .
                                 
                              
                           
                        
                     

The 0th component of every band is not needed for this estimation since it is not influenced by rotation.

At each time step t, C images I
                        
                           t
                        
                        1:
                           C
                         are captured from C different, overlapping viewpoints. In the experiments, C
                        =3. Foreground estimation is done for each image and a volumetric reconstruction of the scene is created by using volume carving. Person detection and tracking is performed in the reconstructed volume space, using the RCTA+ method described in [29]. This method divides the volume space into individual person hypotheses and performs tracking and detection by solving the assignment problem between known person tracks and individual hypotheses for each time step. The visual hull 
                           H
                         of the person of interest is determined by selecting all voxels within a 1m diameter cylinder, positioned at the person's location estimated by RCTA+. The measured person position 
                           
                              
                                 
                                    x
                                    ^
                                 
                                 t
                              
                           
                         is the center of mass of 
                           H
                         and is computed as the center of the principal axis of 
                           H
                        . A constant acceleration Kalman filter is used to filter 
                           
                              
                                 
                                    x
                                    ^
                                 
                                 t
                              
                           
                         over time and gives the filtered person position x
                        
                           t
                        .

In order to use SH to model the person's shape, 
                           H
                         is transformed into spherical shape function f
                        
                           S
                        . To create f
                        
                           S
                        , a unit sphere is centered on the person location x
                        
                           t
                         and the sphere's surface is discretized into a uniformly distributed set of surface points (in our experiments we use 55×55 surface points). For each surface point (u, v), a ray r is cast from x
                        
                           t
                        , through (u, v). The spherical function is defined by f
                        
                           S
                        (u, v)=
                        d, where d is the distance between x
                        
                           t
                         and the most distant voxel in 
                           H
                         along r. This is similar to the method described in [30] except that, in order to maintain rotational information, we do not normalize the 3D shape. To compensate for the fact that volume carving tends to overestimate the shape, the spherical function is scaled down 10% to prevent sampling of the texture outside the object during texture generation.

A sampled texture consists of a spherical texture function f
                        
                           T
                        , created by projecting the surface points of a shape function f
                        
                           S
                         onto images I
                        
                           t
                        
                        1:
                           C
                         and sampling the image values at these locations. Texture is only sampled from image regions containing foreground and the color of surface points visible in multiple cameras is averaged over these cameras.

The goal is to model the person's appearance, consisting of an SH shape model 
                           
                              S
                              t
                           
                         and an SH texture model 
                           
                              T
                              t
                           
                        , and estimate the person's orientation ϕ
                        
                           t
                         (rotation along the vertical axis) based on the estimated appearance.


                        Algorithm 1 describes the SH based appearance modeling and orientation estimation method. The different parts of this method will be explained in the following sub-sections.

First, a constant acceleration Kalman filter is used to get a prediction ϕ
                           
                              t
                           
                           − of person orientation ϕ
                           
                              t
                            based on ϕ
                           
                              t
                              −1. Using the shape model 
                              
                                 S
                                 
                                    t
                                    −
                                    1
                                 
                              
                            from the previous time step, the SH texture measurement 
                              
                                 
                                    
                                       T
                                       ^
                                    
                                    t
                                 
                              
                            at time t is computed and the person's orientation ϕ
                           
                              t
                            is estimated by using this measurement.
                              Algorithm 1
                              Appearance modeling and orientation estimation.
                                    
                                       
                                    
                                 
                              

Texture function f
                           
                              T
                            at t is created by using a rotated spherical shape function 
                              
                                 S
                                 
                                    F
                                    
                                       S
                                       ϕ
                                    
                                 
                              
                            reconstructed from the SH shape model 
                              
                                 S
                                 
                                    t
                                    −
                                    1
                                 
                                 ϕ
                              
                           , acquired by rotating 
                              
                                 S
                                 
                                    t
                                    −
                                    1
                                 
                              
                            using the predicted orientation ϕ
                           
                              t
                           
                           − as follows:
                              
                                 (4)
                                 
                                    
                                       
                                          S
                                          
                                             t
                                             −
                                             1
                                          
                                          ϕ
                                       
                                       =
                                       
                                          S
                                          
                                             t
                                             −
                                             1
                                          
                                       
                                       
                                          e
                                          
                                             im
                                             
                                                ϕ
                                                t
                                                −
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           See also line 9 of Algorithm 1. The exponent in Eq. (4) has been negated in order to rotate 
                              
                                 S
                                 
                                    t
                                    −
                                    1
                                 
                              
                            with negative orientation angle −
                           ϕ
                           
                              t
                           
                           −.

When positioned at x
                           
                              t
                           , 
                              
                                 S
                                 
                                    F
                                    
                                       S
                                       ϕ
                                    
                                 
                              
                           's discretized surface points can be used to sample the person's texture from images I
                           
                              t
                           
                           1:
                              C
                           . Texture regions without color information due to occlusions or sampling outside the foreground regions are filled by using the average color along horizontal scanning lines over the texture. This way, artifacts in the SH texture representation due to lacking information are prevented while vertical color variance is maintained in the texture. 
                              
                                 
                                    
                                       T
                                       ^
                                    
                                    t
                                 
                              
                            is computed by projecting f
                           
                              T
                            onto a 9 band SH subspace (L
                           =8), reducing the dimensionality of the texture space by 97% and smoothing the texture. To use RGB color, the three color channels λ={R, G, B} are modeled as three separate SH.

Since rotation of the SH texture is simplified according to Eq. (2), finding the most likely orientation 
                              
                                 
                                    
                                       ϕ
                                       ^
                                    
                                    t
                                 
                              
                            given the SH texture model from the previous time step 
                              
                                 T
                                 
                                    t
                                    −
                                    1
                                 
                              
                            and the current texture measurement 
                              
                                 
                                    
                                       T
                                       ^
                                    
                                    t
                                 
                              
                            is straightforward and can be solved by minimizing the SSE in Eq. (5) for 
                              
                                 
                                    
                                       ϕ
                                       ^
                                    
                                    t
                                 
                              
                            using a quasi-Newton approach like BFGS [28].
                              
                                 (5)
                                 
                                    
                                       
                                          
                                             ∑
                                             λ
                                          
                                          
                                       
                                       
                                       
                                          
                                             ∑
                                             l
                                          
                                          
                                       
                                       
                                       
                                          
                                             ∑
                                             m
                                          
                                          
                                       
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         T
                                                         ^
                                                      
                                                      t
                                                      
                                                         l
                                                         ,
                                                         m
                                                         ,
                                                         λ
                                                      
                                                   
                                                
                                                
                                                   e
                                                   
                                                      −
                                                      im
                                                      
                                                         ϕ
                                                         ^
                                                      
                                                   
                                                
                                                −
                                                
                                                   T
                                                   
                                                      t
                                                      −
                                                      1
                                                   
                                                   
                                                      l
                                                      ,
                                                      m
                                                      ,
                                                      λ
                                                   
                                                
                                             
                                          
                                          2
                                       
                                    
                                 
                              
                           
                        

A Kalman filter update of ϕ
                           
                              t
                           
                           − using 
                              
                                 
                                    ϕ
                                    ^
                                 
                                 t
                              
                            gives the final orientation ϕ
                           
                              t
                           .

The texture model 
                              
                                 T
                                 t
                              
                            is learned over time by exponential decay using learning rate 
                              
                                 α
                                 T
                              
                           :
                              
                                 (6)
                                 
                                    
                                       
                                          T
                                          t
                                       
                                       =
                                       
                                          
                                             1
                                             −
                                             
                                                α
                                                T
                                             
                                          
                                       
                                       
                                          T
                                          
                                             t
                                             −
                                             1
                                          
                                       
                                       +
                                       
                                          α
                                          T
                                       
                                       
                                          
                                             
                                                T
                                                ^
                                             
                                             t
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

However, to prevent over representation of the first sampled textures in 
                              
                                 T
                                 t
                              
                           , iterative averaging is used to learn 
                              
                                 T
                                 t
                              
                            as long as 
                              
                                 t
                                 <
                                 
                                    
                                       1
                                       
                                          α
                                          T
                                       
                                    
                                 
                              
                           , as shown in Eq. (7).
                              
                                 (7)
                                 
                                    
                                       
                                          T
                                          t
                                       
                                       =
                                       
                                          
                                             t
                                             −
                                             1
                                          
                                          t
                                       
                                       
                                          T
                                          
                                             t
                                             −
                                             1
                                          
                                       
                                       +
                                       
                                          1
                                          t
                                       
                                       
                                          
                                             
                                                T
                                                ^
                                             
                                             t
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Combining models and measurements over time requires each measurement to be rotated into a canonical orientation, matching the model. By sampling the texture using the rotated shape model 
                              
                                 S
                                 
                                    t
                                    −
                                    1
                                 
                                 ϕ
                              
                           , 
                              
                                 
                                    T
                                    ^
                                 
                                 t
                              
                            is in a canonical orientation and can directly be combined with 
                              
                                 T
                                 
                                    t
                                    −
                                    1
                                 
                              
                           . An example of a sampled texture, its SH reconstruction and a learned texture after 100 frames can be found in Fig. 3
                           .

Using orientation ϕ
                           
                              t
                           , the SH shape model 
                              
                                 S
                                 t
                              
                            can be computed. First, the SH shape measurement 
                              
                                 
                                    
                                       S
                                       ^
                                    
                                    t
                                 
                              
                            at time t is constructed by transforming visual hull 
                              H
                           
                           
                              t
                            into a spherical function f
                           
                              S
                            and projecting this function onto the SH space. To reduce feature dimensionality and simultaneously smooth the shape representation, the first 17 bands of the SH space (L
                           =16) are used for projection, reducing feature dimensionality by 90%. Examples of a person's visual hull, the spherical function derived from that visual hull and the reconstruction of the SH representation of the spherical function can be seen in Fig. 4(a)–(c). The top-down views showing the person facing to the left clearly show the excess volume created by volume carving due to shape ambiguities. While Fig. 4(e) shows that, in this frame, the person stands straight with his arms alongside his torso, the top-down view of the 3D volume shows significant extrusions of the estimated shape on the front and back of the person.

Estimating the rigid body over time is done by averaging shape estimates over time, decreasing the influence of non-rigid body parts on the estimated body shape. Since arms and legs will have different positions over time, they will be averaged out in the final shape estimate as can be seen in Fig. 4(d). To combine shape models and measurements, they have to be in canonical orientation. Since the shape is represented in SH components, rotation is straightforward (as mentioned in Section 3.2) and does not yield rotation artifacts which would occur when rotating objects in the discrete volume space consisting of cubic voxels in a regular grid. The rotated SH shape measurement 
                              
                                 
                                    
                                       S
                                       ^
                                    
                                    t
                                    ϕ
                                 
                              
                            is computed as shown in Eq. (8) (line 27 in Algorithm 1), in accordance with (2):
                              
                                 (8)
                                 
                                    
                                       
                                          
                                             
                                                S
                                                ^
                                             
                                             t
                                             ϕ
                                          
                                       
                                       =
                                       
                                          
                                             
                                                S
                                                ^
                                             
                                             t
                                          
                                       
                                       
                                          e
                                          
                                             −
                                             im
                                             
                                                ϕ
                                                t
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

The rigid body shape model 
                              
                                 S
                                 t
                              
                            at time t is computed over time by iteratively combining 
                              
                                 
                                    
                                       S
                                       ^
                                    
                                    t
                                    ϕ
                                 
                              
                            for all time steps under exponential decay, as shown in Eq. (9)
                           
                              
                                 (9)
                                 
                                    
                                       
                                          
                                             
                                                S
                                                0
                                             
                                             =
                                             
                                                
                                                   
                                                      S
                                                      ^
                                                   
                                                   0
                                                
                                             
                                             ;
                                          
                                       
                                       
                                          
                                             
                                                S
                                                t
                                             
                                             =
                                             
                                                
                                                   1
                                                   −
                                                   
                                                      α
                                                      S
                                                   
                                                
                                             
                                             
                                                S
                                                
                                                   t
                                                   −
                                                   1
                                                
                                             
                                             +
                                             
                                                α
                                                S
                                             
                                             
                                                
                                                   
                                                      S
                                                      ^
                                                   
                                                   t
                                                   ϕ
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The shape learning rate, 
                              
                                 α
                                 S
                              
                           , is split up in two parts: one for growing the model and one for shrinking it. Because volume carving reconstructions are convex, artifacts in the reconstruction are much more likely to consist of extrusions of the actual shape than of indentations. Therefore, measuring an indentation in 
                              
                                 
                                    
                                       S
                                       ^
                                    
                                    t
                                    ϕ
                                 
                              
                            compared to 
                              
                                 S
                                 
                                    t
                                    −
                                    1
                                 
                              
                            conveys more information about the actual object shape than when measuring an extrusion. This is reflected in the learning rates. The learning rate for surface points that are more indented in 
                              
                                 
                                    
                                       S
                                       ^
                                    
                                    t
                                    ϕ
                                 
                              
                            than in 
                              
                                 S
                                 
                                    t
                                    −
                                    1
                                 
                              
                            is set to 0.4. The learning rate for more extruded surface points is set to 0.01. In Fig. 4(a), a small artifact is visible near the head of the person. While this artifact gives rise to an erroneous extrusion in the spherical function (Fig. 4(b)) and its SH representation (Fig. 4(c)), the learned body model in Fig. 4(d) shows no influence of this artifact. The figure furthermore shows that the model was able to learn a good approximation of the actual person's volume.

The estimate of the current body orientation made by using Eq. (5) allows for an iterative orientation optimization scheme. Since the texture measurement 
                              
                                 
                                    
                                       T
                                       ^
                                    
                                    t
                                 
                              
                            is sampled by using the reconstruction of 
                              
                                 S
                                 
                                    t
                                    −
                                    1
                                 
                              
                           , oriented according to the Kalman filter prediction ϕ
                           
                              t
                           
                           − instead of the final estimate ϕ
                           
                              t
                           , the sampled texture might be distorted.

In order to optimize the estimation of the person orientation, texture sampling and orientation estimation are repeated multiple times. Each time, the reconstruction of 
                              
                                 S
                                 
                                    t
                                    −
                                    1
                                 
                              
                            is rotated by using the most likely orientation estimate 
                              
                                 
                                    
                                       ϕ
                                       ^
                                    
                                    t
                                 
                              
                            of the previous iteration. While the estimate gets closer to the true orientation, the SSE gets smaller and the difference between orientation estimates reduces. Optimization is stopped when the difference between consecutive orientation estimates is less than 0.001rad or 10 iterations have been done. In Fig. 4, red lines on the ground plane show the resulting orientation estimate on a single frame, while green lines show the ground truth orientation.


                           
                              
                                 S
                                 
                                    t
                                    −
                                    1
                                 
                              
                            could be updated every iteration by using 
                              
                                 
                                    
                                       S
                                       ^
                                    
                                    t
                                    ϕ
                                 
                              
                           , rotated by the previous iteration's estimate of 
                              
                                 
                                    
                                       ϕ
                                       ^
                                    
                                    t
                                 
                              
                           . However, while the orientation estimate is suboptimal, an incorrectly rotated version 
                              
                                 
                                    
                                       S
                                       ^
                                    
                                    t
                                    ϕ
                                 
                              
                            might result in a more distorted shape estimate instead of an improved estimate.

@&#EXPERIMENTS@&#

We evaluate how the SH texture representation, shape information and the iterative estimation procedure influence the quality of the estimated orientation. To this end, our method is compared to a state-of-the-art approach as well as to an alternative method. The first method is based on the Panoramic Appearance Map (PAM) [5]. A fixed size cylinder with a diameter of 35cm and a height of 2m, fitting tightly around a person's torso, is used for sampling the person's texture. This is much smaller than the 1m diameter cylinder used for segmenting the person of interest from the volume space mentioned in Section 3.3, but while the volume segmentation should contain all voxels belonging to the reconstructed person, the PAM cylinder should capture as little background as possible to generate stable textures. Orientation is estimated in a generate-and-test fashion by sampling 360 textures using 1° orientation difference and making a per-pixel weighted comparison between all textures and a learned texture model. The orientation of the best matching texture sample is used as the object orientation.

As a secondary, alternative method, we combine the cylinder based shape model with our SH based texture representation and use the iterative orientation estimation from Section 3.3.3. This method will be referred to as ‘cylinder shape with SH texture’. Like our method, both PAM and the cylinder shape with SH texture provide relative orientation estimates with respect to the initial person orientation.

The following settings are used for the experiments. Textures are sampled in standard RGB colorspace. Preliminary experiments were done using both normalized RGB and C-invariant color spaces [31], but the overall best results were obtained using standard RGB. The voxelspace has a resolution of 2×2×2cm/voxel. Textures and shapes are sampled at a resolution of 55×55 surface points (3025 parameters) and a texture learning rate 
                        
                           
                              α
                              T
                           
                           =
                           0.05
                        
                      is used. For the methods using SH based textures 9 SH bands (81 parameters) are used for texture representation. For the SH shape representation 17 SH bands (289 parameters) are used. For all methods, each frame's orientation estimate is constrained to be within 30° of the previous frame's estimate.

Experiments are done on three artificial datasets as well as on a real-world dataset. Doing experiments on artificial data allows us to test different aspects of the algorithms such as the influence of shape estimation and the use of SH for the texture representation.

As a measure of the orientation estimation performance, the Root Mean Squared (RMS) error between the ground truth orientation and the estimated orientation is used. The difference in orientation is computed by using the angular difference between the two orientation angles. This ensures a correct error computation considering angular wrap-around (0°=360°).

The first artificial dataset uses a pre-generated 2D texture image, shown in Fig. 5(a), in order to test the performance of texture based orientation estimation. Using a fixed texture ensures that the measurements are not influenced by inaccurate shape estimates and the texture sampling method. A texture of 360×360pixels is used, allowing for 1° rotations. Rotation is simulated by circular shifting of this texture a number of pixels to the right at each time step. For two runs of the experiment a static black area is added to simulate a static sampling occlusion (e.g. a part of the object not seen by any camera), shown as a vertical blacked-out bar in Fig. 5(a). The orientation estimation methods treat this part of the texture as if no information is available for this region. Orientation is estimated making use of a 55×55pixel resized version of the texture to simulate sampling. One run is done with a fully visible texture and 1° rotation per iteration. Two runs are done with the occluded texture using either 1° or 7° rotation per iteration. Since shape has no role in this experiment, we only compare between PAM making use of generate-and-test based orientation estimation, and the SH texture making use of our iterative orientation estimation.


                        Fig. 6(a) shows the RMS error per frame for the different scenarios and methods. When using the fully visible texture, estimation results are as good as perfect. As is to be expected, the generate-and-test based method (not shown in the graph) does a perfect job here. The lightly jagged profile of the error of the SH based method on this texture is an effect of the fact that the downsampled texture is used to estimate 1° orientation shifts. When the original 360pixel wide texture is shifted 1pixel per degree, the changes in the downsampled texture (shifted about 0.15pixels per degree) are very small. The SH based method does not pick up on this shift until the downsampled texture is shifted 1pixel.

When a part of the texture is blacked-out, the SH based method is outperformed by the generate-and-test based method. The SH based texture model takes slightly longer to learn, causing a higher error for the first few frames. Tests with an increasing number of SH texture components produce similar results, suggesting that the SH texture combined with the iterative orientation estimation procedure is less accurate than the generate-and-test based method when the estimation task is this much simplified. After the initial offset in error, both methods show a similar gradual increase in the orientation error over time. This is an artifact of doing relative orientation estimation. The use of an incrementally updated texture model as the reference makes it hard to compensate a drift in the estimated orientation over time. Drifting estimates are caused by inaccurate texture matches due to the blacked-out patch, combined with a learning rate that includes this shift into the model.

The second artificial dataset shows a rendered scene containing a textured, rotating cylinder (Fig. 5(b)). For the last artificial dataset, a scene showing a rotating textured human model is rendered (Fig. 5(c)–(d)). The camera viewpoints and calibration for these scenes are the same as for the real-world data, as is the image resolution of 752×560pixels. In all scenes, the object is kept positioned at one location in the scene. For both the rendered cylinder and the human models, sequences are generated by using either 1° or 7° rotation per frame. In one cylinder sequence, the cylinder incrementally stands still, rotates left, stands still and rotates right for 50 frames each. In the last human model sequence, the model is rotated 1° per frame while the limbs show walking motion (Fig. 5(d)).

The system setup for the methods compared is the same as for the real-world experiments, except for the person tracking and detection part. Since foreground segmentation is provided and perfect, the volume reconstruction of the rendered scene only contains the reconstruction of the object of interest. Therefore, the object location is computed as the center of the principle axis of the reconstructed scene. In the cylinder sequences and the first two human sequences, the assumption of a rigid body is fully met. In the last human sequence, rigidity only holds for the torso and head. This allows evaluation of the influence of shape estimation under the rigid body assumption.


                        Fig. 6(b) shows the RMS error per frame for the rendered cylinder data as well as for the human model data. For the cylinder scenario, the SH texture with cylindrical shape model benefits from its matching shape model. No time is needed for the methods to adapt to the object shape, resulting in well sampled textures from the first frame on. PAM also has this benefit, but suffers from texture sampling artifacts resulting in a higher error rate. The low dimensional SH texture is able to learn a more general texture model, resulting in better performance. For this scenario, our method has a drawback from having to estimate the object shape, resulting in lower performance than the SH texture with cylindrical shape model, but it still performs on-par with PAM.

Like in the previous experiment, a gradual increase in error is visible for all methods for both the cylinder shape scenarios as well as the human shape scenarios. Picking the correct learning rate for the shape and texture model can compensate for this to some degree, but over time the error is likely to converge toward the random error.

For the human model, and specifically the articulated human model, the cylindrical assumption does not hold. It causes continuous distortions in the texture, giving rise to drifting orientation estimates. While the need to learn the shape of the object in our method seems to be a drawback for simple shapes, experiments on the more complicated human shape model show the benefit of modeling the object shape together with the texture. Our full SH model shows similar performance on the cylindrical shape and the human shape, but the methods using the cylindrical shape model have a clear decline in performance for the more complex object. The shape estimation still takes time to adapt to the shape, but in the end gives a more stable shape model to sample the texture from.

The real-world data consists of about 2800 frames, distributed over 12 scenarios recorded in an open, outdoors environment with uncontrollable illumination. Three cameras with fully overlapping views are used recording at 20Hz. While some scenarios feature multiple persons, orientation estimation is only performed for one person. The nine different persons involved in the scenes can be found in Fig. 7(a)–(i), while Fig. 7(j) shows sample frames from three scenarios and all camera viewpoints.

Ground-truth (GT) for the real-world data is created by annotating 17 distinctive locations (joint and head positions) on the body of each person of interest in each camera. A fully articulated 3D person model is fitted onto these annotated points by using least-squares optimization. The model's torso orientation is taken to be the person body facing direction. Doing this for a regular interval of frames results in a positional accuracy of about 4cm.

For the experiments on this dataset a third, classifier based orientation estimation method is added. This method, introduced by [13], uses a classifier combining HOG-based features to get an absolute orientation estimate per frame, with respect to a fixed 3D coordinate system. It is complementary to our method, using a fundamentally different way of orientation estimation, and is added as a reference for absolute orientation estimation methods. It uses a mixture of four orientation experts, each trained on images showing pedestrians in one of four canonical orientations (front, back, left, right). The experts' classification results are used as weights in a Gaussian Mixture Model, creating a full 360° orientation probability density function (pdf). A maximum likelihood estimate derived from this pdf is used as the final orientation estimate. The experts are trained making use of data kindly provided by the authors of [13]. We generate regions of interest in the 2D images based on the projection of the volume reconstruction of the person of interest onto the camera plane. Orientation estimates per camera are combined into a 3D orientation estimate using the camera calibration. Applying the constraint on the maximum difference in estimated orientation between consecutive frames to the classifier based approach is done by limiting the range of the pdf when computing the maximum likelihood orientation. Since the classifier based approach is time independent by nature, we also test this method without applying the 30° orientation constraint. For both versions, the results have been Kalman filtered resulting in more stable and smooth orientation estimates over time.

Finally, results of the version of our method as presented in [26] are added for comparison. This version uses manually generated ‘optimal’ foreground segmentations to create the volume reconstruction and does not make use of RCTA+ tracking results. Results from this method are referred to as ‘SH shape and texture (manual fg)’.


                        Fig. 8
                         shows how the number of SH bands used influences orientation estimation performance for our method. Orientation estimation has been performed for all scenarios, using 5, 9, 13, 17 or 25 bands for both the SH texture and the SH shape (L ∈ {4, 8, 12, 16, 24}). This results in 25 performance measurements. Using a small number of bands has a clear negative impact on performance. This is shown by the blue line (only 5 shape bands) and the first part of the green and red lines (only 5 texture bands). However, a comparison with the cyan and magenta lines shows that using less bands for shape has a larger impact in performance than using less bands for texture. This is due to the fact that the texture depends on the shape for sampling, and gets more distorted when using less shape SH. For higher numbers of bands (17, 25), adding extra bands has limited influence on performance since the extra details modeled by the SH representation get decreasingly relevant. For the rest of the experiments 9 texture SH bands and 17 shape SH bands are used, balancing performance and computational efficiency. Using more bands results in a slight increase in performance at the cost of much more parameters (e.g. using 25 instead of 17 bands results in 625 instead of 289 parameters).


                        Fig. 9
                         shows the RMS error per frame for the real-world data, computed over all scenarios. This differs from the results shown in Fig. 4(a) in [26] where the moving RMS error over time was used, giving a more smoothed view of the performance. Since not all sequences have the same length, vertical dashed lines indicate the points where scenarios end and the RMS error is taken over fewer scenarios. Please note that since our method, PAM and the cylinder shape with SH texture all provide relative orientation estimates w.r.t. the first frame, their error is defined to be 0 at that point. Because the classifier based approach gives an absolute orientation estimate for each frame, it also shows an error for the first frame.

Measured over all scenarios, our method shows better performance compared to the other relative orientation methods for almost all frames. Only the version of our method from [26] performs better, which is expected because of the ‘optimal’ foreground segmentations. In the first 40 frames the error rises quickly because of sub-optimal shape and texture models. After this, the error for our method seems to stabilize while the other errors keep rising. A comparison with the method using the cylinder together with SH texture shows that our method benefits from modeling the shape of the person. Representing the texture in low-dimensional SH space gives a comparable performance to PAM's full texture approach while the 9 SH bands reduce the number of features by 97%. However, when the person being tracked has a low-contrast appearance like the one shown in Fig. 7(e), the SH texture lacks detail.

The relative orientation estimation methods using a fixed shape model exhibit a significantly stronger error increase over time. This effect is similar to the one seen in the experiments using the synthetically rendered human model. The incorrect shape model causes more noise in the sampled texture, causing a larger deviation in the estimated orientation over time. Around frame 350, a jump in the RMS error for PAM and the cylinder shape with SH texture is visible. This is caused by the ending of the scenario with the person shown in Fig. 7(h), who's repetitive shirt pattern poses an issue for PAM.

Directly comparing results between the classifier based, absolute estimation methods and the relative estimation methods is difficult, because of the different nature of these methods. In the short term, the frame-by-frame absolute orientation estimates show a bit more erratic behavior. The error spikes show the sensitivity of the classifier based method to orientation ambiguities between opposite orientations (errors of about 180°), caused by the ambiguity in person shape when viewed from the front or the back. In the long term however, the absolute nature of the method makes it invulnerable to drifting of the orientation estimate. For the relative methods, the error is likely to converge to the random error over time since they lack a fixed reference point. Constraining the orientation difference per frame for the classifier based method dampens the erratic behavior and seems to result in slightly more stable orientation estimates.


                        Fig. 10
                         shows each method's cumulative error distribution, obtained by binning the errors over all frames. Our method clearly outperforms the other relative estimation methods with respect to the error distribution. Only our method using manually created foreground segmentations shows better performance. This graph confirms that PAM has the poorest error rates over all. The cylinder shape and SH texture method specifically shows less mid-range errors between 30° and 100° compared to PAM, while our method consistently shows less large errors than the other two relative estimation methods. The graph also confirms the classifier approaches' sensitivity to errors around 180°, shown by the final increase of the graph from 140° onwards. This matches the results from Fig. 6 of [13].

As a final measure, the RMS error measured over all scenarios and all frames together is computed for all methods. These results can be found in Table 1
                        . When comparing the relative orientation estimation methods, the method using manual foreground segmentations performs best. However, using tracking results instead of manually created foreground segmentations only gives a slight performance loss of 6°. Of the methods using real tracking results, our method shows the lowest average error, followed by the cylinder shape with SH texture with a 16° higher RMS error. These results show that relative orientation estimation benefits from the low dimensional SH texture representation and shape modeling. For the classifier based approaches, the constrained version shows the lowest error.

An example of the orientation estimation performance on a frame-to-frame basis in one scenario is shown in Fig. 11
                        . In order not to clutter the graph too much, we only show the constrained classifier results here, since it gave the most stable results in the previous experiments. The graph demonstrates that our method follow the GT orientation closely, while both PAM and the cylinder with SH texture method drift away from the GT. Around frame 40, the person jumps up and down and bends over, resulting in a bad match between the cylindrical shape model and the actual person shape. This causes a drift in the estimated orientation for PAM, which cannot be compensated. While PAM still follows the curve of the GT orientation for the rest of the scene, the offset remains. The SH texture model shows more robustness to this deformation, resulting in a much smaller drift for the cylinder model with SH texture. Around frame 110, the tracked person is occluded in one of the cameras causing a slight drift in orientation for the cylinder shape with SH texture model. Our method shows more robustness to this occlusion. The method using manually created foreground segmentations follows the GT a bit more closely than our method for most of the scene, but does not show significantly better performance. The classifier based approach does a reasonable job following the GT, but its frame-wise estimation approach causes a bit more erratic orientation estimates resulting in less accurate Kalman filtered results.

All experiments were done on a single core of a 2.6Ghz Intel Xeon CPU. Average processing times were: 9.5s/fr for our method, 1s/fr for PAM, 1.7s/fr for the cylinder with SH texture and 0.7s/fr for the classifier based approach. Volume carving and computation of the spherical shape function took about 8s of the 9.5s our method needs per frame, using a crude implementation. A large speed improvement is possible by using a GPU implementation. The classifier based approach was implemented in C++, while the other methods contain unoptimized Matlab code.

@&#CONCLUSION@&#

We presented a novel approach for estimating the relative person orientation, based on a low dimensional shape and texture representation using Spherical Harmonics; this involves a reduction in the number of appearance modeling parameters by 90–97%. Results on synthetic data show that, when the object shape is rigid and known in advance, fixed shape models combined with a learned SH texture model offer the best orientation estimation performance. However, when the object shape is not entirely rigid or precisely known, like in the case of a person's body, the addition of a learned SH shape model is beneficial.

Results could be improved by using more advanced inference methods like particle filters instead of the maximum likelihood approach employed here. The SH based orientation estimation could be extended to estimate the object's rotation along all axis. Finally, the fusion of our relative orientation estimates with classifier based absolute orientation estimates might further improve performance.

@&#ACKNOWLEDGMENTS@&#

The authors would like to thank Leo Dorst from the University of Amsterdam for proof reading the paper. This research has received funding from the EC's Seventh Framework Programme under grant agreement number 218197, the ADABTS project.


                     
                        
                           
                              Description of supplementary videos.
                           
                           
                        
                     
                     
                        
                           
                              Supplementary video showing orientation estimation performance on scenario 8.
                           
                           
                        
                     
                     
                        
                           
                              Supplementary video showing orientation estimation performance on scenario 20.
                           
                           
                        
                     
                  

Supplementary data to this article can be found online at http://dx.doi.org/10.1016/j.imavis.2014.04.007.

@&#REFERENCES@&#

