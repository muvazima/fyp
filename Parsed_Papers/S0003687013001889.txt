@&#MAIN-TITLE@&#Human performance under two different command and control paradigms

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A simulated command and control task was performed thirty times by two teams who were structured differently.


                        
                        
                           
                           Due to the inherent instability of the human/system interaction a strong differential learning effect took place.


                        
                        
                           
                           The hierarchical command and control team were faster but less accurate than the network enabled team.


                        
                        
                           
                           The common ‘one shot’ cross-sectional psychology experiment would not have detected these differences.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Command and control

System design and evaluation

Time series analysis

@&#ABSTRACT@&#


               
               
                  The paradoxical behaviour of a new command and control concept called Network Enabled Capability (NEC) provides the motivation for this paper. In it, a traditional hierarchical command and control organisation was pitted against a network centric alternative on a common task, played thirty times, by two teams. Multiple regression was used to undertake a simple form of time series analysis. It revealed that whilst the NEC condition ended up being slightly slower than its hierarchical counterpart, it was able to balance and optimise all three of the performance variables measured (task time, enemies neutralised and attrition). From this it is argued that a useful conceptual response is not to consider NEC as an end product comprised of networked computers and standard operating procedures, nor to regard the human system interaction as inherently stable, but rather to view it as a set of initial conditions from which the most adaptable component of all can be harnessed: the human.
               
            

@&#INTRODUCTION@&#

This paper is motivated by a number of intriguing observations made during a large-scale simulated command and control exercise (Walker et al., 2009; Stanton et al., 2009). The exercise had the explicit aim of testing a new method of collaborative working supported by a networked ‘infostructure’ commonly referred to as Network Enabled Capability (NEC; e.g. Ferbrache, 2003; Alberts, 2003; Alberts et al., 1999). When the subsequent task analysis was scrutinised it was found that this socio-technical system was exhibiting unusual behaviour. Over time it became progressively more preoccupied with the ‘means’ to achieve a given end (the ‘process’) rather than the massing of objectives or ‘end states’ (the ‘output’). The latter focus on outputs is what is normally expected from NEC and what was expected in this case. Despite the provision of a networked information infrastructure, individuals and teams either used it in unpredictable ways or else adopted more familiar and presumably easier methods of working. To paraphrase Clegg (2000), what was witnessed were “people interpreting the system, amending it, massaging it and making such adjustments as they saw fit and/or were able to undertake” (p. 467). Paradoxically, what was designed to be a highly rational operation end up growing quite irrational (Ritzer, 1993, p.22). Experience over centuries of military command and control (e.g. Regan, 1991) make it possible to go further; the sociotechnical infrastructure put in place to manage large, complex, dynamic resource systems such as these can, if not designed correctly, actively create inefficiency (instead of efficiency), unpredictability (instead of predictability), incalculability (instead of calculability) and a complete loss of control (Ritzer, 1993; Trist and Bamforth, 1951). These are the antithetical problems, ironies, productivity paradoxes and ‘irrationalities of rationality’ (Ritzer, 1993) that, when all else fails – as in this case – fall into the domain of Applied Ergonomics.

On closer inspection findings such as these are common, both in the field of military command and control and more generally in the sociotechnical literature. In the former case several studies have observed sub-optimal performance in terms of performance time (or the so-called Observe, Orient, Decide, Act loop; Stanton et al., 2009), task accuracy (or more specifically fratricide/friendly fire;Rafferty et al., 2012), not to mention overall system effectiveness. The UK's nascent NEC capability has already been the subject of a high profile parliamentary inquiry due to £4.7 bn of expenditure failing to translate into more effective command and control (House of Commons, 2007). The wider sociotechnical literature presents an interesting counterpoint. It abounds with examples of favourable ‘joint optimisation’ of people and management infrastructures (e.g. Walker et al., 2008; Teram, 1991; Trist, 1978; Davis, 1977), demonstrating the contribution a user-centred approach to organisational design can make. Indeed, if military operations really are as enormously complex as commentators feel, and complexity theory is the appropriate response, then by extension command and control should organise best from the bottom-up (Cebrowski and Gartska, 1998, p. 4–5). In other words, the component in these systems best able to cope with complexity is the humans. This creates a different interpretation of the results that motivate this study.

Drawing from the emerging world of networked technologies such as the internet (the world from whence NEC concepts seem to have been derived in most cases) it is possible to discern powerful new trends whereby this form of human adaptability, far from being commanded and controlled out of existence, is instead actively exploited. From the sublime (e.g. the Human Genome Project) to the ephemeral (e.g. Facebook), both are networked, highly distributed systems embodying the diffuse non-linear causality of peers influencing peers (Kelly, 1994; Tapscott and Williams, 2007; Viegas et al., 2007). These are entities where the boundary between designers and users has become “highly blurred, highly permeable, or non-existent” (Scacchi, 2004, p. 6–7). Under these ‘initial conditions’ highly effective and agile forms of organisational infrastructure have ‘emerged’ rather than been created. To use Toffler's (1981) or Tapscott and William's (2007) phraseology, the participants in the motivating case study behaved rather like ‘prosumers’, individuals who see the ability to adapt, massage, cajole and generally ‘hack’ a new technology as a birth right (p. 32). In the Ergonomics world Shorrock and Straeter (2006) remind us that people are still needed in complex command and control systems precisely because of this, and that human adaptability is inevitable (Hollnagel, 1993). So perhaps a more useful way to look at NEC is not to see it as an end product or an entity that ‘is’ something, but rather as a process, something that ‘becomes’ (e.g. Houghton et al., 2006). It seems possible to go even further, to argue that an alternative conception of NEC is not something that can be called a finished article, but rather as the initial conditions from which the most adaptable component of all, the humans in the system, create the end product most useful for their particular set of circumstances. Even then, this adaptation may prove fleeting and highly context dependant.

The purpose of this paper is to take the anecdotal evidence observed in the field and try to recreate, if not the exact situation, then at least conceptually similar conditions in the laboratory. The advantage of this, of course, is the degree of control that can be imposed, control that was almost entirely lacking in the case study example that has brought us to this point. Caution, however, needs to be exercised. Paradoxically, too much control could conceivably prevent the emergence of the adaptive behaviour being sought, so a novel approach to experimental design needs to be adopted. In the present study what might be referred to as a classic hierarchical command and control organisation (so called ‘classic C2’) was created within a simulated environment, then pitted against a peer-to-peer NEC counterpart, both of which contained live actors who had to operate within a complex, adaptive, high tempo scenario. Both conditions represent ‘frameworks’ that people undertake a common task within but different constraints apply to the different conditions. For example, there is relatively little in the way of rigid task specification in the NEC condition (the focus is on outcomes not actions) and the technological infrastructure is configured to facilitate peer-to-peer interaction. The opposite is true for the C2 condition. Here there is a high degree of ‘scripting of tasks’ and a more constrained technological infrastructure within which this occurs.

Manipulations such as these have (and continue) to be of importance within the Ergonomic literature (e.g. Sinclair et al., 2012). The research question links to wider debates within Applied Ergonomics around collaboration (e.g. Patel, Pettitt and Wilson, 2012) and organisational/group learning (Guimaraes et al., 2012). The same artefacts have been observed beyond the field of Ergonomics in a number of recent studies in the specialised command and control literature (Stanton et al., 2012, 2009; Bordetsky and Netzer, 2010). Common to these studies is a break from the traditional human centred approaches wherein the interaction and subsequent representations are generally static (Lee, 2001; Woods and Dekker, 2000). In this study we continue to assume they are dynamic. There is a good basis for this. Patrick, James and Ahmed (2006) for one recognise the ‘unfolding’ nature of command and control in their particular ‘process based’ methodology. They state that, “A critical feature of command and control in safety critical systems is not only the dynamically evolving situation or state of the plant but also the fluctuating responsibilities, goals and interaction of team members” (p. 1396). Our experimental design needs to take such factors into consideration but there is a trade-off. The link between the ‘unfolding’ nature of command and control and the resulting human interaction is no longer a direct one. There is also the likelihood of hidden variables that cannot be known in advance. Despite this there is a wider cybernetic principle at work: “if all the variables are tightly coupled, and if you can truly manipulate one [or a few] of them in all its freedoms, then you can indirectly control all of them” (Kelly, 1994, p. 121). In regard to human performance under different command and control paradigms the central question is related as much to the outright relative performance of the two organisations, the ‘short term’ end product (and what is normally measured) as it is to the pattern of adaptation and how performance changes over time, or the ‘long term’ end product. In other words, the central question relates to the system that the users ‘design for themselves’ by undertaking whatever adaptations they feel able and necessary, factors that are not normally measured. Whilst the promise of NEC leads us to anticipate better initial conditions for more effective adaptation, the sociotechnical ‘model’ needs to be run in order to find out.

@&#METHOD@&#

The experimental task is based around a simplified ‘Military Operations in Urban Terrain’ (MOUT) game called ‘Safe houses’. The game creates a dual task paradigm. The first task involves a commander managing two live fire teams as they negotiate an urban environment en-route to a ‘safe house’. The second task involves the commander managing the activities of ten further simulated fire teams within a much wider Area of Operations. The two tasks interact such that success in one does not necessarily connote success overall. It falls to the commander to effectively balance task demands under the independent, between subjects variable of command and control ‘type’, which has two levels: NEC and C2. The study is longitudinal in nature. The two teams (NEC and C2) separately undertook a total of thirty iterations through the same dynamic task paradigm and a form of time series analysis was employed to reveal the underlying ‘adaptive model’ embedded in the data. Participant matching and task randomisation were employed to control for individual differences and task artefacts respectively. The dependant variables focus on performance and were as follows:
                           
                              •
                              Task completion time,

Attrition

Enemies Neutralised.

Good performance in terms of these task based measures equates to the shortest time taken, all en-route Target Areas of Interest (TAI's) correctly located and effected, and a high ratio of enemy to friendly agents neutralised. In general terms, prior research would lead us to hypothesise that the simple organisation design (NEC) will allow the actors to perform more complex tasks, thereby exhibiting greater degrees of adaptability from initial conditions. This is compared to the more complex organisation (C2) which requires actors to perform a greater number of simpler, more scripted tasks, in which it might be anticipated that greater, more malevolent environmental dynamism and complexity will arise along with poorer agility and performance.

There are five principal roles in the study, three of which were occupied by experimental participants (all aged 21) who were recruited from Brunel University. The remaining two roles were filled by the experimenters.

The participants were recruited via on-campus media and publicity and were not previously known by the experimental team. It should be emphasised that the experimental team did not interact/engage with the participants beyond the requirements of their assigned role, and the study was monitored throughout by a senior lead investigator. The five principal roles were as follows:

In general, the NEC system operator deals with the experimental aspects of the Commander's Primary Task (managing the fire teams) as well as the NEC system itself. Thus the first experimenter effectively ‘drives’ the NEC command wall system, receiving requests to add/append data to the live maps from the commander and helping them to use the system themselves. The system operator also supplies strategic ‘injects’ according to pre-set rules dependent on experimental condition and the state of game play. In the NEC condition, the experimenter also provides situational updates to all team members (ensuring that ‘everyone’ knows ‘everything’). The NEC system operator did not provide any interaction beyond these requirements.

The incumbent of this role was in charge of both fire teams (within the Primary Task), providing guidance and strategy as required, they were also responsible for the larger strategic Secondary Task.

This participant was located within the live battlespace and communicated to the commander, and depending on experimental condition, the other fire team as well, by using an XDA mobile device and MSN Messenger™. The XDA device also enabled the fire team to be live tracked and represented on the commander's command wall representation of the battlespace.

This participant had the same role and capabilities as Fire team Alpha.

This individual, like the NEC system operator, was another member of the experimental team physically separated from the battlespace and from the commander. They were in charge of playing the commander (to the best of their abilities) in the Secondary Task, thus they controlled enemy actions in a ‘wizard of oz’ fashion. The instructions given to this role were explicit: to ‘try and beat their opponent’, whom they did not know or see for the duration of the study.


                           Fig. 1
                            presents a visual representation of the command and control microworld within which the Safe houses game was implemented. Figs. 2 and 3
                           
                           
                        

The system operator and commander were co-located in Brunel University's BIT Lab. Both sat with a clear view of the command wall which contained a Google Earth™ representation of the virtual battlespace with the position of the fire teams represented by an icon derived from GPS data (thus positional tracking of the fire teams was live). This visualisation window was supplemented by a planning window which contained a map based representation of the same environment with a grid square coordinate system. The planning window allowed the system operator and commander to add, delete and move objects as required by the primary and secondary tasks, which were then instantaneously represented on the main visualisation window. The planning window was populated by the experimenter (before the condition commenced) with all the required TAI's and enemy icons according to a pre-set template randomly selected for that trial. The commander and system operator had separate work stations and their own computer, and communicated purely through text based means (using MSN Messenger™).

Both fire teams (Alpha and Bravo) were located in the Brunel Battlespace, away from the co-located commander and system operator. The fire teams carried an XDA mobile phone device. This permitted them to be live tracked using GPS. The XDA device also allowed each fire team to communicate with each other and also, depending on experimental condition, the commander in the control centre (via MSN Messenger™). Fire teams could add icons into their own version of the digital map which would then simultaneously appear on each other's screens and the main visualisation window in the control centre. The digital map shown on the fire teams XDA screens was also used for navigation purposes.

The NATO SAS-050 model of command and control () was used to design command architectures that exhibited appropriate NEC and C2 characteristics as shown in Table 1
                           .

The goal of the primary task (that of the live fire team in the Brunel battlespace) is to:
                              “Execute a concentrated and simultaneous operation to disrupt a named suspect by searching his house in order to gather evidence to disrupt and dislocate enemy force elements within that Area of Operations”(MoD, 2005).
                           
                        

Practically, this involves the fire team choosing and negotiating a route through the urban battlespace in order to correctly locate and effect the safe house, dealing with Target Areas of Interest (TAI's) en-route, with each team having to collaborate by providing cover for each other.

Both fire teams start from the same location. The location of the safe house, the final destination, is randomised for each trial but with distance from start point controlled. The Area of Operations (AO) for the primary task is scattered with numerous TAI's so that an equal number of TAI's will be encountered ‘en-route’ (and regardless of route chosen).

Firstly, each of these ‘en-route’ TAI's has to be correctly located by one of the fire teams. Correct location of a TAI is judged to have occurred when the fire team takes up position at the same grid coordinates as the TAI. Missing out a TAI by failure to locate it will result in the offending fire team being removed from the mission and having to return to the start point for the remainder of the trial.

Secondly, assuming the en-route TAI has been correctly located, the fire team then has to ‘effect’ it in order to make it safe for the other fire team to continue on their route. Although the location of the TAI is known by the fire team and commander a-priori, what is not known is what form the TAI actually takes and the most appropriate way to ‘effect’ it. This can only be judged by the fire team who are on the ground and are able to make that assessment based on a number of simple local characteristics. These are as follows:
                              
                                 •
                                 If the TAI is located on a building over three stories high then a ‘yellow effect’ will neutralise it (signified by the relevant fire team using their XDA to place a yellow icon on the appropriate grid coordinate).

If the TAI is located on a building less than three stories high, then a ‘blue effect’ will neutralise it (signified by a blue icon being placed).

f the TAI is located in a busy thorough fare with retail outlets then a ‘pink effect’ will neutralise it (signified by a pink icon).

After confirmation that this information has been received, the relevant fire team will hold in this position, providing cover for the other fire team as they make their way to the next TAI. This ‘leapfrogging’ effect continues until, finally, the safe house itself is located and affected in the same way. It should be pointed out that in the NEC condition this leap frogging is facilitated by the fire teams being able to communicate directly with each other via their XDA's, in the C2 condition, however, communication (and instructions) have to pass through and/or come from the commander.

In order to further encourage the need for communication and interaction there is also a degree of built-in ambiguity in the positional data. This means that part of the adaptive process of the entire team is to figure out ‘work arounds’ and modes of operation that enable these ambiguities to be resolved in whatever way is found to be most efficient. For example, both teams developed a form of ‘communications protocol’ in which the field operatives reported-in whenever they began to move (due to lag in the positional data), and began to develop abbreviations and code words for locations around the Area of Operations. Some of the adaptations that were possible, and might have been predicted in the NEC condition, failed to emerge in practice, as the Results describe in full.

The need for good time and accuracy performance is embedded in the game by two simple game-play expedients. As mentioned before, if the wrong location is chosen or the TAI is ignored then the fire team allocated to it fails the mission and has to sit out the remainder of the trial. If the right location but wrong effect is applied then the fire team's attrition score, which acts rather like a ‘life score’, starting at five and meaning ‘full strength’ through to zero, meaning ‘neutralised and unable to continue the task’, is decremented. The attrition score is not just affected by accuracy but also speed and time. Five time activated attrition injects occur randomly throughout the 15 min allotted to the trial, these cause both fire team's attrition scores to be decremented. As a result, the longer the fire team takes, the longer they expose themselves to the deleterious effects of the experimental injects. This combines with accuracy: the less accurate they are the lower the score.

What appears to be a relatively complex set of rules becomes considerably simplified as far as the experimental participants are concerned. The system operator (who is a member of the experimental team) undertakes all game play management tasks such as maintaining the formal record of ‘location accuracy’, ‘effect concordance’ (whether the right effect has been applied) the attrition score, enactment of the time based attrition injects, and communicates all of this to the commander as required.

The commanders secondary task is based on the following mission:
                              “Execute a concentrated and simultaneous operation to disrupt named suspects by searching their houses in order to gather evidence to disrupt and dislocate the enemy force within West London”
                           
                        

Whilst the first task concerns the activities of a live fire team in a live environment, the secondary task of the commander is simulated and occurs in a much wider Area of Operations (the boundaries are six miles in either direction from the AO of the first task). This larger strategic mission relies on the commander playing a competitive game against a simulated ‘enemy’, played by the experimenter acting in a wizard of oz fashion. Thus ‘live’ (small AO) and ‘simulated’ (large AO) elements are played simultaneously on the same NEC system.

While the live fire team progress towards their primary target within their smaller AO, wider enemy activity is taking place all around which, if permitted to continue, will eventually impinge on the primary task. Such impingement, under certain specified conditions, will mean that the primary task fails regardless of the efficacy of the fire teams. The onus is thus on the commander to manage both tasks effectively.

In the commander's wider area of operations there are 15 enemy elements/icons dispersed randomly around the environment. These icons are placed according to a random script for that trial by the experimenter acting in the ‘enemy’ role.

Enemy icons, due to the asymmetry of most MOUT-type situation, are free to move across the battlespace at will (no area is restricted to them). The commander has 25 friendly force icons/elements all massed in a defined ‘green zone’, this is their starting position. Although more numerous, the friendly icons are not permitted to enter pre-defined ‘sensitive areas’.

Apart from these constraints, the game plays like a virtual game of ‘draughts’ in which the grid square system of the map serves as a form of draughts board. Only one icon can be moved, one square at a time (in any direction) in enemy reaction, friendly counter action, enemy counter re-action, and so on in sequence.

If the friendly icon enters a grid square occupied by an enemy icon then the friendly icon wins. If the enemy icon enters a grid square occupied by a friendly icon, the enemy wins. The experimenter playing the role of enemy updates the icons/map accordingly (and keeps a record of the game score).

The enemy's’ objectives are to reach several other enemy safe houses dotted around the AO (one of which is the primary target for the live fire team). Every enemy icon that enters the grid square occupied by a safe house is safe and no longer available to be ‘captured or neutralised’ and thus no longer able to contribute to an ‘enemy captured/neutralised’ score. As a result, the onus on the commander is to capture/neutralise the enemies before they reach a safe house, and preferably, neutralising the safe house before enemies start to reach it, hence the phrase ‘disrupt’ and ‘dislocate’.

@&#PROCEDURE@&#

The aims and objectives of the study are introduced in broad terms along with health and safety preliminaries and informed consent. Detailed instructions on the task are then provided to all participants, supplemented with demonstrations and hands-on examples. The experimenter then uses the pre-populated command wall to begin the first full trial which is identical in all respects to the experimental trials but serves as a practice (both teams are measured subsequently as an internal check on concordance between them).

The sensor is equipped with the XDA and briefed by the commander, with the help of the study team, as to the mission objectives. The start point of the study is Brunel University's BIT Lab: the study is timed from the moment the fire teams leave. The commander is seated in front of their own laptop computer and the command wall. With all participants ready the practice trial commences with help, facilitation and intervention from the experimental team as required. The System Operator manages the experimental tasks associated with the Primary Task (attrition scores, communications updates – where required and permitted – and timing). The Enemy plays the commander concurrently according to the rules of the game. After a maximum of 15 min (or sooner if the Primary Task is complete) the trial is halted, the MSN transcripts are saved/archived along with those of the command wall's system logs.

With the participants familiar with the broad paradigm the teams begin to undertake the repeated iterations of the experimental trials in both conditions. Issues and questions are dealt with before the trial starts and during it if required. The task gets underway and the participants interact in the manner prescribed by the organisational type they are currently working within.

@&#RESULTS@&#

To briefly recap, participants took part in a simulated MOUT mission over thirty successive iterations. The analysis, therefore, focuses on how the different constraints of NEC and C2 influenced the direction of team adaptation and performance. It is hypothesised that NEC provides better conditions for adaptation but linear regression is used as a form of time series analysis (see also Agha and Alnahhal, 2012) in order to uncover the underlying theory behind the data and thus to test this supposition.

Both teams (C2 and NEC) were measured in terms of how long it took the live fire team to perform their task. When this first task was complete then both tasks of the dual task paradigm were halted. The maximum amount of time that was allowed to be spent on the task was 15 min (900 s). As one would expect, over the course of the thirty iterations both teams sped up considerably and continued to do so for every trial. A strong association between task time and trial was obtained for both conditions (NEC r = −0.84 and C2 r = −0.85), both of which were significant at beyond the 1% level. Furthermore, the regression ANOVA supports the hypotheses that this association is linear in nature for the NEC condition: F(1,28) = 64.74; p < 0.01 and F(1,28) = 73.53; p < 0.01 for the C2 condition.

The linear regression model fitted to the data accounted for 69% of the variance in the NEC condition (Adjusted R
                        2 = 0.69) and 71% of the variance in the C2 condition (Adjusted R
                        2 = 0.71). Both values represent a large effect size and both regression models were statistically significant to beyond the 1% level. The regression equation, however, differed between the two conditions. The intercept for the C2 condition was at b
                        0 = 862 s, somewhat nearer the maximum value of 900 s permissible for the task than the NEC condition, whose intercept was at b
                        0 = 762 s. However, the regression line for the C2 condition had a slightly more precipitous slope than that for the NEC condition, b
                        1 = −14.26 compared to b
                        1 = −19.73, thus despite the higher intercept the regression lines actually crossed at trial 17 meaning that by trial 30 the regression model predicts the task being completed in 270 s for the C2 condition compared to 334 s for the NEC condition (approximately a minute faster). This represents a small effect (r
                        bis = 0.17) but one that an analysis of covariance reveals as significant at the 7% level (t = 1.88, df = 56, p = 0.07). This is an encouraging finding given the exploratory nature of the study, its novel methodology and small sample size. Taken in isolation it appears that that the NEC condition favours initial time adaptation with the C2 condition yielding longer term improvements and faster times.

The live fire teams performing the primary task were given an attrition score rather similar to the kind of ‘lives left’ score given in computer games. The attrition score, which begins at five, is diminished through a) time based injects, so the longer that is spent on the task the more chance there is of having the score decremented, and b), if locations of TAI's and markers placed at them are inappropriate and/or inaccurate then the attrition score is also subtracted. A high attrition score connotes better performance.

In both conditions the attrition score is positively correlated with the number of trials undertaken, r = 0.8, p < 0.01 for the NEC condition and r = 0.53, p < 0.01 for the C2 condition. The regression ANOVA supports the hypothesis of linearity in both cases: F(1,28) = 48.55; p < 0.01 for NEC and F(1,28) = 11.09; p < 0.01 for C2. Note, however, that despite the statistical significance of these regression diagnostics the C2 condition possess less statistical power in terms of its associative performance (r), linearity (F) and also in the amount of variance explained by the regression model (R
                        2 = 0.26 compared to NEC's 0.62).

In model terms the regression coefficient (the slope of the regression line) was similar for NEC and C2, being 0.08 and 0.09 respectively. The intercept values were, however, different. The C2 model starts with a lower attrition score of 1.5 and maintains a subordinate position to NEC (whose intercept is 1.98) for the duration of the longitudinal testing and data collection intervals. Both of these regression models were statistically significant to beyond the 1% level. An analysis of covariance confirms the visual impression of the data by detecting a very strong and statistically significant effect (t = 47.2 × 104, df = 56, p < 0.01, r
                        bis = 1.0). So despite C2's favourable evolution towards faster task completion times (as shown above) it is not all good news. Attrition in the C2 condition is statistically worse than the NEC condition, in other words, additional organisational structure in the C2 condition does not necessarily lead to better performance.

This factor relates most strongly to the commander's performance in the secondary task (within the wider area of operations). It can be immediately noted that the commander was able to manage these competing tasks satisfactorily, with the secondary task at no time causing the premature cessation of the primary task. That said, it did compete for the commander's attention and thus influenced their performance.

The assumptions underlying a linear approach to time series analysis are not met in the case of friendly versus enemy capture ratio. In other words, trial number or task iteration appeared not to be a good predictor of this factor's performance. In both cases only small (r = 0.3/0.24) correlations were detected for NEC and C2 respectively, albeit statistically significant. However, the fact that the resultant regression model only explained around 6–9% of the variance in the data (R
                        2 = 0.09/0.06), the hypothesis regarding linearity was not supported: F(1,28 = 2.75/1.64); p = ns and, furthermore, both regression models failed to reach significance (p = ns) means that this form of analysis is abandoned. Given the lack of a linear relationship between trial and capture ratio a simple cross-sectional approach can be taken.

Even here, however, an independent samples t-test failed to detect a statistically significant difference in capture ratio between NEC and C2 (t = 1.48; df = 58; p = ns). Given that such a test possesses in excess of 80% power to detect medium effect sizes or larger, and that only a very small effect was actually detected (r
                        bis = 0.04), means that there is a high degree of confidence in stating that the constraints imposed by both NEC and C2 conditions are not a particularly meaningful determinant of enemy versus friendly capture ratio. In other words, C2 evolves towards faster task completion times, but with poorer accuracy, but both NEC and C2 are comparable in terms of the numbers of ‘friendlies’ and ‘enemies’ captured.

@&#CONCLUSIONS@&#

The results, in summary, show that the traditional command and control condition is good at optimising task completion times, starting off slower than NEC but catching up and overtaking it. Optimisation of task time, however, comes with no benefit to other factors, in particular, the C2 condition is less accurate. What the NEC condition loses in task time (approximately 1 min) it ‘gains’ in accuracy and remains stable in terms of enemies neutralised. The main lesson to learn from this study is the instability of the human-system interaction and the impact this has on how future studies could or should be considered. If the human-system interaction had been assumed to be stable the results would have been quite different. Even allowing for not just one but several practice or control trials, the NEC condition would have posted faster task completion times than the traditional command and control condition. The trade-off, of course, is that what is gained with a repeated measures study is lost in terms of the diversity and size of the sample. To gain this back would inevitably require considerably more experimental resources than is presently common. Thus a broader lesson can be extracted, which is to adopt a contingent approach. To determine, first of all, the likely instability in the human-system interaction of interest and approach the study design with that in mind. For ‘stable’ problems the conventional cross-sectional study will perform well. For ‘unstable’ interactions, the type of study undertaken here may well reveal the real underlying phenomena of interest. In doing so, the present study gives us insights into the following:
                        
                           •
                           The emergent nature of command and control: Participants did indeed perform adaptations to the way they carried out their task within the confines of the study. This methodological success shows that these open-systems properties can be instantiated (and measured) in a laboratory setting, which...

…led to a degree of unpredictability in the NEC condition: This behaviour matched that observed in the previous live case study which motivated the current paper. It was hypothesised that NEC would increase tempo, and had a static view of the human system interaction been taken this view could be upheld. But with NEC representing an initial condition from which adaptations could take place, this hypothesis was not supported, however…

…are we comparing like with like? With more than one factor to optimise is the fact that NEC was slower really very meaningful? Perhaps not. Whereas the more traditional hierarchical command and control condition accelerated task completion times this came at a cost to other factors, a cost that the NEC condition was able to trade-off and optimise. How these two organisation types are able to balance and optimise more factors than those present in the current study requires further research, but the possible relationship between the ‘rigidity’ of a team structure (i.e. hierarchical command and control) versus its output flexibility (i.e. ability to optimise more than one process outcome) is an intriguing one with numerous analogues in the literature.

As the case study(s) that have motivated this work have already hinted, NEC-like organisations often exhibit paradoxical behaviour. The NEC condition ‘should’ have been faster, but actually, the scope of adjustments available (and actually made by the incumbents of the team roles) meant that task time was not elevated to the status of single most important priority. This, surely, is a desirable outcome given the inherent complexity, dynamism and asymmetry present in the context within which these organisations operate. Perhaps this is the hallmark of what NEC is really all about? The greater extent of open systems behaviour is what seems to enable the NEC organisation to undertake a more complex process of optimisation compared to its hierarchically organised counterpart. The take home message seems to be that this situation can be made to arise by creating the optimum conditions for the most adaptable, open-systems component of all in NEC: the human.

This paper operates in the domain of Effects-Based Operations (EBO) from which terms such as ‘effect’, ‘effector’ etc. are drawn. The author's acknowledge that ‘affect’ and ‘affector’ are more linguistically appropriate in most cases.

@&#ACKNOWLEDGEMENT@&#

This work from the Defence Technology Centre for Human Factors Integration (DTC-HFI) was part funded by the Human Sciences Domain of the UK Ministry of Defence.

@&#REFERENCES@&#

