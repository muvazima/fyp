@&#MAIN-TITLE@&#Semi-supervised learning for refining image annotation based on random walk model

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A unified framework for refining annotation by fusing GMM with random walk.


                        
                        
                           
                           A semi-supervised learning is used to enhance the quality of training data.


                        
                        
                           
                           Gaussian mixture model is learned to estimate initial annotations.


                        
                        
                           
                           Integrating label and visual similarities of images associated with the labels.


                        
                        
                           
                           Random walk is implemented on graph to further mine the semantic correlation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Automatic image annotation

Semi-supervised learning

Gaussian mixture model

Expectation-maximization

Random walk

Image retrieval

@&#ABSTRACT@&#


               
               
                  Automatic image annotation has been an active research topic in recent years due to its potential impact on both image understanding and semantic based image retrieval. In this paper, we present a novel two-stage refining image annotation scheme based on Gaussian mixture model (GMM) and random walk method. To begin with, GMM is applied to estimate the posterior probabilities of each annotation keyword for the image, during which a semi-supervised learning, i.e. transductive support vector machine (TSVM), is employed to enhance the quality of training data. Next, a label similarity graph is constructed by a weighted linear combination of label similarity and visual similarity of images associated with the corresponding labels. In this way, it can seamlessly integrate the information from image low-level visual features and high-level semantic concepts. Followed by a random walk process over the constructed label graph is implemented to further mine the correlation of the candidate annotations so as to capture the refining results, which plays a crucial role in semantic based image retrieval. Finally, extensive experiments carried out on two publicly available image datasets bear out that this approach can achieve marked improvement in annotation performance over several state-of-the-art methods.
               
            

@&#INTRODUCTION@&#

Automatic image annotation (AIA) is a promising solution to enable the semantic based image retrieval via keywords. It is generally believed that AIA refers to a process to automatically generate textual words to describe the content of a given image. In recent years, the state-of-the-art research on AIA has broadly proceeded along two categories. The first one views image annotation as a supervised classification problem [1] that treats each semantic concept as an independent class and constructs different classifiers for different concepts. This approach predicts the annotations of a new image by computing similarity at the visual level and propagating the corresponding keywords. The second category treats the words and visual tokens in each image as equivalent features in different modalities. Image annotation is then formalized by modeling the joint distribution of visual and textual features on the training data and predicting the missing textual features for a new image. As the representative work of this perspective, Duygulu et al. [2] propose a translation model (TM) to treat AIA as a process of translation from a set of blob tokens, obtained by clustering image regions, to a set of keywords. Jeon et al. [3] put forward cross-media relevance model (CMRM) to annotate images, assuming that the blobs and words are mutually independent given a specific image. Subsequently, CMRM is improved through continuous-space relevance model (CRM) [4], multiple-Bernoulli relevance model (MBRM) [5] and dual cross-media relevance model (DCMRM) [6], etc. In addition, several nearest-neighbor-based methods have also been proposed in the most recent years [7,8]. Despite most of these methods have achieved encouraging results, there are still two problems remain to be solved. First, in most cases, labeled images are often hard to obtain or create in large quantities while the unlabeled ones are easier to collect. So how to efficiently use the unlabeled images to improve the annotation performance is a key issue to formulate effective semantic models. Second, little effort focuses on the semantic context and semantic correlation between image annotations. Even if considered, the image visual information related to the annotation often tends to be ignored, which is liable to cause the phenomenon that different images with the same candidate annotations
                        1
                        Candidate annotations denote the initial annotations generated by some image annotation models.
                     
                     
                        1
                      would obtain the same refinement results.

To address the above issues, we present a novel two-stage refining image annotation scheme based on Gaussian mixture model and random walk. On the one hand, GMM is employed to estimate the posterior probabilities of each annotation keyword for the image, which can be seen as the initial annotation stage. It is worth noting that a semi-supervised learning, viz. transductive support vector machine, is introduced into GMM to enable the unlabeled images to be fully exploited to improve the GMM performance to some degree. On the other hand, a random walk process over the constructed label graph is implemented to further mine the correlation of the candidate annotations for precise annotation, which can be regarded as the refining annotation stage. Experimental results on two publicly available image datasets validate the effectiveness of the proposed approach. To the best of our knowledge, this study is the first attempt to integrate GMM with random walk as well as semi-supervised learning in the task of refining image annotation.

The rest of this paper is organized as follows. Section 2 discusses some related work about image annotation. In Section 3, Gaussian mixture model is first introduced, and then the TSVM as well as random walk is elaborated respectively, especially for the proposed SGMM-RW refining annotation framework. Experimental results are reported and analyzed in Section 4. Finally, the paper is ended with some important conclusions and future work in Section 5.

@&#RELATED WORK@&#

During recent years, many methods have been developed for refining image annotation. As a pioneer work, Jin et al. [9] employ WordNet to estimate the semantic correlation between annotation keywords. This method, however, can only achieve a fairly limited effect as it totally ignores the visual content of images. In the approach [10], Wang et al. apply random walk with restarts to refine candidate annotations by integrating word correlations with the original candidate annotation confidences together. Followed by they propose a content based approach by formulating annotation refinement as a Markov process [11]. Later on Jin et al. [12] exploit randomized weighted-max cut algorithm to complete image annotation refinement. Subsequently it is extended by a new methodology for knowledge based refining image annotation in a deterministic polynomial time [13], in which they further investigate various semantic similarity measures between keywords and fuse the outcomes of all these measures together to make a final decision by using Dempster–Shafer evidence combination. Liu et al. [14] rank the image tags according to their relevance with respect to the associated images by tag similarity and image similarity in a random walk model. Xu et al. [15] come up with a new graphical model termed as regularized latent Dirichlet allocation (rLDA) for tag refinement. In recent work [16], an efficient iterative approach is put forward for image tag refinement by pursuing low-rank, content consistency, tag correlation and error sparsity through solving a constrained yet convex optimization problem. Especially in our previous work [17,18], a unified refining image annotation technique is proposed by combining probabilistic latent semantic analysis (pLSA) with random walk/max-bisection model respectively, experiments on several image datasets have validated its computational efficiency and annotation accuracy. In more recent work [19], a survey on refining image annotation techniques is reviewed. Particularly the key aspects of various methods, including their original intentions and annotation models, are comprehensively summarized and analyzed. For more details please refer to the corresponding literature.

Alternatively, semi-supervised learning has been an active topic of research in computer vision and machine learning for decades [20,21]. As the representative work, Li and Sun [22] formulate image annotation problem as a joint classification task based on two-dimensional conditional random fields together with semi-supervised learning, in which the semi-supervised learning is utilized to exploit the unlabeled data to improve the joint classification performance. In [23], a semi-supervised ensemble of classifiers is constructed based on the AdaBoost and naive Bayes is leveraged as its base classifiers. One of the main advantages is that the weights of unlabeled instances are dynamic and proportional to the probability given by the previous stage. Besides, TSVM-HMM [24] is proposed for automatic image annotation by integrating the discriminative classification with the generative model to mutually complete their advantages for better annotation performance. Zhu and Liu [25] conduct image annotation based on a semi-supervised learning model and random walk with restart algorithm so as to well integrate the information of both candidate annotations and the corpus. Recently, Shao et al. [26] put forward a semi-supervised topic modeling for image annotation by introducing a harmonic regularizer based on the graph Laplacian of the data into the probabilistic semantic model for learning latent topics of the images. Ismail and Bchir [27] present a semi-supervised possibilistic clustering and feature weighting algorithm for AIA. More recently, to make up for the drawback of ignoring manifold structures revealed by the unlabeled data, Yuan et al. [28] propose a semi-supervised cross-domain learning method with group sparsity for image annotation by applying both labeled and unlabeled training data with their manifold structural information. Meanwhile, a multi-view semi-supervised bipartite ranking model, which allows using the information contained in unlabeled sets of images, is proposed for refining image annotation [29]. Following this work, Chen et al. [30] treat image annotation as a multi-instance semi-supervised learning problem by constructing a graph based semi-supervised learning classifier to produce several keywords for each unseen image. Table 1
                      summarizes several automatic image annotation methods reported in the literatures.

In this paper, we propose an approach for refining image annotation by integrating Gaussian mixture model (GMM) with random walk (abbreviated as SGMM-RW). This method mainly involves two stages. Specifically, Gaussian mixture model is leveraged to estimate the posterior probabilities of each annotation keyword for the image in the first stage, during which TSVM is applied to enhance the quality of training data. In the second stage, random walk is employed over the constructed label graph to further mine the correlation of the candidate annotations for precise results. To summarize, the novelty of SGMM-RW lies in two aspects: exploiting GMM to accomplish the initial semantic annotation task and implementing random walk process over the constructed label similarity graph to refine the candidate annotations generated by the GMM. Fig. 1
                      illustrates the framework of SGMM-RW proposed in this paper.

A Gaussian mixture model (GMM) is a parametric statistical model which assumes that the data originates from a weighted sum of several Gaussian sources. More formally, a GMM is a weighted sum of M component Gaussian densities as given by the following equation.
                           
                              (1)
                              
                                 p
                                 (
                                 x
                                 |
                                 λ
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 
                                    
                                       ω
                                    
                                    
                                       i
                                    
                                 
                                 g
                                 (
                                 x
                                 |
                                 
                                    
                                       μ
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       Σ
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                           
                        where x is a D-dimensional continuous-valued data vector, wi
                        , i
                        =1, 2, … , M, denote the mixture weights and satisfy the constraint 
                           
                              
                                 
                                    ∑
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    M
                                 
                              
                              
                                 
                                    ω
                                 
                                 
                                    i
                                 
                              
                              =
                              1
                              ,
                              
                              g
                              (
                              X
                              |
                              
                                 
                                    μ
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    Σ
                                 
                                 
                                    i
                                 
                              
                              )
                              ,
                              
                              i
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              M
                           
                        , are the component Gaussian densities. Each component density can be represented as a D-variate Gaussian function:
                           
                              (2)
                              
                                 g
                                 (
                                 x
                                 |
                                 
                                    
                                       μ
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       Σ
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       
                                          
                                             (
                                             2
                                             π
                                             )
                                          
                                          
                                             D
                                             /
                                             2
                                          
                                       
                                       |
                                       
                                          
                                             Σ
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             |
                                          
                                          
                                             1
                                             /
                                             2
                                          
                                       
                                    
                                 
                                 exp
                                 
                                    
                                       
                                          -
                                          
                                             
                                                1
                                             
                                             
                                                2
                                             
                                          
                                          
                                             
                                                (
                                                x
                                                -
                                                
                                                   
                                                      μ
                                                   
                                                   
                                                      i
                                                   
                                                
                                                )
                                             
                                             
                                                T
                                             
                                          
                                          
                                             
                                                Σ
                                             
                                             
                                                i
                                             
                                             
                                                -
                                                1
                                             
                                          
                                          (
                                          x
                                          -
                                          
                                             
                                                μ
                                             
                                             
                                                i
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        where μi
                         and ∑i
                         represent the mean vector and covariance matrix respectively. The component number of the GMM M is determined by the minimum description length (MDL) principle. And the parameter set λ of the GMM is generally estimated by the expectation–maximization (EM) algorithm [31] following the maximum-likelihood criterion as follows.


                        E-step. Compute the posterior probabilities for each Gaussian component gj
                         (j
                        =1, 2, … , M) based on the current estimation of the parameters.
                           
                              (3)
                              
                                 p
                                 (
                                 
                                    
                                       g
                                    
                                    
                                       j
                                    
                                 
                                 |
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       θ
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             ω
                                          
                                          
                                             j
                                          
                                          
                                             (
                                             n
                                             )
                                          
                                       
                                       p
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               θ
                                                            
                                                            
                                                               j
                                                            
                                                            
                                                               (
                                                               n
                                                               )
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             M
                                          
                                       
                                       
                                          
                                             ω
                                          
                                          
                                             k
                                          
                                          
                                             (
                                             n
                                             )
                                          
                                       
                                       p
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               θ
                                                            
                                                            
                                                               k
                                                            
                                                            
                                                               (
                                                               n
                                                               )
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     


                        M-step. Update the parameters to maximize the expectation.
                           
                              (4)
                              
                                 
                                    
                                       ω
                                    
                                    
                                       j
                                    
                                    
                                       (
                                       n
                                       +
                                       1
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       M
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 p
                                 
                                    
                                       
                                          
                                             
                                                g
                                             
                                             
                                                j
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         θ
                                                      
                                                      
                                                         j
                                                      
                                                      
                                                         (
                                                         n
                                                         )
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    
                                       μ
                                    
                                    
                                       j
                                    
                                    
                                       (
                                       n
                                       +
                                       1
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             M
                                          
                                       
                                       p
                                       
                                          
                                             
                                                
                                                   
                                                      g
                                                   
                                                   
                                                      j
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         ,
                                                         
                                                            
                                                               θ
                                                            
                                                            
                                                               j
                                                            
                                                            
                                                               (
                                                               n
                                                               )
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       ∗
                                       
                                          
                                             x
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             M
                                          
                                       
                                       p
                                       
                                          
                                             
                                                
                                                   
                                                      g
                                                   
                                                   
                                                      j
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         ,
                                                         
                                                            
                                                               θ
                                                            
                                                            
                                                               j
                                                            
                                                            
                                                               (
                                                               n
                                                               )
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 
                                    
                                       Σ
                                    
                                    
                                       j
                                    
                                    
                                       (
                                       n
                                       +
                                       1
                                       )
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             M
                                          
                                       
                                       p
                                       
                                          
                                             
                                                
                                                   
                                                      g
                                                   
                                                   
                                                      j
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         ,
                                                         
                                                            
                                                               θ
                                                            
                                                            
                                                               j
                                                            
                                                            
                                                               (
                                                               n
                                                               )
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   -
                                                   
                                                      
                                                         μ
                                                      
                                                      
                                                         j
                                                      
                                                      
                                                         (
                                                         n
                                                         +
                                                         1
                                                         )
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             T
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                -
                                                
                                                   
                                                      μ
                                                   
                                                   
                                                      j
                                                   
                                                   
                                                      (
                                                      n
                                                      +
                                                      1
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             M
                                          
                                       
                                       p
                                       
                                          
                                             
                                                
                                                   
                                                      g
                                                   
                                                   
                                                      j
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         ,
                                                         
                                                            
                                                               θ
                                                            
                                                            
                                                               j
                                                            
                                                            
                                                               (
                                                               n
                                                               )
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

In general, the above process is repeated until some convergence threshold is reached. In the scenario of image annotation, given that the training image is represented by both a visual feature X
                        ={x
                        1, x
                        2, … , xm
                        } and a keyword list W
                        ={w
                        1, w
                        2, … , wn
                        }, where xi
                         (i
                        =1, 2, … , m) denotes the visual feature for region i and wj
                         (j
                        =1, 2, … , n) is the jth keyword in the annotation. For a test image I is represented by its visual feature vector X
                        ={x
                        1, x
                        2, … , xm
                        }, according to Bayesian rule, the posterior probability p(wi
                        |I) can be calculated based on the conditional probability p(I|wi
                        ) and prior probability p(wi
                        ).
                           
                              (7)
                              
                                 p
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 X
                                 )
                                 ∞
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          m
                                       
                                    
                                 
                                 p
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       j
                                    
                                 
                                 |
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 p
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                           
                        From Eq. (7), the top n keywords can be selected as the initial annotations for image X.

Semi-supervised learning, which is a family of algorithms that take advantage of both labeled and unlabeled data, has been extensively studied for a couple of years [32]. Among which the transductive support vector machine (TSVM), also called semi-supervised support vector machine (S3VM) located between supervised learning with fully labeled training data and unsupervised learning without any labeled training data, is a promising way to find out the underlying relevant data from the unlabeled ones. TSVM implements the idea of transductive learning by including test points in the computation of the margin rather than finds maximum margin between labeled points, which is able to provide useful prior information for learning [33]. In addition, it is worth noting that an important issue should be highlighted is that, although there is still some debate about whether the transductive inference can be successful in semi-supervised classification, it has been proved both empirically and theoretically that TSVM can be effective in handling problems where few labeled data are available while the unlabeled data are easy to be obtained [34]. Therefore, TSVM is a simple yet very efficient semi-supervised learning algorithm that can mine more relevant image regions included in the expanded set with only a few labeled relevant regions. It is a promising way to fulfill the task of refining image annotation with less tedious labeling efforts involved.

To be specific, TSVM works as follows to mine the relevant image regions: Given a keyword w, several labeled regions are taken as the relevant examples and the initial non-relevant examples are randomly sampled from the remaining regions. A two-class SVM classifier is trained first. Then based on the learnt SVM classifier, the most confident relevant regions and the most non-relevant ones are added into the relevant and non-relevant training set respectively. With the expanded training set, SVM classifier will be re-trained until the maximum time of iteration is reached. Finally, an expanded set of labeled regions can be obtained to benefit for modeling the visual feature distribution of the keyword w. Based on this recognition, TSVM is adopted to explore more relevant image regions to enhance the annotation performance of GMM. The pseudo-code of it is summarized in Algorithm 1.
                           
                              
                                 
                                 
                                 
                                    
                                       
                                          Algorithm 1. Pseudo-code of TSVM for mining relevant image regions
                                       
                                    
                                    
                                       
                                          Input: 
                                             
                                                
                                                   
                                                      R
                                                   
                                                   
                                                      L
                                                   
                                                   
                                                      0
                                                   
                                                
                                             
                                           and 
                                             
                                                
                                                   
                                                      R
                                                   
                                                   
                                                      U
                                                   
                                                   
                                                      0
                                                   
                                                
                                             
                                           denote the sets of labeled and unlabeled regions for the keyword w; S is a SVM classifier; m, n and K denote control parameters
                                    
                                    
                                       
                                          Output: 
                                             
                                                
                                                   
                                                      R
                                                   
                                                   
                                                      L
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                           denote an expanded set of labeled image regions
                                    
                                    
                                       
                                          Process:
                                    
                                    
                                       1
                                       
                                          for 
                                          k
                                          =1 to K 
                                          do
                                       
                                    
                                    
                                       2
                                       Learning a SVM classifier S from 
                                             
                                                
                                                   
                                                      R
                                                   
                                                   
                                                      L
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       3
                                       Using S to classify regions in 
                                             
                                                
                                                   
                                                      R
                                                   
                                                   
                                                      U
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       4
                                       Selecting m most confidently predicted regions from 
                                             
                                                
                                                   
                                                      R
                                                   
                                                   
                                                      U
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                           which are labeled as relevant examples
                                    
                                    
                                       5
                                       Selecting n most confidently predicted regions from 
                                             
                                                
                                                   
                                                      R
                                                   
                                                   
                                                      U
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                           which are labeled as non-relevant examples
                                    
                                    
                                       6
                                       Adding m
                                          +
                                          n regions with their corresponding labels into 
                                             
                                                
                                                   
                                                      R
                                                   
                                                   
                                                      L
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       7
                                       Removing these m
                                          +
                                          n regions from 
                                             
                                                
                                                   
                                                      R
                                                   
                                                   
                                                      U
                                                   
                                                   
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       8
                                       
                                          end for
                                       
                                    
                                 
                              
                           
                        
                     

Based on the above description, it is obvious that the initial annotation of images can be easily obtained by the trained GMM. To capture more precise annotation, random walk is utilized over the label graph to further mine the correlation of the candidate annotations, which plays a critical role in semantic based image retrieval. To construct the label similarity graph, i.e. the initial annotation similarity graph, each candidate is transformed to a vertex and the pairwise label similarity is used as the weight of the corresponding edge. For now we focus on how to reasonably measure the similarity between pairwise concepts associated with an image, which is still a tough problem in the field of image annotation and retrieval. As is well known, the most common methods include WordNet [35] and normalized Google distance (NGD) [36]. However, from their definitions it is easy to see that NGD is actually a measure of the contextual relation while WordNet focuses on the semantic meaning of the keyword itself. Furthermore, both of them build word correlations only based on the textual descriptions whereas the visual information of images in the dataset is not considered at all, which is very likely to result in the phenomenon that different images with the same candidate annotations would obtain the same refinement results. To this end, an effective pairwise similarity strategy is devised by calculating a weighted linear combination of label similarity and visual similarity of images associated with the corresponding labels, in which the label similarity between wi
                         and wj
                         is defined as follows:
                           
                              (8)
                              
                                 
                                    
                                       s
                                    
                                    
                                       l
                                    
                                 
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 exp
                                 (
                                 -
                                 d
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 )
                              
                           
                        where d(wi
                        , wj
                        ) represents the distance between two labels wi
                         and wj
                         and it is defined similarly to NGD as:
                           
                              (9)
                              
                                 d
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          max
                                       
                                       (
                                       log
                                       f
                                       (
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       ,
                                       log
                                       f
                                       (
                                       
                                          
                                             w
                                          
                                          
                                             j
                                          
                                       
                                       )
                                       )
                                       -
                                       log
                                       f
                                       (
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             w
                                          
                                          
                                             j
                                          
                                       
                                       )
                                    
                                    
                                       log
                                       G
                                       -
                                       
                                          min
                                       
                                       (
                                       log
                                       f
                                       (
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       ,
                                       log
                                       f
                                       (
                                       
                                          
                                             w
                                          
                                          
                                             j
                                          
                                       
                                       )
                                       )
                                    
                                 
                              
                           
                        where f(wi
                        ) and f(wj
                        ) denote the numbers of images containing labels wi
                         and wj
                         respectively, f(wi
                        , wj
                        ) is the number of images containing both wi
                         and wj
                        , G is the total number of images in the dataset.

Similar to [14], for a label w associated with an image x, we collect the K nearest neighbors from the images containing w, and these images can be regarded as the exemplars of the label w with respect to x. Thus from the point view of labels associated with an image, the visual similarity between labels wi
                         and wj
                         is given as below:
                           
                              (10)
                              
                                 
                                    
                                       s
                                    
                                    
                                       v
                                    
                                 
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 exp
                                 
                                    
                                       
                                          -
                                          
                                             
                                                1
                                             
                                             
                                                K
                                                ×
                                                K
                                             
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   x
                                                   ∈
                                                   
                                                      
                                                         Γ
                                                      
                                                      
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                   ,
                                                   y
                                                   ∈
                                                   
                                                      
                                                         Γ
                                                      
                                                      
                                                         
                                                            
                                                               w
                                                            
                                                            
                                                               j
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             
                                                ‖
                                                x
                                                -
                                                y
                                                
                                                   
                                                      ‖
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      σ
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where Γw
                         is the representative image collection of label w, x and y denote image features corresponding to the respective image collections of labels wi
                         and wj
                        , σ is the radius parameter of the Gaussian kernel function. To benefit from each other of the two similarities described above, a weighted linear combination of label similarity and visual similarity is defined as follows:
                           
                              (11)
                              
                                 
                                    
                                       s
                                    
                                    
                                       ij
                                    
                                 
                                 =
                                 s
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 =
                                 λ
                                 
                                    
                                       s
                                    
                                    
                                       l
                                    
                                 
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 +
                                 (
                                 1
                                 -
                                 λ
                                 )
                                 
                                    
                                       s
                                    
                                    
                                       v
                                    
                                 
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                 
                                 )
                              
                           
                        where λ
                        ∈[0, 1] controls the weights for each measurement and the corresponding performance with different λ values is to be discussed in Section 4.1.

Suppose that the constructed label graph with n nodes, rk
                        (i) is used to denote the relevance score of node i at iteration k, P denotes a n-by-n transition matrix, whose element pij
                         indicates the probability of the transition from node i to node j and it can be computed as below:
                           
                              (12)
                              
                                 
                                    
                                       p
                                    
                                    
                                       ij
                                    
                                 
                                 =
                                 
                                    
                                       s
                                    
                                    
                                       ij
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   k
                                                
                                             
                                          
                                          
                                             
                                                s
                                             
                                             
                                                ik
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where sij
                         is the pairwise label similarity (defined in Eq. (11)) between node i and node j. Thereafter the random walk process can be formulated as:
                           
                              (13)
                              
                                 
                                    
                                       r
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 j
                                 )
                                 =
                                 α
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    
                                       r
                                    
                                    
                                       k
                                       -
                                       1
                                    
                                 
                                 (
                                 i
                                 )
                                 
                                    
                                       p
                                    
                                    
                                       ij
                                    
                                 
                                 +
                                 (
                                 1
                                 -
                                 α
                                 )
                                 
                                    
                                       v
                                    
                                    
                                       j
                                    
                                 
                              
                           
                        where α
                        ∈(0, 1) is a weight parameter to be determined, vj
                         denotes probabilistic scores calculated by GMM for the initial annotation. In the process of refining annotation, it should be noted that the random walk process keeps executing until it reaches the steady-state probability distribution. After that the top several candidates with the highest probabilities can be chosen as the final refining image annotation results.

In this section, we first evaluate SGMM-RW on Corel5k dataset, which consists of 5000 images from 50 Corel Stock Photo CD’s provided by [2]. Each CD contains 100 images with a certain theme, of which 90 are designated to be in the training set and 10 in the test set, resulting in 4500 training images and a balanced 500-image test collection. Finally, the dictionary contains 260 words that appear in both the training and testing set. It is worth noting that the normalized cuts algorithm (Ncuts) [37] rather than JSEG [38] is applied to segment images into a number of meaningful regions. The reason lies in that JSEG only focuses on local features and their consistencies while Ncuts aims at extracting the global impression of an image data. So Ncuts, to some extent, can get a better segmentation result than that of JSEG (as can be seen from Fig. 2
                     ). In addition, since the focus of this paper is not on image feature selection, for each image at most the 10 largest regions are selected and 809-dimensional visual features
                        2
                        
                           http://appsrv.cse.cuhk.edu.hk/~jkzhu/felib.html.
                     
                     
                        2
                      
                     [39] (color, texture, shape and saliency) are extracted for each region, which include 81-dimensional grid color moment features, 59-dimensional local binary pattern texture features, 120-dimensional Gabor wavelets texture features, 37-dimensional edge orientation histogram features and 512-dimensional GIST features respectively. Followed by these features are clustered with the help of CLUTO toolkit
                        3
                        
                           http://glaros.dtc.umn.edu/gkhome/views/cluto/.
                     
                     
                        3
                      so as to train the GMM. Besides, σ is empirically set to the median value of all pairwise Euclidean distances between images and K is set to 50 to get the best performance by trial and error.

Without loss of generality, recall and precision of every word in the test set are computed and the mean of these values is exploited to measure the performance of the model. Similar to [5], for a given semantic word, recall=
                     B/C and precision=
                     B/A, where A is the number of images automatically annotated with a given keyword in the top 5 returned word list, B is the number of images correctly annotated with that keyword in the top 5 returned word list, and C denotes the number of images having that keyword in the ground truth annotation. In addition, the top n precision and coverage rate [40] are also employed to evaluate the performance of refining image annotation, in which top n precision (denoted as top_n_p(n)) evaluates the precision of top n ranked annotations for one image whereas top n coverage rate (denoted as top_n_c(n)) is defined as the percentage of images that are correctly annotated by at least one word among the first n ranked annotations.
                        
                           (14)
                           
                              top
                              _
                              n
                              _
                              p
                              (
                              n
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       ∈
                                       T
                                    
                                 
                              
                              
                                 
                                    precision
                                    (
                                    i
                                    ,
                                    n
                                    )
                                 
                                 
                                    |
                                    T
                                    |
                                    ·
                                    n
                                 
                              
                           
                        
                     
                     
                        
                           (15)
                           
                              top
                              _
                              n
                              _
                              c
                              (
                              n
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       ∈
                                       T
                                    
                                 
                              
                              
                                 
                                    coverage
                                    (
                                    i
                                    ,
                                    n
                                    )
                                 
                                 
                                    |
                                    T
                                    |
                                 
                              
                           
                        
                     where precision(i, n) is the number of correct annotations in top n ranked annotations for image i, T is the test image set and |T| denotes its size. On the contrary, coverage(i, n) judges whether image i contains correct annotations in the top n ranked ones. If at least one correct annotation of image i belongs to the top n ranked annotations, then coverage(i, n) is set to 1, otherwise by 0.

Since there are two variable weights λ and α to be determined, it should be first fixed one of them so as to observe the other’s varied trend and vice versa. Suppose that α is set to 0.5, then we range λ from 0 to 1. As shown in Fig. 3
                         (left), it is clear to observe that the performance is better when λ
                        ∈(0, 1) than λ
                        =0 or λ
                        =1 individually. In particular, the best result can be achieved when λ
                        =0.7, which demonstrates the complementary nature of label similarity and visual similarity of images associated with the corresponding labels. On the other hand, we range α from 0 to 1 with a fixed λ
                        =0.7. From the curve in Fig. 3 (right), it should be noted that the performance improves consistently before 0.5, followed by it almost keeps in a smooth state, afterwards the performance begins to decrease when α exceeds 0.7. Thus α
                        =0.5 can be chosen as the optimal parameter in our experiment.

To validate the effectiveness of this approach, we compare it with several previous approaches [3–5,8,17,18]. The experimental results listed in Table 2
                         are based on two sets of words: the subset of 49 best words and the complete set of all 260 words that occur in the training set. Note that “–” denotes no corresponding values can be directly acquired from the literature. From Table 2, it is easy to see that SGMM-RW apparently outperforms all the others, especially the first two approaches. Meanwhile, it is also superior to MBRM, LASSO, PLSA-RW and PLSA-MB by the gains of 8, 3, 4 and 3 words with non-zero recall, 28%, 10%, 19% and 10% mean per-word recall together with 13%, 13%, 8% and 4% mean per-word precision on the sets of 260 words respectively. Compared to PLSA-RW on the set of 49 best words, we can also get improvements in mean per-word recall, despite the mean per-word precision of SGMM-RW is the same as our previous work PLSA-RW.

In particular, we also compare SGMM-RW with several state-of-the-art refining image annotation methods WNM [9], RWRM [10] and PLSA-RW [17] respectively. As illustrated in Fig. 4
                         (left), it shows the precision of the top n ranked annotations for the image (n
                        =3, 4, 5, … , 9). Note that the precision decreases gradually with n increasing from 3 to 9, which reflects that the ranking of the words is on average consistent with the true level of accuracy. It is also worth noting that the precision of SGMM-RW is consistently higher than the corresponding ones of WNM, RWRM and PLSA-RW respectively. On the other hand, all the coverage rates displayed in Fig. 4 (right) behave uptrend. Specifically, it increases from 53.5% to 69.5% for SGMM-RW, 51% to 65.5% for PLSA-RW, 49% to 64.5% for RWRM and 39% to 64% for WNM, which further demonstrates the proposed refining image annotation method, by incorporating visual similarity of images associated with the corresponding labels into the refining process, can efficiently boost the performance of semantic based image annotation and retrieval.

To further investigate the performance of SGMM-RW, the mean average precision (mAP) is utilized as a metric to evaluate the performance of single word retrieval. Here, we only compare our model with CMRM [10], CRM [14], MBRM [7] and PLSA-RW [28] because mAPs of other models cannot be accessed directly from the literature. As shown in Table 3
                        , SGMM-RW is able to achieve significant improvements of 88%, 33%, 7% and 23% mean average precision on all 260 words over CMRM, CRM, MBRM and PLSA-RW respectively. Correspondingly, the gains of 85%, 37%, 6% and 23% mean average precision on the set of words that have positive recalls can be obtained.

From the perspective of probability theory, image retrieval can be seen as a procedure of ranking images in the database according to their posterior probabilities of being relevant to the query concept. To illustrate the effect of SGMM-RW, Fig. 5
                         presents the retrieval results obtained with single word queries on several challenging visual concepts being queries. Notice that each row displays the top five matches to the semantic query ‘coast’, ‘tiger’, ‘mountain’ and ‘flower’ from top to bottom respectively. The diversity of visual appearance of the returned images demonstrates that our model also has good generalization ability.

In this section, we test SGMM-RW for refining image annotation on Mirflickr25k dataset,
                           4
                           Download from http://press.liacs.nl/mirflickr/dlform.php.
                        
                        
                           4
                         which contains 25,000 images with 1386 labels. For the sake of fair comparison, the following features are extracted [39]: 225-dimensional block-wise color moment features, 128-dimensional wavelet texture features and 75-dimensional edge distribution histogram features based on an image partitioned into 5×5 fixed blocks. In total, a 428-dimensional visual feature is generated to represent an image. Next, these features are clustered by k-means algorithm. At the same time, 205 unique tags are obtained in total for Mirflickr25k by removing those tags whose occurrence numbers are less than 50. Table 4
                         summarizes the average performance measured by precision, recall and F-value for different refining methods. As can be seen from Table 4, the F-value of our model is 0.481, which gives statistically significant better result than the values obtained by UT (original tags) [41] and RWR [10]. In the meanwhile, it compares favorably with the state-of-the-art approaches PLSA-RW [17] and PLSA-MB [18] respectively, which further proves the good scalability and robustness of SGMM-RW when applied to large-scale image datasets.


                        Table 5
                         shows some exemplars of image annotation refinement (only four cases are listed here due to the limited space). For clarity, note that the enriched and re-ranked annotations compared to those produced by UT and LR-ES-CC-TC [16] are italic and underlined respectively. It can be easily observed that SGMM-RW is able to generate more accurate annotation results. To take the second image for example, there exists only one tag ‘girl’ in the original annotation. However, after refining by SGMM-RW, its annotation is enriched by the other three keywords ‘portrait’, ‘child’ and ‘face’, all of which are very appropriate and reasonable to describe the visual content of the image. In summary, the experiments on Mirflickr25k indicate that SGMM-RW is fairly stable and efficient with respect to its parameter settings.

@&#CONCLUSIONS@&#

This paper has proposed a refining image annotation method by integrating GMM with random walk as well as TSVM. Specifically, GMM is first used to estimate posterior probabilities of each annotation keyword for the image, during which TSVM is employed to improve the quality of training dataset. Next, a label similarity graph is constructed by a weighted linear combination of label similarity and visual similarity of images associated with the corresponding labels, which enables to construct an accurate semantic correlation of initial annotations. Afterwards a random walk process over the built label graph is exploited to further mine the correlation of the candidate annotations so as to capture the refining results. Experiments on Corel5k and Mirflickr25k datasets have shown its outstanding annotation performance over several state-of-the-art methods.

Except for the summary mentioned above, several important conclusions can be drawn as follows. First, similar to [5], when only a 36-dimensional feature vector for each image region/block is extracted for experiment, the results are nearly the same as that generated in previous text by trial and error, which indicates that there is no direct relationship between low-level visual features and high-level annotation performance, mainly involving the feature types and dimensions extracted. Second, measuring the similarity between pairwise concepts in the context of image annotation and retrieval is a challenging task if not impossible. Compared to WordNet and NGD, the pairwise similarity strategy proposed in this paper can efficiently avoid the noise data introduced by polysemous words. Third, since image segmentation is still an open issue in computer vision, Ncuts rather than JSEG is utilized to segment images into a number of meaningful regions here. So to explore more efficient image segmentation methods is helpful to boost the performance of SGMM-RW. Furthermore, image segmentation itself is a worthy of further research direction. Last but not the least, the proposed refining image annotation framework, in actual fact, can be extended with other methods to evolve new annotation models. Owing to this generality, we consider to connect the algorithm to process other multimedia data such as audio and video for our future work.

@&#ACKNOWLEDGEMENTS@&#

The author would like to sincerely thank the editor and anonymous reviewers for their valuable comments and insightful suggestions that have helped to improve the paper. In addition, this work is supported by the National Natural Science Foundation of China (Nos. 61035003, 61072085, 60933004, 60903141), the National Program on Key Basic Research Project (973 Program) (No. 2013CB329502), the National High-tech R&D Program of China (863 Program) (No. 2012AA011003) and the National Science and Technology Support Program of China (2012BA107B02).

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.knosys.2014.08.023.


                     
                        
                           Supplementary data 1
                           
                        
                     
                     
                        
                           Supplementary data 2
                           
                        
                     
                     
                        
                           Supplementary data 3
                           
                        
                     
                     
                        
                           Supplementary data 4
                           
                        
                     
                  

@&#REFERENCES@&#

