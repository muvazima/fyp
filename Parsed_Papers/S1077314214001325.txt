@&#MAIN-TITLE@&#Nonparametric label propagation using mutual local similarity in nearest neighbors

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Label propagation by means of nearest-neighbor search and “mutual local similarity”.


                        
                        
                           
                           Effectiveness and efficiency of quantized HoG for large scale image retrieval.


                        
                        
                           
                           Example application to low-quality underwater images and fish classification.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Big data

Nearest-neighbor

Object classification

Data-driven approaches

@&#ABSTRACT@&#


               
               
                  The shift from model-based approaches to data-driven ones is opening new frontiers in computer vision. Several tasks which required the development of sophisticated parametric models can now be solved through simple algorithms, by offloading the complexity of the task to the amount of available data. However, in order to develop data-driven approaches, it is necessary to have large annotated datasets. Unfortunately, manual labeling of large scale datasets is a complex, error prone and tedious task, especially when dealing with noisy images or with fine-grained visual tasks.
                  In this paper we present an automatic label propagation approach that transfers labels from a small set of manually labeled images to a large set of unlabeled items by means of nearest-neighbor search operating on HoG image descriptors. In particular, we introduce the concept of mutual local similarity between the labeled query image and its nearest neighbors as the condition to be verified for propagating labels.
                  The performance evaluation, carried out on the COREL 5K dataset and on a dataset of 20 million underwater low-quality images, showed how big data combined to simple nonparametric approaches allows to solve effectively complex visual tasks.
               
            

@&#INTRODUCTION@&#

In the last decade we have witnessed an explosion of the amount of visual content publicly available through the Internet. Such a huge amount of information has led the computer vision community towards a new research trend, aimed at exploiting the intrinsic variability of the data (both in terms of the number of image/video subjects and in the number of available unique portraits of each subject) to make the algorithms simpler.

As an example, let us consider the case of object classification, which, typically is tackled by training parametric models from a set of learning data, in order to make the model learn discriminative features associated to each object class [1]. This approach shows a few disadvantages, such as the need of a representative learning data set (large enough to cover the variability of possible class instances, but small enough to keep the learning time reasonable), the need to optimize the model’s parameters to suit the data distribution, and the complexity of the mathematical functions employed to check whether a given input image matches a certain class (or classes).

Nonparametric approaches exploit data to reduce the complexity of managing class models. For example, with big datasets it is possible to translate the classification task to something like “assign to an image a class from its most similar images” [2,3]. The reliability of such systems lies entirely on the data: if no images exist which are similar to the query, the closest sample might be unrelated and produce a wrong result. On the other hand, this is also the reason why these approaches are becoming popular only now: so far, the absence of datasets large enough to satisfy the hypothesis of “completeness over object class and pose” had pushed researchers towards model-based solutions; now, the high availability of images and video which can be simply retrieved by feeding specific keywords to web search engines (such as Google and Flickr) is drawing more and more attention towards the application of simple and well-known algorithms (such as k-means clustering [4] and nearest-neighbor search [5]) to large datasets for solving complex visual tasks, such as scene recognition, object detection, image annotation and location recognition (see Fig. 1
                     ).

However, all of these approaches depends on image quality (see examples in Fig. 2
                     , taken from [2,3]). In these applications, noise is mainly represented as labeling noise, whereas images are relatively clean, clear and at good resolution. However, this is not something that can be taken for granted for all applications, especially with data coming from sources operating in real-life conditions. One such example is the dataset collected in the EU-funded Fish4Knowledge
                        1
                        
                           http://fish4knowledge.eu.
                     
                     
                        1
                      project, which aimed at studying fish populations in their natural environment using videos taken by underwater cameras. In such scenario, weather conditions, water status, network bandwidth, storage capabilities and, most importantly, light propagation in water significantly compromise the visual level of details of detected fish, as shown in the examples of Fig. 2.

Nevertheless, the size of the resulting dataset (about 1.5 billion images) was large enough to cover a wide range of combinations of fish species, fish poses, lighting conditions, and scene backgrounds. From this consideration comes the idea behind this paper: why not try to label each image with an object class by exploiting the size and variability (albeit noisy) of the dataset itself?

The idea to achieve this objective is as follows: let us define a certain label to be assigned to each image; for example, this label may be, in our case, as simple as either “fish” or “non-fish”, or something more complex, such as the fish species. Since the size of the dataset allows us to assume that, given an input image, the most similar images will likely share the same label, we can exploit this assumption to label the whole dataset by expanding small sets of manually-labeled “seed” images through an iterative propagation approach, which consists in repeatedly searching for the nearest neighbors of seed images, applying a filtering approach to select the most relevant results (and reject false positives) and including the results into the corresponding label group. After each execution of this search-and-add procedure, newly-labeled images included in a given label group can be used as seeds for the next propagation runs, thus increasing the variability of the seeding sets and making the algorithm capable of including, from time to time, object instances not represented in the original labeled sets.

More in detail, this paper advances the recent state of the art on nonparametric label propagation methods in that:
                        
                           –
                           We propose an iterative label propagation method operating at large scale and extremely robust to false positives by using mutual local similarity between the query labeled image and the nearest neighbors as the condition to be verified for propagating labels; in particular, in contrast to image classification [2], where precision is relatively important, in our application we have a strict requirement on the precision as wrong label assignments might cascadingly affect further annotations and, therefore, the entire labeled dataset.

We prove that simple nonparametric approaches and feature descriptors (such as quantized HoG [6]) operating on big data allows to solve extremely complex tasks such as label propagation, which is still an open problem in computer vision and machine learning.

In this work, we mean to show, at a lower scale than the size of the whole dataset generated within the F4K project, a proof of concept of the automatic labeling process described above and of the solutions adopted to deal with the difficulties that the task presents. The method was tested on a subset of 20 million images with large variability of object classes and poses to make the “completeness over object class and pose” hypothesis still valid. However, we would like to highlight that the application to fish images is only presented as a proof of concept of the approach, which is generally applicable to any similar labeling task; for the same reason, implementation choices may be changed according to different application domains (e.g. the choice of other image features than HoG).

@&#RELATED WORK@&#

Data-driven approaches generally rely on comparing high-dimensional vectors in large datasets and have been applied in several applications: image classification [7,8], object recognition [2,9,10], 3D object detection [11] and recognition [12], image retrieval [13–16], scene recognition [2,17–19], graph building and partitioning [20,21].

A key method to support these tasks is k-nearest-neighbor (k-NN) search and it was theoretically demonstrated [22] that the error rate of a k-NN classifier tends to the Bayes optimal as the sample size tends to infinity. However, k-NN is a complex and challenging problem when the descriptor dimensionality and/or the size of the dataset increase. When the search space size dimensionality is relatively low (up to 10–15 dimensions), a classic solution is provided by k-d tree [23], which defines a recursive partition of the search space through a space- and time-efficient tree structure. However, the so-called “curse of dimensionality” makes NN algorithms hardly scalable as the number of dimensions increases, if not meaningless [5]; besides, in these cases no exact NN approach seems to perform better than linear search (i.e. brute force) [24].

Approaches have been suggested to make the search more efficient but less accurate through approximate NN (ANN) methods [25], which guarantee to find the nearest neighbor with “high probability” only; these approaches are suitable to tasks for which a close match to the real nearest neighbor will do, which is often the case for image classification or retrieval. Several ANN approaches employ tree structures similar to the k-d tree: priority-search trees [10,25,26] limit branch backtracking to a fixed number of candidate paths; some approaches partition the actual data rather than the search space, trying to exploit the data distribution to optimize the search process and perform better at high dimensionality [27–30]; randomized trees [11,31] exploit multiple independent trees to reduce the number of nodes to check; hierarchical k-means clustering (a.k.a. vocabulary trees) decompose the flat k-means clustering algorithm into a hierarchy of simpler (i.e. with lower k) k-means runs [9]; other approaches build k-d trees where partition hyperplanes are not necessarily aligned with the coordinate axis, through variance direction analysis [32,33].

In large databases, besides query time requirements, the amount of memory needed to manage the data structure becomes a critical factor: when dealing with millions or billions of images, described by hundreds of real-values features, memory optimization becomes a critical factor. Recently, efforts have been devoted to the problem of compact representations of descriptors for data retrieval scenarios [34–36]. Binary representations of high-dimensional image descriptors have been proposed for scene recognition [37] and image retrieval [16,38,39] in large databases, where NN search uses the Hamming distance for descriptor comparisons. Similarly, quantization approaches aim at obtaining short binary codes by discretizing the cardinality of the search space [40,41]. A recent approach for low-dimensional embedding has been proposed which performs exact NN search, rather than the approximate one [42].

Automatic image annotation or label propagation are the tasks of transferring information from labeled (manually or automatically) items to unlabeled ones based on similarity or closeness criteria. The information represented by the label(s) may vary: a set of tags associated to the image [43], the class of the portrayed object [44], a description of the scene, a set of objects present in the picture, etc. Existing approaches to perform such task can be classified as:
                        
                           •
                           Search-based approaches which adopt techniques to find images similar to the query and then process their labels in order to transfer the most suitable one.

Learning-based approaches which treat the annotation problem as a learning task.

Nearest-neighbor search has been extensively used as basis for the former type of approaches. In particular, NN-search-based label propagation retrieves similar images and then assigns a subset of their labels to the target item. However, most of the proposed methods are mainly focused on identifying the most suitable distance metrics to be used and do not rely entirely on the information coming from the data: several methods work in an image-descriptor space [44]; others exploit similarity based on a semantic distance function, learned by analyzing the relationship between visual features and image labels in a ground truth set [45]; the work in [46] presents baseline methods to combine multiple metrics; other approaches [43,47] apply metric learning techniques [48–51] for k-NN retrieval. Graph-based methods, in conjunction to k-NN search, have also been employed in order to post-process the nearest-neighbor set, by taking relative distance values into consideration or modeling label correlations [20,52]. For example, a recent work [53] employs Flickr metadata (photo data and tags, user information, comment threads, etc.) to build an enhanced correlation graph and improve label assignment; similarly in [54] dense semantic labeling and image patch correspondences are used for label propagation. Recently, random forests (i.e. sets of randomized trees) have been used to enhance semantic similarity in NN search tasks for image annotation [55].

Beside search based label propagation, also model-based solutions have been investigated where the propagation task has been seen as a learning problem, by computing statistical models which describe the correlation between image appearance and labels [56–61].


                     Table 1
                      summarizes some of the existing methods, highlighting the approach used, the adopted datasets and the results achieved in terms of precision/recall or 
                        
                           
                              
                                 F
                              
                              
                                 1
                              
                           
                        
                     -measure.

In general, search-based methods have shown better performance of many complex models and learning-based methods [46], although the former need large datasets to achieve good performance, and issues related to scalability and feature description at very large scale are still open.

@&#METHOD@&#

Our label propagation approach is based on a two-step nearest-neighbor search on a big dataset. In the first step, given a labeled image I, we perform a global (i.e. in the whole unlabeled dataset) k-NN search to find the set of the K images which are most similar to I, where K is a relatively large number; we will refer to 
                        
                           
                              
                                 N
                              
                              
                                 e
                              
                           
                        
                      as the “extended set” of I’s neighbors. This preliminary search is not meant to find images which are strictly similar to I, but to extract a large subspace which contains images that could actually share I’s label, while trying to keep some of the data variability of the whole unlabeled dataset. In the second step of the algorithm, we analyze the images in 
                        
                           
                              
                                 N
                              
                              
                                 e
                              
                           
                        
                      in order to decide to which we can safely assign I’s label. This decision depends on a condition of mutual local similarity: given image 
                        
                           
                              
                                 I
                              
                              
                                 e
                              
                           
                           ∈
                           
                              
                                 N
                              
                              
                                 e
                              
                           
                           ,
                           
                              
                                 I
                              
                              
                                 e
                              
                           
                        
                      will be assigned I’s label if I belongs to the set of the most similar images to 
                        
                           
                              
                                 I
                              
                              
                                 e
                              
                           
                        
                     , and 
                        
                           
                              
                                 I
                              
                              
                                 e
                              
                           
                        
                      belongs to the set of the most similar images to I. The “locality” of this requirement refers to the fact that this check is performed within the 
                        
                           
                              
                                 N
                              
                              
                                 e
                              
                           
                        
                      set, rather than the whole dataset. The “mutuality” constraint is necessary because 
                        
                           x
                           ∈
                           k
                           -NN
                           
                              
                                 
                                    y
                                 
                              
                           
                        
                      does not imply 
                        
                           y
                           ∈
                           k
                           -NN
                           
                              
                                 
                                    x
                                 
                              
                           
                        
                     .

Both mutual conditions can be easily verified: in order to make sure that 
                        
                           
                              
                                 I
                              
                              
                                 e
                              
                           
                        
                      belongs to the set of I’s most similar images, we can choose 
                        
                           
                              
                                 I
                              
                              
                                 e
                              
                           
                        
                      from the set of the first 
                        
                           
                              
                                 T
                              
                              
                                 1
                              
                           
                           ≪
                           K
                        
                      elements in 
                        
                           
                              
                                 N
                              
                              
                                 e
                              
                           
                        
                      (assuming elements are sorted by their distance from I); similarly, in order to make sure that I belongs to set of the most similar images to 
                        
                           
                              
                                 I
                              
                              
                                 e
                              
                           
                        
                     , we can perform a k-NN search on the 
                        
                           
                              
                                 N
                              
                              
                                 e
                              
                           
                        
                      set using 
                        
                           
                              
                                 I
                              
                              
                                 e
                              
                           
                        
                      as query, and verify whether I is within the first 
                        
                           
                              
                                 T
                              
                              
                                 2
                              
                           
                        
                      of 
                        
                           
                              
                                 I
                              
                              
                                 e
                              
                           
                        
                     ’s neighbors. For this second check, it is important that, if images exist which are more similar to 
                        
                           
                              
                                 I
                              
                              
                                 e
                              
                           
                        
                      than to I, they are included in 
                        
                           
                              
                                 N
                              
                              
                                 e
                              
                           
                        
                     : this is the reason why we choose a large value for K in the first step of the algorithm.

Let 
                           
                              V
                              =
                              
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             v
                                          
                                          
                                             2
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             v
                                          
                                          
                                             n
                                          
                                       
                                    
                                 
                              
                              ⊂
                              
                                 
                                    R
                                 
                                 
                                    d
                                 
                              
                           
                         be the set of feature vectors describing the appearance of the n images belonging to the “big” dataset.

Each element 
                           
                              
                                 
                                    v
                                 
                                 
                                    i
                                 
                              
                           
                         in the dataset can be assigned a label from the set 
                           
                              L
                              =
                              
                                 
                                    
                                       
                                          
                                             L
                                          
                                          
                                             0
                                          
                                       
                                       ,
                                       
                                          
                                             L
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       
                                          
                                             L
                                          
                                          
                                             2
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             L
                                          
                                          
                                             m
                                          
                                       
                                    
                                 
                              
                           
                        , and the corresponding label will be identified as 
                           
                              
                                 
                                    l
                                 
                                 
                                    i
                                 
                              
                           
                        ; the 
                           
                              
                                 
                                    L
                                 
                                 
                                    0
                                 
                              
                           
                         value will be used to mean “no label has been assigned” (so the number of labels is m). Therefore, the dataset can be split into a labeled subset 
                           
                              
                                 
                                    V
                                 
                                 
                                    l
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                       |
                                       
                                          
                                             l
                                          
                                          
                                             i
                                          
                                       
                                       
                                       ≠
                                       
                                       
                                          
                                             L
                                          
                                          
                                             0
                                          
                                       
                                    
                                 
                              
                           
                         and an unlabeled subset 
                           
                              
                                 
                                    V
                                 
                                 
                                    u
                                 
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                       |
                                       
                                          
                                             l
                                          
                                          
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             L
                                          
                                          
                                             0
                                          
                                       
                                    
                                 
                              
                           
                        .

The proposed label propagation method starts from a small 
                           
                              
                                 
                                    V
                                 
                                 
                                    l
                                 
                              
                           
                         set, which initially contains 
                           
                              
                                 
                                    n
                                 
                                 
                                    l
                                 
                              
                           
                         elements. At each iteration, 
                           
                              
                                 
                                    n
                                 
                                 
                                    l
                                 
                              
                           
                         elements are selected randomly from this set and used as propagation seeds. Limiting the number of seeds per iteration to 
                           
                              
                                 
                                    n
                                 
                                 
                                    l
                                 
                              
                           
                         (rather than scanning the whole 
                           
                              
                                 
                                    V
                                 
                                 
                                    l
                                 
                              
                           
                         set every time) prevents from labeling too many elements in a single iteration, thus maintaining an approximately linear (rather than exponential) labeling rate.

Summarizing, the method consists of a sequence of iterations of the following steps:
                           
                              1.
                              Select a random seeding set 
                                    
                                       
                                          
                                             V
                                          
                                          
                                             s
                                          
                                       
                                    
                                  from 
                                    
                                       
                                          
                                             V
                                          
                                          
                                             l
                                          
                                       
                                    
                                 , containing 
                                    
                                       
                                          
                                             n
                                          
                                          
                                             l
                                          
                                       
                                    
                                  elements.

For each seed sample 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             s
                                          
                                       
                                    
                                  in 
                                    
                                       
                                          
                                             V
                                          
                                          
                                             s
                                          
                                       
                                    
                                 :
                                    
                                       (a)
                                       Perform k-NN search in 
                                             
                                                
                                                   
                                                      V
                                                   
                                                   
                                                      u
                                                   
                                                
                                             
                                           in order to find the K elements which are most similar to 
                                             
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      s
                                                   
                                                
                                             
                                          . Let us call the retrieved set 
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      e
                                                   
                                                
                                                =
                                                
                                                   
                                                      
                                                         
                                                            
                                                               n
                                                            
                                                            
                                                               1
                                                            
                                                         
                                                         ,
                                                         
                                                            
                                                               n
                                                            
                                                            
                                                               2
                                                            
                                                         
                                                         ,
                                                         …
                                                         ,
                                                         
                                                            
                                                               n
                                                            
                                                            
                                                               K
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                           the “extended” set of 
                                             
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      s
                                                   
                                                
                                             
                                          ’s neighbors.

Let 
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      s
                                                   
                                                
                                             
                                           be the subset of 
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      e
                                                   
                                                
                                             
                                           containing the closest 
                                             
                                                
                                                   
                                                      T
                                                   
                                                   
                                                      1
                                                   
                                                
                                             
                                           samples; this will be the set of candidates to propagate 
                                             
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      s
                                                   
                                                
                                             
                                          ’s label to.

Analyze each element in 
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      s
                                                   
                                                
                                             
                                          , in order to remove false positives. To do so, for each element 
                                             
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      i
                                                   
                                                
                                                ∈
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      s
                                                   
                                                
                                             
                                          , compute its 
                                             
                                                
                                                   
                                                      T
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                           nearest neighbors, using 
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      e
                                                   
                                                
                                             
                                           as search space. Let 
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                           be the retrieved set; if 
                                             
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      s
                                                   
                                                
                                                
                                                ∉
                                                
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          , remove 
                                             
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                           from 
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      s
                                                   
                                                
                                             
                                          .

This step enforces the mutual local similarity requirement explained earlier.

Assign 
                                             
                                                
                                                   
                                                      v
                                                   
                                                   
                                                      s
                                                   
                                                
                                             
                                          ’s label to all remaining elements in 
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      s
                                                   
                                                
                                             
                                           (thus automatically adding them to 
                                             
                                                
                                                   
                                                      V
                                                   
                                                   
                                                      l
                                                   
                                                
                                             
                                          ).

If 
                                    
                                       
                                          
                                             V
                                          
                                          
                                             u
                                          
                                       
                                       
                                       ≠
                                       
                                       ∅
                                    
                                 , go back to the first step.

Label propagation algorithm: the label of a known element is propagated to the most similar unlabeled elements in the dataset. 
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                
                                                   
                                                
                                             
                                             
                                          
                                       
                                    
                                 
                              
                           

The approach described in Algorithm 1 allows to label the whole dataset by propagating a known label to the set of most similar unlabeled images. However, as the process goes on, fewer and fewer unlabeled images remain, which may affect the effectiveness of the nearest-neighbor search, since the “completeness over object class and pose” hypothesis might not hold anymore as the size of the unlabeled dataset decreases.

For this reason, after a certain percentage P of the dataset has been labeled, we switch to a different labeling approach, where labels are propagated through a “reverse” nearest-neighbor search, where the query object is an unlabeled element and the search space is the set of previously-labeled elements, 
                           
                              
                                 
                                    V
                                 
                                 
                                    l
                                 
                              
                           
                        . In other words, when the unlabeled dataset’s size is close to becoming too small, we circumvent the problem by turning our task from “find all unlabeled images which might share this label” to “find the possible labels which may be assigned to this unlabeled image”. This set of “possible labels” is obtained through k-NN search in the labeled dataset, and the label decision is based on conditional majority voting: the most common label within the retrieved set is assigned to the input unlabeled image, subject to a condition: all images within the retrieved set must be closer to the query image than a given distance threshold 
                           
                              
                                 
                                    T
                                 
                                 
                                    d
                                 
                              
                           
                        . Without this check, an unlabeled image could be assigned a certain label even if the closest labeled images are completely different. The distance threshold is computed at the beginning of the “reverse” label propagation phase, as the average distance between elements satisfying “mutual local similarity” labeled up to that point, plus two times the standard deviation of those distances.

The following is a more detailed description of this second labeling stage:
                           
                              1.
                              Select a random unlabeled element 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             u
                                          
                                       
                                    
                                  from 
                                    
                                       
                                          
                                             V
                                          
                                          
                                             u
                                          
                                       
                                    
                                 .

Perform k-NN search in 
                                    
                                       
                                          
                                             V
                                          
                                          
                                             l
                                          
                                       
                                    
                                  in order to find the 
                                    
                                       
                                          
                                             k
                                          
                                          
                                             r
                                          
                                       
                                    
                                  elements which are most similar to 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             u
                                          
                                       
                                    
                                 . Let us call the retrieved set 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             u
                                          
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   
                                                      n
                                                   
                                                   
                                                      1
                                                   
                                                
                                                ,
                                                
                                                   
                                                      n
                                                   
                                                   
                                                      2
                                                   
                                                
                                                ,
                                                …
                                                ,
                                                
                                                   
                                                      n
                                                   
                                                   
                                                      
                                                         
                                                            k
                                                         
                                                         
                                                            r
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 .

If the distance between 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             u
                                          
                                       
                                    
                                  and 
                                    
                                       
                                          
                                             n
                                          
                                          
                                             
                                                
                                                   k
                                                
                                                
                                                   r
                                                
                                             
                                          
                                       
                                    
                                  is higher than the distance threshold 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             d
                                          
                                       
                                    
                                 , go back to the first step.

Assign to 
                                    
                                       
                                          
                                             v
                                          
                                          
                                             u
                                          
                                       
                                    
                                  the most common label found in 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             u
                                          
                                       
                                    
                                 .

If 
                                    
                                       
                                          
                                             V
                                          
                                          
                                             u
                                          
                                       
                                       
                                       ≠
                                       
                                       ∅
                                    
                                 , go back to the first step.


                        Algorithm 2 shows the pseudo-code version of this second approach.

It should be noted that the label propagation method is independent from the employed image descriptors and it may be used with any feature descriptors for which it is possible to use k-NN search.
                           Algorithm 2
                           “Reverse” variant of the label propagation algorithm: the label of an unknown element is inferred by those of its most similar labeled elements. 
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                
                                                   
                                                
                                             
                                             
                                          
                                       
                                    
                                 
                              
                           

@&#EXPERIMENTAL RESULTS@&#

The performance evaluation of the proposed method consisted of the following steps:
                        
                           •
                           First, we tested, as to serve as baseline, our label propagation method on the COREL 5K dataset (containing 5K images of 50 object classes, and commonly employed for automatic image annotation evaluation) and on our underwater image dataset selecting randomly 1K (in order to have the same number of images per class of the COREL 5K dataset) images equally distributed among ten fish species.

Secondly, we tested our approach in two cases, when transferring, from a small manual labeled dataset to the 20 million one, simple labels such as “fish” and “non-fish” and richer semantic labels such as fish species.

We also evaluated the sensitivity of our method to parameters and compared HoG descriptors to a binary hashing method for efficient and accurate image retrieval.

Finally, we showed the effectiveness of a simple fish species classification method based on k-NN search in the automatically-labeled dataset.

As image descriptor for underwater images we used Histogram of Gradients (HoG) [6]. Before computing HoG descriptors, images were resized so that they all have the same number of blocks and would produce constant-sized feature vectors. The target resolution was set to 32×32, which has been shown in [2] to be the smallest resolution at which visual tasks can be performed. In detail, after resizing the image to 32×32 pixels and converting it to grayscale, our HoG descriptor is computed by dividing the image into a 4×4 block grid with no overlap (block size: 8×8 pixels), and in turn dividing each block into a 4×4 cell grid (cell size: 4×4 pixels); 9 bins are used for the orientation histograms. In order to reduce memory requirements, 8-bit quantization is performed on HoG values. Color features were not considered since, in “real-life” underwater environment, light propagation in water strongly affects how colors appear, so it often happens that fish of different species look exaclty the same and vice versa as shown in Fig. 3
                     .

In order to assess the complexity of the proposed underwater image dataset and also to provide a baseline for our label propagation method, we tested its performance on COREL 5K and a small subset of our underwater image dataset with only 1K images (“Fish 1K”). For each dataset, we selected 20% of labeled images (distributed equally over the object classes) as initial seeds and then propagated their labels to the remaining image dataset (80% of images); HoG representation was used on both datasets to compare the respective results. The performance were measured as average precision and average recall and the results on the COREL 5K are given in Table 2
                         compared to the ones achieved by state of the art methods.

The performance of our iterative label propagation method reaches state of the art performance; after, [46] had already demonstrated how simple combinations of features for finding nearest neighbors may achieve results comparable to those by complex learning-based approaches for automatic image labeling. As shown in Table 2, the achieved recall was sensibly lower than the precision because our approach is strongly conservative as it tends to include into the labeled dataset only images with a very high similarity (guaranteed by the mutual local similarity condition) to the query images, to avoid that wrong assignments affect the quality of the entire dataset.

The average precision and recall obtained on our fish dataset (Fish 1K) were, respectively, 0.71 and 0.35, thus higher than the ones obtained on COREL 5K. This is due to: (a) each image of the Fish 1K dataset contains only one fish and the background is relatively simple, while in COREL 5K the background is more complex and images may contain more than one object, thus requiring upstream segmentation modules and (b) the number of classes in the fish dataset is much less (10 classes) than in the COREL 5K dataset (50 classes). The COREL 5K dataset contains also object classes where images contain only one object, and on these classes (see Fig. 4
                        ) the performance were comparable (in some cases even better) to the ones obtained on the Fish 1K dataset. This, in particular, shows the complexity of the underwater image dataset, where fish of different species may look very similar and, as such, be misclassified. Fig. 5
                         shows some examples of the COREL 5K object classes where our approach obtained the lowest performance. However, the relatively low performance on COREL 5K dataset was partly due to an incorrect choice of image descriptors; in fact, just employing HoG of color images allowed to increase the performance by about 10%.

The second phase of our performance evaluation involved the assessment of the label propagation method in two cases of increasing difficulty: (a) simple labels such as “fish” and “non-fish” and (b) fish species labels.

As mentioned earlier, the nearest-neighbor search problem in high-dimensional spaces is still largely debated, since in practice no solutions seem to perform better than “brute force” search, which has a complexity of 
                           
                              O
                              (
                              nd
                              )
                           
                         (
                           
                              n
                           
                         and 
                           
                              d
                           
                         being respectively the size of the dataset and the dimensionality of the feature space). As this is not an essential point for this work, we chose not to investigate the adoption of efficient structures for nearest-neighbor search: using linear search over the 20-million dataset, each query took about 5s.

As shown in Fig. 2, the 20-million dataset contains images of both fish and underwater background elements. The first task we tested our label propagation method for was the separation between the two object classes.

The initial set of labeled elements 
                           
                              
                                 
                                    V
                                 
                                 
                                    l
                                 
                              
                           
                         was generated manually and contained 5,000 items, equally divided between “fish” and “non-fish” images.

Precision and recall per object class and their averaged values across all classes, obtained by comparing the labels assigned by the algorithm to 200,000 manually-labeled images, were considered for performance analysis and the results are shown in Table 3
                        , together with the number of labeled images.

After obtaining the “filtered” dataset containing about 15 million images of fish (see Table 3), we ran the label propagation algorithm to propagate the labels of the 8 fish species shown in Fig. 6
                        . Although this could look like an image classification problem, it actually is not: the aim of this process was to expand a small set of labeled images by robustly propagating those labels to the unknown dataset. In this context, precision is much more important than recall: increasing the initial labeled significantly and with correct labels is more than trying to label the whole dataset.

In this case, we used 4000 images split into 8 fish species (500 images per species) as initial seeds to propagate labels to the 15-million “fish” dataset.


                        Table 4
                         shows precision and recall per fish species. Similarly to the “fish”/“non-fish” case, these values were obtained by comparing the labels assigned by the algorithm to 32,000 manually-labeled images (equally divided among the 8 fish species).

The high precision obtained by the label propagation method shows the effectiveness of our mutual local similarity constraint, which was exactly targeted at strictly propagating the labels only to reliable candidate, in order to prevent that wrong labels would cascadingly affect the quality of the annotated dataset.

Also, as a direct consequence of this constraint, average recall was not as high as precision, i.e. we missed images which might have been included in the labeled set but were not. Although this might seem a downside of the approach, achieving optimal recall was not the main objective of the label propagation method, which had been designed to robustly expand the initial labeled set.

The results presented in this section were obtained by running the proposed label propagation algorithm on an Intel i7 3.4GHz CPU and 16GB RAM; the code was written in Matlab 2013. Using the 8-bit feature representation the whole 20-million dataset required 2.8GB of memory.

The label propagation algorithm presented in Section 3 depends on several parameters (e.g. nearest-neighbor set sizes and thresholds) used to force the “mutual local similarity” constraint on label transfers. Before we present the experimental results obtained with different parameter configurations, it is important to understand their effect on the overall algorithm:
                           
                              •
                              
                                 
                                    
                                       K
                                    
                                  is the size of the “extended neighbor set” of a labeled item. This set should be large enough to retain a similar internal variability as the whole dataset, so that it contains both good propagation candidates and items closer to false positives than the labeled seed; thus, the value of 
                                    
                                       K
                                    
                                  depends on the dataset size and characteristics. A high value of 
                                    
                                       K
                                    
                                  favors data variability, but increases execution times, since each propagation candidate has to perform 
                                    
                                       k
                                    
                                 -NN search on the extended neighbor set.


                                 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             1
                                          
                                       
                                    
                                  is the number of propagation candidates for a given labeled seed. Typically, 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             1
                                          
                                       
                                    
                                  should be small enough to make sure that only elements which are very similar to the seed can be assigned the same label; however, the smaller 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             1
                                          
                                       
                                    
                                  is, the more nearest-neighbor searches across the whole dataset are required to label the same amount of items. Interestingly, Table 6b shows that the computation time rises when 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             1
                                          
                                       
                                    
                                  is too high. This happens because each propagation candidate has to be checked against the mutual local similarity constraint, which requires performing each time a nearest-neighbor search in the extended neighbor set; however, the further the candidate is from the seed element, the more visually different the two are, and the less likely it is that the candidate will actually be assigned the seed’s label. In practice, if 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             1
                                          
                                       
                                    
                                  is too high, we perform much more processing while still labeling the same number of elements that we would with a lower 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             1
                                          
                                       
                                    
                                 .


                                 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             2
                                          
                                       
                                    
                                 , given seed 
                                    
                                       s
                                    
                                  and candidate 
                                    
                                       c
                                    
                                 , is the number of nearest neighbors of 
                                    
                                       c
                                    
                                 ’s within which 
                                    
                                       s
                                    
                                  must reside for 
                                    
                                       s
                                    
                                 ’s label to be propagated to 
                                    
                                       c
                                    
                                 . If it is too large, a label might be propagated to wrong candidates; similarly to 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             1
                                          
                                       
                                    
                                 , if it is too low we are increasing the strictness of the mutual local similarity constraint, which causes less items to be labeled at each cycle.


                                 
                                    
                                       P
                                    
                                  is the percentage of completion of the labeling process at which we switch from the “forward” approach (i.e. propagate a label by finding a seed’s unlabeled neighbors) to the “reverse” one (i.e. propagate a label by finding the labeled neighbors of an unlabeled item); that is, the switch occurs after a 
                                    
                                       P
                                    
                                  fraction of the whole dataset has been labeled. The choice of 
                                    
                                       P
                                    
                                  depends on three considerations: (1) for the forward propagation to work, enough unlabeled elements should exist (otherwise dataset variability is compromised), so reverse propagation should be enabled at a certain point; (2) for the reverse propagation to work, enough elements should have already been labeled, so it cannot be enabled too early; (3) reverse propagation is much slower the forward one, since you have to perform a whole-dataset nearest-neighbor search for each unlabeled item.


                                 
                                    
                                       
                                          
                                             k
                                          
                                          
                                             r
                                          
                                       
                                    
                                  is the size of the voting set used to label an unlabeled element with reverse propagation; in other words, given an unlabeled element, the voting set is composed of its first 
                                    
                                       
                                          
                                             k
                                          
                                          
                                             r
                                          
                                       
                                    
                                  nearest neighbors among the labeled portion of the dataset. This parameter has no effect on computation time; the size of the voting set should be small, in order to take into consideration only the most similar neighbors; however, too small a voting set may give too much weight to “intruders” in the nearest-neighbor set.

The choice of the above parameters represents a trade-off between false-positive avoidance and processing time. In order to evaluate this trade-off, we performed several runs of the fish-species label propagation algorithm, with different parameter configurations, on a reduced dataset containing one million elements, using a ground-truth set of 32,000 manually-labeled fish images.


                        Table 5
                         shows, for each parameter, the set of values we tested and, in bold, the value used to obtain the results shown in Section 4.2. To compare different parameter configurations, we used the F-measure score, which combines precision and recall into a single value, making it easier to sort the results:
                           
                              (1)
                              
                                 
                                    
                                       F
                                    
                                    
                                       1
                                    
                                 
                                 =
                                 2
                                 ·
                                 
                                    
                                       Precision
                                       ·
                                       Recall
                                    
                                    
                                       Precision
                                       +
                                       Recall
                                    
                                 
                              
                           
                        For each parameter, Table 6
                         shows how F-measure and computation time change as each parameter changes, while keeping the others to their default values; an exception is Table 6e, where 
                           
                              
                                 
                                    k
                                 
                                 
                                    r
                                 
                              
                           
                         is evaluated: in order to increase as much as possible 
                           
                              
                                 
                                    k
                                 
                                 
                                    r
                                 
                              
                           
                        ’s influence on the results, for those experiments 
                           
                              P
                           
                         was set to 0.4 (rather than its default value, 0.8), which is why the corresponding score for 
                           
                              
                                 
                                    k
                                 
                                 
                                    r
                                 
                              
                           
                        
                        =10 is not 88.9%, as would be if all parameters were set to their default values.

The results show that the default parameter configuration (highlighted in bold in Table 6) represented the best trade-off between accuracy and processing times: some configurations yielded slightly better results (for example, by decreasing 
                           
                              
                                 
                                    T
                                 
                                 
                                    1
                                 
                              
                           
                        , i.e. by making the mutual local similarity constraint stricter), but the execution time rose excessively; conversely, more efficient configurations unavoidably caused a decrease in the accuracy (since they ended up relaxing the mutual local similarity constraint).

In order to enable the labeling of the whole 20-million dataset on a single machine, we employed a simple 8-bit quantization method to reduce the size of the 144-dimensional HoG descriptors. In fact, a lot of interest has been recently put by researchers in the study of how to scale search algorithms with respect to both query time and memory, when the size of datasets is in the order of tens or hundreds of millions. The main trend of research has focused in locality-sensitive hashing algorithms, which are designed to hash a real-valued vector (e.g. a point in a feature space) into a short binary code, in such a way that similar hash codes correspond to similar input vectors; the compact binary codes can then be used directly to perform exhaustive nearest-neighbor search using efficient metrics (such as the Hamming distance) [66], or to build an index for performing fast neighbor lookups [67].

As a further contribution of this paper and a preliminary step for future work, we tested the application of the recently-proposed Multi-Dimensional Spectral Hashing (MDSH) [68] to the label propagation task described in Section 3, in order to provide an indication of the actual utility of hashing methods in real problems and to compare its performance to the simple HoG quantization used in our iterative label propagation approach.

First of all, we evaluated how well the Hamming distance on the descriptors’ hashes models the Euclidean distance in a nearest-neighbor retrieval task. In practice, for a given query descriptor 
                           
                              q
                           
                         from a test set 
                           
                              Q
                           
                        , we retrieved the nearest-neighbor sets 
                           
                              
                                 
                                    N
                                 
                                 
                                    H
                                    ,
                                    q
                                 
                              
                           
                         (using the Hamming distance on the hashed descriptors) and 
                           
                              
                                 
                                    N
                                 
                                 
                                    E
                                    ,
                                    q
                                 
                              
                           
                         (using the Euclidean distance on the quantized HoG descriptors), and we computed the accuracy indicator 
                           
                              A
                           
                         as the average number of elements in 
                           
                              
                                 
                                    N
                                 
                                 
                                    H
                                    ,
                                    q
                                 
                              
                           
                         which were also found in 
                           
                              
                                 
                                    N
                                 
                                 
                                    E
                                    ,
                                    q
                                 
                              
                           
                        :
                           
                              (2)
                              
                                 A
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       
                                          
                                             
                                                Q
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          q
                                          ∈
                                          Q
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      H
                                                      ,
                                                      q
                                                   
                                                
                                                ∩
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      E
                                                      ,
                                                      q
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      N
                                                   
                                                   
                                                      E
                                                      ,
                                                      q
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        Of course, the results depend on the MDSH parameters: the length of the binary codes 
                           
                              b
                           
                         (measured in bits) and a 
                           
                              σ
                           
                         parameter, which we set to 0.5, as suggested by the authors of [68]. Fig. 7
                         shows the accuracy values for different bit lengths of the hashes when training MDSH with 50,000 random descriptors (which were then excluded from the test set) and computing the 
                           
                              k
                           
                         nearest neighbors (
                           
                              k
                              =
                              200
                           
                        ) on 10,000 random test descriptors.

The results show that even when using a relatively high number of bits, the MDSH-based 
                           
                              k
                           
                        -NN algorithm returned only about 40% of the exact nearest-neighbor set. Although this might seem a low value, it should be noted that the remaining 60% of retrieved items are not necessarily unrelated to the exact neighbors; whether this relevance is sufficient depends on the specific task one needs these neighbors for.

For the label propagation process on the one-million test dataset, we hashed our descriptors to 384-bit binary strings, which was a good compromise between the average accuracy and bit length (which of course has little influence on a small dataset, but easily becomes a decisive factor with larger ones). Without any particular hardware optimizations, the running time for the hashing-based label propagation algorithm was reduced to less than a third of the time needed by the default parameter configuration on the same one-million dataset (59h vs 198h), while the F-measure score was 83.3%, not much less than the value of 88.9% obtained using quantized HoG descriptors (see Table 6).

To conclude, although this experiment was only meant as a proof of concept on a small dataset, it is still a very promising result, which shows both the potentiality of hashing techniques and the robustness of our labeling approach to the approximation introduced by dimensionality reduction and the use of Hamming distance.

In order to show the utility of the obtained labeled dataset (and to confirm the effectiveness of nonparametric approaches based on big data in solving complex tasks), we evaluated the performance of a simple voting-based fish classification method. The classification scheme employed a similar strategy as the label propagation method: for each image to be classified, 
                           
                              k
                           
                        -NN search was used to retrieve the 
                           
                              K
                              =
                              50
                           
                         most similar images in the labeled dataset and, according to the majority of the labels of the retrieved images, the corresponding object class was assigned to the query image.

As testing set we used 32,000 manually-labeled fish images, while the search space of the 
                           
                              k
                           
                        -NN classifier was a modified version of the dataset shown in Table 4. In particular, we removed from that dataset all near-duplicate images (i.e. images taken from the same videos or which were part of the same trajectory of a fish) resulting in a dataset of about 2.8 million images. Despite the presence of some inaccurate labels in the automatically-labeled dataset, the relative simplicity of the classification method, and the low quality of the images, we obtained high performance as shown in Table 7
                        .

Of course, given the originality of the dataset, performing a comparison of these results with existing methods is problematic. The closest work is the one proposed in [69], which employed hierarchical classification based on a Balance-Guaranteed Optimized Tree for fish species recognition on a dataset derived from the same underwater cameras used in this work. In particular, the authors reported an average precision and recall of, respectively, 0.90 and 0.91 on a dataset with 3179 fish images. This dataset was strongly unbalanced, with more than a half of the images belonging to only one fish species (Dascyllus Reticulatus), on which they obtained about 0.96 of both precision and recall, whereas for fish species with fewer labeled images (e.g. Acanthurus nigrofuscus) the results were lower, i.e. about 0.7 for precision and 0.78 for recall.

Unfortunately, the method described in [69] could not be tested on our dataset, since the training step would have taken years to complete; nevertheless, besides the exact numerical comparison between the two methods – which would be inappropriate due to the different datasets – it is interesting to note that the simple method presented in this paper, used in conjunction to a large dataset, is able to reach performance comparable to (when not better than) a much more complex approach in similar conditions, thus confirming the motivations behind the switch of the recent trend of research towards data-driven methods.

In this paper we presented a label propagation approach able to operate robustly at large scale and with low-quality images. Our method relies on the assumption, that, in datasets big enough to cover a wide range of object appearances, given a labeled image it is possible to find very similar images to which propagate its label. According to this concept, our method expands small sets of manually-labeled “seeds” through an iterative propagation process based on a mutual local similarity between query image and nearest neighbors.

The method was first tested on the COREL 5K dataset where it showed satisfactory performance when compared to state-of-the-art approaches. Then, we tested it on a dataset of 20-million underwater images showing high performance, especially in terms of precision. We, therefore, proved that with big data, simple techniques and descriptors (such as HoG) are able to achieve very accurate results also in complex visual tasks such as identifying objects at the subordinate level.

As future work, we plan to extend this work to the whole 1.5-billion-image dataset and to different object classes. The issues related to this large-scale workflow are not trivial. First of all, the memory required to store the whole 1.5-billion-image dataset needs the use of dedicated computing cluster infrastructures. For example, using 128-dimensional 32-bit image descriptors (e.g. HoG as in [6]) would take about 700GB RAM. As a consequence, suitable image description methods need to be investigated as well, to ease memory requirements. Another aspect related to the size of the dataset is the nearest-neighbor search time: assuming that high-dimensional image description vectors will be needed, no search algorithms currently exist which guarantee to perform better than linear search (i.e. brute force) [24], which can be a problem with such a large number of items. For this reason, locality-sensitive hashing schemes for the nearest neighbor problem will be further investigated taking into account that the preliminary results, reported on this paper, are encouraging, especially concerning the trade-off between processing times and label propagation accuracy.

@&#REFERENCES@&#

