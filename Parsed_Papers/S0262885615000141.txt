@&#MAIN-TITLE@&#Unrestricted pose-invariant face recognition by sparse dictionary matrix

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We have presented a method for facial expression invariant face recognition.


                        
                        
                           
                           We propose a novel method for real-world pose-invariant face recognition.


                        
                        
                           
                           Proposed method use a single image in gallery with any facial expressions.


                        
                        
                           
                           We generate a sparse dictionary matrix for each people based on triplet pose angles.


                        
                        
                           
                           Convincing results were acquired to handle pose on the FERET, CMU PIE and LFW databases.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Real-world

Facial expression generic elastic models

Face synthesis

Pose-invariant face recognition

Sparse representation

Sparse dictionary matrix

@&#ABSTRACT@&#


               
               
                  In this paper, a novel method is proposed for real-world pose-invariant face recognition from only a single image in a gallery. A 3D Facial Expression Generic Elastic Model (3D FE-GEM) is proposed to reconstruct a 3D model of each human face using only a single 2D frontal image. Then, for each person in the database, a Sparse Dictionary Matrix (SDM) is created from all face poses by rotating the 3D reconstructed models and extracting features in the rotated face. Each SDM is subsequently rendered based on triplet angles of face poses. Before matching to SDM, an initial estimate of triplet angles of face poses is obtained in the probe face image using an automatic head pose estimation approach. Then, an array of the SDM is selected based on the estimated triplet angles for each subject. Finally, the selected arrays from SDMs are compared with the probe image by sparse representation classification. Convincing results were acquired to handle pose changes on the FERET, CMU PIE, LFW and video face databases based on the proposed method compared to several state-of-the-art in pose-invariant face recognition.
               
            

@&#INTRODUCTION@&#

Real-world pose-invariant face recognition is one of the most difficult and challenging tasks in computer vision due to large changes in poses of human faces. Typical face recognition methods have been prosperous at working under controlled situations. However, carrying out pose-invariant face recognition is very difficult in real-world situations when alternations in illumination and facial expression exist.

Performances of face recognition systems are great under controlled situations, but expressively reduce for images displaying large pose, illumination and facial expression variations. Handling of head pose change is thus one of the most essential aspects for a concrete real-time face recognition method. Although there have been many advances in the last few years to incorporate robustness into head pose change in face recognition approaches, it is still a challenge to attain this manner under unrestrained situations due to precision and speed problems.

Available pose-invariant face recognition methods can be mostly categorized into two separate types: 1) 2D methods which include subspace-based techniques which present nonlinearities into a 2D shape to extract new poses [21,2], view-based methods which utilize a subset of shapes to demonstrate appearance from a diverse view [3,4] and etc., and 2) 3D model-based methods which employ a 3D model to present new poses of faces [1,5–18]. In this paper, a novel method from the third type is proposed.

A main problem of the above studies is that face images acquired under controlled conditions (e.g., the FERET database) are considered, which are usually frontal, occlusion-free, with a clean background, consistent lighting, and limited facial expressions. However, in real-world applications, face recognition needs to be performed on real-life face images captured in unconstrained scenarios. There are significant appearance variations on real-world faces, which include facial expressions, illumination changes, head pose variations, and etc. Therefore, face recognition in real-world faces is much more challenging compared to the case for faces captured in constrained conditions.

In this paper, a novel approach is proposed for unconstrained face recognition from real-world face image by 3D face reconstruction and sparse representation. Accordingly, a 3D model is initially reconstructed from frontal face images in a real-world gallery. To reconstruct a 3D model from each human face in real-world scenarios, a Facial Expression Generic Elastic Model (FE-GEM) is proposed. The FE-GEM method for facial expression-invariant 3D face reconstruction is the extension of the Generic Elastic Model (GEM) [6] to resolve the drawback of handling facial expression in 3D face reconstruction. Then, each 3D reconstructed face in the gallery is synthesized to all possible views and a Sparse Dictionary Matrix (SDM) is generated based on triplet angles of face pose for each person. On the other hand, automatic head pose estimation by the Constrained Local Model (CLM) [19] is used to extract the triplet angles (including yaw, pitch and roll) of face poses. Therefore, for each person, an array of SDM is selected based on triplet angles of face pose which were estimated from automatic head pose estimation. Finally, the face recognition is performed by sparse representation classification [20] between the selected arrays of SDM and probe images. The main contribution of this paper is the generation of the SDM matrix based on triplet angles of face pose for each registered input image by sparse representation and 3D reconstructed depth to recognize face across pose changes.

The experimental evaluation was provided to evaluate the proposed method on FERET, CMU PIE and LFW face databases. Promising results were acquired to handle pose changes based on the proposed method compared with several state-of-the-art in pose-invariant face recognition. Also, evaluating the LFW database demonstrated that the proposed method is very rapid and efficient for real-world scenarios in pose-invariant face recognition. Furthermore, to real-time experiments, the proposed method was evaluated on video databases with 30 subjects. Evaluating video database demonstrated that the proposed method is very rapid and real-time for pose-invariant face recognition.

In this paper, the main contributions that are proposed are as follow:
                        
                           1)
                           The FE-GEM method is presented to handle facial expression variations for 3D face reconstruction in real-world scenarios in which reconstructed models are more accurate than the original GEM method.

The manner of generation of the SDM matrix is another contribution of this paper. The SDM is generated based on triplet angles of face pose for each gallery image by 3D face reconstruction and adopting the sparse representation and LBP on synthesized face for carrying out pose-invariant face recognition in real-world scenarios.

Speed is a main contribution of this paper that is very rapid and closely real-time in comparison with the state-of-the-art approaches to handle pose changes in face recognition.

The presented approach improves the recognition rate of the best 3D model-based across pose on the CMU-PIE and FERET databases. Also, the proposed approach obtained results that outperformed the best approaches on the LFW database. Moreover, the proposed method is evaluated on the video databases to recognize the face in real-time videos.

This paper is organized as follows: Section 2 describes the related works. In Section 3, automatic head pose estimation by the CLM is explained. Section 4 describes the 3D face modeling method from a single frontal face image. In Section 5, the generation manner of SDM and the training method for each human face in the database are proposed for pose-invariant face recognition. Experimental evaluations are given in Section 6 and conclusions are presented in Section 7.

@&#RELATED WORKS@&#

In the 2D method context, Sharma et al. [21] proposed the discriminant multiple coupled latent subspace method for pose-invariant face recognition. They find the sets of projection directions for diverse views such that the projected images of the same subject in diverse views are maximally correlated in the latent space. They claimed that the discriminant analysis with artificially simulated pose errors in the latent space makes it robust to small pose errors caused due to a subject's incorrect view estimation. Also, they do a comparative analysis of three common latent space learning methods: Partial Least Squares (PLSs), Bilinear Model (BLM) and Canonical Correlational Analysis (CCA) in their coupled latent subspace method.

Also, Arashloo et al. [22] proposed an MRF-based classification method which employs the energy of the established match between a pair of images as a criterion of goodness-of-match. They claimed that by incorporating an image matching approach as part of the recognition process, the system is made robust to moderate global spatial transformations. Accordingly, by adopting a multi-scale relaxation scheme based on super coupling transform, the inference using sequential tree reweighted the message passing method [23] is accelerated. Then, by taking advantage of a statistical shape prior to the matching, the results are regularized and constrained, making the system robust to spurious structures and outliers. For classification, both textural and structural similarities of the facial images are taken into account.

On the other hand, the surviving 3D model-based approaches can be mostly separated into four categories based on how to utilize the 3D models:
                        
                           1)
                           Pose Normalization: in this method, the probe face images are normalized to frontal pose based on the 3D model, and after that the normalized probe is matched to the gallery face image [9–12].

Pose Synthesis: in this method, several virtual face images are created by synthesis of the 3D model to different poses for the gallery, and then the probe is matched to the virtual face images [5–8].

Recognition by Fitting: in this method, the gallery and probe images were fitted by the 3D model. Hence, both texture and shape parameters are utilized for performing face recognition [13–17].

Filter Transformation: in this method, the filters are transformed based on the pose and shape of probe face image, and after that the pose adapted filters are used for feature extraction from probe images to be compared with features of the gallery image [18].

Among the attempts to generalize 3D model-based face recognition, Blanz and Vetter [13] presented the 3D Morphable Model (3DMM), where each face was provided as a linear combination of a 3D face reconstructed model. Then, the 3DMM was fitted for each input image and, then the shape and texture parameters were reconstructed based on the analysis-by-synthesis framework. Several methods [14–17] used the 3DMM to carry out the face recognition. The major drawback of these approaches is that it requires high computational complexity to reconstruct image fitting parameters. Also, Asthana et al. [10] proposed an automatic method for pose-invariant face recognition that was a 3D pose normalization approach. This method is completely automatic and leverages the accurate 2D facial landmarks found by the method that can handle 3D pose difference up to ±30° in pitch and ±45° in yaw angles.

Also, Prabhu et al. [7] suggested a novel approach for real-world pose-invariant face recognition. In this method, a 3D face model was built for each person in a database utilizing only a 2D frontal face image by Generic Elastic Model (GEM). Then, a new 2D pose face was synthesized from these 3D reconstructed face models for the best matching. Also, head pose was estimated in the test images by a linear regression method according to face landmark localization. Then, each 3D reconstructed face model was synthesized at specific estimated poses. Finally, cosine distance similarity was calculated between the synthesized 3D face models and the test image to display usefulness of the pose synthesis approach for real-world scenarios. However, this method is not very fast, real-world and accurate for face recognition. For example, for matching a single test image to all poses of a 3D face model, this method will take 1s, and for matching a single test image to all poses of 249 face models, it will approximately take 4min. Also, accuracy and performance of this method are not very high in test images which have a large pose of face and facial expressions.

Recently, Yi et al. [18] proposed an approach for pose robust face recognition towards realistic applications, which is rapid, pose robust and can work well under unrestrained environments. Accordingly, a 3D deformable model is generated and a fast 3D model fitting algorithm is proposed to estimate the pose of the face image. Then, a set of Gabor filters are transformed according to the pose and shape of the face image for feature extraction. Finally, the Principal Component Analysis (PCA) is applied on the pose adaptive Gabor features to eliminate the redundancies and the Cosine metric is utilized to compare the similarity.

The existing methods for automatic head pose estimation still have difficulties in detecting individual independent facial landmarks and in the attendance of large pose and lighting alternations. There have been many efforts with different levels of achievement to resolve this restriction; one of the most favorable one being CLM presented by Saragih et al. [19]; followed by several extensions [24–27]. Recent advances in CLM fitting and response functions have shown good results in terms of accuracy and convergence rates in the task of person-independent facial feature tracking.

In this paper, a CLM [19] method was used to automatically estimate head pose that took full advantage of intensity data for detecting facial landmarks in images and tracking them through the frame of video sequences. Also, Baltrusaitis et al. [27] reported that mean error of CLM head pose estimation toward ground-truth face pose on the ICT-3DHP database was less than 10%. The main reason for using the CLM in this work is that the CLM method is closely real-time for head pose estimation. Also, the accuracy of the CLM method is outperforming other pose estimators that are evaluated in [27]. For further details of the CLM method, refer to [19] and [27]. Therefore, to facilitate the proposed method for real-time and real-world face recognition, the CLM is used.

In order to construct the 3D database from images in real-world scenarios, a Facial Expression Generic Elastic Model (FE-GEM) is proposed which is extending the Generic Elastic Model (GEM) [6] method. In this section, the GEM method is initially reviewed for 3D face reconstruction. Then, the FE-GEM method is proposed for facial expression invariant 3D face reconstruction from a single frontal face image in real-world scenarios. Finally, the constructing manner of a 3D database from 2D images is discussed.

In this section, the GEM framework [6] is described for 3D face reconstruction from a single frontal image. While the former methods for 3D face reconstruction have difficulty in reconstructing images under illumination, Heo and Savvides [6] demonstrated that the GEM method can reconstruct the 3D human face under illumination and also in partial face pose. For more details related this problem refer to [6]. The general technique of the original GEM method is displayed in Fig. 1
                        . On this basis, the face was initially detected by facial landmarks. Then, each face (I) is divided into a mesh of triangular polygons (P). Correspondingly, the generic depth-model (D) of face is divided to a mesh (M) from facial landmark points. When facial landmarks are extracted between input face images and the generic depth-model, the density of meshes M and P concurrently increase utilizing loop subdivision [28]. The subdivision method utilized in the GEM method can be considered a middle stage for creating dense correspondence between input mesh of face and mesh of the depth model.

A piecewise affine conversion (W) is employed for warping the GEM depth map (D) sampled in face landmarks of M onto input triangle mesh (P) in order to approximate depth data. Each pixel in the input face image has an exact corresponding pixel in the depth model and intensity of the depth model can be utilized for estimating depth in the input face image. Finally, the 3D reconstructed model can be interpolated using intensity of the input image I(P(x, y)) experimented in 2D facial landmarks of P(x, y).

Therefore, 3D face reconstruction by the GEM method is expressed by
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             S
                                             r
                                          
                                          =
                                          
                                             
                                                x
                                                ,
                                                y
                                                ,
                                                z
                                                =
                                                D
                                                
                                                   
                                                      M
                                                      
                                                         
                                                            x
                                                            ˜
                                                         
                                                         
                                                            y
                                                            ˜
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          ,
                                       
                                    
                                    
                                       
                                          
                                             T
                                             r
                                          
                                          =
                                          I
                                          
                                             
                                                P
                                                
                                                   x
                                                   y
                                                   z
                                                
                                             
                                          
                                          =
                                          
                                             
                                                R
                                                
                                                   x
                                                   ,
                                                   y
                                                   ,
                                                   z
                                                
                                             
                                             
                                                G
                                                
                                                   x
                                                   ,
                                                   y
                                                   ,
                                                   z
                                                
                                             
                                             
                                                B
                                                
                                                   x
                                                   ,
                                                   y
                                                   ,
                                                   z
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where x̃ and ỹ in M are registered pixels x and y in image P. The main drawback of this method is the problem of 3D face reconstruction from images with a variety of facial expressions.

In this section, facial expression depth alterations in 3D human face models are examined and the method is presented for facial expression invariant 3D reconstruction from only frontal images. By investigating and analyzing the 3D human face with facial expression by Zhao et al. [29] and Fang et al. [30], the conclusion is obtained that maximum depth variation of neutral models is related to happy expression models and surprised expression models. Therefore, three models are employed to be utilized in the GEM framework. Fig. 2
                         shows three models including Surprised Mean Model (SMM), Neutral Mean Model (NMM) and Happy Mean Model (HMM), which are generated from the 3D Bosphorus database [31] by the proposed method for generating the generic model in [6].

By employing three generic models in the GEM framework and then achieving three GEM models from input images, the desired model is generated with a mixture of these three new models according to the similarity coefficients. An illustration of the problem is shown in Fig. 3
                        . Therefore, the desired model is achieved as:
                           
                              (2)
                              
                                 D
                                 =
                                 S
                                 ×
                                 
                                    D
                                    s
                                 
                                 +
                                 N
                                 ×
                                 
                                    D
                                    n
                                 
                                 +
                                 H
                                 ×
                                 
                                    D
                                    H
                                 
                              
                           
                        where D is the desired depth model, Ds
                         is the surprised output depth model of GEM, Dn
                         is the neutral output depth model of GEM, DH
                         is the happy output depth model of GEM and S, N and H are surprised, neutral and happy coefficients, respectively.

By investigating the 2D face image and corresponding depth image, reconstruction coefficients are obtained by computing Distance Ratio (DR) in 2D face input images as:
                           
                              (3)
                              
                                 D
                                 R
                                 =
                                 
                                    
                                       d
                                       
                                          C
                                          R
                                          −
                                          C
                                          L
                                       
                                    
                                    
                                       d
                                       
                                          S
                                          U
                                          −
                                          S
                                          D
                                       
                                    
                                 
                              
                           
                        where dCR-CL
                         is the distance between Cheek Right landmark (CR) and Cheek Left landmark (CL), dSU-SD
                         is the distance between Stomion Upper landmark (SU) and Stomion Down landmark (SD), as shown in Fig. 4
                        .

Similarity parameter (SP) is written as:
                           
                              (4)
                              
                                 S
                                 
                                    P
                                    i
                                 
                                 =
                                 ±
                                 1
                                 −
                                 
                                    
                                       D
                                       
                                          R
                                          i
                                       
                                       −
                                       D
                                       R
                                    
                                 
                                 ,
                                 i
                                 =
                                 S
                                 ,
                                 N
                                 ,
                                 H
                              
                           
                        where DRi
                         is the distance ratio in the Surprise Mean Model (i
                        =
                        S), Neutral Mean Model (i=
                        N) and Happy Mean Model (i=
                        H), and DR is the distance ratio in the 2D face input image. Note that the values of DR and DRi
                         are normalized between 0 and 1. Also, in Eq. (4), +1 and −1 are used when DRi-DR is positive and negative, respectively.

Thus, the reconstruction coefficient was obtained as:
                           
                              (5)
                              
                                 S
                                 =
                                 
                                    
                                       S
                                       
                                          P
                                          S
                                       
                                    
                                    
                                       S
                                       
                                          P
                                          S
                                       
                                       +
                                       S
                                       
                                          P
                                          N
                                       
                                       +
                                       S
                                       
                                          P
                                          H
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 N
                                 =
                                 
                                    
                                       S
                                       
                                          P
                                          N
                                       
                                    
                                    
                                       S
                                       
                                          P
                                          S
                                       
                                       +
                                       S
                                       
                                          P
                                          N
                                       
                                       +
                                       S
                                       
                                          P
                                          H
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 H
                                 =
                                 
                                    
                                       S
                                       
                                          P
                                          H
                                       
                                    
                                    
                                       S
                                       
                                          P
                                          S
                                       
                                       +
                                       S
                                       
                                          P
                                          N
                                       
                                       +
                                       S
                                       
                                          P
                                          H
                                       
                                    
                                 
                              
                           
                        where S
                        +
                        N
                        +
                        H
                        =1, S, N, H can be positive or negative. In fact, that FE-GEM method is a linear combination of three models from three expressions with coefficients of S, N and H.

In order to obtain optimum S, N and H coefficients from 2D input face, reconstruction results of a face sample are compared by reconstructing the 3D faces and then, computing the depth error (zE
                        ) values. This comparison is performed to achieve the best models of the 3D desired model both qualitatively and quantitatively. Also, depth error is computed per pixel as:
                           
                              (8)
                              
                                 
                                    z
                                    E
                                 
                                 =
                                 
                                    
                                       
                                          
                                             z
                                             
                                                r
                                                e
                                                c
                                             
                                          
                                          
                                             x
                                             y
                                          
                                          −
                                          
                                             z
                                             
                                                G
                                                T
                                             
                                          
                                          
                                             x
                                             y
                                          
                                       
                                    
                                    
                                       
                                          z
                                          
                                             G
                                             T
                                          
                                       
                                       
                                          x
                                          y
                                       
                                    
                                 
                              
                           
                        where z
                           GT(x, y) denotes ground truth 3D scanned depth values and z
                           rec(x, y) is depth value of reconstructed models.


                        Fig. 5
                         displays qualitative outcomes of a sample for evaluation of the proposed method. In this figure, the number of reconstruction coefficients is enumerated in order to examine the quality of 3D reconstructed models. As can be seen in the rows, the reconstruction outcomes in Surprised Mean Model, Happy Mean Model, Neutral Mean Model, mean of three generic models, distance ratio with minimum error and distance ratio of the sample are respectively shown. For each row, a coefficient, 3D shape generated, depth error map between the generated 3D shape and original 3D shape can be observed. Color range goes from dim blue to dim red (corresponding to depth error beginning from 0 to 50) while means and standard deviations of these differences are in percentage. In the fifth row, the reconstruction coefficient is obtained from Fig. 6
                        . In Fig. 6, the number of distance ratio (DR) is enumerated to compute reconstruction coefficients using Eqs. (3)–(7). Then, the 3D face model is generated by these coefficients (according to Eq. (2)) and corresponding depth error (according to Eq. (8)) is obtained for each corresponding DR from the sample, as shown in Fig. 5. Thus, in the fifth and sixth rows, the reconstructed models with minimum error and the reconstructed models with a DR of the original 3D face are generated from the corresponding sample, respectively.

These results demonstrated that, in reconstructing 3D face models, the FE-GEM method is very close to the reconstructed model with minimum error. Also, the proposed method could combine three generic models (Surprise, Neutral and Happy) for 3D face model reconstruction robust to seven traditional facial expressions by computing distance ratio (DR) of input face images and achieving the reconstruction coefficient with DR using Eqs. (3)–(7). Thus, using facial expression models in the original GEM framework can model not only face images with a variety of facial expressions but also face images without facial expression (neutral face) with higher accuracy than the GEM approach.

Examples of facial expression invariant 3D reconstruction images of five images of famous people which were downloaded from the Internet using the FE-GEM method are also shown in Fig. 7
                        . In the figure, the first column images contain image of famous people, the second column are the values of DR that were computed from the input image by Eq. (3), the third column include values of DRS
                        , DRH
                         and DRN
                         that were computed from three facial expression GEM by Eq. (3), the fourth and fifth column include the values SPi
                         and coefficients that were computed from the input image by Eqs. (4)–(7), the sixth column images contain the 3D estimated model and the seventh column images contain 3D texture models in different new poses from −75° up to +75°. These 3D models were generated from their corresponding input images in the first column.

Also, to further evaluate the proposed method in Fig. 8
                        , outcomes of the 3D reconstruction were exhibited for images which were rendered from the Bosphorus database in a variety of facial expressions. In the figure, four examples are shown and in each example, face image and triplet of models are given from left to right, respectively: ground truth 3D scanned model, reconstruction by the GEM shape and presented reconstructed (FE-GEM) shape, difference between ground truth 3D scanned model and GEM reconstruction model (left) and difference between ground truth 3D scanned model and FE-GEM reconstruction model (right). Values under each of the differences indicate mean square error and the corresponding standard deviation. Moreover, two depth error images were exhibited: between the ground truth 3D scanned model and GEM reconstruction model (left) and between the proposed reconstruction (FE-GEM) model and ground truth 3D scanned model (right). The values under each of the error images indicate mean error and the corresponding standard deviation (in percentage). As in Fig. 8, the reconstructed models by the GEM method failed from images with facial expression. To some extent, the FE-GEM method resolved this drawback of the GEM method, and the reconstructed models by FE-GEM are quantitatively and qualitatively better than the reconstructed model by the GEM method. Depth error images and difference image and also values which were indicated under each of the images illustrated which FE-GEM is more improved than the GEM method.

Furthermore, for extending the initial supposition that FE-GEM is a linear combination of three models from three expressions, more models of more expressions are employed in the FE-GEM framework by a linear combination of several models. For this purpose, entire persons (105 subjects) from the Bosphorus database in seven facial expressions (including Anger, Disgust, Fear, Happy, Neutral, Sadness and Surprise) were used for 3D face reconstruction by the FE-GEM method. Then, the mean depth error was calculated from 735 (105∗7 (subjects∗expressions)) models that were reconstructed by the FE-GEM from the Bosphorus database. The results of this evaluation are given in Fig. 9
                        , in which results of combinations of several models are given. For instance, H
                        +
                        S
                        +
                        N
                        +
                        D
                        +
                        Sa state in Fig. 9 are combinations of models including Happy (H), Surprised (S), Neutral (N), Disgust (D) and Sad (Sa). Also, since there might be an error in the computation of mean depth reconstruction errors due to acquisition errors in the inner mouth region (surface details in the tongue, and teeth region) from the Bosphorus database, the computation of results without this region (inner mouth region) is also given in Fig. 9 in which mean depth errors were calculated for entire models from the Bosphorus database for seven facial expressions. As it is evident from Fig. 9, though the depth error partially decreased by increasing more models, the delay time (that is mean processing time for 3D reconstruction from a face image) subsequently increased by enhancing the computational complexity. Also, the best state of combination of models in terms of minimum depth error is related to a linear combination of models seven expressions that are Surprise, Neutral, Happy, Fear, Disgusted, Sad and Angry (S
                        +
                        N
                        +
                        H
                        +
                        F
                        +
                        D
                        +
                        Sa
                        +
                        A) models. In contrast this combination has a relatively long delay time. Therefore, the optimum state of linear combination of models in terms of both accuracy and delay time of reconstructed models is related to a combination of surprise, neutral and happy models. Moreover, the obtained results of depth error with/without inner mouth regions demonstrate that the FE-GEM method is not only useful for inner mouth regions but entire of face is also accurately reconstructed.

In order to construct a 3D database from frontal images with/without facial expression in real-world scenarios, the FE-GEM method is used. To construct a 3D database, one 2D frontal image is registered for each person, in which the type of facial expression was not significant. Therefore, 2D images for constructing the 3D database included one frontal image with any facial expression. Therefore, a visual illustration of constructing the 3D face database from arbitrary frontal face images that was registered, is shown in Fig. 10
                        . Based on each 2D frontal face image with/without facial expression which is registered, a 3D face is reconstructed and a 3D face database is made.

In this section, the sparse representation is initially represented. Then, the feature extraction method and the manner of generating SDMs are proposed from the 3D database by sparse representation for pose-invariant face recognition. Then, the method for real-world pose-invariant face recognition is proposed.

The sparse representation was described by Wright et al. [20] for face recognition. A collection of labeled training samples is given from k discrete classes. Then, the task is done to decide that a new unseen probe sample is belongs to which the class. Let Ai = [vi,
                           1, vi,2
                        , …, vi,ni
                        ] is an m
                        ×
                        ni
                         training matrix from the i-th class in the gallery in which the ni
                         training samples are set as columns. Every column vi,j
                         in matrix Ai
                         is vectorized to intensity image or a number of appropriate description of the intensity image. One effective method to use the organization of the matrix Ai
                         for face recognition is to model the samples from a single class as presenting on a linear subspace. Meanwhile, if there are adequate samples from the i-th class , any novel test sample y from the same class is estimated to present in the linear length of the columns of matrix Ai
                        , i.e.,
                           
                              (9)
                              
                                 y
                                 =
                                 
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          n
                                          i
                                       
                                    
                                    
                                       
                                          a
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                       
                                          v
                                          
                                             i
                                             ,
                                             j
                                          
                                       
                                    
                                 
                              
                           
                        for some scalars αi,j
                         ∈ R, j
                        =1, 2, …, ni
                        . Because the identity of the probe sample is initially unknown, a novel matrix A was clarified by [32] which is the incorporation of the training samples from all the classes:
                           
                              (10)
                              
                                 
                                    A
                                    
                                       m
                                       ×
                                       n
                                    
                                 
                                 
                                 =
                                 
                                 
                                    
                                       
                                          A
                                          1
                                       
                                       ,
                                       
                                       
                                          A
                                          2
                                       
                                       ,
                                       
                                       …
                                       ,
                                       
                                       
                                          A
                                          k
                                       
                                    
                                 
                                 
                                 =
                                 
                                 
                                    
                                       
                                          v
                                          
                                             1
                                             ,
                                             1
                                          
                                       
                                       ,
                                       
                                       …
                                       ,
                                       
                                       
                                          v
                                          
                                             1
                                             ,
                                             n
                                          
                                       
                                    
                                 
                                 …
                                 
                                 
                                    
                                       
                                          v
                                          
                                             k
                                             ,
                                             1
                                          
                                       
                                       ,
                                       
                                       …
                                       ,
                                       
                                       
                                          v
                                          
                                             k
                                             ,
                                             n
                                             k
                                          
                                       
                                    
                                 
                              
                           
                        where. With this explanation of A, y in Eq. (9) is rewritten as:
                           
                              (11)
                              
                                 y
                                 
                                 =
                                 
                                 Ax
                              
                           
                        where x
                        =[0, …, 0, …, αi,1, …, αi,ni, …, 0, …, 0]
                           T
                         is a coefficient vector whose corresponding arrays to the i-th class are not zero and all remaining arrays are zero. The purpose is to acquire x from a novel test sample y and matrix A which is special information about the identity of the test sample to help in the face recognition. To this end, there are numerous decomposition approaches. The sparse representation methods discover the sparsest answer to the linear systems of equations y = Ax. This is obvious that test sample y is adequately represented by only the samples from its correct class, which if the total number of classes in A is great, it is obviously guided to sparse x. Whatever the recovered x is more sparse, it demonstrates that improves the identity of the unlabeled test sample, which guides to solve the following optimization problem:
                           
                              (12)
                              
                                 
                                    x
                                    ^
                                 
                                 =
                                 arg
                                 
                                 
                                    min
                                    
                                       x
                                       ∈
                                       
                                       
                                          R
                                          n
                                       
                                    
                                 
                                 
                                    
                                       
                                          x
                                       
                                    
                                    1
                                 
                                 
                                 subject
                                 
                                 t
                                 o
                                 
                                 Ax
                                 
                                 =
                                 
                                 y
                                 .
                              
                           
                        
                     

Actually, l0
                        -minimization instead of l1
                        -minimization provides the sparsest solution but it guides to a NP-hard trouble even for estimation. Recent researches in compressed sensing [33] demonstrate that if the solution required is sparse sufficient, the solution to the l0
                        -minimization problem is like to that of the l1
                        -minimization problem. For more details refer to [20]. The optimization problem in Eq. (12) is acknowledged as Basis Pursuit (BP) and it is solved in polynomial time by standard linear encoding techniques [34].

Visual illustration of the proposed method for extracting the feature by Local Binary Pattern (LBP) [41] and sparse representation [20] is shown in Fig. 11
                        . Based on the proposed method, the process can be summarized as follows:
                           
                              1)
                              Input: a set of 3D faces which the LBP operator is applied to face texture (subject i).

For each subject (subject i), 3D faces with LBP texture is synthesized across pose and possible views in all poses of the face are extracted with steps of 5 (is initialized with s
                                 =5) degrees on either side (at three directions including yaw, pitch and roll) in the subject pose matrix Si(Y,P,R) in which Si (Y,P,R) is a cubic matrix with a size of x-by-x-by-x and total array of Si(Y,P,R) is x
                                 ×
                                 x
                                 ×
                                 x
                                 =
                                 x3
                                 . In fact, arrays of the Si(Y,P,R) matrix for subject i are arranged based on triplet angles of face pose form, where i indicates the number of subject and Y, P and R are the number of array in the corresponding dimension (yaw, pitch and roll) of the Si(Y,P,R) matrix (see Fig. 11). To cover all poses of the face, the range of yaw, pitch and roll angles is between −90 to +90°. Therefore, the sizes of Si (Y,P,R) are x
                                 =180/s=180/5=36, in which s is step size and 180° is the cover pose angles for each direction of face pose. For example, Si(Y
                                 =5,P
                                 =4,R
                                 =6) indicates that the pose angle of the face is yaw
                                 =5×5(s
                                 =5)=25, pitch
                                 =4×5=20 and roll
                                 =6×5=30°.

In each Si(Y,P,R) for each specific triplet angle of face pose (y, p, r), a sub-training matrix si(y,p,r) is selected with size 3×3×3 (ni
                                 
                                 =27, in which n is total array of si(y,p,r)) around the array to the center of (y, p, r). A sample of the sub-training matrix of si(y,p,r) with the corresponding LBP images inside the specific triplet angles is shown in Fig. 11 and is as:
                                    
                                       (13)
                                       
                                          
                                             s
                                             i
                                          
                                          
                                             
                                                y
                                                ,
                                                
                                                p
                                                ,
                                                
                                                r
                                             
                                          
                                          
                                          =
                                          
                                          
                                             S
                                             i
                                          
                                          
                                             
                                                Y
                                                
                                                =
                                                
                                                y
                                                ‐
                                                1
                                                
                                                :
                                                
                                                y
                                                
                                                +
                                                
                                                1
                                                ,
                                                
                                                P
                                                
                                                =
                                                
                                                p
                                                ‐
                                                1
                                                
                                                :
                                                
                                                p
                                                
                                                +
                                                
                                                1
                                                ,
                                                
                                                R
                                                
                                                =
                                                
                                                r
                                                ‐
                                                1
                                                
                                                :
                                                
                                                r
                                                
                                                +
                                                
                                                1
                                             
                                          
                                          .
                                       
                                    
                                 
                              

One of the main requirements of the sparse representation approach is the availability of multiple samples for each class in dictionary A. In this paper, a cubic matrix of sparse dictionary A is generated based on a triplet angle of face poses. The cubic matrix A(Y,P,R) is defined as a sparse dictionary matrix (SDM) in which a dictionary A was saved in each array. Therefore, for each specific pose (y, p, r), an array of Ai(Y
                                 =
                                 y,P
                                 =
                                 p,R
                                 =
                                 r) is created by generating the sparse dictionary Ai
                                  from si(y,p,r) matrix in which Ai
                                 
                                 =[vi,1, vi,2, …, vi,ni
                                 ]=[si(y−1, p−1, r−1), …, si(y, p, r), …, si(y
                                 +
                                 1, p
                                 +
                                 1, r
                                 +
                                 1)] with ni
                                 
                                 =27 (3×3×3) samples for training images from the i-th class. In fact, sparse dictionary A from si(y,p,r) in (y,p,r) pose is saved in Ai(Y
                                 =
                                 y,P
                                 =
                                 p,R
                                 =
                                 r). Similarly, this manner is repeated for entire poses and arrays of Si(Y,P,R). Therefore, the matrix of sparse dictionary (that is SDM) is generated for each subject by sparse representation and FE-GEM 3D face reconstruction. Thus, for each subject, a SDM is created based on triplet angles of face pose with the size of the matrix Y
                                 ×
                                 P
                                 ×
                                 R, in which i indicates number of subjects. Hence, the size of each SDM is similar to the subject pose matrix Si(Y,P,R).

Output: SDMs with A
                                 
                                    m
                                    ×
                                    n
                                 (Y,P,R)=[A1(Y,P,R), A2(Y,P,R), …, Ai(Y,P,R)] where i is the number of subjects in the database according to Eq. (10) in which a cubic matrix based on triplet angles of pose is generated.

Visual illustration of the pose-invariant face recognition system by FE-GEM and SDM (FE-GEM+SDM) is shown in Fig. 12
                        . The proposed system operated in two offline and online stages. In the offline stage, a single frontal face image of each person is registered from available 2D databases. Then, for each registered image, 3D face reconstruction is performed by the FE-GEM to produce a 3D face database. Afterwards, feature extraction is performed and then, the SDM is created for each person according to Section 4.1.

In the online stage, pre-processing and face detection are performed for each probe image by the CLM method for landmark and face detection. Then, pose angles of the face are estimated by the CLM head pose estimation method. Afterwards, for each person, an array (Ai(Y
                        =
                        yaw, P
                        =
                        pitch, R
                        =
                        roll)) of SDM is selected based on pose estimated angles (including yaw, pitch and roll). On the other hand, LBP is applied on the probe image for which the LBP image is y. Finally, the identity is determined independently for each facial part as follows. Given a test LBP sample y and the training matrix A(Y
                        =
                        yaw, P
                        =
                        pitch, R
                        =
                        roll)
                        =[A1(Y
                        =
                        yaw, P
                        =
                        pitch, R
                        =
                        roll), A2(Y
                        =
                        yaw, P
                        =
                        pitch, R
                        =
                        roll),...., Ai(Y
                        =
                        yaw, P
                        =
                        pitch, R
                        =
                        roll)], the sparse coefficient vector 
                           
                              
                                 x
                                 ^
                              
                              i
                           
                         is obtained by solving (12). Final classification is performed by determining which class present in A(Y
                        =
                        yaw, P
                        =
                        pitch, R
                        =
                        roll) best represents the probe sample using the recovered 
                           
                              x
                              ^
                           
                        . Representation error for the i-th class is computed by reconstructing the probe sample using the samples belonging to that class only, as follows:
                           
                              (14)
                              
                                 
                                    e
                                    i
                                 
                                 
                                    y
                                 
                                 =
                                 
                                    
                                       y
                                       −
                                       A
                                       
                                          
                                             Y
                                             =
                                             yaw
                                             ,
                                             
                                             P
                                             =
                                             pitch
                                             ,
                                             
                                             R
                                             =
                                             roll
                                          
                                       
                                       
                                          
                                             x
                                             ^
                                          
                                          i
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 x
                                 ^
                              
                              i
                           
                           
                           =
                           
                           
                              
                                 
                                    0
                                    ,
                                    
                                    0
                                    ,
                                    
                                    …
                                    ,
                                    
                                    0
                                    ,
                                    
                                    
                                       
                                          x
                                          ^
                                       
                                       I
                                    
                                    ,
                                    
                                    1
                                    ,
                                    
                                    …
                                    ,
                                    
                                    
                                       
                                          x
                                          ^
                                       
                                       
                                          i
                                          ,
                                          n
                                          i
                                       
                                    
                                    ,
                                    
                                    0
                                    ,
                                    
                                    …
                                    ,
                                    
                                    0
                                 
                              
                              T
                           
                           
                           ∈
                           
                              R
                              n
                           
                        . The probe sample is classified to the class that produces the smallest representation error.

@&#EXPERIMENTS@&#

The performance of the proposed method is evaluated for pose-invariant face recognition on three databases: CMU-PIE [35], FERET [36] and LFW [37]. Then, the impact of s (step size) and ni
                      (size of sub-training matrix) parameters is investigated on the LFW database by changing them. Also, the impact of 3D face reconstruction by the FE-GEM is investigated through utilizing the reconstructed model by the GEM in the proposed method. Moreover, the proposed method is evaluated on video databases to demonstrate that proposed method is real-world and closely real-time.

The FERET database consists of 200 subjects at 9 different poses of the face in yaw direction. These 9 poses include: (bi=−60°), (bh=−40°), (bg=−25°), (bf=−15°), (ba: 0°), (be=+15°), (bd=+25°), (bc=+40°) and (bb=+60°). In this evaluation of the proposed method, 200 frontal images denoted as ba are utilized as the gallery, and there are a total of 1600 (200×8) images in 8 of the remaining poses as the probe. To evaluate accuracy of the proposed method for handling pose on the FERET database from only a single image in the gallery, the obtained results were compared with four state-of-the-art methods, the results of which have been reported in the FERET database: 3D Pose Normalization (3DPN) [10], 3D Morphable Model (3DMM) [13], Correspondence Latent Subspace (CLS) [21] and Pose Adaptive Filter (PAF) [18]. Table 1
                         shows the recognition rate of the proposed method (SDM+FE-GEM) on the FERET database in comparison with these four state-of-the-art methods. As is obvious from the results, overall performance of the SDM+FE-GEM for pose-invariant face recognition on 8 poses of the FERET database outperforms state-of-the art methods including 3DPN, 3DMM, CLS and PAF. Also, it is demonstrated that SDM+FE-GEM is better than these six methods for handling the pose variations in large angles such as bb and bi. Furthermore, the impact of FE-GEM 3D face reconstruction is evaluated on the proposed method, for which the results on the FERET database are given in Table 1. For this purpose, the reconstructed models by GEM were used in the proposed method instead of the FE-GEM method that is named SDM+GEM. As it is obvious from Table 2
                        , in the SDM framework the recognition rate is improved by the FE-GEM method in comparison with GEM methods for 3D face reconstruction.

The CMU-PIE database consists of 68 subjects in 8 different poses of the face in yaw direction, 2 different poses of the face in pitch direction and 2 different poses of the face in both yaw and pitch directions. These 12 poses are denoted as: c34 (yaw
                        =−90°), c31 (yaw
                        =−67.5°, pitch
                        =+22.5°), c14 (yaw
                        =−67.5°), c11 (yaw
                        =−45°), c29 (yaw
                        =−22.5°), c09 (pitch
                        =+22.5°), c07 (pitch
                        =−22.5°), c05 (yaw
                        =+22.5°), c37 (yaw
                        =+45°), c25 (yaw
                        =+67.5°, pitch
                        =+22.5°), c02 (yaw
                        =+67.5°), and c22 (yaw
                        =+90°). In this evaluation of the proposed method, 68 frontal images denoted as c27 (frontal) are utilized as the gallery, and there are a total of 816 (68×12) images in the 12 poses as probe. To evaluate accuracy of the proposed method for handling pose on the CMU-PIE database from only a single image in the gallery, the obtained results were compared with four state-of-the-art methods and their results have been reported on 68 face images of the CMU-PIE database: 3DPN [10], MRF-based classification (MRF) [22], CLS [21], and PAF [18]. Table 2 shows the recognition rate of the proposed method (SDM+FE-GEM) on the CMU-PIE database in comparison with these four state-of-the-art methods. As it is obvious from the results, overall performance of the SDM+FE-GEM for pose-invariant face recognition on 12 poses and 68 subjects of the CMU-PIE database outperforms the state-of-the art methods including CLS on 12 poses and 34 subjects, and MRF and PAF on 12 poses and 68 subjects. Although the overall rate of 3DPN (99%) is higher than the SDM+FE-GEM method, 3DPN is only evaluated on 6 poses, for which the overall rate of SDM+FE-GEM is 100% in these six poses. The SDM+FE-GEM method is comparable to: the PAF method in c25, c37, c05, c07, c09, c29 and c11, the CLS method in c37, c05, c07, c09, c29 and c11, the 3DPN method in c05, c09 and c29, and the MRF method in c25, c09. Also, it is demonstrated that SDM+FE-GEM is better than the four state-of-the-art methods for handling the pose variations in large angles especially in c22, c02, c25, c14, c31 and c34. Furthermore, the impact of FE-GEM 3D face reconstruction is evaluated on the proposed method, for which the results on the CMU-PIE database are given in Table 2. For this purpose, the reconstructed models by GEM were used in the proposed method instead of the FE-GEM method that is named SDM+GEM. As it is obvious from Table 2, in the SDM framework the recognition rate is improved by the FE-GEM method in comparison with GEM methods for 3D face reconstruction.

The LFW database is one of the most challenging databases in face recognition that closely comprises all variations of the face image in real-world situations including large variations in pose, lighting, etc. The LFW consists of 13,233 face images from 5749 different subjects of different ages, gender, etc. The LFW database was prepared in [39] two disjoint sets: ‘View 1’ is used for gallery whereas ‘View 2’ is used for probe. These sets were selected from the aligned version of the faces as provided by Wolf et al. [42]. The ‘View 1’ that was prepared by Hussain et al. [39] has closely frontal pose with partial pose of about 20° and in proper illumination. In the proposed FE-GEM method, these partial non-frontal faces are almost treated as frontal face for 3D face modeling (for more detailed see Fig. 7). Thus, note that in this paper, a set of LFW database with 5749 different subjects is used that only have two images from each person (‘View 1’ and ‘View 2’) that was prepared by Hussain et al. [39]. Hence, there is one image per subject in the gallery set for 3D face reconstruction and the offline process based in Fig. 12 against one image per subject that is in the probe set for the online process. Therefore, to evaluate the performance of the proposed method on real-world scenarios for pose-invariant face recognition, the LFW database is used under unconstrained environments besides the pose. Since the proposed method by the SDM+FE-GEM method is an unsupervised technique, its performance is compared to the approaches belonging to the same category of benchmarks: PAF [18], the best result of the I-LQP approach [40] and the best result of the BIF approach (Brain-Inspired Features) [38]. The recognition rate accuracy of the methods on View 2 of the LFW database is shown in Table 3
                        . As is evident from the results, the proposed method by SDM-FE-GEM outperforms PAF, BIF and the best unsupervised method I-LQP [39]. Furthermore, the impact of FE-GEM 3D face reconstruction is evaluated on the proposed method, for which the results on the LFW database are given in Table 3. For this purpose, the reconstructed models by GEM were used in the proposed method instead of the FE-GEM method that is named SDM+GEM. As it is obvious, in the SDM framework the recognition rate is improved by the FE-GEM method in comparison with GEM methods for 3D face reconstruction. This difference of results is due to expression-invariant 3D face reconstruction (FE-GEM) that was adopted for handling the real-world images in the LFW database. While the GEM method cannot model the real-world images that have facial expression from LFW database.

The proposed method for pose-invariant face recognition is very rapid for real-world scenarios. Hence, in the LFW database, the speed for 480×640 image, the mean processing time is 89.4ms on a laptop with P5 CPU Core 2 Duo@2.0GHz by implementation with OpenCV. The details of the online stage based on Fig. 12 are as follows. 1) pre-processing and face detection with head pose estimation by CLM: 42.9ms, 2) array selection from SDMs: 8.8ms, 3) LBP on probe image: 4.6ms, and 4) sparse representation classification for classification: 33.1ms. Therefore, the proposed method is closely real-time for unconstrained pose-invariant face recognition in comparison with the PAF method that is the fastest in state-of-the-art methods with 104.2ms delay time on LFW database by a laptop with P4 CPU@2.0GHz which is comparable with our platform.

In the SDM+FE-GEM method for pose-invariant face recognition, the size of the sub-training matrix si(Y, P, R) (n
                        =
                        ni
                        ) (which was n
                        =27) in Fig. 11 and also the size of the sparse dictionary matrix A (Y, P, R) which was planned using step 5 (s
                        =5) degrees on either side is an important factor for the recognition performance. Hence, impact of changing the number of n and s together on LFW database is shown in Fig. 13
                        . As can be seen in Fig. 13, the highest identification rate appeared around steps of 5, 6 and 7° that can be due to the accuracy of pose estimator with estimation error of less than 10% (Section 3).

For more evaluation of the proposed on video databases, 30 videos of 30 people from the Biwi Kinect Head Pose [43] and ICT-3DHP [27] databases are used. In each video, more than 1000 frames existed for each person. Head pose range in these videos covered about ±75° in yaw direction and ±60° in pitch direction. Face recognition was performed using SDM-FE-GEM method and performance evaluation was compared with the following methods: 1) SDM-GEM method in which the reconstructed models by GEM were used in the proposed SDM instead of the FE-GEM method and 2) the Estimated Pose+Search Range by GEM (EPSR+GEM) [7] method in which a 3D face was reconstructed by the GEM method. Then, a new 2D pose face was synthesized from these 3D reconstructed face models for the best matching. Hence, each 3D reconstructed face model was synthesized in specific estimated poses. Finally, cosine distance similarity was calculated between synthesized 3D face models and probe image. 3) The ESPR by FE-GEM (EPSR+FE-GEM) method in which reconstructed models by FE-GEM method that have been proposed in this work was used in ESPR method instead of the reconstructed models by GEM method.

The value of the minimum representation error in sparse representation classification that was obtained by Eq. (14) was tracked from all the rendered poses for each frame by SD+FE-GEM and SDM+GEM methods. The result of real-time pose-invariant face recognition on four videos of these experiments is shown in Fig. 14
                        . Also, identification rate results with respect to different poses in yaw and pitch rotations from 30 videos are given in Fig. 15a and b, respectively. For instance, in Fig. 15a, pose angle changes not only are in yaw direction but also there are pose angle changes in both pitch and yaw direction and only the figure was plotted based on frames in which there are face pose changes in yaw direction. As can be seen from Fig. 15, identification rate under yaw and pitch different poses in SDM+FE-GEM method outperforms the compared approaches especially in large face poses. Also, Fig. 14 demonstrates that the proposed method for pose-invariant face recognition is very rapid and real-time so that the face recognition process for each testing frame in each pose is performed with average frame per second of 9 to 14 (FPS=9–14). The FPS=14 is related to frames that face pose are small and the FPS=9 is related to frames that face pose are large in which pose estimator was slower. Hence, the proposed method is real-time for unrestricted pose-invariant face recognition against EPSR+FE-GEM and EPSR+GEM methods, which were time consuming and non-real-time.

@&#CONCLUSION@&#

In this paper, a novel approach was proposed for real-world pose-invariant face recognition from only a single frontal image in the gallery which was very rapid and real-time. To handle the pose in face recognition, the SDM+FE-GEM method is proposed that is efficient for this purpose. Accordingly, a 3D model is initially reconstructed from frontal face images in real-world situations. To reconstruct a 3D model from each human frontal face in a real-world situation, the FE-GEM was proposed. Then, the SDMs are generated based on the proposed method, and finally, face recognition is performed by sparse representation classification.

The proposed method was tested on CMU-PIE, FERET and LFW databases in order to perform pose-invariant face recognition. Also, the proposed method was tested on the video databases in order to perform unrestricted real-time face recognition. Promising results were obtained to handle the face pose in control and non-control (real-world) situations for face recognition. It was demonstrated that performance of the proposed method for pose-invariant face recognition was improved in comparison to state-of-the-art approaches. In the future, the proposed face recognition system will be extended to unconstrained face recognition that is robust to a wide range of face variations including: makeup, plastic surgery, facial expression, face occlusion, and etc.

@&#REFERENCES@&#

