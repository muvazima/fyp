@&#MAIN-TITLE@&#Linking bottom-up intonation stylization to discourse structure

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           New contour-based, parametric, and superpositional approach for automatic intonation analysis and partially automatic synthesis.


                        
                        
                           
                           Biunique relation between the F0 contour and its representation.


                        
                        
                           
                           Extraction of an optimum number of contour classes by subtractive and k-means clustering.


                        
                        
                           
                           Data-driven linking of the stylization to discourse structure by means of simple prediction models.


                        
                        
                           
                           Successful evaluation of this linguistic anchoring in two perception experiments.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Computational intonation stylization

Data-driven

Contour-based

Superposition

Discourse structure

@&#ABSTRACT@&#


               
               
                  A new approach for intonation stylization that enables the extraction of an intonation representation from prosodically unlabeled data is introduced. This approach yields global and local intonation contour classes arising from a contour-based, parametric and superpositional intonation stylization. Based on findings about the linguistic interpretation of the contour classes derived from corpus statistics and perception experiments, we created simple prediction models for the partial generation of intonation contours from discourse structure defined by discourse segment boundaries and the information status of nouns within these segments. The predicted intonation contours were evaluated by human judgments of adequacy that yielded a high accordance.
               
            

@&#INTRODUCTION@&#

Established intonation models can be classified along three dimensions:
                           
                              1.
                              the chosen units: tone targets vs. contours,

their description: symbolic vs. parametric, and

their arrangement: single-layered vs. superpositional.

In the following, a selection of high impact intonation models will be described with respect to these dichotomies. The tone sequence model (TSM) of Pierrehumbert (1980), Gussenhoven (2004) and Ladd (2008) can be characterized as target-based, symbolic and single-layered, since fundamental frequency (F0) within an intonation phrase is described as a single-layered sequence of tone symbols that are assigned to pitch accented and phrase boundary syllables. The PENTA model, conceptually introduced in Xu (2005) and quantified in the form of the qTA model by Prom-on et al. (2009), also considers the F0 contour as a sequence of targets that are static (horizontal) or dynamic (rising or falling) and are assigned to each syllable. While in the TSM the F0 contour generally results from connecting the targets by an interpolation function, PENTA considers the F0 contour as a result of target approximation which can be realized in different forms such as a third-order critically damped linear system as in Prom-on et al. (2009). The PENTA model thus is target-based, parametric, and arranges intonation units in a single layer. The Fujisaki model (Fujisaki, 1987; Möbius, 1993; Mixdorff, 1998), in every respect opposite to the TSM, is contour-based, parametric, and superpositional. It considers F0 as a superposition of a global phrase component related to declination and a local accent component related to F0 movements on accented and phrase-final syllables. The components are parametrically represented as critically damped systems activated by phrase and accent commands, respectively. The TILT model (Taylor, 2000) as well as the PaintE model (Möhler, 1998) yield a contour-based, parametric and single-layered intonation representation. They provide a parameterization of the F0 contour in the scope of accented and phrase-final syllables.

Ideally, intonation models should allow for (1) an appropriate abstraction, (2) the interpretability of the abstraction, and (3) its automation.

In the following sections the models presented thus far are discussed with respect to these requirements.

An appropriate abstraction from the signal means to capture relevant aspects of the signal, to allow for signal reproducibility, and to be reproducible itself if repeated on the same signal.


                           Relevant signal aspects: Tone-based approaches yield a comparably high degree of abstraction but hence have to face the criticism to neglect relevant properties of the F0 contour between tone targets due to underspecification. For example, they cannot account for the shape between the prenuclear and nuclear tone accents in Neapolitan being concave in questions and linear in statements (Petrone and D’Imperio, 2008). So far two major proposals to face this issue had been made: the insertion of intervening targets (Petrone and D’Imperio, 2008) and the usage of interpolation rules (Pierrehumbert, 1981). Since tones are associated to accented and phrase boundary syllables only, additional targets require a reorganisation of the prosodic structure which needs justification. This reorganisation can for example consist in the insertion of an additional prosodic level of accentual phrases as in Petrone and D’Imperio (2008). The usage of interpolation rules raises the question why not to work directly with contours instead of tones.


                           Dainora (2002) argues for a contour approach due to the high predictability of tones given their predecessors indicating that the relevant intonation unit is not the tone but a tone sequence and thus a contour. Further support for this perspective is given by Dombrowski and Niebuhr (2005) who found for German dialog data that contours are more appropriate than tones to reveal the underlying intonation systematics for turn holding and turn yielding.


                           Signal reproducibility: Since parametric models are inherently more closely related to the signal, they are more appropriate for signal reproduction from the abstract intonation representation. While a parametric representation can be transformed directly into the F0 contour by parameter assignment as in an analysis-by-synthesis framework, symbolic approaches depend on additional mediation which may be hand-crafted rule-based (Jilka et al., 1999) or data-driven (Black and Hunt, 1996).


                           Abstraction reproducibility: Symbolic approaches relying on manual labeling have to face the risk of low intra- and inter-labeler agreement. A common proposal to address this issue of low abstraction reproducibility is to reduce the size of the label inventory (Dainora, 2001; Wightman, 2002).

Established approaches do not guarantee the reproducibility of the abstraction either, since the parameters for the chosen stylization functions are not derivable analytically but have to be estimated numerically by local optimization of the fit between representation and original contour (Mixdorff, 1998; Dusterhoff et al., 1999). Thus the abstraction depends strongly on the parameter value initialization. Furthermore, the relation between the parameter values and the F0 contour is usually not biunique, i.e. different parameter assignments can lead to the same contour (see Reichel, 2010, p. 60 for an example).


                           Linguistic interpretability: Empirical findings suggest that linguistic phenomena show categorical as well as gradual correspondence in the production and perception of intonation. For example, Baumann et al. (2006) found that the realizations of broad vs. narrow focus show categorical differences in terms of different pitch accent types as well as gradual differences in F0 alignment and segment durations. Therefore, both symbolic and parametric intonation representations are principally accessible for linguistic interpretation, which is discussed in depth by Taylor (1995). Pierrehumbert and Hirschberg (1990) can be referred to for the linguistic interpretation of symbolic approaches, and Möbius (1993) and Mixdorff (1998) for the interpretation of parametric models.

To make use of possible advantages of more intuitive linguistic interpretations on the symbol level, approaches transforming parametrical transcriptions into symbolic ones might be helpful. The PaintE model for example offers an additional symbolic F0 representation by means of parameter vector clustering (Möhler and Conkie, 1998).


                           Phonetic interpretability: A superpositional arrangement of intonation units as in the Fujisaki model can account for long-term phenomena like declination. Furthermore, it offers the possibility to incorporate findings of pre-planning in intonation production (Cooper and Sorensen, 1981), amongst others reflected by the relation between utterance length and declination slope.

Automation is highly desirable if a model is to be tested on a larger amount of data. Furthermore, it allows for fast applicability to new data of other languages. In contrast, manual adaptations of symbolic label inventories like ToBI (Silverman et al., 1992) for other languages (Grice and Benzmüller, 1995) are very laborious.

As said above in the context of signal reproducibility, the advantage of a parametric unit description is its direct linking to the signal. While symbolic representations need experts or additional modules to derive symbols from the signal (Schweitzer and Möbius, 2009) or to generate F0 contours (Black and Hunt, 1996), parameters can be directly inferred from and transformed into F0 values.


                        Grosz and Sidner (1986) proposed three parallel discourse structures: The linguistic structure is given by the written or spoken text, the attentional structure represents the relative salience of discourse entities, and the intentional structure subdivides the discourse into segments of coherent speaker intentions. The linguistic concepts into which we attempt to embed our intonation stylization are discourse segmentation and the information status of words within these segments. These concepts can be linked to the intentional and the attentional structure, respectively.

Discourse segmentation serves to group together coherent parts of an utterance. One way to address coherence consists in a linear discourse segmentation into subtopic units (Hearst, 1997). Concerning the intonational marking of discourse segmentation it has been found in production and perception studies for several languages that subtopics generally start with a higher F0 register and end in a lower register (Nakajima and Allen, 1993; Swerts et al., 1994; Botinis et al., 2001), so that pitch reset is more pronounced at topic shifts (Nakajima and Allen, 1993). The strength of coherence of adjacent discourse segments is prosodically encoded amongst others by boundary tones (Pierrehumbert and Hirschberg, 1990; Féry, 1993), and the degree of final lowering (Hirschberg and Pierrehumbert, 1986). High boundary tones generally mark continuation and thus a high degree of cohesion of adjacent discourse segments, while low boundary tones and final lowering indicate low coherence. A more extensive overview on intonation in discourse segmentation is given by Venditti and Hirschberg (2003).

The information status of a discourse entity describes whether it contains new information, which is not yet present in the discourse context, or given information, that is already available. Prince (1981) proposes a major categorical division of givenness into evoked (already mentioned or directly available as part of the dialog situation) and inferable (only accessible via an evoked entity). Chafe (1994) distinguishes between different activation levels of discourse entities on an ordinal scale: given (active), accessible (semi-active), and new (inactive). A more detailed six-level scale is given by Gundel et al. (1993).

Numerous studies mainly within the TSM framework have revealed how information status is expressed by means of intonation. Different pitch accent type preferences (including deaccentuation) have been identified for given and new information for several languages as English (Brown, 1983; Pierrehumbert and Hirschberg, 1990) and German (Féry, 1993; Grice et al., 2005). Hirschberg and Pierrehumbert (1986) and Hirschberg et al. (1987) relate these findings to the attentional structure of Grosz and Sidner (1986) by expressing given and new in terms of different degrees of salience. The intonational marking of different degrees of givenness is explored e.g. by Pierrehumbert and Hirschberg (1990), and Baumann (2006) who found amongst others degree-dependent pitch accent type preferences.

The aim of this study is to link our approach to simplified forms of the introduced discourse concepts. The approach was originally developed and presented in Reichel (2010, 2011) (in the first source it is referred to as the PKS stylization) and is based on the considerations formulated in Section
                           1.2. It will be described in Section
                           2. In Section
                           3 our previous work on its linguistic interpretation with respect to discourse is summarized. Based on these findings in the current study simple prediction models for global and local contour classes were handcrafted, which are introduced in Section
                           4 together with their perceptual evaluation.

The results are discussed in Section
                           5 with a focus on the relevance of our approach concerning adequate intonation representation for linguistic interpretation.

Our CoPaSul approach provides a contour-based (Co), parametric (Pa), and superpositional (Sul) F0 representation. F0 contours are treated as a superposition of global and local components. These components are anchored in a hierarchical prosodic structure defined by global and local segments which roughly correspond to intonation phrases and potential accent groups, respectively, where ‘potential’ means that the allowed number of accented syllables within such a group is zero or one. The stylization of the F0 contours is carried out as follows: Within each global segment a linear F0 base contour is fitted. After the subtraction of this global baseline within each local segment a third order polynomial is fitted to the F0 residual. Subsequently, a symbolic description of the intonation inventory in the form of global and local contour classes is derived by polynomial coefficient clustering. On the phonetic level, linear regression models adjust these abstract units to the respective prosodic context.

CoPaSul thus stands in the tradition of parametric (Fujisaki, PaintE, PENTA, Tilt) and superpositional (Fujisaki) models. Like the Fujisaki model it explicitly distinguishes between a global and a local intonation component and thus allows for addressing them separately. Its parametric and contour-based nature closely connects the stylization to the signal level. Furthermore, parameter clustering has been adopted from the PaintE approach yielding a symbolic intonation representation that allows for linking the stylization to the linguistic level.

The training data originates from the SI1000P corpus (Schiel, 1999) containing 190min of German read speech by a professional standard German male newsreader. F0 contours were extracted by the Schaefer–Vincent algorithm (Schaefer-Vincent, 1983) and transformed to semitones (base 50Hz). F0 errors and voiceless segments were bridged by a shape-preserving piecewise cubic Hermite spline interpolation (de Boor, 1978). The contours were smoothed by a Savitzky–Golay filter (Persson and Strang, 2003) of order 3 and window length 5. A main advantage of this filter type over other standard smoothing methods like moving average is that it is only a little prone to temporal shifts of local peaks and valleys and therefore better preserves the F0 envelope.

Pauses and syllable nuclei were detected automatically by energy (root mean squared deviation; RMS) comparison between an analysis window 
                           
                              w
                              a
                           
                         and a longer reference window 
                           
                              w
                              r
                           
                         with the same time midpoint. For pause assignment the energy in 
                           
                              w
                              a
                           
                         had to be considerably lower than in 
                           
                              w
                              r
                           
                        , more precisely 
                           RMS
                           (
                           
                              w
                              a
                           
                           )
                           <
                           RMS
                           (
                           
                              w
                              r
                           
                           )
                           ·
                           0.06
                        . The length of 
                           
                              w
                              r
                           
                         was set to 5 s, the length of 
                           
                              w
                              a
                           
                         to 150ms in order to prevent the confusion of pauses and shorter stop consonant closures. For syllable nucleus assignment the energy in the relevant frequency range from 245 to 3125Hz had to be considerably higher in 
                           
                              w
                              a
                           
                         than in 
                           
                              w
                              r
                           
                        , and additionally had to surpass a threshold which was defined relative to the maximum energy of the utterance, i.e. 
                           RMS
                           (
                           
                              w
                              a
                           
                           )
                           >
                           0.15
                           ·
                           max
                           (
                           RMS
                           )
                           ∧
                           RMS
                           (
                           
                              w
                              a
                           
                           )
                           >
                           RMS
                           (
                           
                              w
                              r
                           
                           )
                           ·
                           1.2
                        . Here, the lengths of 
                           
                              w
                              a
                           
                         and 
                           
                              w
                              r
                           
                         were set to 50 and 250ms, respectively.

All length, factor, bandwidth, and threshold parameters were estimated by the non-linear Nelder–Mead Simplex optimization (Nelder and Mead, 1965) on a SI1000P subcorpus comprising 20 hand-segmented sentences with 1011 syllables and 86 speech pauses. The error for pause detection in terms of insertions and deletions in non-final position amounts 10%, the error for syllable nucleus detection 7%. These error rates are regarded as not too severe: First, in our read speech data every punctuation mark co-occurred with a speech pause, so that a global segment boundary was set even if pause detection failed. Second, almost all nucleus detection errors occurred in low prominence function words which are expected to be of minor relevance for determining intonation events. Details of the optimization and the evaluation are described in Reichel (2010).

On the text level, part of speech tagging was carried out by a tagger described in Reichel (2007). Signal and text were aligned by the Munich Automatic Segmentation System (MAUS) (Schiel, 1999), and a grapheme-phoneme converter (Reichel, 2012) served to locate the word-stressed syllables within this alignment.

The segmentation into global and local segments was carried out automatically based on the preceding alignment of the signal and the tagged text and on pause detection. Global intonation segments are delimited by speech pauses and by punctuation. Local intonation segments were defined as a chunk of function words terminated by a content word or a global segment boundary. This notion roughly corresponds to chunking approaches as those given in Gee and Grosjean (1983), Abney (1991) and ensures in most cases that each local segment contains one accented syllable as a maximum. As an example, the utterance illustrated in Fig. 1
                            is divided into global segments at punctuation marks, and each global segment is further divided into local segments by placing a boundary behind each content word yielding the following structure: [[Die Tiere] [verstummen]], [[ein Unheil] [naht]] ([[The animals] [hush]], [[a disaster] [is approaching]]).

All stylizations are based on the F0 values in 110ms frames centered on the detected syllable nuclei. The advantage of this approach is that its demands on preprocessing are low. It requires only a syllable nucleus detection, which is robust and can be carried out automatically. There is no need for an exact syllable segmentation or for a weighting of more and less important parts of the F0 contour. Thus, ad-hoc approaches like intensity-based weighting (Hermes, 1998) are dispensable. The choice of a frame length of 110ms is a trade-off between the need of enough input data for reliable polynomial fitting and the need to avoid frame overlaps of consecutive syllables. If an overlap still occurs, neighboring frames are shortened by an equal amount.

Global and local contour stylizations that will be described in the next paragraphs are shown in Figs. 2 and 3
                           
                           .


                           Global contours: Within each global intonation segment, a declination baseline is derived as follows: For each syllable nucleus window the F0 minimum is taken as an F0 base level. The baseline then is adjusted as the flattest bottom tangent of the sequence of these base level values that passes through two points of the sequence chosen from all linear connections of pairs of F0 minima (Reichel, 2010). In order to make baseline slopes comparable across different segment lengths, time is normalized to the interval from 0 to 1. This baseline is then subtracted from the F0 contours, and its slope is recorded for subsequent clustering (see Section
                           2.3.3).


                           Local contours: Within each local segment a third-order polynomial is fitted to the time-normalized residuum contour. As illustrated in Fig. 3 time is normalized as follows: The time span of the local segment is set from −1 to 1. −1 is assigned to the left boundary of the first syllable nucleus window, and 1 to the right boundary of the last syllable nucleus window. 0 is placed on the nucleus of the syllable of the segment-final word (the content word), that carries the word stress. Thus, the peak of the F0 contour can be interpreted relative to the accent position. This approach requires separate normalizations of the pre- and post-accent parts of the local segment. By means of normalization it is possible to compare utterance segments of different lengths. Furthermore, it allows to ignore unstable polynomial behavior outside the chosen interval it is fitted to. On the other hand, this normalization obscures the influence of phonetic segment durations and syllable number on peak alignment (see Niebuhr, 2007 for an overview). However, due to the high number of different phonetic segmental contexts, this influence is considered as noise that overall does not affect the analysis in a systematic way.

The selected polynomial order is motivated by the trade-off between capturing relevant F0 movements and avoiding data overfitting. First and second order polynomials are not powerful enough to cover all relevant aspects of local F0 contours. Polynomials of fourth or higher order run the risk not to be well-conditioned and to complicate subsequent clustering, since they demand a larger amount of data due to the increased coefficient vector length, as well as a weighting schema for more and less relevant coefficients. In any case, the danger of oversimplification by third-order polynomials is diminished by the specification that a local segment contains at most one pitch accent, which limits the expected complexity of the F0 contour.

Contour classes were derived by k-means clustering of the range-normalized coefficients. For global classes the baseline slope values and for local classes the polynomial coefficient vectors were clustered with respect to their squared Euclidean distances. Cluster initialization was carried out by subtractive clustering (Chiu, 1994) that iteratively locates initial centers in the parameter space at regions with high data density. The parameters for subtractive clustering were derived by Nelder–Mead Simplex optimization. Details on the application of this procedure are given in Appendix A.


                           Fig. 4
                            shows the centroids of the resulting three global and five local contour classes g
                           1 to g
                           3 and c
                           1 to c
                           5, respectively. The centroid coefficient values are listed in Appendix A. In Fig. 1 intonation within the two global segments is illustrated as the superposition of the global contour class sequence g
                           2, g
                           2 and the local class sequence c
                           2, c
                           4, c
                           5, c
                           1.

The chosen squared Euclidean distance measure is not primarily perceptually motivated, since reliable perceptually grounded distance measures are not yet available (although attempts were made to develop such metrics, e.g. Hermes, 1998; Reichel et al., 2009), but it is in line with the general bottom-up character of the current approach. In Reichel et al. (2009) the Euclidean distance turned out to correspond slightly better to perceptual distance judgments than other metrics.

The cluster center initialization by means of subtractive clustering guarantees stable results across disjunct data subsets and input vector randomization. The number of clusters was constant over all subsets and randomizations. All pairwise correlations of corresponding centroids are above 0.97 for the data subsets and above 0.99 for data randomization.

The phonetic realization of the contour classes on the basis of linear regression models is illustrated in Fig. 5
                           . The regression models serve to map the abstract contour class centroids to the intonation surface level.


                           Contour realizations: In order to constrain the deviance of a contour realization from the underlying class, the regression models for global and local contour realizations predict absolute values for the polynomial coefficients. The algebraic sign is taken over from the underlying centroid coefficient.

The linear regression model for the slope realizations |b
                           
                              r
                           | of global contours is given by 
                              |
                              
                                 b
                                 r
                              
                              |
                              =
                              
                                 w
                                 0
                              
                              +
                              
                                 w
                                 1
                              
                              ·
                              |
                              
                                 b
                                 c
                              
                              |
                              +
                              
                                 w
                                 2
                              
                              ·
                              |
                              
                                 b
                                 rp
                              
                              |
                              +
                              
                                 w
                                 3
                              
                              ·
                              l
                           . This means that the absolute slope realization |b
                           
                              r
                           | of a global contour is derived from the underlying absolute centroid slope |b
                           
                              c
                           | of the contour class, the realized slope of the preceding contour |b
                           
                              rp
                           |, and the length of the current global segment l. 
                              
                                 w
                                 i
                              
                            are the predictor weights, which were calculated by minimizing the overall error between |b
                           
                              r
                           | and absolute values of the observed baseline slopes obtained from the stylization. For weight calculation the realized slope of the preceding contour b
                           
                              rp
                            was set to the value of the respective stylization coefficient. In application contour slopes are estimated sequentially from left to right, so that b
                           
                              rp
                            is given by the output of the regression model for the preceding global contour.

For each local contour coefficient a separate linear regression model was trained: 
                              |
                              
                                 a
                                 r
                              
                              |
                              =
                              
                                 w
                                 0
                              
                              +
                              
                                 w
                                 1
                              
                              ·
                              |
                              
                                 a
                                 c
                              
                              |
                              +
                              
                                 w
                                 2
                              
                              ·
                              |
                              
                                 a
                                 rp
                              
                              |
                              +
                              
                                 w
                                 3
                              
                              ·
                              |
                              
                                 b
                                 r
                              
                              |
                              +
                              
                                 w
                                 4
                              
                              ·
                              p
                           . The absolute value of the contour coefficient realization |a
                           
                              r
                           | is predicted from its underlying contour class coefficient |a
                           
                              c
                           |, the realized value of the preceding corresponding coefficient |a
                           
                              rp
                           | the realized slope of the current global contour |b
                           
                              r
                           |, and the relative position p of the local segment within the global segment. The predictor weights are calculated by minimizing the overall error between |a
                           
                              r
                           | and the corresponding value obtained from the stylization.


                           Pitch reset: At junctions between global segments the pitch reset r is modeled by 
                              r
                              =
                              
                                 w
                                 0
                              
                              +
                              
                                 w
                                 1
                              
                              ·
                              
                                 b
                                 
                                    r
                                    1
                                 
                              
                              +
                              
                                 w
                                 2
                              
                              ·
                              
                                 b
                                 
                                    r
                                    2
                                 
                              
                              +
                              
                                 w
                                 3
                              
                              ·
                              pl
                           . b
                           
                              r1 and b
                           
                              r2 are the realized slopes of the adjacent global contours. Their absolute values have been predicted by the global contour model introduced above. pl is the length of the interjacent pause. The predicted pitch reset is added to the final F0 value of the preceding global contour in order to derive the F0 starting level of the current global contour. The predictor weights are calculated by minimizing the overall error between r and the corresponding observed pitch reset values.

For all linear regression models, the predictors are range-normalized. The resulting regression weights are shown in Table 6
                            in Appendix A along with the correlations between predictions and targets that range between 0.63 and 0.82.


                           Gain from the realization models: The regression models serve to set the concrete realization of a contour class in the context of extrinsic influence factors. By this it can be accounted for the negative correlation of global segment length (predictor l) and global contour slope (Cooper and Sorensen, 1981). As can be seen in Appendix A, Table 6, this reverse relation is captured by the negative contour length weight 
                              
                                 w
                                 3
                              
                              =
                              −
                              0.0221
                           .

An implicit F0 topline (Connell and Ladd, 1990) can be modeled by relating the local contour coefficients to the relative position of the contour (predictor p) within the global segment. Accordingly, the corresponding regression weights 
                              
                                 w
                                 4
                              
                            turned out to be negative for all local contour coefficients. Thus, the more a local contour is located to the end of a global segment, the lower its height, and the less pronounced its steepness and shape. Moreover, the regression models serve to smooth contour sequences, since contour parameters are calculated not only from the underlying centroids but also from the correspondent coefficients of the preceding contour.

The pitch reset model accounts for the co-operation of pitch reset (de Pijper and Sandermann, 1994) and pause length (Swerts and Geluykens, 1994) in encoding prosodic boundary strength. This positive impact of pause length on the pitch reset is reflected by the positive value of 
                              
                                 w
                                 3
                              
                              =
                              0.2166
                            shown in Table 6. In principle, the pitch reset model could be extended by a predictor representing the presence or absence of a discourse segment boundary in order to incorporate the findings of Nakajima and Allen (1993) (cf. Section
                           1.3.1). However, this is not yet possible on the basis of the given training data since each sentence was produced in isolation so that pitch reset could not been measured across topic shifts.


                           Local segment junctions: Depending on the distance between the two syllable nuclei adjacent to a segment boundary a contour gap (as in Fig. 5) or overlap occurs. Both are bridged by linear interpolation. Start and endpoint of this linear bridge are smoothed by a moving median filter.

The text-based local contour class prediction suggested in the current study (see Section
                     4) integrates the findings of two previous perception experiments (Reichel, 2011, 2012) that will be reviewed in the following.

We had tested the linguistic adequacy of the local contour classes with respect to semantic weight (Reichel, 2011) (defined in terms of word predictability (Bolinger, 1951)), utterance finality (Reichel, 2011), and information status (Reichel, 2012). Due to strong correlations between the contour classes’ relations to semantic weight and information status and the inherent correlations of these two concepts (Reichel, 2010), only the two discourse structure concepts discourse segmentation and information status are further considered in this study. For these initial attempts to relate the data-driven approach to discourse structure, terminology was strongly simplified: First, we reduced information status to the dichotomy given vs. new information not further distinguishing between different levels of givenness. Second, we considered discourse segments to be subtopics in a linear sequential order as in Hearst (1997) leaving aside hierarchical discourse structures or more sophisticated discourse coherence analyses reflecting its intentional structure. Third, we set equal utterance and discourse segment boundaries based on the assumption that the intonation of declarative utterance ends is accepted as a marker of potential discourse segment ends, due to their common coincidence.

The general procedure to link the intonation stylization to linguistic units can be described as follows: First, linguistic concepts were extracted by means of simple sentence segmentation and natural language processing (NLP) methods. Then, based on co-occurrence statistics between the linguistic events and the contour classes, hypotheses were formulated about the linguistic function of the classes. For global classes these hypotheses concern discourse segmentation, for local classes discourse segmentation and information status are covered. For the local classes these hypotheses were subsequently tested by perception experiments. 24 German mother tongue subjects between 22 and 47 years took part in these experiments. All subjects were students or scientists of phonetics in Munich.

From the studies referred to in Section
                        1.3.1 it can be concluded that in intonation segment finality is encoded on a local level by means of boundary tones (Pierrehumbert and Hirschberg, 1990; Féry, 1993) and on a global level by means of F0 register (Nakajima and Allen, 1993; Swerts et al., 1994; Botinis et al., 2001). Segment starts are predominantly marked on the global level in terms of F0 register (Nakajima and Allen, 1993; Swerts et al., 1994). Therefore, we examined the function of local contour classes in segment final position only, with the aim to classify them by the dichotomy final vs. non-final (continuation). The function of global contour classes was examined in segment final and additionally in initial position using the two dichotomies final vs. non-final and initial vs. non-initial, respectively. Non-final and non-initial both subsume medial positions.

Since in our data the reader produced each sentence in isolation, the last global, respectively local segment of each sentence was classified as discourse segment final, and the others as non-final. Accordingly, the first global segment of a sentence was labeled as discourse segment initial, all others as non-initial.

For each class it was tested separately (a) whether there is a significant relation to initial position (i.e. initial vs. non-initial the latter subsuming the medial position) and (b) whether there is a significant relation to final position (i.e. final vs. non-final the latter subsuming medial). In case a significant relation between a contour class and a position was revealed, the direction of this relation was determined by comparing the observed occurrence count of the class in the respective position with the count to be expected in the case of independence. By this comparison a positional preference (e.g. initial) or obstruction (non-initial) was concluded. For both positions each class showed significant preferences or obstruction (χ
                           2 values between 28.43 and 257.18 were obtained; α
                           =0.001). The relations are summarized in Table 1
                           . One can see from this table, that in our data the global contour marking of discourse segment initial and final parts is disjunct only for class g
                           1, that preferably occurs in final position but is only reluctantly used in initial position. Class g
                           2 in contrast has a preference for both positions from which can be inferred that it is rarely to be observed in medial position. Both classes are characterized by a negative slope to establish low F0 register values at the end of discourse segments. g
                           3 in contrast has a preference to occur neither in initial nor in final position from which can be inferred its preference for the medial position to mark continuation by its positive slope.

In line with these findings χ
                           2 tests relating contour classes and medial position showed a significant preference for this position only for class g
                           3 while g
                           1 and g
                           2 significantly more often occur in non-medial (i.e. initial or final) position (χ
                           2 values from 24.91 to 124.38, α
                           =0.001).

Unfortunately, the impact of pitch reset on discourse segmentation allowing for high topic initial F0 register (Nakajima and Allen, 1993) could not be addressed directly, since in the training data each sentence was produced in isolation, so that pitch reset values were not available for discourse segment boundaries.

Testing the significance of co-occurrences of contour classes and utterance finality by χ
                           2 tests (α
                           =0.05; Pearson, 1900) resulted in the hypotheses: Finality is encoded by c
                           1 and c
                           5, continuation by c
                           2, c
                           3, and c
                           4, which is summarized in Table 2
                           .

In a perceptual validation of these hypotheses single local segment utterances of the form ‘Eine X (an X).’ were presented. The stimuli had been synthesized by means of MBROLA (Dutoit et al., 1996) using a male German voice database (de4). Segment durations had been calculated by a regression tree which will be described in Section 4.2. For all F0 contours the F0 baseline was constantly set to 80Hz (declination slope 0), so that the contour variation was determined solely by the local contour classes. The class-related contours were laid on the time-normalized utterances as illustrated in Fig. 3. Time 0 was associated with the nucleus midpoint of the stressed syllable in the target word X.

The 60 target words X were amongst others controlled for uniform syllable number and structure, word frequency, and voicing, in order to rule out that word-intrinsic properties interfere with its prosodic realization. The task was to allocate the stimuli on a 5-point bipolar scale with the end points ‘Eine X und eine Y (an X and a Y)’ and ‘Eine X’. Allocating a stimulus to ‘Eine X’ implies that it is considered as a completed utterance and is thus characterized by segment-final intonation, whereas allocating it to ‘Eine X und eine Y’ implies that subjects expect more to come triggered by an intonation contour signaling continuation.

The subjects’ judgments for each class are shown in Fig. 6
                            in the right boxplots together with the corpus-derived hypotheses marked by filled circles placed at 1 (non-final) or 5 (final). Except for class c
                           5 all hypotheses were perceptually verified (Kruskal–Wallis test, 
                              
                                 χ
                                 4
                                 2
                              
                              =
                              316.9
                              ,
                              p
                              <
                              0.001
                            
                           Kruskal and Wallis, 1952; Dunnett post-hoc test, α
                           =0.01; Dunnett, 1955). Furthermore, all classes were perceptually identified with respect to finality, since all judgments differed significantly from the mean level 3 of undecidedness (sign tests, α
                           =0.05, Bonferroni-corrected, |z|>3.4, p
                           <0.001; Karas and Savage, 1967; Dunn, 1961).

As described in Reichel (2012) we automatically labeled nouns co-referring to preceding ones with the information status given, and the others with new. To achieve this, first, the text was segmented into thematic units using an adapted version of the TextTiling algorithm (Hearst, 1997). Subsequently, coreference resolution was carried out within each of these units by means of an iterative pattern matching procedure proposed by Hearst (1992), extended by a transitive closure based on compound analyses.

Considering the stylization parameters, only the offset polynomial coefficient differed significantly across the conditions and was as expected higher in the new condition compared to given. For local contour classes corpus statistics yielded that givenness was encoded by c
                        1 and c
                        4, whereas the classes c
                        2, c
                        3, and c
                        5 encoded new information (χ
                        2 tests, α
                        =0.05). These results are summarized in Table 2.

In order to perceptually verify these hypotheses the same subjects as introduced in Section
                        3.3 were asked to participate. They had to judge a stimulus Yes, an X (e.g. Yes, a flower) on a five level bipolar scale whether it is rather an answer to the question ‘Is this an X? (Is this a flower?’) or to ‘Is this a hypernym(X)? (Is this a plant?)’. If the stimulus ‘Yes, a flower’ is perceived as an answer to ‘Is this a flower?’, it is considered as a confirmation not containing any new information. However, as an answer to ‘Is this a plant?’ it contains new information that specifies the hypernym ‘plant’. Thus, the judgment of the stimulus on the bipolar scale reflects the subjects’ opinion whether its intonation encodes rather given or new information.

The generation of the stimulus part ‘…an X’ was carried out as described in the previous section. The initial particle ‘Yes, …’ received a linear F0 contour falling from 90 to 80Hz and was followed by a 300ms pause.

As is shown by the left boxplots of Figure 6, except for class c
                        4 the results were in line with the hypotheses: c
                        2, c
                        3, and c
                        5 were clearly perceptually bound to new information, and c
                        1 to givenness (Kruskal–Wallis test, 
                           
                              χ
                              4
                              2
                           
                           =
                           217.1
                           ,
                           p
                           <
                           0.001
                        , Dunnett post-hoc test, α
                        =0.05). As with finality all classes were perceptually attributed to information status, again expressed by the significant differences of all judgments from the mean level 3 (sign tests, Bonferroni-corrected, |z|>4.3, p
                        <0.001).

For the global classes the predictions simply emerge from the Table 1. g
                        1 can be chosen for any non-initial (thus medial or final) position within a discourse segment. g
                        2 is suggested for both initial and final but not medial position, whereas g
                        3 is complementary restricted to medial (thus non-initial and non-final) positions.

For the local classes in accordance with our hypotheses in Table 2 derived from corpus statistics a tree was handcrafted for choosing the appropriate class based on the simplified discourse structure of an utterance. The linguistic concepts are represented by the non-terminal nodes of the tree, their values by the outgoing branches, and local contour classes by the leafs. Thus, each path through the tree represents one discourse structure setting and ends in a leaf suggesting a corresponding local contour class. This tree is presented in Fig. 7
                        .


                           Subjects: 14 subjects took part in the perceptual evaluation of global classes, and 15 subjects in the evaluation of local classes. All subjects are German native speakers between 21 and 53 years, and all of them are phonetic experts working at the Institute of Phonetics and Speech Processing in Munich or post-graduate students of Phonetics. The subject groups for both evaluations are not identical but overlap. The author did not take part as a subject.


                           Method for global class evaluation: The subjects were presented with pairs of sentences that were either coherent or not. They had to judge the intonation with respect to whether or not it can correctly express the connectedness of the sentences. Coherence was established by pronouns and topic relatedness, separation by topic incongruity.

Examples for a coherent and an incoherent sentence pair used in this validation are:
                              
                                 coherent: Dort gibt es Bienen. Ihr Honig ist lecker. (There are bees. Their honey is tasty.)
                              
                                 incoherent: Dort gibt es Bienen. Die Fähre kam pünktlich. (There are bees. The ferry arrived in time.)
                           
                        

All eight sentence pairs are listed in Appendix B.1. Each sentence is considered as a global segment, within which a global contour is fitted. For each sentence pair four global contour variant types V
                           * were generated:
                              
                                 •
                                 
                                    V: the global contour classes suggested by the prediction model in Section
                                    4.1 in order to establish sentence coherence or incoherence,


                                    V
                                    1: only the contour class of the first sentence matches the prediction,


                                    V
                                    2: only the contour class of the second sentence matches the prediction, and


                                    V
                                    0: both contour classes contradict the prediction.

This can be related to the coherent sentence pair example above by the following schema:
                              
                                 
                                    
                                       
                                          
                                          
                                          
                                          
                                             
                                                
                                                Sentence 1
                                                Sentence 2
                                             
                                             
                                                Discourse
                                                Non-final
                                                Non-initial
                                             
                                          
                                          
                                             
                                                
                                                   Variants
                                                
                                             
                                             
                                                
                                                   V
                                                
                                                
                                                   g
                                                   3
                                                
                                                
                                                   g
                                                   1
                                                
                                             
                                             
                                                
                                                   V
                                                   1
                                                
                                                
                                                   g
                                                   3
                                                
                                                
                                                   g
                                                   2
                                                
                                             
                                             
                                                
                                                   
                                                
                                             
                                             
                                                
                                                   V
                                                   2
                                                
                                                
                                                   g
                                                   1
                                                
                                                
                                                   g
                                                   1
                                                
                                             
                                             
                                                
                                                
                                                   g
                                                   2
                                                
                                                
                                                   g
                                                   1
                                                
                                             
                                             
                                                
                                                   
                                                
                                             
                                             
                                                
                                                   V
                                                   0
                                                
                                                
                                                   g
                                                   1
                                                
                                                
                                                   g
                                                   2
                                                
                                             
                                             
                                                
                                                
                                                   g
                                                   2
                                                
                                                
                                                   g
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Since in this case sentences 1 and 2 are coherent, a non-final contour should be used for the first sentence, and a non-initial contour for the second. The variant type V in line with these recommendations thus assigns the non-final g
                           3 class to the first sentence and the non-initial g
                           1 class to the second. Variant type V
                           1 agrees with the recommendations only concerning the finality-status of the first contour class g
                           3 but contradicts the recommendations in assigning the initial class g
                           2. Variant type V
                           2 agrees with V with respect to the contour class of the second sentence (non-initial g
                           1) but not for the first sentence, to which it assigns final g
                           1 or g
                           2. V
                           0 does not agree with the recommendations at all assigning a final class to sentence 1 (g
                           1 or g
                           2) and the initial class g
                           2 to sentence 2. All four variation types are shown in Appendix B.1. Since the subject's task was to judge the intonation marking of the sentence coherence and not the marking of the end of the utterance or the sentence mode, for the final sentence only the falling slope classes g
                           1 and g
                           2 were applied.

The F0 contour of a stimulus was generated in the following way: Each sentence was assigned a global contour according to the schema described above. No phonetic realization model (cf. Section 2.3.4) was applied for the global contours in order to directly examine the impact of the contour class on the listener judgments. The initial F0 value of the first sentence was set in dependence of the first contour class to 75Hz for g
                           1, 85Hz for g
                           2 and 70Hz for g
                           3 to ensure a realistic pitch range throughout the whole utterance. For the second sentence the first F0 value results from the pitch reset regression model described in Section 2.3.4.

Within each local segment a local contour was added to the declination lines. For all global contour variants the F0 values of these local contours were determined by the tree predictor in Fig. 7. In the example given above the contour specification of variant V is thus given by
                              [[Dort gibt es Bienen]
                                    c2]
                                    g3. [[Ihr Honig]
                                    c2 [ist lecker]
                                    c5]
                                    g1.
                              
                                 (There are bees. Their honey is tasty.)
                              
                           
                        


                           Bienen (bees) and Honig (honey) provide new information and are both in non-final discourse segment position. Thus class c
                           2 is assigned to both local segments. Note that we have not yet elaborated an intonation class prediction for other word classes than nouns, since the concept of information status is not easily transferable. Nevertheless, in an informal pretest the class c
                           5 turned out to be appropriate for the adjective lecker (tasty) with respect to its discourse characteristics (new, final) as well as to the assigned F0 contour. This could be an indication that the automatic contour predictions may be extended to local segments containing predicative adjectives.

For all incoherent cases the local contour class c
                           5 was assigned to sentence 1 since it is suggested by the tree for discourse segment final position. A variant V for the incoherent cases is thus:
                              [[Dort gibt es Bienen]
                                    c5]
                                    g1]. [Die Fähre]
                                    c2 [kam pünktlich]
                                    c5]
                                    g2.
                              
                                 (There are bees. The ferry arrived in time.)
                              
                           
                        

Thus, the local contour classes were always set to be in line with the tree predictions, whereas the global classes were systematically varied.

To anchor the local contour classes within each local segment the time point 0 of the time-normalized contours was aligned to the midpoint of the nucleus in stressed syllable of the head word, i.e. the final content word. The local contours were subsequently adjusted to the context by means of the corresponding regression models (cf. Section 2.3.4). As described in Section 2.3.4 the joints of neighboring local contours were bridged by linear interpolation and smoothed by a moving median filter.

The segment durations needed for the synthesis were derived from the model 
                              
                                 
                                    
                                       
                                          d
                                          x
                                       
                                    
                                    ˆ
                                 
                              
                              =
                              
                                 
                                    
                                       d
                                       x
                                    
                                 
                                 ¯
                              
                              ·
                              f
                           , where 
                              
                                 
                                    
                                       d
                                       x
                                    
                                 
                                 ˆ
                              
                            is the predicted duration of phoneme x, 
                              
                                 
                                    
                                       d
                                       x
                                    
                                 
                                 ¯
                              
                            its intrinsic duration set to the mean duration found in a manually segmented sub-part of the SI1000P corpus. f is a factor adjusting the intrinsic duration to the given context defined by accentuation, phrase finality and phoneme class. This factor is predicted by a regression tree (Breiman et al., 1984) trained on a SI1000P sub-part. With one exception (one schwa occurrence was considered to be too long by one subject) the subjects did not report any duration abnormalities.

Between the coherent sentences a pause of length 400ms was inserted, while for the non-coherent sentences the pause duration was set to 250ms. This duration difference reduces the impact of pause duration on coherence marking, thus the realization of incoherent sentences is not only judged to be adequate due to a long pause, and vice versa for the realization of coherent sentences. By reducing the reliability of the pause as a cue for coherence, the subjects’ attention is expected to be drawn to the intonation contours.

As in the preceding studies the stimuli were synthesized using MBROLA (Dutoit et al., 1996) and a German database (de4) available on the MBROLA project web page.

Eight test items were presented to the subjects: one for each of the eight sentence pairs shown in Appendix B.1. In each of these trials the appropriateness of each of the four intonation contour variant types had to be judged with respect to how well it marks the presence or absence of coherence. Since always two variant types consist of two class pairs (final, non-initial: g
                           1, g
                           1 and g
                           2, g
                           1; final, initial: g
                           1, g
                           2 and g
                           2, g
                           2) actually six contour variants were presented. The subjects were asked to assign one value for each variant on a five-level bipolar Likert scale between the endpoints non-adequate 1 and adequate 5. Thus, in each trial six judgments had to be made. The stimuli were presented via closed headphones, and no upper limit was set to stimulus repetition.


                           Method for local class evaluation: The subjects had to judge the intonation adequacy of local target segments within different discourse contexts as in:
                              
                                 Dort steht eine Buche. [Die Buche]
                                    s1 
                                 verliert [ihre Blätter]
                                    s2.
                              
                                 (There is a beech tree. [The beech tree]
                                 
                                    s1 
                                 loses [its leaves]
                                 
                                    s2
                                 .)
                              
                           
                        

The discourse context is provided by the preceding sentence, in this example There is a beech tree. It determines the information status of the referents in the local segments s
                           1 and s
                           2. For each of the segments four local contour variant types V
                           * were generated:
                              
                                 •
                                 
                                    V: the local contour class suggested by the tree,


                                    V
                                    
                                       i
                                    : a contour class only matching the information status encoding requirements,


                                    V
                                    
                                       s
                                    : a contour class only matching the discourse segmentation encoding requirements, and


                                    V
                                    0: a contour class neither matching the information status nor the segmentation encoding requirements.

This can be related to the example above by the following schema:
                              
                                 
                                    
                                       
                                          
                                          
                                          
                                          
                                             
                                                
                                                   Context
                                                
                                                
                                                   There is a beech tree.
                                                
                                             
                                             
                                                
                                                   Carrier
                                                
                                                
                                                   [The beech tree]
                                                   
                                                      s1 
                                                   loses [its leaves]
                                                   
                                                      s2
                                                   .
                                                
                                             
                                             
                                                
                                                   s
                                                   1
                                                
                                                Discourse
                                                Given, non-final
                                             
                                             
                                                
                                                Variants
                                                
                                                   V: c
                                                   4, V
                                                   
                                                      i
                                                   : c
                                                   1, V
                                                   
                                                      s
                                                   : c
                                                   2, c
                                                   3, V
                                                   0: c
                                                   5
                                                
                                             
                                             
                                                
                                                   s
                                                   2
                                                
                                                Discourse
                                                New, final
                                             
                                             
                                                
                                                Variants
                                                
                                                   V: c
                                                   5, V
                                                   
                                                      i
                                                   : c
                                                   2, c
                                                   3, V
                                                   
                                                      s
                                                   : c
                                                   1, V
                                                   0: c
                                                   4
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The local segment s
                           1 is located in the non-final position and carries given information. As can be seen in Figure 7 the tree suggestion is a c
                           4 type contour. Three types of contrastive variants to this prediction were generated: V
                           0 (class c
                           5) differs from c
                           4 regarding information status as well as discourse segmentation encoding. V
                           
                              s
                            differs with respect to information status encoding (classes c
                           2 and c
                           3), and V
                           
                              i
                            regarding segmentation (class c
                           1). All four context-carrier configurations are shown in Appendix B.2.

Again the stimuli were synthesized using MBROLA (Dutoit et al., 1996) and the German database de4. F0 contours were generated in the following way: Each context and carrier sentence was assigned a global contour component in accordance with the global class prediction model, thus the first sentence received an “initial” g
                           2 contour and the second a “final” g
                           1 contour. Both contours were adjusted amongst others to sentence length by the regression model introduced in Section 2.3.4. The first onset was set to 85Hz, and the second onset was derived dynamically from the pitch reset regression model of Section 2.3.4.

On these declination lines one local contour per local segment was added. The F0 values of these contours were determined by the underlying local contour class. Opposed to the declination lines no phonetic realization models had been applied in order to directly examine the impact of the contour classes in the target segments.

As documented in Appendix B.2 in detail, constant classes were chosen for the context sentences and the verbs in the carrier sentences, and varying classes for the target segments. Within each local segment the time point 0 of the time-normalized contours was aligned to the midpoint of the nucleus in stressed syllable of the head word, i.e. the final content word. The joints of neighboring local contours were bridged as described above. The segment durations were derived from the duration model introduced in the preceding section. The pause between the context and the carrier sentence was constantly set to 200ms.

Eight test items were presented to the subjects: For each of the four sentence pairs shown in Appendix B.2 one item for the target segment s
                           1, and a one item for s
                           2. In each of these trials the appropriateness of each of the four intonation contour variant types on the marked target segment had to be judged with respect to information status and discourse segmentation. Since always one variant type consists of two classes, c
                           2 and c
                           3, actually five contour variants were presented. The subjects were asked to assign one value for each variant on a five-level bipolar Likert scale between the endpoints non-adequate 1 and adequate 5. Thus, in each trial five judgments had to be made. The stimuli were presented via closed headphones, and no upper limit was set to stimulus repetition.

@&#RESULTS@&#


                           Global class prediction The judgments for all variants are presented as boxplots in Fig. 8
                           . Results were the following:
                              
                                 •
                                 The predicted global contours were generally accepted. The judgment median is 4 and thus significantly higher than the mean judgment level 3 (one-sided one-sample sign test for median comparison, z
                                    =4.12, p
                                    <0.001; Mann and Whitney, 1947).

The predictions V were significantly higher rated than alternatives V
                                    2 and V
                                    0 (Kruskal–Wallis test, 
                                       
                                          χ
                                          3
                                          2
                                       
                                       =
                                       9.36
                                       ,
                                       p
                                       <
                                       0.05
                                    , Scheffé post-hoc test, α
                                    =0.05; Scheffé, 1959).

There was no significant difference between V and V
                                    1 indicating that listeners rather relate the declination pattern of the first sentence to topic coherence or shift.


                           Local class prediction: The judgments for all variants are presented as boxplots in Fig. 9
                           . Results were the following:
                              
                                 •
                                 The contours suggested by the tree were generally accepted. The judgment median is 4 and thus significantly higher than the mean judgment level 3 (one-sided one-sample sign test for median comparison, z
                                    =7.28, p
                                    <0.001).

The tree predictions V were rated significantly higher than variant types V
                                    
                                       i
                                     and V
                                    
                                       o
                                     (Kruskal–Wallis test, 
                                       
                                          χ
                                          3
                                          2
                                       
                                       =
                                       117.12
                                       ,
                                       p
                                       <
                                       0.001
                                    , Scheffé post hoc test, α
                                    =0.05). V showed also a tendency to be rated higher than V
                                    
                                       s
                                     which is reflected in the arithmetic mean rating difference of these variants (3.88 vs. 3.52) and a significant difference yielded by the less conservative Dunnett post-hoc test, α
                                    =0.05.

All other variant ratings differed significantly (Kruskal–Wallis test, 
                                       
                                          χ
                                          3
                                          2
                                       
                                       =
                                       117.12
                                       ,
                                       p
                                       <
                                       0.001
                                    , Scheffé post-hoc test, α
                                    =0.05). Variant V
                                    
                                       s
                                     was rated higher than variant V
                                    
                                       i
                                     indicating that the subjects focused on the discourse segmentation function of intonation rather than on the information status. As to be expected V
                                    0 received the lowest scores.

@&#DISCUSSION AND CONCLUSIONS@&#

In order to get started with a knowledge-free bottom-up approach of intonation modeling, several restrictions and simplifications concerning the data and its analyses were made. So far only a single speaker was examined. However, due to the choice of a professional newsreader producing clear and easily intelligible speech, it can safely be stated that the data contains intonation patterns that are linked to linguistic concepts with high proficiency. In total, 10 out of 12 corpus-based predictions of these links were confirmed in subsequent perception experiments by Reichel (2011, 2012). As in the current study in these experiments not the original voice but an MBROLA voice had been presented, which indicates that the found relations are not just speaker-idiosyncratic but more general.

Inter-speaker variability has not yet been addressed, but will be investigated in future studies. In principle, our approach can capture variability by distinguishing between abstract contour classes and concrete F0 realizations. Within this framework it has to be tested whether it is possible to derive speaker-independent classes which can be mapped to the F0 surface by speaker-dependent regression models as described in Section 2.3.4.

Concerning prosodic structure, the global segmentation guided only by speech pauses and punctuation needs further improvement at the current state, since pauses are neither necessary nor sufficient boundary markers. In a current study (Reichel and Mády, 2013) we explore how automatically derived F0 discontinuity features can be used to supplement the identification of prosodic boundaries.

In subsequent studies prosodic structure could be extended by the insertion of an intermediate layer between local and global segments, that is defined directly in information structure terms. This layer would contain a sequence of theme (topic) and rheme (comment) segments (Halliday, 1967), that has been proven to have an impact on prosodic phrasing. To give an example, Steedman (2000) found for English that the prosodic attachment of a verb depends on its information status. It is attached to the subject (theme) if its status is given, and to the object (rheme), if its status is new. Furthermore, the intermediate layer could serve to examine how information structure is encoded by the syntagmatic combination of local contour classes.

The discourse structure concepts examined here were highly simplified. Again this simplification is considered to be an appropriate starting point given the bottom-up nature of the present approach. Next steps should include interpretations in the context of more fine-grained discourse concepts like different degrees of givenness (Pierrehumbert and Hirschberg, 1990; Baumann, 2006). Since rather accent type preferences instead of unique mappings of intonation and linguistic events are reported, it can be concluded that principally already a small number of contour classes can be sufficient to encode more complex information status concepts.

Also the NLP methods to generate the hypotheses for the linguistic anchoring were rather crude. They are definitely not sufficient to replace sophisticated methods used for example in speech synthesis for text-based intonation prediction, but nevertheless, they are of use for discovering relations between intonation events and their linguistic functions, since in two former studies of the author, Reichel (2011) and Reichel (2012), in total 83% of the predictions had been verified by the subsequent perceptual validations.

Taken altogether, we argue that these simplifications are justified to find an appropriate point of departure for our bottom-up modeling approach.

To anchor the polynomial contour stylization that contains a high degree of variation to linguistic concepts the CoPaSul framework provides an intermediate abstraction level by inference of discrete classes from the continuous parameter space. The quality of this mapping depends amongst others on the choice of the stylization function. Due to its analytical nature, the polynomial contour fit is more suited for parameter clustering than the numerical fit of more complex functions (Möhler and Conkie, 1998), since by the latter parameter vector distances are more loosely linked to contour distances.

Other parametric approaches like the Fujisaki model allow for a direct parameter interpretation (Möbius, 1993; Mixdorff, 1998). But again this semantically rich parameterization does not guarantee the reproducibility of the intonation abstraction due to the underlying numerical optimization. If different parameter values can be derived from the same underlying original contour, the linking of the parameters to linguistics may not be possible in a straight-forward linear manner.

Generally, there is no straightforward solution for an appropriate derivation of categories amongst others due to the various kinds of objects that could be clustered. We followed Möhler and Conkie (1998) in clustering stylization coefficient vectors, but alternatively, the stylized F0 vectors themselves could have been used. First, there is a methodological justification for using coefficient and not F0 vectors: In machine learning dimension reduction and orthogonalization of the feature vectors is beneficial (Cunningham, 2007). While the F0 vectors consist of a high number of elements that are highly correlated among each other, polynomial coefficients are exactly the result of a dimension reduction and orthogonalization of these vectors, and are thus expected to be more appropriate.

Second, to test the linguistic adequacy of this alternative classification, we clustered the stylized F0 vectors in exactly the same way as the coefficients. As introduced in section 3.3 we tested by means of χ
                           2 tests, whether or not the four derived contour co-occur significantly with the examined discourse events. The relation to utterance finality was significant for all four classes (α
                           =0.05). However for none of the classes a significant relation to information status has been found (χ
                           <1.2). It can thus be concluded that for our data coefficient vector classes are more appropriate to reflect discourse structure than F0 vector classes.

Analogously to the approach of Dainora (2001) for ToBI label sequences, we trained a contour class trigram probability model on the local contour class sequences of our data. Given that the vocabulary consists of only five classes all trigram probabilities turned out to be relatively low (all maximum likelihood estimates are below 0.34). This means that the contour class predictability in a given intonation context is much lower than was found in Dainora (2002) for pitch accent and boundary tone sequences within the tone sequence framework. One can draw two conclusions from this observation: First, the intonotactics of CoPaSul intonation events is not as restricted as found for tone sequence approaches and for other models like the IPO model (t’Hart et al., 1990; Noteboom, 1997). Second, following the argument of Dainora (2002) low overall n-gram predictability can be seen as an indication for having extracted proper basic building blocks and not block fragments co-occurring with probabilities near one. Thus, these findings support the position that elementary intonation units are better expressed as contours than tones.

So far the local contour have been linked to semantic weight and elementary discourse structure concepts (Reichel, 2011, 2012). As already pointed out the test of many other links like sentence mode, contrast constructions and paralinguistics is pending. It is not yet clear how far one can get with linguistic interpretations of this knowledge-free bottom-up approach of intonation modeling, but nevertheless, the initial attempts are promising.

Since the local contour classes cover all combinations of information status (given, new) and discourse segmentation (non-final, final), the derived set is sufficient at least for a crude encoding of discourse structure. It was further possible to perceptually attribute a linguistic function to each contour class since all mean perceptual judgments differed significantly from the undecidedness level 3 on the 5 point scales. The relations turned out to be stable, since in the present study the predicted contours received high scores by the subjects across different intonation contexts.

There is no one-to-one mapping between local intonation classes and linguistic concepts: More than one class can be used to encode the same concept (class correlations) and the same class can be used to encode several concepts (concept correlations). Concept correlations are in line with the results in Ward and Hirschberg (1985) and Pierrehumbert and Hirschberg (1990) within the tone sequence framework revealing that the same sequence of tones can have different pragmatic functions context-dependently and can encode different sentence modes. The class correlations that were found here do not allow for a compositional approach as in Pierrehumbert and Hirschberg (1990) distinguishing between pitch accents connected to the information status of a discourse referent and boundary tones encoding finality or continuation. All local contour classes derived in this study serve at the same time to encode information status and discourse segmentation.

It is to be pointed out that our contour prediction is not yet fully automated, since local contour classes can so far only automatically be assigned to local segments containing nouns. Thus, further effort is needed to extend the predictions to local segments of any type. As stated in Section
                        4.2 the prediction was already applicable also to adjectives in predicative position.

This study aimed to integrate previous findings about the linguistic anchoring of the CoPaSul approach which had been derived by perception experiments with a higher number of subjects. These findings were incorporated into a tree for text-based intonation prediction whose output was evaluated by phonetic experts. The choice of expert subjects allowed for a compact experimental setup. The relatively complex demands on the subject decisions would have made it necessary to run several experiments with naive subjects. Only simple sentences have been presented in order to allow for a straightforward linking of the subjects’ judgments to the responsible intonation characteristics which eases the interpretation of the results. In other perception studies (Niebuhr, 2007; Welby, 2003) the discourse context is specified in more detail by longer texts. The advantage over the single-sentence contexts of the current study is, that the subjects are constrained not to add other contextual factors than the ones being specified. On the other hand, processing more complex discourse contexts is more demanding for the subjects, and thus may lead to misunderstandings or even interferences across different contexts. In any case, in the experiments introduced in this study no subject reported any difficulties arising from vague linguistic contexts.

Despite the small sample sizes and the required conservative statistical tests, significant results emerged concerning the acceptability of the generated contours. The intonation that conformed to the predictions was judged to be adequate significantly above the midpoint of the rating scale for global and local contour classes. Furthermore, for local contour classes it was judged to be significantly more adequate than intonation patterns contradicting the predictions. It is very likely that these differences remain significant with increasing sample size, which would additionally allow for less conservative tests. Thus, the claim that within our purely data-driven framework acceptable and adequate intonation contours can be predicted from text is supported by these results. For global contour classes it turned out that the intonation prediction before and after potential topic shifts (variant V) does not offer any gain compared to the prediction of the preceding intonation only (variant V
                        1). The global contour class prediction model may thus be reduced to cover only the sentence final global segments.

The CoPaSul approach is partly eclectic in the sense that it aggregates characteristics of established intonation models presented in Section 1. Regarding the requirements defined in Section 1.2, namely an appropriate abstraction from the signal, the interpretability of the abstraction, and its automation, in our opinion the current approach provides several additional benefits: Opposed to the other parametric models PaintE, PENTA, TILT, and the Fujisaki model, CoPaSul offers a biunique mapping of the F0 contour to the parameter values which is expected to ease linguistic interpretation. Like PaintE, CoPaSul additionally offers a symbolic representation in form of intonation contour categories, that are derived by clustering. However, while for PaintE the cluster number was set manually in an ad hoc manner, in the current approach it arises directly from the data, using a subtractive clustering technique. This approach is suitable to obtain repeatable results for disjunct data subsets concerning contour class number and shapes.

For PaintE, TILT, and most applications of the Fujisaki model, the domain of local F0 behavior is defined in terms of realized pitch accents and boundary tones, while CoPaSul operates on potential accent groups containing zero or one accent. The advantages of this approach are that no preceding labeling of these prosodic events is required, and not only pitch accents but also deaccentuation can be captured as an intonation event. In contrast to the PENTA model which can also capture deaccentuation by describing local F0 behavior for each syllable, CoPaSul makes contour characteristics spanning over several syllables more explicit, which is considered to be favorable at least for languages that do not contain lexical tones.

@&#CONCLUSION@&#

It was possible to generate a perceptually acceptable intonation representation in a data-driven partly automatic way. This representation can be interpreted linguistically with respect to discourse structure and can thus be derived from the signal as well as from text. Therefore, this approach can be of relevance for intonation analysis and synthesis and can be useful for speech technology applications as well as for phonetic fundamental research for the automatic analysis of speech data.

For cluster initialization subtractive clustering (Chiu, 1994) was applied, that iteratively locates initial centers in the parameter space at regions with high data density. It was carried out using the SUBCLUST Matlab function. The following four parameters need to be initialized: (1) the cluster radius, that determines the size of the clusters, (2) the squash factor, which is to be multiplied with the radius and defines the neighborhood of cluster centers within which new centers are discouraged, (3) the accept ratio to set the minimum neighborhood density as a fraction of the density of the first cluster center, above which another data point will be accepted as a center, and (4) the reject ratio, that gives the neighborhood density as a fraction of the density of the first cluster center, below which a data point will be rejected as a center. Generally, a low number of non-overlapping clusters is derived by reasonably high values for all four parameters. To find a local optimum for these parameter values for a sample of 20% of the data, the Nelder–Mead Simplex optimization (Matlab function FMINSEARCH) was utilized. The error to be minimized was derived from the mean clustering silhouette as in (Reichel, 2010). The silhouette for the data point i is defined as 
                        S
                        (
                        i
                        )
                        =
                        
                           
                              
                                 d
                                 B
                              
                              (
                              i
                              )
                              −
                              
                                 d
                                 A
                              
                              (
                              i
                              )
                           
                           
                              max
                              (
                              
                                 d
                                 A
                              
                              (
                              i
                              )
                              ,
                              
                                 d
                                 B
                              
                              (
                              i
                              )
                              )
                           
                        
                     . d
                     
                        A
                     (i) is the mean squared Euclidean distance of point i to all points of the same cluster. d
                     
                        B
                     (i) is mean distance of point i to all points of the most i-similar cluster B
                     ≠
                     A. Its values fall within the range from −1 to 1. The closer the silhouette approaches 1 the more clearly and thus better is the assignment of i to its cluster. Values close to −1 indicate and erroneous assignment. The mean silhouette 
                        
                           S
                           ¯
                        
                      over all data points was transformed to an error measure e ranging from 0 to 1 by the following equation: 
                        e
                        =
                        (
                        
                           1
                           −
                           
                              S
                              ¯
                           
                        
                        )
                        /
                        2
                     . The optimized parameter values and the errors are listed separately for global and local contour clustering in Table 3
                     .

Subsequently, k-means clustering was carried out starting with these initial cluster centers. In Tables 4 and 5
                     
                      the polynomial coefficients of the global and local contour class centroids are presented.


                     Table 6 shows the coefficients of the linear regression models mapping the contour class centroids to context dependent realizations.


                        Coherent sentence pairs
                        
                           
                              •
                              
                                 ‘Dort steht eine Buche. Ihre Blätter sind grün. (There is a beech tree. Its leafs are green.)’
                              


                                 ‘Da liegt eine Geige. Ihre Saiten sind neu. (There is a violin. Its strings are new.)’
                              


                                 ‘Dort gibt es Bienen. Ihr Honig ist lecker. (There are bees. Their honey is tasty.)’
                              


                                 ‘Hier gibt es Birnen. Sie schmecken sehr saftig. (Here we have pears. They are very juicy.)’
                              


                        Incoherent sentence pairs
                        
                           
                              •
                              
                                 ‘Dort steht eine Buche. Meine Schwester hat Fieber. (There is a beech tree. My sister suffers from fever.’
                              


                                 ‘Da liegt eine Geige. Es gibt Nudeln aus der Dose. (There is a violin. We’ll have noodles from the can.)’
                              


                                 ‘Dort gibt es Bienen. Die Fähre kam pünktlich. (There are bees. The ferry arrived in time.)’
                              


                                 ‘Hier gibt es Birnen. Der Kellner trägt keine Socken. (Here we have pears. The waiter does not wear socks.)’
                              


                        
                           
                              •
                              
                                 Sentence pair 1: 
                                 ‘Dort steht eine Buche. Die Buche verliert ihre Blätter. (There is a beech tree. The beech tree is losing its leaves.)’
                                 
                                    
                                       
                                          
                                             
                                                
                                                
                                                
                                                
                                                   
                                                      
                                                         Context
                                                      
                                                      
                                                         Dort steht eine Buche.
                                                      
                                                   
                                                   
                                                      
                                                         Carrier
                                                      
                                                      
                                                         [Die Buche]
                                                         
                                                            s1 
                                                         verliert [ihre Blätter]
                                                         
                                                            s2
                                                         .
                                                      
                                                   
                                                   
                                                      
                                                         s
                                                         1
                                                      
                                                      Discourse
                                                      Given, non-final
                                                   
                                                   
                                                      
                                                      Variants
                                                      
                                                         V: c
                                                         4; V
                                                         
                                                            i
                                                         : c
                                                         1; V
                                                         
                                                            s
                                                         : c
                                                         2, c
                                                         3; V
                                                         0: c
                                                         5
                                                      
                                                   
                                                   
                                                      
                                                         s
                                                         2
                                                      
                                                      Discourse
                                                      New, final
                                                   
                                                   
                                                      
                                                      Variants
                                                      
                                                         V: c
                                                         5; V
                                                         
                                                            i
                                                         : c
                                                         2, c
                                                         3; V
                                                         
                                                            s
                                                         : c
                                                         1; V
                                                         0: c
                                                         4
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              


                                 Sentence pair 2: 
                                 ‘Dort steht eine Buche. Auch ein Traktor und ein Ochse. (There is a beech tree. Also a tractor and an ox.)’
                                 
                                    
                                       
                                          
                                             
                                                
                                                
                                                
                                                
                                                   
                                                      
                                                         Context
                                                      
                                                      
                                                         Dort steht eine Buche.
                                                      
                                                   
                                                   
                                                      
                                                         Target
                                                      
                                                      
                                                         [Auch ein Traktor]
                                                         
                                                            s1 
                                                         [und ein Ochse]
                                                         
                                                            s2
                                                         .
                                                      
                                                   
                                                   
                                                      
                                                         s
                                                         1
                                                      
                                                      Discourse
                                                      New, non-final
                                                   
                                                   
                                                      
                                                      Variants
                                                      
                                                         V: c
                                                         2, c
                                                         3; V
                                                         
                                                            i
                                                         : c
                                                         5; V
                                                         
                                                            s
                                                         : c
                                                         4; V
                                                         0: c
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         s
                                                         2
                                                      
                                                      Discourse
                                                      New, final
                                                   
                                                   
                                                      
                                                      Variants
                                                      
                                                         V: c
                                                         5; V
                                                         
                                                            i
                                                         : c
                                                         2, c
                                                         3; V
                                                         
                                                            s
                                                         : c
                                                         1; V
                                                         0: c
                                                         4
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              


                                 Sentence pair 3: 
                                 ‘Dort steht eine Buche. Die Kinder bewundern die Buche. (There is a beech tree. The children admire the beech tree.)’
                                 
                                    
                                       
                                          
                                             
                                                
                                                
                                                
                                                
                                                   
                                                      
                                                         Context
                                                      
                                                      
                                                         Dort steht eine Buche.
                                                      
                                                   
                                                   
                                                      
                                                         Target
                                                      
                                                      
                                                         [Die Kinder]
                                                         
                                                            s1 
                                                         bewundern [die Buche]
                                                         
                                                            s2
                                                         .
                                                      
                                                   
                                                   
                                                      
                                                         s
                                                         1
                                                      
                                                      Discourse
                                                      New, non-final
                                                   
                                                   
                                                      
                                                      Variants
                                                      
                                                         V: c
                                                         2, c
                                                         3; V
                                                         
                                                            i
                                                         : c
                                                         5; V
                                                         
                                                            s
                                                         : c
                                                         4; V
                                                         0: c
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         s
                                                         2
                                                      
                                                      Discourse
                                                      Given, final
                                                   
                                                   
                                                      
                                                      Variants
                                                      
                                                         V: c
                                                         1, V
                                                         
                                                            i
                                                         : c
                                                         4, V
                                                         
                                                            s
                                                         : c
                                                         5, V
                                                         0: c
                                                         2, c
                                                         3
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              


                                 Sentence pair 4: 
                                 ‘Dort steht eine Buche und eine Scheune. Die Buche verdunkelt die Scheune. (There is a beech tree and a barn. The beech tree darkens the barn.)’
                                 
                                    
                                       
                                          
                                             
                                                
                                                
                                                
                                                
                                                   
                                                      
                                                         Context
                                                      
                                                      
                                                         Dort stehen eine Buche und eine Scheune.
                                                      
                                                   
                                                   
                                                      
                                                         Target
                                                      
                                                      
                                                         [Die Buche]
                                                         
                                                            s1 
                                                         verdunkelt [die Scheune]
                                                         
                                                            s2
                                                         .
                                                      
                                                   
                                                   
                                                      
                                                         s
                                                         1
                                                      
                                                      Status
                                                      Given, non-final
                                                   
                                                   
                                                      
                                                      Variants
                                                      
                                                         V: c
                                                         4; V
                                                         
                                                            i
                                                         : c
                                                         1; V
                                                         
                                                            s
                                                         : c
                                                         2, c
                                                         3; V
                                                         0: c
                                                         5
                                                      
                                                   
                                                   
                                                      
                                                         s
                                                         2
                                                      
                                                      Status
                                                      Given, final
                                                   
                                                   
                                                      
                                                      Variants
                                                      
                                                         V: c
                                                         1; V
                                                         
                                                            i
                                                         : c
                                                         4; V
                                                         
                                                            s
                                                         : c
                                                         5; V
                                                         0: c
                                                         2, c
                                                         3
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

For the background segments, i.e. the local segments other than the target segments s
                        1 and s
                        2 following contour classes have been used:
                           
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             Background Segment
                                             Contour class
                                          
                                          
                                             [Dort steht eine Buche]
                                             
                                                c
                                                5
                                             
                                          
                                          
                                             [Dort stehen eine Buche] …
                                             
                                                c
                                                4
                                             
                                          
                                          
                                             … [und eine Scheune]
                                             
                                                c
                                                5
                                             
                                          
                                          
                                             … [verliert] …
                                             
                                                c
                                                1
                                             
                                          
                                          
                                             … [bewundern] …
                                             
                                                c
                                                5
                                             
                                          
                                          
                                             … [verdunkelt] …
                                             
                                                c
                                                5
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     


                        
                           Remark
                           The verb steht (is) was treated as an auxiliary as in English, so that it does not demand its own local segment. In order to avoid that the background segments affect the adequacy judgments of the target segments (1) the first sentence was separated from the second one, that contains the targets, by means of the finality encoding contour class c
                              5. (2) For the other background segments the 3 least prominent contour classes c
                              1, c
                              4, and c
                              5 had been chosen, whereas class prominence had been derived from a former perception experiment (Reichel, 2010).

@&#REFERENCES@&#

