@&#MAIN-TITLE@&#Learning low-rank and discriminative dictionary for image classification

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Learn a discriminative dictionary with low-rank regularization


                        
                        
                           
                           Fisher discriminant function is applied to the coding coefficients.


                        
                        
                           
                           IPM and ALM algorithms are adopted to solve our objective function.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sparse representation

Dictionary learning

Low-rank regularization

Image classification

@&#ABSTRACT@&#


               
               
                  Dictionary learning plays a crucial role in sparse representation based image classification. In this paper, we propose a novel approach to learn a discriminative dictionary with low-rank regularization on the dictionary. Specifically, we apply Fisher discriminant function to the coding coefficients to make the dictionary more discerning, that is, a small ratio of the within-class scatter to between-class scatter. In practice, noisy information in the training samples will undermine the discriminative ability of the dictionary. Inspired by the recent advances in low-rank matrix recovery theory, we apply low-rank regularization on the dictionary to tackle this problem. The iterative projection method (IPM) and inexact augmented Lagrange multiplier (ALM) algorithm are adopted to solve our objective function. The proposed discriminative dictionary learning with low-rank regularization (D
                     2
                     L
                     2
                     R
                     2) approach is evaluated on four face and digit image datasets in comparison with existing representative dictionary learning and classification algorithms. The experimental results demonstrate the superiority of our approach.
               
            

@&#INTRODUCTION@&#

Sparse representation has been extensively studied in recent years due to its promising performance [26,55,36]. Given a test signal and an over-complete dictionary with prototype signals as atoms, sparse representation seeks a sparsest representation of the test signal among all the linear combinations of the dictionary atoms. As a result, sparse representation can reveal the underlying structure of high dimensional signals. This is especially significant in the big data age when massive high dimensional data (image, video, web, bioinformatic data, etc.) are emerging which require fast processing. Sparse representation has been applied to many problems, ranging from speech denoising [19] to super resolution [54], and from blind source separation [31] to bioinformatics [56]. In this paper, we focus on image classification based on sparse representation which demands the representation to be discriminative as well. Wright et al. [51] propose sparse representation based classifier (SRC) for face recognition based on sparse signal representation theory. This sparsity has been supported by research in human visual system which found that nerve cells in the connecting pathway only react to a certain amount of stimuli [43].

The essence of sparse representation is to recover a signal from a small number of linear measurements. Given an over-complete dictionary D and a query sample y, the problem can be formulated as the following objective function:
                        
                           (1)
                           
                              
                                 
                                    min
                                    α
                                 
                                 ∥
                                 α
                                 
                                    ∥
                                    1
                                 
                                 
                                 s
                                 .
                                 t
                                 .
                                 
                                 y
                                 =
                                 Dα
                                 ,
                              
                           
                        
                     where α is the coding coefficient whose non-zero elements are those corresponding to the category y belongs to. Here the dictionary D can either be pre-specified or gradually adapted to fit the training samples given. Wright et al. [51] pre-specified the dictionary as the original training samples. A problem with this strategy is that the original images in the training set may not faithfully represent the test samples due to the noise and uncertainty in it. Besides, the distinctive message resided in the training set might be ignored in this way and the dictionary cannot guarantee the sparse property. As a result, we need to learn the dictionary adaptively from the specific training sample set.

Research progress on dictionary learning has been made on learning a well adapted dictionary for discriminative representation of test samples. Generalizing K-means clustering process, Aharon et al. [1] presented K-SVD algorithm to learn an over-complete dictionary by updating dictionary atoms and sparse representations iteratively. Recently, a discriminative K-SVD method that considers classification error when learning the dictionary was proposed [58]. Jiang et al. [21] associated label information with each dictionary atom to enforce discriminability. To reduce computational complexity, Lee et al. and Wang et al. [26,50] emphasized specific discriminative criteria to learn an over-complete dictionary. In Ref. [55], the authors introduced the Fisher criterion to learn a structured dictionary. Studer and Baraniuk [48] investigated dictionary learning from sparsely corrupted signals. However, the methods above can only work well on clean training samples or with small noise and sparse corruption. Imagine the training samples are corrupted with large noise, then in order for representing the training samples, the dictionary atoms will also get corrupted.

Recent advances in low-rank learning for the purpose of visual representation have shown excellent performance for handling large noise. Given a matrix M of low rank, matrix completion aims at recovering it from noisy observations of a random small portion of its elements. It has been proved that under certain assumptions, the problem can be exactly solved and several methods have been proposed [23,5,6]. In our case of dictionary learning for image classification, training samples in the same class are linearly correlated and lie in a low dimensional manifold. Therefore, a sub-dictionary for representing samples from one class should reasonably of low rank. Ma et al. (2012) integrated rank minimization into sparse representation and achieved impressive face recognition results especially when corruption existed.

Inspired by the previous work, we aim at learning a discriminative dictionary for image classification that can handle training samples corrupted with large noise. We propose a discriminative dictionary learning with low-rank regularization (D
                     2
                     L
                     2
                     R
                     2) approach and illustrate it in Fig. 1
                     . In the figure, the training sample set can be approximately recovered by the multiplication of the dictionary and the coding coefficient matrix. Each sub-dictionary is of low rank (can be seen as multiplication of two matrices of smaller size) to reduce the negative effect of noise contained in training samples. The coding coefficients conform to Fisher discrimination criterion. Benefiting from the above design, our approach has the following advantages. First, the Fisher discriminant function can help us achieve a small ratio of the within-class scatter to between-class scatter on the coefficients, making the dictionary learned has strong discerning power. Second, low-rank regularization will output a compact and pure dictionary that can reconstruct the denoised images even when the training samples are contaminated.

Unlike FDDL proposed in Ref. [55], our D
                     2
                     L
                     2
                     R
                     2 approach can well cope with training samples with large noise and can still achieve impressive performance due to the low-rank regularization on the sub-dictionaries. Our approach also differs from the recently proposed DLRD_SR algorithm proposed in Ref. [36]. Though DLRD_SR was claimed to be able to handle noisy samples as well, it may suffer from certain information loss because of the low-rank regularization, our D
                     2
                     L
                     2
                     R
                     2 approach compensates this by enforcing the Fisher criterion on the coding coefficients of the training sets.

This paper is a substantial extension of our previous conference paper [30]. Compared with Ref. [30], we give more details of our approach in this paper, and extensive experimental results are reported. In addition, we introduce the background of our work more comprehensively, and add more discussions in methodology and experimental parts. Beyond face recognition applications in Ref. [30], we also evaluate the performance of our approach on digit recognition. Thus, this is a more systematic and comprehensive paper of our work. The rest of this paper is organized as follows. Section 2 gives a brief review of some related work. Section 3 introduces our discriminative dictionary learning with low-rank regularization (D
                     2
                     L
                     2
                     R
                     2) approach. Section 4 presents the optimization algorithm for our model. Section 5 describes the classification scheme. Section 6 shows experimental results on several image datasets. Finally, we draw conclusions in Section 7.

@&#RELATED WORK@&#

In this section, we briefly review related work on sparse representation, dictionary learning and low-rank learning.

We briefly review robust face recognition using sparse representation proposed in Ref. [51]. Define matrix Y as the entire training set which consists of n training samples from all c different classes: Y
                        =[Y
                        1, Y
                        2, …, Y
                        
                           c
                        ] where 
                           
                              
                                 Y
                                 i
                              
                              ∈
                              
                                 R
                                 
                                    d
                                    ×
                                    
                                       n
                                       i
                                    
                                 
                              
                           
                         is all the training samples from i-th class, d is the dimension of samples, and n
                        
                           i
                         is the number of samples from i-th class. To classify a test sample y, we need to go through two phases: coding and classification.
                           
                              (a) Coding phase: we obtain the coding coefficient of y by solving the following minimization problem:
                                    
                                       (2)
                                       
                                          
                                             α
                                             =
                                             argmin
                                             ∥
                                             α
                                             
                                                ∥
                                                0
                                             
                                             
                                             subject
                                             
                                             to
                                             
                                             ∥
                                             y
                                             −
                                             Yα
                                             
                                                ∥
                                                2
                                             
                                             ≤
                                             ϵ
                                             .
                                          
                                       
                                    
                                 
                              

The model seeks the sparsest representation for y among all the possible linear combinations of the atoms in the dictionary. For general over-complete dictionary, the determination for this sparsest representation is shown to an NP-hard problem [11]. Instead, approximation methods are proposed to tackle the problem. Among them, the simplest are matching pursuit [38] and orthogonal matching pursuit [10]. The most widely used approach is to replace the l
                        0 norm with its convex envelope l
                        1 norm, then the minimization problem Eq. (2) becomes:
                           
                              (3)
                              
                                 
                                    α
                                    =
                                    argmin
                                    ∥
                                    α
                                    
                                       ∥
                                       1
                                    
                                    
                                    subject
                                    
                                    to
                                    
                                    ∥
                                    y
                                    −
                                    Yα
                                    
                                       ∥
                                       2
                                    
                                    ≤
                                    ϵ
                                    ,
                                 
                              
                           
                        which can be reformulated using Lagrange multiplier as following:
                           
                              (4)
                              
                                 
                                    α
                                    =
                                    argmin
                                    ∥
                                    y
                                    −
                                    Yα
                                    
                                       ∥
                                       2
                                    
                                    +
                                    λ
                                    ∥
                                    α
                                    
                                       ∥
                                       1
                                    
                                    .
                                 
                              
                           
                        
                     

As reviewed in Ref. [53], there are three widely used approaches to the above l
                        1 minimization problem: the interior-point method [17,22], the Homotopy method [42,15,37,13] and first-order methods [41,16,12,2]. Primal–dual interior-point method converts problem with inequality constraints to the one with equality constraints in an iterative fashion and then apply Newton's barrier method. However, the interior-point method cannot scale well to large-scale real-world problem. Homotopy method uses the fact that as the balance parameter λ decreases, problem (4) is a homotopy from l
                        2 to l
                        1. However, Homotopy method is also computational expensive. First-order methods look into the l
                        1 norm's structure and significantly reduce the computational cost of each iteration.
                           
                              (b) Classification phase: y is classified as the category with the smallest residual:
                                    
                                       (5)
                                       
                                          
                                             
                                                min
                                                i
                                             
                                             
                                                r
                                                i
                                             
                                             
                                                y
                                             
                                             =
                                             ∥
                                             y
                                             −
                                             Y
                                             
                                                δ
                                                i
                                             
                                             
                                                α
                                             
                                             
                                                ∥
                                                2
                                             
                                             ,
                                          
                                       
                                    
                                 where δ
                                 
                                    i
                                 (α) is a function that picks the coefficients corresponding to i-th class.

Sparse representation has found applications in many domains, e.g., blind image deblurring [57], face alignment [49], and human action recognition [52].

The goal of dictionary learning in sparse representation is to learn a dictionary which can yield sparse representation for the training samples. Probabilistic approach to dictionary learning learns a dictionary either by maximum likelihood [28,29] or by maximum a-posterior [24,39]. Given the training samples Y
                        ={y
                        
                           i
                        }
                           i
                           =1
                        
                           N
                        , the maximum likelihood learns a dictionary that maximizes the likelihood function p(Y|D)=∏
                        
                           i
                           =1
                        
                           N
                        
                        p(y
                        
                           i
                        |D). Each p(y
                        
                           i
                        |D) is calculated by integrating out its sparse coefficient which assumes to be a hidden variable. Maximum a-posterior finds the dictionary that instead maximizes the posterior p(D|Y)∝
                        p(Y|D)p(D). Different choices of the prior p(D) will lead to different formulas of the dictionary.

Many methods generalizes the K-Means clustering algorithm to learn the dictionary in sparse representation because sparse representation actually generalizes K-Means by selecting more than one clusters and the coefficients can take arbitrary values when minimizing the mean square error (MSE). The methods all first learn the sparse coding and then update the dictionary alternatively. K-SVD [1] applies SVD decomposition after computing the overall representation error matrix to update each atom of the dictionary. K-SVD is shown to converge by reducing MSE monotonically, but a global minimum is not guaranteed. Besides, K-SVD only considers good representation but is not optimal for classification. The classification error is integrated with the reconstruction error in the objective to learn a dictionary that is suitable for sparse representation but also has the discriminative power [58]. Discriminative K-SVD [58] can get around the issues of getting stuck at local minima and slow convergence. Label consistent K-SVD [21] further more adds a discriminative sparse-code error term, i.e. label consistency of the sparse codes so that the training samples from the same class have similar sparse coefficients. In this way, it can explore the underlying structure of the training set, and generate discriminative sparse representation optimal for classification. Pairwise sparse codes' similarity/dissimilarity constraints is considered in Ref. [18] and combines with the classification error to achieve a discriminative dictionary. The idea is that signals from the same class should share similar sparse codes while those from different classes have dissimilar ones.

Recently, theoretical advances on low-rank matrix recovery and completion enable us to correctly recover underlying low-rank structure in data [7,8], and low-rank matrix recovery has been applied in many areas, such as background modeling [7], shadow removal [7], subspace clustering [33], image processing [59] and multimedia analysis [60]. Robust PCA [7] can be used to recover corrupted data in a single subspace by solving a matrix rank minimization problem. This can be regarded as an extension of sparse representation from vector to matrix. Low-rank representation (LRR) [33] recovers data from multiple subspaces, and it shows very impressive results on subspace segmentation. Latent LRR (LatLRR) [34] is an extension of LRR that can recover the effects of unobserved hidden data. LatLRR can also extract salient features from corrupted images for use in classification tasks. Based on LRR, a fixed-rank representation (FRR) method [35] is proposed for unsupervised learning on image features. In Ref. [61], a non-negative low-rank and sparse graph is constructed for semi-supervised learning. Also, low-rank constraints have been introduced to the visual domain adaption in Ref. [20] and transfer subspace learning in Ref. [46]. A discriminative low-rank dictionary learning (DLRD) method is proposed in Ref. [36], which seeks low-rank dictionary for each class. In Ref. [9], a low-rank approximation method with structural incoherence is proposed for face recognition.

Given an observed and usually corrupted sample set XO
                        , low-rank learning methods solve the nuclear norm regularized optimization problem, which can be generally formulated as:
                           
                              (6)
                              
                                 
                                    min
                                    ∥
                                    X
                                    
                                       ∥
                                       ∗
                                    
                                    +
                                    λ
                                    ∥
                                    E
                                    
                                       ∥
                                       l
                                    
                                    ,
                                    
                                    s
                                    .
                                    t
                                    .
                                    
                                    
                                       X
                                       O
                                    
                                    =
                                    A
                                    
                                       X
                                    
                                    +
                                    E
                                 
                              
                           
                        where ∥⋅∥∗ is the nuclear norm (trace norm), X and E are unknown matrices to learn, A is a linear operator, ∥⋅∥1 is used to measure the noise, and λ
                        >0 is a balance parameter. In Robust PCA, A is an identity matrix and ∥⋅∥
                           l
                         is expressed as ∥E∥1. In LRR, A(X)=
                        AX where A is a given dictionary, and ∥⋅∥2,1 is chosen for E.

Several optimization algorithms have been proposed to solve the above problem, such as semi-definite programming (SDP) and accelerated proximal gradient (APG). However, these algorithms suffer large computational burden. Recently, Lin et al. proposed an augmented Lagrange multipliers (ALM), which solves the nuclear norm optimization problem efficiently. By introducing the singular value thresholding (SVT) operator, ALM has a computational complexity of (O)(n
                        3). In this paper, we adopt the ALM algorithm to solve the low-rank regularized problem.

To learn a discriminative dictionary even when large noise exists in the training samples, we propose a novel approach to learn a low-rank and discriminative dictionary.

Let Y denote the entire training set which consists of n training samples from all c different classes: Y
                        =[Y
                        1, Y
                        2,…,Y
                        
                           c
                        ], where 
                           
                              
                                 Y
                                 i
                              
                              ∈
                              
                                 R
                                 
                                    d
                                    ×
                                    
                                       n
                                       i
                                    
                                 
                              
                           
                         is all the training samples from i-th class, d is the dimensionality of each sample vector, and ni
                         is i-th class' sample size.

We aim to learn a discriminative dictionary from Y for future image classification task. Rather than learning the dictionary as a whole from all the training samples, we separately learn a sub-dictionary Di
                         for the i-th class. With all the sub-dictionaries learned, we will get the whole dictionary as D
                        =[D
                        1, D
                        2,…,D
                        
                           c
                        ], where c is the number of classes, Di
                         is the sub-dictionary for the i-th class, each Di
                         is of size d
                        ×
                        m
                        
                           i
                        , d is the dimension of each dictionary atom which is the same with the feature dimension of each training sample, and mi
                         is the number of atoms in the i-th sub-dictionary. Table 1
                         summarizes the notations used in this paper.

We represent the entire training set Y using the whole dictionary D and denote by X the sparse coefficient matrix we obtain. We should have Y
                        ≈
                        DX, and X could be written as X
                        =[X
                        1, X
                        2,…,X
                        
                           c
                        ], where Xi
                         is the sub-matrix that is the coefficients for representing Yi
                         using D. In this paper, we propose the following D
                        2
                        L
                        2
                        R
                        2 model:
                           
                              
                                 
                                    
                                       J
                                       
                                          D
                                          ,
                                          X
                                       
                                    
                                    =
                                    
                                       
                                          argmin
                                          
                                             D
                                             ,
                                             X
                                          
                                       
                                    
                                    
                                    
                                       
                                          R
                                          
                                             D
                                             X
                                          
                                          +
                                          
                                             λ
                                             1
                                          
                                          ∥
                                          X
                                          
                                             ∥
                                             1
                                          
                                          +
                                          
                                             λ
                                             2
                                          
                                          F
                                          
                                             X
                                          
                                          +
                                          α
                                          
                                             
                                                ∑
                                                i
                                             
                                             
                                          
                                          ∥
                                          
                                             D
                                             i
                                          
                                          
                                             ∥
                                             ∗
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where R(D, X) is reconstruction error term for expressing the discrimination power of D, ∥X∥1 is the l
                        1 regularization on coding coefficient matrix, F(X) is the Fisher discriminant function of the coefficients X, and ∥D
                        
                           i
                        ∥∗ is the nuclear norm of each sub-dictionary Di
                        , which is the convex envelope of its matrix rank. We will break down the model in the following subsections.

Sub-dictionary Di
                         should have the capacity to well represent samples from i-th class. To illustrate this mathematically, we rewrite Xi
                        , the coding coefficient matrix of Yi
                         over D, as X
                        
                           i
                        
                        =[X
                        
                           i
                        
                        1; X
                        
                           i
                        
                        2;…;X
                        
                           i
                        
                        
                           c
                        ], where 
                           
                              
                                 X
                                 i
                                 j
                              
                              ∈
                              
                                 R
                                 
                                    
                                       m
                                       j
                                    
                                    ×
                                    
                                       n
                                       i
                                    
                                 
                              
                           
                         is the coding coefficient matrix of Yi
                         over Dj
                        . We will have to minimize ∥Y
                        
                           i
                        
                        −
                        D
                        
                           i
                        
                        X
                        
                           i
                        
                        
                           i
                        ∥
                           F
                        . On the other hand, Di
                         should not be able to represent samples from other classes, that is ∑ 
                           j
                           =1,j
                           ≠
                           i
                        
                        
                           c
                        ∥D
                        
                           i
                        
                        X
                        
                           j
                        
                        
                           i
                        ∥F
                        2 should be as small as possible, where each X
                        
                           j
                        
                        
                           i
                         has nearly zero elements. Lastly, it is obvious that the whole dictionary D can well represent samples from any class Yi
                        , so we require the minimization of ∥Y
                        
                           i
                        
                        −
                        DX
                        
                           i
                        ∥F
                        2. Denote R(D
                        
                           i
                        , X
                        
                           i
                        )=∥Y
                        
                           i
                        
                        −
                        D
                        
                           i
                        
                        X
                        
                           i
                        
                        
                           i
                        ∥F
                        2
                        +∑
                        
                           j
                           =1,j
                           ≠
                           i
                        
                        
                           c
                        ∥D
                        
                           i
                        
                        X
                        
                           j
                        
                        
                           i
                        ∥F
                        2
                        +∥Y
                        
                           i
                        
                        −
                        DX
                        
                           i
                        ∥F
                        2 as the discriminative reconstruction error term for sub-dictionary Di
                        , we want to minimize the value of R(D
                        
                           i
                        , X
                        
                           i
                        ).

In addition to the discriminative reconstruction term, we want to make the coding coefficient matrix X discriminative as well. In this way, D will have discriminative power for training samples Y. We apply Fisher discrimination criterion [14] on the coding coefficient matrix X so that the ratio of within-class scatter to between-class scatter will be minimized and samples from different classes can be well separated.

Let S
                        
                           W
                        (X) and S
                        
                           B
                        (X) denote the within-class scatter matrix and the between-class scatter matrix of X, we have
                           
                              
                                 
                                    
                                       
                                          
                                             S
                                             W
                                          
                                          
                                             X
                                          
                                          =
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                c
                                             
                                             
                                          
                                          
                                          
                                             
                                                ∑
                                                
                                                   
                                                      x
                                                      k
                                                   
                                                   ∈
                                                   
                                                      X
                                                      i
                                                   
                                                
                                             
                                             
                                          
                                          
                                          
                                             
                                                
                                                   x
                                                   k
                                                
                                                −
                                                
                                                   
                                                      
                                                         x
                                                         ¯
                                                      
                                                      i
                                                   
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      x
                                                      k
                                                   
                                                   −
                                                   
                                                      
                                                         
                                                            x
                                                            ¯
                                                         
                                                         i
                                                      
                                                   
                                                
                                             
                                             T
                                          
                                          ,
                                       
                                    
                                    
                                       
                                          
                                             S
                                             B
                                          
                                          
                                             X
                                          
                                          =
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                c
                                             
                                             
                                          
                                          
                                          
                                             n
                                             i
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         x
                                                         ¯
                                                      
                                                      i
                                                   
                                                
                                                −
                                                
                                                   x
                                                   ¯
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            x
                                                            ¯
                                                         
                                                         i
                                                      
                                                   
                                                   −
                                                   
                                                      x
                                                      ¯
                                                   
                                                
                                             
                                             T
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    x
                                    ¯
                                 
                                 i
                              
                           
                         is the mean sample of X
                        
                           i
                        , 
                           
                              x
                              ¯
                           
                         is the mean sample of X, and ni
                         is the number of samples in i-th class.

As we have mentioned, we introduce Fisher criterion to X by defining a discriminative term F(X) as
                           
                              (7)
                              
                                 
                                    F
                                    
                                       X
                                    
                                    =
                                    tr
                                    
                                       
                                          
                                             S
                                             W
                                          
                                          
                                             X
                                          
                                       
                                    
                                    −
                                    tr
                                    
                                       
                                          
                                             S
                                             B
                                          
                                          
                                             X
                                          
                                       
                                    
                                    +
                                    η
                                    ∥
                                    X
                                    
                                       ∥
                                       F
                                       2
                                    
                                    .
                                 
                              
                           
                        
                     

Note here that minimizing tr(S
                        
                           W
                        (X))−
                        tr(S
                        
                           B
                        (X)) is equivalent to minimizing the ratio of within-class scatter to between-class scatter. The last term η∥X∥F
                        2 is to make the function convex and stable in which η is set as η
                        =1 [55].

In image classification, training samples in the same class are linearly correlated and reside in a low dimensional subspace. Therefore, a sub-dictionary for representing samples from one class should be reasonably of low rank.

Besides, requiring that the sub-dictionaries are of low rank can separate the noisy information and make the dictionary more pure and compact. Of all the possible sub-dictionary Di
                         that can represent samples from i-th class, we want to find the one with the most compact bases, that is to minimize ∥D
                        
                           i
                        ∥∗.

Considering the discriminative reconstruction error, Fisher discrimination criterion on the coding coefficients and the low-rank regularization on the dictionary all together, we have the following D
                        2
                        L
                        2
                        R
                        2 model:
                           
                              (8)
                              
                                 
                                    
                                       J
                                       
                                          D
                                          X
                                       
                                    
                                    =
                                    
                                       
                                          argmin
                                          
                                             D
                                             ,
                                             X
                                          
                                       
                                    
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      c
                                                   
                                                   
                                                
                                                
                                                R
                                                
                                                   
                                                      D
                                                      i
                                                   
                                                   
                                                      X
                                                      i
                                                   
                                                
                                                +
                                                
                                                   λ
                                                   1
                                                
                                                ∥
                                                X
                                                
                                                   ∥
                                                   1
                                                
                                                +
                                             
                                          
                                          
                                             
                                                
                                                   λ
                                                   2
                                                
                                                F
                                                
                                                   X
                                                
                                                +
                                                α
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      c
                                                   
                                                   
                                                
                                                ∥
                                                
                                                   D
                                                   i
                                                
                                                
                                                   ∥
                                                   ∗
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

In the next section, we solve our model by alternatively optimizing D and X.

To solve the optimization problem in Eq. (8), we divide it into two sub-problems. First, we optimize X
                     
                        i
                     (i
                     =1,2,…,c) when the dictionary D and all X
                     
                        j
                     (j
                     ≠
                     i) are fixed. We can get coding coefficient matrix X by putting all the X
                     
                        i
                     (i
                     =1,2,…,c) together. Second, we optimize Di
                      when X and D
                     
                        j
                     (j
                     ≠
                     i) are fixed. We describe the detailed implementations of solving these two sub-problems in this section.

Assumed that D is fixed, the original objective function Eq. (8) is then reduced to a sparse coding problem. We update each Xi
                         one by one and make all X
                        
                           j
                        (j
                        ≠
                        i) fixed. This can be done by solving the following problem:
                           
                              (9)
                              
                                 
                                    
                                       J
                                       
                                          
                                             X
                                             i
                                          
                                       
                                    
                                    =
                                    
                                       
                                          argmin
                                          
                                             X
                                             i
                                          
                                       
                                    
                                    
                                    
                                       
                                          
                                             
                                                ∥
                                                
                                                   Y
                                                   i
                                                
                                                −
                                                
                                                   D
                                                   i
                                                
                                                
                                                   X
                                                   i
                                                   i
                                                
                                                
                                                   ∥
                                                   F
                                                   2
                                                
                                                +
                                                ∥
                                                
                                                   Y
                                                   i
                                                
                                                −
                                                D
                                                
                                                   X
                                                   i
                                                
                                                
                                                   ∥
                                                   F
                                                   2
                                                
                                                +
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         j
                                                         =
                                                         1
                                                         ,
                                                         j
                                                         ≠
                                                         i
                                                      
                                                      c
                                                   
                                                   
                                                
                                                ∥
                                                
                                                   D
                                                   j
                                                
                                                
                                                   X
                                                   i
                                                   j
                                                
                                                
                                                   ∥
                                                   F
                                                   2
                                                
                                                +
                                             
                                          
                                          
                                             
                                                
                                                   λ
                                                   1
                                                
                                                ∥
                                                
                                                   X
                                                   i
                                                
                                                
                                                   ∥
                                                   1
                                                
                                                +
                                                
                                                   λ
                                                   2
                                                
                                                
                                                   F
                                                   i
                                                
                                                
                                                   
                                                      X
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              
                                 F
                                 i
                              
                              
                                 
                                    X
                                    i
                                 
                              
                              =
                              ∥
                              
                                 X
                                 i
                              
                              −
                              
                                 
                                    
                                       X
                                       ¯
                                    
                                    i
                                 
                              
                              
                                 ∥
                                 F
                                 2
                              
                              −
                              
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    c
                                 
                                 
                              
                              ∥
                              
                                 
                                    
                                       X
                                       ¯
                                    
                                    k
                                 
                              
                              −
                              
                                 X
                                 ¯
                              
                              
                                 ∥
                                 2
                              
                              +
                              η
                              ∥
                              
                                 X
                                 i
                              
                              
                                 ∥
                                 F
                                 2
                              
                           
                         and 
                           
                              
                                 
                                    X
                                    ¯
                                 
                                 k
                              
                           
                         and 
                           
                              X
                              ¯
                           
                         are matrices composed of the mean of vectors of k-th class and all classes.

This reduced objective function can be solved by using iterative projection method (IPM) in Ref. [44] by rewriting it as
                           
                              
                                 
                                    
                                       J
                                       
                                          
                                             X
                                             i
                                          
                                       
                                    
                                    =
                                    
                                       
                                          argmin
                                          
                                             X
                                             i
                                          
                                       
                                    
                                    
                                    
                                       
                                          Q
                                          
                                             
                                                X
                                                i
                                             
                                          
                                          +
                                          2
                                          τ
                                          ∥
                                          
                                             X
                                             i
                                          
                                          
                                             ∥
                                             1
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The detailed implementations of IPM can be referred to Refs. [55,44].

When X is fixed, we can update Di
                         by fixing all the other D
                        
                           j
                        (j
                        ≠
                        i). Notice that the coding coefficient of Yi
                         over Di
                         should be updated at the same time. In addition, when Di
                         is updated, the coding coefficient of Yi
                         over Di
                         should also be updated to reflect this change, that is, X
                        
                           i
                        
                        
                           i
                         is also updated.

The objective function Eq. (8) is then reduced to
                           
                              
                                 
                                    
                                       J
                                       
                                          
                                             D
                                             i
                                          
                                       
                                    
                                    =
                                    
                                       
                                          argmin
                                          
                                             
                                                D
                                                i
                                             
                                             ,
                                             
                                                X
                                                i
                                                i
                                             
                                          
                                       
                                    
                                    
                                    
                                       
                                          
                                             
                                                ∥
                                                
                                                   Y
                                                   i
                                                
                                                −
                                                
                                                   D
                                                   i
                                                
                                                
                                                   X
                                                   i
                                                   i
                                                
                                                −
                                                
                                                   
                                                      ∑
                                                      
                                                         j
                                                         =
                                                         1
                                                         ,
                                                         j
                                                         ≠
                                                         i
                                                      
                                                      c
                                                   
                                                   
                                                      
                                                         D
                                                         j
                                                      
                                                      
                                                         X
                                                         i
                                                         j
                                                      
                                                      
                                                         ∥
                                                         F
                                                         2
                                                      
                                                   
                                                
                                                +
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         j
                                                         =
                                                         1
                                                         ,
                                                         j
                                                         ≠
                                                         i
                                                      
                                                      c
                                                   
                                                   
                                                      ∥
                                                      
                                                         D
                                                         j
                                                      
                                                      
                                                         X
                                                         i
                                                         j
                                                      
                                                      
                                                         ∥
                                                         F
                                                         2
                                                      
                                                   
                                                
                                                +
                                                ∥
                                                
                                                   Y
                                                   i
                                                
                                                −
                                                
                                                   D
                                                   i
                                                
                                                
                                                   X
                                                   i
                                                   i
                                                
                                                
                                                   ∥
                                                   F
                                                   2
                                                
                                             
                                          
                                          
                                             
                                                +
                                                α
                                                ∥
                                                
                                                   D
                                                   i
                                                
                                                
                                                   ∥
                                                   ∗
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Denote r(D
                        
                           i
                        )=∥Y
                        
                           i
                        
                        −
                        D
                        
                           i
                        
                        X
                        
                           i
                        
                        
                           i
                        
                        −∑ 
                           j
                           =1,j
                           ≠
                           i
                        
                        
                           c
                         
                        D
                        
                           j
                        
                        X
                        
                           i
                        
                        
                           j
                        ∥F
                        2
                        +∑ 
                           j
                           =1,j
                           ≠
                           i
                        
                        
                           c
                        ∥D
                        
                           j
                        
                        X
                        
                           i
                        
                        
                           j
                        ∥F
                        2, the above objective function can be converted to the following:
                           
                              (10)
                              
                                 
                                    
                                       
                                          
                                             min
                                             
                                                
                                                   D
                                                   i
                                                
                                                ,
                                                
                                                   E
                                                   i
                                                
                                                ,
                                                
                                                   X
                                                   i
                                                   i
                                                
                                             
                                          
                                          ∥
                                          
                                             X
                                             i
                                             i
                                          
                                          
                                             ∥
                                             1
                                          
                                          +
                                          α
                                          ∥
                                          
                                             D
                                             i
                                          
                                          
                                             ∥
                                             ∗
                                          
                                          +
                                          β
                                          ∥
                                          
                                             E
                                             i
                                          
                                          
                                             ∥
                                             
                                                2
                                                ,
                                                1
                                             
                                          
                                          +
                                          λr
                                          
                                             
                                                D
                                                i
                                             
                                          
                                          ,
                                       
                                    
                                    
                                       
                                          s
                                          .
                                          t
                                          .
                                          
                                          
                                             Y
                                             i
                                          
                                          =
                                          
                                             D
                                             i
                                          
                                          
                                             X
                                             i
                                             i
                                          
                                          +
                                          
                                             E
                                             i
                                          
                                          ,
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    
                                       
                                          E
                                          i
                                       
                                    
                                    
                                       2
                                       ,
                                       1
                                    
                                 
                              
                              =
                              
                                 
                                    ∑
                                    
                                       q
                                       =
                                       1
                                    
                                    n
                                 
                                 
                              
                              
                              
                                 
                                    
                                       
                                          ∑
                                          
                                             p
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                    
                                    
                                    
                                       
                                          
                                             
                                                
                                                   E
                                                   i
                                                
                                             
                                             pq
                                          
                                       
                                       2
                                    
                                 
                              
                           
                         is the l
                        2,1-norm that is usually adopted to measure the sample-specific corruption or noise.

To facilitate the optimization, we introduce two relaxation variables J and Z, and then Eq. (10) can be rewritten as:
                           
                              (11)
                              
                                 
                                    
                                       
                                          
                                             min
                                             
                                                
                                                   D
                                                   i
                                                
                                                ,
                                                
                                                   E
                                                   i
                                                
                                                ,
                                                
                                                   X
                                                   i
                                                   i
                                                
                                             
                                          
                                          ∥
                                          Z
                                          
                                             ∥
                                             1
                                          
                                          +
                                          α
                                          ∥
                                          J
                                          
                                             ∥
                                             ∗
                                          
                                          +
                                          β
                                          ∥
                                          
                                             E
                                             i
                                          
                                          
                                             ∥
                                             
                                                2
                                                ,
                                                1
                                             
                                          
                                          +
                                          λr
                                          
                                             
                                                D
                                                i
                                             
                                          
                                          ,
                                       
                                    
                                    
                                       
                                          s
                                          .
                                          t
                                          .
                                          
                                             Y
                                             i
                                          
                                          =
                                          
                                             D
                                             i
                                          
                                          
                                             X
                                             i
                                             i
                                          
                                          +
                                          
                                             E
                                             i
                                          
                                          ,
                                          
                                             D
                                             i
                                          
                                          =
                                          J
                                          ,
                                          
                                             X
                                             i
                                             i
                                          
                                          =
                                          Z
                                          .
                                       
                                    
                                 
                              
                           
                        
                     

The above problem can then be solved by the inexact Augmented Lagrange Multiplier (ALM) [4] method. The augmented Lagrangian function of Eq. (11) is
                           
                              (12)
                              
                                 
                                    
                                       
                                          
                                             
                                                min
                                                
                                                   
                                                      D
                                                      i
                                                   
                                                   ,
                                                   
                                                      E
                                                      i
                                                   
                                                   ,
                                                   
                                                      X
                                                      i
                                                      i
                                                   
                                                
                                             
                                          
                                          
                                             ∥
                                             Z
                                             
                                                ∥
                                                1
                                             
                                             +
                                             α
                                             ∥
                                             J
                                             
                                                ∥
                                                ∗
                                             
                                             +
                                             β
                                             ∥
                                             
                                                E
                                                i
                                             
                                             
                                                ∥
                                                
                                                   2
                                                   ,
                                                   1
                                                
                                             
                                             +
                                             λr
                                             
                                                
                                                   D
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          
                                          
                                             +
                                             tr
                                             
                                                
                                                   
                                                      T
                                                      1
                                                      t
                                                   
                                                   
                                                      
                                                         
                                                            Y
                                                            i
                                                         
                                                         −
                                                         
                                                            D
                                                            i
                                                         
                                                         
                                                            X
                                                            i
                                                            i
                                                         
                                                         −
                                                         
                                                            E
                                                            i
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                          
                                             +
                                             tr
                                             
                                                
                                                   
                                                      T
                                                      2
                                                      t
                                                   
                                                   
                                                      
                                                         
                                                            D
                                                            i
                                                         
                                                         −
                                                         J
                                                      
                                                   
                                                
                                             
                                             +
                                             tr
                                             
                                                
                                                   
                                                      T
                                                      3
                                                      t
                                                   
                                                   
                                                      
                                                         
                                                            X
                                                            i
                                                            i
                                                         
                                                         −
                                                         Z
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                          
                                             +
                                             
                                                μ
                                                2
                                             
                                             (
                                             ∥
                                             
                                                Y
                                                i
                                             
                                             −
                                             
                                                D
                                                i
                                             
                                             
                                                X
                                                i
                                                i
                                             
                                             −
                                             
                                                E
                                                i
                                             
                                             
                                                ∥
                                                F
                                                2
                                             
                                          
                                       
                                       
                                          
                                          
                                             +
                                             ∥
                                             
                                                D
                                                i
                                             
                                             −
                                             J
                                             
                                                ∥
                                                F
                                                2
                                             
                                             +
                                             ∥
                                             
                                                X
                                                i
                                                i
                                             
                                             −
                                             Z
                                             
                                                ∥
                                                F
                                                2
                                             
                                             )
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where T
                        1, T
                        2 and T
                        3 are Lagrange multipliers and μ(μ
                        >0) is a balance parameter.

The details of solving the problem can be referred to Algorithm 1. Each atom of the dictionary is normalized to a unit vector. A similar proof to demonstrate the convergence property of Algorithm 1 can be found in Lin et al.'s work [32].

Once the dictionary D is initialized, we can proceed by iteratively repeating the above process until a stopping criterion is reached. We summarize our approach for D
                        2
                        L
                        2
                        R
                        2 in Algorithm 2.
                           
                              
                           
                        
                        
                           
                              
                           
                        
                     

We code a query sample y against the dictionary D learned and obtain the coding coefficient by solving
                        
                           (13)
                           
                              
                                 x
                                 =
                                 
                                    
                                       argmin
                                       x
                                    
                                 
                                 
                                 
                                    
                                       ∥
                                       y
                                       −
                                       Dx
                                       
                                          ∥
                                          2
                                          2
                                       
                                       +
                                       γ
                                       ∥
                                       x
                                       
                                          ∥
                                          1
                                       
                                    
                                 
                                 .
                              
                           
                        
                     
                  

Denote by x
                     =[x
                     1;x
                     2;…;x
                     
                        c
                     ], where x
                     
                        i
                      is the coefficient vector over sub-dictionary Di
                     , we can calculate the residual associated with i-th class as
                        
                           (14)
                           
                              
                                 
                                    e
                                    i
                                 
                                 =
                                 ∥
                                 y
                                 −
                                 
                                    D
                                    i
                                 
                                 
                                    x
                                    i
                                 
                                 
                                    ∥
                                    2
                                    2
                                 
                                 +
                                 w
                                 ∥
                                 x
                                 −
                                 
                                    
                                       
                                          x
                                          ¯
                                       
                                       i
                                    
                                 
                                 
                                    ∥
                                    2
                                    2
                                 
                                 ,
                              
                           
                        
                     where 
                        
                           
                              
                                 x
                                 ¯
                              
                              i
                           
                        
                      is the learned mean coefficient of class i, and w is a preset weight parameter.

The identity of testing sample y is determined according to
                        
                           (15)
                           
                              
                                 identity
                                 
                                    y
                                 
                                 =
                                 arg
                                 
                                    min
                                    i
                                 
                                 
                                    
                                       e
                                       i
                                    
                                 
                                 .
                              
                           
                        
                     
                  

@&#EXPERIMENTAL RESULTS@&#

We apply our approach to face recognition and digit recognition on ORL [45], Extend Yale B [27], CMU PIE [47] and MNIST datasets [25] to verify its performance. The robustness of our D
                     2
                     L
                     2
                     R
                     2 approach to illumination changes, pixel corruptions, block corruptions, and noise will be evaluated. Experimental results will be presented along with discussions.

We set the number of dictionary columns of each class as training size. There are 5 parameters in our approach: λ
                        1 and λ
                        2 in Eq. (9) and α, β and λ in Eq. (10). In the experiment, we found that changing α and λ wouldn't affect the result that much, and we set them both as 1. Parameters of the comparison algorithms are chosen by cross validation in 5-fold fashion. For ORL, λ
                        1
                        =0.005, λ
                        2
                        =0.05, β
                        =0.1; for Extended Yale B, λ
                        1
                        =0.005, λ
                        2
                        =0.005, β
                        =0.01; for PIE, λ
                        1
                        =0.025, λ
                        2
                        =0.025, β
                        =0.1; and for MNIST, λ
                        1
                        =0.005, λ
                        2
                        =0.005, β
                        =0.01.

We compare D
                        2
                        L
                        2
                        R
                        2 with DLRD_SR [36], FDDL [55], LDA [3], SRC [51] and LRC [40]. DLRD_SR applies low-rank regularization on the dictionary but without Fisher criterion on the coefficients, FDDL introduces Fisher criterion but has no low-rank requirement for the dictionary, LDA has Fisher criterion alone but no discriminative reconstruction error minimization. LRC is a recently proposed classifier which falls in the category of nearest subspace classification.

The ORL dataset contains 400 images in total, ten different images for each of 40 different subjects. The background of the images is uniform and dark while the subjects are in frontal, upright posture. The images were shot under different lighting condition and with various facial expression and details [45]. For each class, we select half of the images randomly as training samples and rest as testing and repeat the experiment on five random splits. The images are normalized to 32×32. The images are manually corrupted by an unrelated block image at a random location. Fig. 2
                         shows an example of images with 20% block corruptions.

We list the recognition accuracies under different levels of occlusions in Table 2
                        . From the table, we can see that our approach constantly performs the best under different levels of corruptions (>0%). FDDL achieves the best result when there is no corruption, however, when the percentage of occlusions increases, performance of FDDL along with that of LRC, SRC and LDA drops rapidly, but D
                        2
                        L
                        2
                        R
                        2 and DLRD_SR can still obtain much better recognition rates. This demonstrates the effectiveness of low-rank regularization when noise exists. Comparing D
                        2
                        L
                        2
                        R
                        2 with DLRD_SR, D
                        2
                        L
                        2
                        R
                        2 performs better due to the Fisher criterion on the coefficients.

The CMU PIE dataset consists of 41,368 images of 68 subjects, each person under 13 different poses, 43 different illumination conditions, and with 4 different expressions. We use the first 15 subjects and select those images with frontal position and various expression and illumination. Each subject has 50 images, and we randomly select 10 images as training and the rest as testing and repeat our experiment on five random splits. The images are normalized to size 32×32. We replace a certain percentage of randomly selected pixels of each image with pixel value of 255. Fig. 3
                         exemplifies random pixel corruption on both training and test face samples.


                        Fig. 4
                         shows the recognition accuracy under various noise percentage. D
                        2
                        L
                        2
                        R
                        2 performs the best most of the time, but when there is no corruption or the percentage of corruption is very small, D
                        2
                        L
                        2
                        R
                        2 cannot beat FDDL. We see that the low-rank regularization doesn't help much in this case, but can on the hand degrade the performance. However, compared with DLRD_SR, D
                        2
                        L
                        2
                        R
                        2 still obtain better accuracy due to the benefit from Fisher criterion. This can also be validated from LDA's good performance.

The Extended Yale B dataset contains 2414 frontal-face images of 38 subjects captured under various laboratory-controlled lighting conditions. We choose the first 15 subjects and each subject has around 60 images. We randomly take half as training samples, and the rest as testing samples and repeat the experiment five times. The images are normalized to size 32×32. We replace a certain percentage of randomly selected pixels from the images with pixel value of 255. Fig. 5
                         shows training and testing face samples with pixel corruptions.

We list the recognition accuracies in Table 3
                        . Both D
                        2
                        L
                        2
                        R
                        2 and DLRD_SR perform well when noise exists whereas recognition rates of LRC, SRC and LDA decrease fast with increasing noise, which demonstrates the superiority of low-rank regularization in terms of handling noise.

The MNIST handwritten digit dataset used in this experiment is a popular subset that contains 60,000 training samples and 10,000 test samples. In our experiments, we randomly select 30 samples of each digit to construct the training set, and select 2000 test samples in total. The size of each digit image is 28×28. As before, we replace a certain percentage (from 10% to 40%) of randomly selected pixels from the images with pixel value of 255. Fig. 6
                         shows training and testing face samples with pixel corruptions.


                        Table 4
                         shows the average recognition rates (with standard deviations) of all compared methods. We can observe that our approach constantly outperforms all the other competing methods under different levels of corruptions. The result demonstrates the low-rank regularization's capability of handling noise and the further discrimination Fisher criterion can introduce.

@&#DISCUSSIONS@&#

Both D
                        2
                        L
                        2
                        R
                        2 and DLRD_SR perform better under noise condition from the results of the four datasets (both face and digit datasets), which no doubt demonstrates low-rank regularization's advantage in dealing with noise. Comparing D
                        2
                        L
                        2
                        R
                        2 with DLRD_SR, the former one can achieve better results almost all the time. This is due to the Fisher discriminant function on the coefficient matrix, which can make the dictionary learned more discerning. However, this function is defined on the coefficients of training set, so when the training set is not sufficient compared to the testing set, D
                        2
                        L
                        2
                        R
                        2 and FDDL might not perform that well.

To investigate how sensitive the parameters are, we experiment on PIE dataset without occlusion to see how different values of parameters λ
                        1 and λ
                        2 affect the recognition accuracy. Fig. 7
                         shows the recognition accuracy using different values of parameter λ
                        1. Other parameters are fixed. The accuracy reaches a plateau as λ
                        1 grows from 0.025, which indicates our method is insensitive to the choice of λ
                        1. Similarly, Fig. 8
                         shows the recognition accuracy using different values of parameter λ
                        2. Again, the accuracy reaches a plateau after λ
                        2 grows to 0.025, thus our method is also insensitive to λ
                        2. Also notice that when λ
                        1
                        =0, the accuracy drops relatively 21%, which shows the importance of the sparsity of the coefficients.

We also evaluate the computational cost of D
                        2
                        L
                        2
                        R
                        2 and other compared methods on the PIE dataset. Table 5
                         shows the running time
                           2
                        
                        
                           2
                           The machine used is installed with 24GB RAM and Intel Xeon W3350 CPU.
                         of different methods. We can observe that sparse representation based methods usually consumes more time than linear methods like LDA and LRC. As expected, our method's running time is between that of FDDL and DLRD_SR.

In addition, we only adopt images under controlled environment in the experiments, since low-rank constraint is somewhat sensitive to images that are not well aligned. Designing models to learn noise-free and discriminative dictionary from images taken in uncontrolled environment could be our future work.

@&#CONCLUSION@&#

In this paper, we present a novel approach to learn a discriminative dictionary with low-rank regularization (D
                     2
                     L
                     2
                     R
                     2) for face and digit image recognition. Our D
                     2
                     L
                     2
                     R
                     2 approach can learn a structured dictionary where each sub-dictionary is of low rank, which can separate the noise information in the training samples. The discrimination power comes as two-fold. First, the correlation between each sub-dictionary and samples from other classes is minimized; Second, Fisher discrimination criterion is applied to the coding coefficient matrix. Our approach can be effectively solved by the IPM and inexact ALM algorithms. We evaluate the classification performance of our approach and several compared methods on four face and digit image datasets, and the experimental results clearly demonstrate that our approach is superior to many other state-of-the-art dictionary learning and classification methods, especially when there is noise or corruption present.

@&#ACKNOWLEDGMENTS@&#

This research is supported in part by the NSF CNS award 1314484, Office of Naval Research award N00014-12-1-1028, Air Force Office of Scientific Research award FA9550-12-1-0201, the U.S. Army Research Office under grant number W911NF-13-1-0160 and IC Postdoctoral Research Fellowship award 2011-11071400006.

@&#REFERENCES@&#

