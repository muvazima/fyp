@&#MAIN-TITLE@&#Breast cancer diagnosis in digitized mammograms using curvelet moments

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Mammogram analysis based on curvelet moments, with particular attention to the choice of curvelet subband and moment order.


                        
                        
                           
                           A feature selection step to keep the most discriminative moments.


                        
                        
                           
                           An empirical comparison against state-of-the-art curvelet-based methods on two mammogram databases.


                        
                        
                           
                           The effectiveness and the superiority of curvelet moment for abnormality and malignancy detection.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Curvelet moments

Mammography

Feature reduction

Breast cancer diagnosis

Curvelet transform

@&#ABSTRACT@&#


               
               
                  
                     Background: Feature extraction is a key issue in designing a computer aided diagnosis system. Recent researches on breast cancer diagnosis have reported the effectiveness of multiscale transforms (wavelets and curvelets) for mammogram analysis and have shown the superiority of curvelet transform. However, the curse of dimensionality problem arises when using the curvelet coefficients and therefore a reduction method is required to extract a reduced set of discriminative features.
                  
                     Methods: This paper deals with this problem and proposes a feature extraction method based on curvelet transform and moment theory for mammogram description. First, we performed discrete curvelet transform and we computed the four first-order moments from curvelet coefficients distribution. Hence, two feature sets can be obtained: moments from each band and moments from each level. In this work, both sets are studied. Then, the t-test ranking technique was applied to select the best features from each set. Finally, a k-nearest neighbor classifier was used to distinguish between normal and abnormal breast tissues and to classify tumors as malignant or benign. Experiments were performed on 252 mammograms from the Mammographic Image Analysis Society (mini-MIAS) database using the leave-one-out cross validation as well as on 11553 mammograms from the Digital Database for Screening Mammography (DDSM) database using 2×5-fold cross validation.
                  
                     Results: Experimental results prove the effectiveness and the superiority of curvelet moments for mammogram analysis. Indeed, results on the mini-MIAS database show that curvelet moments yield an accuracy of 91.27% (resp. 81.35 %) with 10 (resp. 8) features for abnormality (resp. malignancy) detection. In addition, empirical comparisons of the proposed method against state-of-the-art curvelet-based methods on the DDSM database show that the suggested method does not only lead to a more reduced feature set, but it also statistically outperforms all the compared methods in terms of accuracy.
                  
                     Conclusions: In summary, curvelet moments are an efficient and effective way to extract a reduced set of discriminative features for breast cancer diagnosis.
               
            

@&#INTRODUCTION@&#

Breast cancer is the most common female cancer and the leading cause of cancer-related deaths among women worldwide. It affected 1.67 million new cases (25% of all cancers) and killed 522,000 women (14.71% of cancer-related deaths) worldwide in 2012 [1]. Early detection of breast cancer increases treatment options and recovery rates [2]. So far, mammography is the best available tool and the gold standard for screening. For instance, nationwide mammogram screening programs decreased breast cancer mortality in many developed countries [3,4]. However, the difficulty of mammogram interpretation leads to high rates of missed cancers and misinterpreted non-cancerous lesions. Therefore, a second opinion is usually required to reduce false positive and false negative rates [5].

However, even though it is effective clinically, double reading is not practicable because of the huge amount of mammograms to be analyzed and the limited number of qualified radiologists [5]. Thus, computer-aided diagnosis (CAD) systems can be used to reduce the expense and to assist radiologists in mammogram interpretation [6]. Typically, a CAD system is composed of three steps: preprocessing, feature extraction and classification. In particular, extraction of a representative feature set is a key step that can greatly enhance the diagnosis performance. The feature set must be compact and discriminative to improve the speed and the accuracy of CAD systems [7].

This paper deals with the problem of extracting a reduced set of discriminative features from curvelet transform for breast cancer diagnosis. We propose a method that uses moment theory to characterize the distribution of curvelet coefficients. After decomposing the mammogram into subbands using curvelet transform, moments can be computed from the distribution of coefficients either at each level or at each band. Both techniques were investigated in this paper. Then, we performed a ranking step to select optimal subsets from curvelet moment sets. Thereafter, a k-nearest neighbor (k-NN) classifier was used to distinguish between normal and abnormal tissues and to classify tumors as malignant or benign. To demonstrate the effectiveness of the proposed method, comparison was made with other feature sets extracted from curvelet transform. Experimental results have shown that the proposed method does not only lead to a far more reduced sets but it also achieves better performance and the difference is statistically significant. Note that the idea of using curvelet moments was previously studied in [8] for aggregate mixture grading. This idea is extended here not only by adapting it for breast cancer diagnosis but also by investigating two techniques to compute curvelet moments: from each level (Curvelet Level Moments-CLM) vs. from each band (Curvelet Band Moments-CBM). As far as we know, this is the first work that suggests the use of curvelet moments to analyze mammograms, with particular attention to the choice of curvelet subband and moment order. Besides, a feature selection step was used to select the most discriminative moments. In fact, selected moments from curvelet levels allow the smaller and the more discriminative feature set for mammogram classification. The contributions of this work are as follows:
                        
                           •
                           A new method for extracting a reduced set of features for mammogram analysis based on curvelet moments, while investigating two techniques to compute curvelet moments: from each level vs. from each band.

A statistical analysis of the distribution of curvelet coefficients of mammograms in order to justify the use of higher order moments, and a feature selection step to keep the most discriminative moments.

An experimental evaluation of the proposed method on mammograms from the mini-MIAS database.

An empirical comparison of this method against state-of-the-art curvelet-based methods for mammogram analysis on a large database of 11,553 mammograms from the challenging DDSM dataset, demonstrating the superiority of the suggested method.

@&#RELATED WORK@&#

Several studies have focused on the extraction of mammogram descriptors from wavelet and curvelet transforms [9,10]. Obviously, the use of all the curvelet coefficients is not practical since it leads to a curse of dimensionality problem, which increases the computational cost and deteriorates the performance of the classification method. Many works studied the challenging issue of extracting a reduced feature from curvelet transform for mammograms analysis. Proposed methods can be regrouped into two main classes: feature selection and feature extraction.

First attempts to cope with this problem were devoted to select a subset of relevant coefficients. Given a feature vector 
                           x
                           =
                           
                              
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 ‥
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       N
                                    
                                 
                                 )
                              
                              
                                 T
                              
                           
                        , feature selection methods aim to choose a subset of features 
                           
                              
                                 x
                              
                              
                                 
                                    
                                       s
                                    
                                    
                                       ^
                                    
                                 
                              
                           
                           =
                           
                              
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       
                                          
                                             s
                                          
                                          
                                             ^
                                          
                                       
                                       1
                                    
                                 
                                 ,
                                 ‥
                                 ,
                                 
                                    
                                       x
                                    
                                    
                                       
                                          
                                             s
                                          
                                          
                                             ^
                                          
                                       
                                       M
                                    
                                 
                                 )
                              
                              
                                 T
                              
                           
                         that maximize an objective function 
                           J
                           (
                           
                              
                                 x
                              
                              
                                 s
                              
                           
                           )
                         (Eq. (1)).
                           
                              (1)
                              
                                 
                                    
                                       x
                                    
                                    
                                       
                                          
                                             s
                                          
                                          
                                             ^
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   ^
                                                
                                             
                                             1
                                          
                                       
                                       ,
                                       ‥
                                       ,
                                       
                                          
                                             x
                                          
                                          
                                             
                                                
                                                   s
                                                
                                                
                                                   ^
                                                
                                             
                                             M
                                          
                                       
                                       )
                                    
                                    
                                       T
                                    
                                 
                                 =
                                 arg
                                 
                                 
                                    
                                       max
                                    
                                    
                                       M
                                       ,
                                       s
                                    
                                 
                                 (
                                 J
                                 
                                    
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             s
                                             1
                                          
                                       
                                       ,
                                       ‥
                                       ,
                                       
                                          
                                             x
                                          
                                          
                                             sM
                                          
                                       
                                       )
                                    
                                    
                                       T
                                    
                                 
                                 )
                                 ,
                              
                           
                        where J is a measure of the information in a feature vector and 
                           M
                           ⪡
                           N
                        .

For instance, the biggest wavelet coefficients were used in [11] to represent mammograms. A k-NN classifier was used to test different fraction of the biggest coefficients at each level (from 10% to 90%). This method was later adapted to curvelet transform [12]. A k-NN classifier used a percentage of the biggest coefficients from each curvelet level (from 10% to 90%) to classify breast tumors. A comparative study between curvelet and wavelet for breast cancer diagnosis was performed in [13]. For each transform (wavelet and curvelet), the mammogram is decomposed into four different levels and the 100 biggest coefficients from each level were used, resulting in a set of 400 features for each transform. Experimental results using a k-NN classifier have shown that curvelet statistically outperforms wavelet. In the aforementioned methods, feature selection from both wavelet and curvelet transforms relies on the intuitive assumption that the most relevant coefficients for mammogram description are the biggest ones. Even though the image can be approximatively reconstructed from its biggest coefficients without a significant loss, there is no proof that the biggest coefficients are the most discriminative ones [14]. Indeed, more sophisticated feature selection methods can be more appropriate to select the most relevant curvelet coefficients. Accordingly, the statistical t-test was used in [15] to select the most significant curvelet coefficients. The method ranked the features according to their capability to differentiate between two different classes. However, a challenging problem with all the above described methods is the large size of feature sets. Indeed, [13] used 400 coefficients, [11] used 821 and 7391 coefficients corresponding to 10–90% of the biggest wavelet coefficients, whereas [15] used 5663 and 333 coefficients for abnormality and malignancy detection, respectively. Another problem with feature selection methods is the information lost since some of relevant features may be omitted. Furthermore, selecting the most discriminative subset from the original 46,080 coefficients in [15] is computationally very demanding.

Another approach to perform dimensionality reduction for multiscale-based mammogram analysis is the feature extraction. It aims to find a mapping that transforms a feature space x into a lower dimensional feature vector y such that y preserves most of the contained information in x
                        
                           
                              (2)
                              
                                 f
                                 :
                                 x
                                 =
                                 
                                    
                                       (
                                       
                                          
                                             x
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       ‥
                                       ,
                                       
                                          
                                             x
                                          
                                          
                                             N
                                          
                                       
                                       )
                                    
                                    
                                       T
                                    
                                 
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       N
                                    
                                 
                                 ⟶
                                 y
                                 =
                                 
                                    
                                       (
                                       
                                          
                                             y
                                          
                                          
                                             1
                                          
                                       
                                       ,
                                       ‥
                                       ,
                                       
                                          
                                             y
                                          
                                          
                                             M
                                          
                                       
                                       )
                                    
                                    
                                       T
                                    
                                 
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       M
                                    
                                 
                                 /
                                 J
                                 (
                                 x
                                 )
                                 ≅
                                 J
                                 (
                                 y
                                 )
                                 ,
                              
                           
                        where J is a measure of the information in a feature vector and 
                           M
                           ⪡
                           N
                        .

Hence, feature extraction methods are more appropriate because they decrease the size of the feature space while keeping most of the original information. Thus, Gedik and Atasoy 
                        [16] performed a Principal Component Analysis (PCA) and a Linear Discriminant Analysis (LDA) on the curvelet coefficients of the approximation subband. Then, a k-NN and a support vector machine (SVM) classifiers used the feature set composed of the first components of PCA or LDA to classify mammograms. In [17], curvelet coefficients were first divided into 200-coefficient groups and then some statistical features (e.g. energy, entropy, etc.) were computed from each group. More recently, Gardezi et al. [9] used the statistical features (e.g. energy, entropy, etc.) of the co-occurrence matrices of curvelet subbands to differentiate between normal and abnormal tissues.

@&#PROPOSED METHOD@&#

The main motivation behind this work is to extract a reduced set of discriminative features for breast cancer diagnosis based on curvelet transform and moment theory. The proposed method consists of the following steps (Fig. 1
                     ). First, the suspicious region of interest (ROI) is extracted. Second, curvelet transform is performed on this ROI. Then, moment values from each curvelet decomposition level or band are computed. After that, a ranking method is applied to select the most relevant moments. Finally, a k-NN classifier is used to distinguish between normal and abnormal tissues as well as to classify tumors as malignant or benign.

Since an important part of the whole mammographic image comprises the pectoral muscle and the background with a lot of noise, the curvelet decomposition were performed on a limited ROI that contains the prospective abnormality. Therefore, mammograms were first cropped to remove the unwanted parts. Image cropping was performed manually based on the physician annotation provided in the data set. Indeed, the ROI is simply the rectangular area centred on the coordinates of the given lesion. The size of the ROI was experimentally chosen to ensure that the ROI covers the entire lesion without including so much of normal tissue surrounding this lesion.

The curvelet transform represents an image at different scales and angles [18]. It provides an efficient representation of smooth objects with discontinuities along curves. As an extension of the wavelet concept, the curvelet transform was not only successfully applied in several image processing fields but also outperformed wavelet-based methods [19,20]. To perform fast discrete curvelet transform, two implementations were proposed [21]. These implementations differ in the way curvelet at a given scale and angle are translated with respect to each other. The first one is based on unequally spaced fast Fourier transforms (USFFT). The second one is based on the wrapping of specially selected Fourier samples. Since the USFFT implementation was commonly used in similar studies on mammogram analysis using curvelet [12,13,15], it was adopted here for comparison purposes, while decomposing each image into 4 levels. The obtained curvelet coefficients at each level or band are then used to extract a set of moments to represent the input mammogram.

Moment theory had been successfully used in several fields ranging from statistics and mechanics to image analysis and pattern recognition [8,22]. Indeed, a common method to characterize a distribution is computing a reduced set of its moments. Let 
                           X
                           =
                           
                              
                                 (
                                 
                                    
                                       X
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                              
                                 1
                                 ≤
                                 i
                                 ≤
                                 N
                              
                           
                         be a distribution of N curvelet coefficients. The statistical moment 
                           
                              
                                 M
                              
                              
                                 p
                              
                           
                         of order p is defined as follows:
                           
                              (3)
                              
                                 
                                    
                                       M
                                    
                                    
                                       p
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       N
                                    
                                 
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       N
                                    
                                 
                                 
                                    
                                       (
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       p
                                    
                                 
                                 .
                              
                           
                        The mean μ represents an estimate of the location where central clustering occurs. It is equal to the first moment M
                        1. The central moments of order p are defined as
                           
                              (4)
                              
                                 
                                    
                                       μ
                                    
                                    
                                       p
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       N
                                    
                                 
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       N
                                    
                                 
                                 
                                    
                                       (
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                       
                                       −
                                       μ
                                       )
                                    
                                    
                                       p
                                    
                                 
                                 .
                              
                           
                        The variance characterizes the variation around the mean. It is equal to the second central moment μ
                        2. The skewness illustrates the degree of asymmetry of the distribution around its mean. It is computed based on the third central moment,
                           
                              (5)
                              
                                 skewness
                                 =
                                 
                                    
                                       
                                          
                                             μ
                                          
                                          
                                             3
                                          
                                       
                                    
                                    
                                       
                                          
                                             (
                                             
                                                
                                                   μ
                                                
                                                
                                                   2
                                                
                                             
                                             )
                                          
                                          
                                             3
                                             /
                                             2
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        The kurtosis measures the relative peakedness or flatness of the distribution to a normal one. It is computed based on the fourth moment,
                           
                              (6)
                              
                                 kurtosis
                                 =
                                 
                                    
                                       
                                          
                                             μ
                                          
                                          
                                             4
                                          
                                       
                                    
                                    
                                       
                                          
                                             (
                                             
                                                
                                                   μ
                                                
                                                
                                                   2
                                                
                                             
                                             )
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 −
                                 3
                                 .
                              
                           
                        
                     

To justify the use of higher order curvelet moments such as skewness and kurtosis, we first tested whether the distribution of curvelet coefficients in each level or band is Gaussian or not. The test was conducted on 252 mammograms from the mini-MIAS database. For this, the normal probability plot and the Jarque–Bera test, which are respectively intuitive and objective way to assess normality, were used. In the normal probability plot, data are plotted against a theoretical normal distribution. If the data is approximately normally distributed, the plot should appear linear. By visually checking the normal probability plots of all the given curvelet coefficient distributions, it was concluded that the distributions of curvelet coefficients seem to be non-Gaussian. Fig. 2
                         illustrates an example of such plots corresponding to randomly chosen normal and abnormal mammograms. It is clear that the normal probability plots (the blue curve) are faraway to form a straight line (the red line) and that the distributions are thus not normal. For the Jarque–Bera test, the null hypothesis states that the samples come from a normal distribution with unknown mean and standard deviation, against the alternative hypothesis that it does not come from a normal distribution [23]. At the 5% significance level, the test rejects the null hypothesis in 99.4% of cases (1002 among the 1008 available distributions), which means that the distribution of curvelet coefficients is not Gaussian. Thus, for the case of curvelet level moments (CLM), four moments descriptors (mean, variance, skewness and kurtosis) of the distribution of all the coefficients at each level were computed. For the case of curvelet band moments (CBM), the same four moments of the distribution of the coefficients of each band were used. Note that moments of order higher than 4 were not considered because of their computation instabilities [24].

To improve the learning performance and to reduce the computation cost, a feature selection step was performed to select a smaller subset of highly discriminative curvelet moments. Ranking methods are fast feature selectors which had proven their efficiency in similar works [15,25]. Hence, a selection method that ranks the features in descending order according to their capability to separate different classes is used. The capability C of a feature to separate two classes C
                        0 and C
                        1 is calculated as the absolute value of two sample t-test with pooled variance estimate,
                           
                              (7)
                              
                                 C
                                 =
                                 
                                    
                                       
                                          
                                             μ
                                          
                                          
                                             0
                                          
                                       
                                       −
                                       
                                          
                                             μ
                                          
                                          
                                             1
                                          
                                       
                                    
                                    
                                       
                                          
                                             (
                                             
                                                
                                                   σ
                                                
                                                
                                                   0
                                                
                                                
                                                   2
                                                
                                             
                                             /
                                             
                                                
                                                   n
                                                
                                                
                                                   0
                                                
                                             
                                             )
                                             +
                                             (
                                             
                                                
                                                   σ
                                                
                                                
                                                   1
                                                
                                                
                                                   2
                                                
                                             
                                             /
                                             
                                                
                                                   n
                                                
                                                
                                                   1
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where, μ
                        0 (resp. μ
                        1) is the mean, σ
                        0 (resp. σ
                        1) is the standard deviation, and n
                        0 (resp. n
                        1) is the number of images, in the class C
                        0 (resp. C
                        1).

In this paper, a k-NN classifier was used to evaluate the discriminative ability of the different features sets to distinguish between normal tissues and tumors, and to classify mammograms as benign or malignant. Although it is possible to use other types of classifiers to ascertain the optimal features, the k-NN classifier was chosen for its stability which distinguishes it from competitors such as trees and neural networks [26]. Another advantage of this classifier is its flexibility, which makes it a classical choice in similar research studies focusing on breast cancer diagnosis [11–13] and on comparison of feature extraction methods [27]. Additionally, in spite of its simplicity, the k-NN classifier has been shown to be among the best performers in a large number of classification problems [28].

The main idea of the k-NN classifier is to predict the class of a test case based on its k nearest neighbors. It consists of two steps: the first step aims to find the k nearest neighbors, whereas the second one predicts the class of the test sample based on these neighbors. Several distances could be used to estimate the k nearest neighbors and several strategies could be taken to predict the class of a test image. In this paper, the Euclidean distance and a weighted voting strategy were used. Indeed, let 
                           {
                           
                              
                                 T
                              
                              
                                 ij
                              
                           
                           ,
                           i
                           =
                           1
                           …
                           A
                           ,
                           j
                           =
                           1
                           …
                           B
                           }
                         be a training dataset, where A is the number of cases and B is the size of the feature vector. Let 
                           {
                           
                              
                                 L
                              
                              
                                 i
                              
                           
                           ,
                           i
                           =
                           1
                           …
                           A
                           }
                         be its known corresponding labels, where 
                           
                              
                                 L
                              
                              
                                 i
                              
                           
                           ∈
                           {
                           
                              
                                 C
                              
                              
                                 0
                              
                           
                           ,
                           
                              
                                 C
                              
                              
                                 1
                              
                           
                           }
                        . Given a feature vector 
                           {
                           
                              
                                 S
                              
                              
                                 j
                              
                           
                           ,
                           j
                           =
                           1
                           …
                           B
                           }
                         of a sample image S to be classified, the classification algorithm can be summarized as follows: Step 1: Finding the k nearest neighbors
                        
                           
                              1.
                              Compute the Euclidean distance 
                                    {
                                    
                                       
                                          d
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    i
                                    =
                                    1
                                    …
                                    A
                                    }
                                  between S and all the available training cases 
                                    {
                                    
                                       
                                          T
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    i
                                    =
                                    1
                                    …
                                    A
                                    }
                                 :
                                    
                                       (8)
                                       
                                          ∀
                                          i
                                          ∈
                                          {
                                          1
                                          …
                                          A
                                          }
                                          ,
                                          
                                          
                                             
                                                d
                                             
                                             
                                                i
                                             
                                          
                                          =
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   
                                                      B
                                                   
                                                
                                                
                                                   
                                                      (
                                                      
                                                         
                                                            S
                                                         
                                                         
                                                            j
                                                         
                                                      
                                                      −
                                                      
                                                         
                                                            T
                                                         
                                                         
                                                            ij
                                                         
                                                      
                                                      )
                                                   
                                                   
                                                      2
                                                   
                                                
                                             
                                          
                                          .
                                       
                                    
                                 
                              

Keep only the k nearest cases 
                                    {
                                    
                                       
                                          T
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    i
                                    =
                                    1
                                    …
                                    k
                                    }
                                  corresponding to the lowest 
                                    {
                                    
                                       
                                          d
                                       
                                       
                                          i
                                       
                                    
                                    }
                                  values.

Compute the weight w
                                 
                                    i
                                  for the i-th nearest neighbor as a multiplicative-inverse of its distance to the sample S 
                                 [29]:
                                    
                                       (9)
                                       
                                          
                                             
                                                w
                                             
                                             
                                                i
                                             
                                          
                                          =
                                          
                                             
                                                1
                                             
                                             
                                                
                                                   
                                                      d
                                                   
                                                   
                                                      i
                                                   
                                                
                                             
                                          
                                          .
                                       
                                    
                                 This weighting function ensures that closer neighbors (smaller distances) are weighted more heavily than the farther ones (greater distances).

Assign the class C
                                 
                                    S
                                  for the sample S using the majority weighted voting:
                                    
                                       (10)
                                       
                                          
                                             
                                                C
                                             
                                             
                                                S
                                             
                                          
                                          =
                                          arg
                                          
                                          
                                             
                                                max
                                             
                                             
                                                C
                                             
                                          
                                          
                                             
                                                ∑
                                             
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                k
                                             
                                          
                                          
                                             
                                                w
                                             
                                             
                                                i
                                             
                                          
                                          ·
                                          δ
                                          (
                                          
                                             
                                                L
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          C
                                          )
                                          ,
                                       
                                    
                                 where
                                    
                                       (11)
                                       
                                          δ
                                          (
                                          
                                             
                                                L
                                             
                                             
                                                i
                                             
                                          
                                          ,
                                          C
                                          )
                                          =
                                          {
                                          
                                             
                                                
                                                   
                                                      1
                                                   
                                                   
                                                      if
                                                      
                                                      
                                                         
                                                            L
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      =
                                                      C
                                                   
                                                
                                                
                                                   
                                                      0
                                                   
                                                   
                                                      otherwise
                                                      .
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

To validate the discrimination effectiveness of the proposed curvelet sets, a k-NN classifier was used to perform two kinds of experiments. The first experiment aims to distinguish between normal tissues and tumors, whereas the goal of the second one is to classify each tumor as benign or malignant. Comparison was also made with state-of-the-art methods dealing with curvelet-based feature set extraction for mammogram analysis. Experiments were conducted on two mammogram datasets. Indeed, since all the compared works performed their experiments on the mini-MIAS database, we begin our investigation on this database. Seen the limited size of this dataset, we conducted a second series of experiments on the DDSM database, which is a very large mammographic dataset.

For comparison purposes, six methods that use Curvelet Coefficients (CC) to describe mammograms were implemented and compared to our proposal. In what follows, we briefly recall the principle of these methods. 
                           
                              Co-Occurrence matrix of Curvelet Coefficients (COCC)
                           
                              : The co-occurrence matrices of each curvelet subband were generated. The feature set consists of the statistical features (e.g. energy, entropy, etc.) computed from the co-occurrence matrices of curvelet subbands [9].

: A statistical t-test ranking technique was performed directly on curvelet coefficients [15]. The feature set is composed of the top ranked coefficients.

: The feature set is composed of 400 features corresponding to the 100 biggest coefficients from each curvelet level [13].

: Curvelet coefficients were first divided into 200-coefficient groups and then some statistical features (e.g. energy, entropy, max value, etc.) were computed from each group [17].

: A principal component analysis was performed on the curvelet coefficients of the approximation subband [16]. The feature set is composed of the first components.

: A linear discriminant analysis was applied on the curvelet coefficients of the approximation subband [16]. The feature set consists of the first component.

In this section, we evaluate our method on mammograms from the mini-MIAS database. Thus, after describing this dataset and the experimental setup, results achieved by the suggested method as well as comparisons with state-of-the art methods are presented.

In this study, mammograms from the mini-MIAS (Mammographic Image Analysis Society) database were used [30]. This database is publicly available and widely used in similar works [11–13,15]. Indeed, the original MIAS mammograms, which are digitized at 50 micron pixel edge, has been reduced to 200 micron pixel edge and each image has been clipped or padded so that it is of size 1024×1024 pixels [30]. Mini-MIAS is composed of 322 mammograms of right and left breasts from 161 women. An expert radiologist diagnosed all the available mammograms based on his experience and biopsy, and labeled 51 mammograms as malignant, 64 as benign, and 207 as normal. The coordinates of the center of each diagnosed abnormality are provided along with the approximate radius of a circle enclosing the abnormality. We used these information to manually extract the ROI. Indeed, the suspicious ROI in each treated mammogram is simply the rectangular area of size 128× 128 around the seed pixel. This size was experimentally chosen to ensure that the ROI covers the entire lesion. In this way, 252 suspicious ROIs were selected from the mini-MIAS database, consisting of 116 abnormal ROIs (50 malignant and 66 benign) and 136 normal ROIs. Normal ROIs include fatty, fatty-glandular, and dense-glandular tissues. Fig. 3
                            illustrates some examples of the ROIs from the mini-MIAS database used in the experiments. These ROIs belong to different types and severities of pathologies.

The estimation of the accuracy of the classifier is very challenging for small datasets, as it is the case for the mini-MIAS database. In this study, the Leave-One-Out Cross-Validation (LOOCV) technique was used. This technique was chosen because it yields an almost unbiased estimation of accuracy with good generalization performance [31]. In the LOOCV strategy, an image is left out and classified based on all the other images. This process is repeated so that each image in the database is left out once. In addition, since there is no straightforward way to select the optimal number of neighbors k 
                           [32], all values of k ranging from 1 to 35 were tested. Values of k higher than 35 were not considered since the k value must be less than the cardinal of the smaller class [33], which is in our case the malignant class with 50 images.

@&#RESULTS@&#


                           Table 1
                            presents the ranking results of the curvelet level moments using the statistical t-test for abnormality and malignancy detection. In this table, m
                           
                              i
                           , v
                           
                              i
                           , k
                           
                              i
                            and s
                           
                              i
                            denote the mean, the variance, the skewness and the kurtosis of the curvelet level L
                           
                              i
                           , respectively. It is clear that the mean values of the fourth and the third level are the top ranked ones. Besides, features computed from the first level are ranked lower than those computed from higher levels. Indeed, except v
                           1, all the features computed from the first level are lower ranked. In addition, except the mean of the first level, lower order moments (mean and variance) are more highly ranked than higher order moments.


                           Fig. 4
                            summarizes the classification results achieved by different subsets of the highest ranked features of curvelet level moments. In this figure, the obtained AUC (Area Under Curve) values are plotted against the number of ranked features. For malignancy detection, the optimal AUC value (=0.841) is obtained with 5 features. For abnormality detection, an optimal AUC of 0.989 is achieved with 10 features. We can remark that increasing the number of features upon the optimal values decreased the classification results, mainly in the case of malignancy detection. In addition, the use of higher order moments improves the classification results for abnormality detection but not for malignancy detection. Besides, the most pronounced increase in AUC values for malignancy and abnormality detection is obtained when adding the fifth and the fourth features respectively, which correspond in both cases to v
                           1. This showed the relevance of this feature even if it is not highly ranked. To confirm the relevance of the feature v
                           1, a principal component analysis projection was performed on curvelet coefficients. The percents of the total variability explained by the first and the second principal components are 75.69% and 23.20%, respectively. This shows that the majority of variance (
                              =
                              98.89
                              %
                           ) is explained by the two principal components. Fig. 5
                           , which visualizes the principal components for curvelet coefficients, proves that higher levels (2, 3 and 4) together contribute to the first principal component, whereas the first level explains alone 23.20% of total variability. That is why the feature v
                           1, which corresponds to the second principal component, is so relevant even if it is not highly ranked. In addition, since the feature selector does not take into account feature redundancy, the features corresponding to the first principal component are ranked higher than v
                           1 even if they are redundant.

Moreover, the performance of curvelet band moments was analyzed by recording classification results achieved by different subsets of the top-ranked features of curvelet band moments (in Fig. 6
                           ). For malignancy detection, the optimal AUC (
                              =
                              0.842
                           ) is obtained with 68 features. For abnormality detection, an optimal AUC of 0.973 was achieved with 3 features. This proves that the increase of the number of features upon the optimal values decreases considerably the classification results in both cases, what confirms the importance of the feature selection step.

This section is devoted to compare the performance of the suggested method with the aforementioned state-of-the-art methods on the mini-MIAS database. Nevertheless, even though the experiments in [34,15,13,17,16] were performed on the mini-MIAS dataset, different validation strategies (50% training–50% test, 10-fold and 2× 5-fold cross validation) and different samples of mammograms have been used. Tables 2 and 3
                           
                            provide an overview (e.g. samples, validation strategy, number of features, accuracy, etc) of the results reported in the compared works on the mini-MIAS database for abnormality and malignancy detection, respectively.

However, it is important to bear in mind that, for a fair comparison of the cited methods, the same samples and the same validation strategy should be used in all the works. As described in Section 4.2.2, the performance of the proposed method was evaluated based on a series of experiments performed on samples from the mini-MIAS database using the LOOCV technique. The same series of experiments were also conducted to evaluate the performance of the state-of-the-art curvelet-based feature extraction methods. In addition, since all the related works used accuracy as classification performance measure, this criteria was used in this paper for comparison purposes. For each set, we computed the accuracy values through the optimal configuration among all the realized experiments described above. Tables 4 and 5
                           
                            summarized the accuracy values for the compared sets for abnormality and malignancy detection, respectively.

In these tables, the optimal feature set size, which corresponds to the size of the subset that yields the highest accuracy, is obtained through the evaluation of several subsets from the original set in the training step. Thus, having smaller sizes of the original set (resp. the optimal subset) may reduce the computation cost in the training (resp. the test) step. We can see that, except for LDA-CC which used one feature, the original CLM set (16 features) and the size of the optimal subsets from CLM (10 features for abnormality and 5 features for malignancy) are the smallest sets.

For the accuracy, when compared on the same samples with the same validation strategy, CLM outperforms all the compared methods for both abnormality and malignancy detection. Besides, the very high values reported by the compared works for both abnormality (e.g. 100% for PCA-CC) and malignancy detection (e.g. 98% for PCA-CC) are optimistically biased because of the used validation strategy (50% training–50% test and k-fold cross validation). However, validation based on the LOOCV strategy yields lower but unbiased accuracy values.

In this section, we evaluate the performance of our method against all the compared methods on the DDSM database. After describing this dataset as well as the experimental setup, the classification results achieved by the evaluated methods for abnormality and malignancy detection are presented.

The second experimental dataset is constructed from the Digital Database for Screening Mammography (DDSM), which is currently the largest public mammogram database [35]. DDSM contains 2604 cases with associated ground truth and other information. Each case consists of four views corresponding to Cranial-Caudal (CC) and Medio-Lateral-Oblique (MLO) views for right and left breasts. Recently, 2340 ROIs depicting masses (1127 benign and 1273 malignant) and 9213 suspicious normal tissues ROIs were extracted in [36]. Each ROI depicting mass is a rectangular area centred on the coordinates given in the ground truth. The normal regions are false positives asserted by a CAD system from health cases. This experiment setting is more consistent with practice and more challenging than experiments based on randomly selected normal regions [36]. In our study, all the used ROIs were resized to 256× 256 pixels. Thus, the total numbers of images used in abnormality and malignancy detection are 11553 and 2340, respectively. Fig. 7
                            illustrates examples of ROIs extracted from the DDSM database and used in the experiments.

The classification results of the tested sets depend on the feature selection step and on the number k of neighbors within the k-NN classifier. However, to estimate correctly the generalization error for the classifier, the data used for selecting tuning parameters must be independent of data used for assessment [37]. Therefore, the available ROI dataset was split into two disjoint sets: the training set and the test one. The training set, used to select the tuning parameters of the classifier, consists of 200 ROIs (100 normal, 50 malignant and 50 benign) for abnormality detection and 100 ROIs (50 malignant and 50 benign) for malignancy detection. The test set consists of all the remaining ROIs. The optimal configuration for each feature set was firstly estimated based on 10-fold cross validation performed on the training set. For CLM, CBM, COCC, and SCC, the features were first ranked and all possible subsets of top-ranked features were tested. For TRCC, subsets of the top ranked feature of sizes up to 6000 were examined. For PCA-CC, subsets of sizes up to 100 of the first components were tested. For each subset, all values of k ranging from 1 to 35 were investigated. Once the optimal configuration (k and the size of the subset) was estimated based on the training set, this configuration was used to assess the performance of each feature set on the test dataset based on 2× 5-fold cross validation. For this, the test dataset is randomly partitioned into 5 disjoint folds of equal size. For each fold, the accuracy is estimated through the classification of the samples of this fold based on the other four folds. This process is repeated twice. Besides, to evaluate the significance of difference between the performances of the suggested method and the compared methods, for comparison purposes, a paired t-test was performed on the results of the 2× 5-fold cross validation. Let μ
                           
                              CLM
                            and μ
                           
                              Comp
                            be respectively the mean accuracies of CLM and the compared set. The null hypothesis is that the means of the accuracy values of the compared two methods are equal (
                              H
                              0
                              :
                              
                                 
                                    μ
                                 
                                 
                                    CLM
                                 
                              
                              =
                              
                                 
                                    μ
                                 
                                 
                                    Comp
                                 
                              
                           ). The alternative hypothesis is that the means are not equal (
                              Ha
                              :
                              
                                 
                                    μ
                                 
                                 
                                    CLM
                                 
                              
                              ≠
                              
                                 
                                    μ
                                 
                                 
                                    Comp
                                 
                              
                           ). At the significance level of 5%, the null hypothesis is rejected if the p-value is lower that 0.05, otherwise it is accepted.

@&#RESULTS@&#


                           Table 6
                            compared the performance of the different feature sets on the DDSM dataset for abnormality detection. The optimal configurations of the feature size as well as the number of neighbors obtained in the training step and the accuracy values of each test fold were given. This shows that CLM yields the highest accuracy (=86.46%) with only 12 features. The PCA-CC achieves the second highest accuracy (=83.8%) but with 41 features. The LDA-CC, which uses one feature, gives an accuracy of only 81.26%.


                           Table 7
                            compared the performance of the different feature sets on DDSM dataset for malignancy detection. Again, the CLM yields the highest accuracy (=60.43%) with only 7 features. The CBM achieves the second highest accuracy (=58.36%) with 20 features, and the LDA-CC records the smallest accuracy (=50.67%).

Besides, a paired t-test was performed to prove the significance of the improvement made by CLM. Indeed, when comparing CLM with all the tested feature sets, the null hypothesis is rejected at 
                              5
                              %
                            significance level for the two classification problems (
                              p
                              <
                              0.001
                           ). This means that CLM statistically outperformed all these methods. Moreover, all the tested feature sets perform better for abnormality detection than for malignancy detection.

@&#DISCUSSION@&#

This paper deals with the problem of high dimensionality of curvelet coefficients space. It proposes an efficient and effective method to extract a compact set of discriminative features from curvelet transform for breast cancer diagnosis. In previous works, feature selection and reduction methods were performed to cope with the problem of curse of dimensionality of curvelet transform. However, this study proposes curvelet moments as a straightforward way to obtain a compact set of discriminative descriptors for breast cancer diagnosis. For both malignancy and abnormality detection, experimental results on 11553 ROIs from the DDSM database showed that curvelet moments method yields more reduced features set and it statistically outperforms state-of-the-art curvelet-based feature extraction methods. This is due to the fact that selection methods cause information loss since some of the coefficients are omitted, whereas moment values are computed from all the coefficients. Furthermore, since the morphology of breast lesions is highly variable, individual curvelet coefficients are unlikely to perform well. In addition, the poor results obtained by LDA and PCA may be due to two reasons. The first reason is the fact that LDA and PCA are linear feature reduction techniques whereas the problem in hand is non-linear. The second reason is related to the fact that LDA and PCA were computed from the approximation subband only and the other subbands were omitted even though they contain relevant information. This result is consistent with our finding that levels 2, 3 and 4 correspond to the first PCA component and thus contain most of curvelet coefficients informations. For the mini-MIAS database, our experimental results showed that curvelet moments outperform state-of-the-art curvelet-based feature extraction methods. For this database, the compared works reported higher accuracies than our results because the validation method optimistically biased the results. Indeed, the 50% train–50% test and k-fold cross validation are not suited for small databases such as mini-MIAS, because they give a biased estimation of performance with poor generalization [31]. In addition, all the feature sets perform better in mini-MIAS database than DDSM database. This is due to the fact that DDSM database is more challenging [36]. For the feature size, except the LDA-CC which gives poor classification results, the suggested method leads to a smaller set than all the compared ones, which may not only reduce the storage cost, but also decrease the computation expense of the classification step.

Moreover, our findings confirm the utility of feature selection step since in all tested feature sets, the selection of a reduced feature subset increases the accuracy values.

Finally, the encouraging results obtained without region segmentation of the suspicious lesions are consistent with the work of [38], who stated that the texture around the lesion is useful for breast cancer diagnosis.

@&#CONCLUSION@&#

This study proposes the use of curvelet moments as an efficient way to extract a reduced set of discriminative features for breast cancer diagnosis. After decomposing the image with curvelet transform, moment values from the distribution of curvelet coefficients at each subband are computed. The distributions of curvelet coefficients at each level or at each band were investigated. Furthermore, a statistical t-test ranking technique selected the most discriminative moments. A k-NN classifier is then used to distinguish between normal and abnormal mammograms, and to classify lesions as malignant and benign. The performance is evaluated using the LOOCV on 252 mammograms from the mini-MIAS database. Experimental results showed the efficiency and the effectiveness of curvelet level moments for breast cancer diagnosis. Empirical comparison on 11553 ROIs from the DDSM database using 2× 5-fold cross validation showed the statistically significant superiority of the proposed method over the state-of-the-art curvelet-based feature extraction methods. In future work, we will incorporate the proposed method into a computer aided breast cancer diagnosis system. Another direction is the investigation of non-linear feature reduction methods.

@&#SUMMARY@&#

Feature extraction is a key issue in designing a computer aided diagnosis system. Recent researches on breast cancer diagnosis have reported the effectiveness of multiscale transforms (wavelets and curvelets) for mammogram analysis and have shown the superiority of curvelet transform. However, the curse of dimensionality problem arises when using the curvelet coefficients and therefore a reduction method is required to extract a reduced set of discriminative features. State-of-the-art methods suggested the use of feature selection methods such as ranking (e.g. statistical t-test, biggest coefficients.) as well as feature extraction methods (e.g. PCA, LDA). This paper deals with this problem and proposes a new feature extraction method for mammogram description based on curvelet transform and moment theory. First, we performed discrete curvelet transform and we computed the four first order moments from curvelet coefficients distribution. Hence, two feature sets can be obtained: moments from each band and moments from each level. In this paper, both sets are tested. Then, we used the t-test ranking technique to select the best features from each set. Finally, the obtained feature sets are input to k-nearest neighbor classifier to distinguish between normal and abnormal mammograms, and to classify lesions as malignant and benign. Experiments performed on 252 ROIs from the mini-MIAS database using the leave-one-out cross validation showed the efficiency and the effectiveness of curvelet level moments for breast cancer diagnosis. Indeed, on the mini-MIAS database, curvelet moments yields an accuracy of 91.27% with 10 features for abnormality detection, and an accuracy of 81.35 % with 8 features for malignancy detection. Besides, we performed an empirical comparison of the proposed method with the most relevant state-of-the-art curvelet-based feature extraction methods on 11553 ROIs from the DDSM database using 2× 5-fold cross validation. Experimental results show the statistically significant superiority of the proposed method for mammogram analysis .

None declared.

@&#REFERENCES@&#

