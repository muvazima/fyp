@&#MAIN-TITLE@&#Efficient modeling of visual saliency based on local sparse representation and the use of hamming distance

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A method for the construction of saliency maps is proposed.


                        
                        
                           
                           Features are extracted via local sparse coding on image patches.


                        
                        
                           
                           The overcomplete dictionary is trained using natural images.


                        
                        
                           
                           A bio-plausible scheme based on the Hamming distance is used to compare patch representations.


                        
                        
                           
                           The algorithm is efficient both in terms of computational cost and of detection performance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Visual saliency

Attention modeling

Sparse representation

Hamming distance

@&#ABSTRACT@&#


               
               
                  Modeling of visual saliency is an important domain of research in computer vision, given the significant role of attention mechanisms during neural processing of visual information. This work presents a new approach for the construction of image representations of salient locations, generally known as saliency maps. The developed method is based on an efficient comparison scheme for the local sparse representations deriving from non-overlapping image patches. The sparse coding stage is implemented via an overcomplete dictionary trained with a soft-competitive bio-inspired algorithm and the use of natural images. The resulting local sparse codes are pairwise compared using the Hamming distance as a gauge of their co-activation. The calculated distances are used to quantify the saliency strength for each individual patch, and then, the saliency values are non-linearly filtered to form the final map. The evaluation results obtained on four image databases, demonstrate the competitive performance of the proposed approach compared to several state-of-the-art saliency modeling algorithms. More importantly, the proposed scheme is simple, efficient, and robust under a variety of visual conditions. Thus, it appears as an ideal solution for a hardware implementation of a frontend saliency modeling module in a computer vision system.
               
            

@&#INTRODUCTION@&#

Visual environment inundates humans with a complex ensemble of stimuli that should be detected, processed, and further analyzed on a daily basis. Evolution equipped the human brain with various mechanisms for coping efficiently with these demands, minimizing in parallel the waste of valuable processing resources. In this context, whenever a visual environment is encountered, the processing procedure is facilitated by the selection of a small number of salient regions allowing for an acceleration in the neural calculations, and advancing image content understanding. This procedure can be computationally modeled via the construction of a probabilistic representation in the form of an activation map of prominent locations, known as saliency map. The assumption that a saliency map is in reality constructed in specific regions of the brain is supported by studies in neuroscience, arguing for the existence of neural mechanisms in early vision that process visual information through the formation of ‘feature maps’. In the past, different regions of the brain were proposed as suitable candidates for the formation of a saliency map, e.g. the lateral geniculate nucleus of the thalamus [1], the pulvinar thalamic nucleus [2], and the posterior parietal cortex [3]. Given the existence of top-down influences during attention control, the exact relation of visual saliency with the attentive procedures is still an open field of research. There are studies even questioning the necessity for the existence of an explicit saliency map during attention control [4]. However, recent scientific data strongly support the formation of saliency maps via the neural activity [5], and specifically during the representation of low-level features.

A saliency modeling procedure begins with the extraction of a set of attention-attracting features from the visual scene under consideration. The features of most concern are usually contrast characteristics, color information cues, and irregular texture patterns. Motion is also a very important attention-attracting characteristic when the visual input changes dynamically (e.g. video sequence). A number of different techniques can be used for the combination and normalization of the extracted features, leading on the formation of the final probabilistic representation of a saliency map. The constructed saliency map can then be employed in a wide gamut of applications. For example, it can be incorporated as a frontend in a computer vision system for lowering the image processing computational burden and improve scene interpretation [6]. Furthermore, a saliency map can be employed to facilitate image retrieval tasks in large databases. The large volume of images in such databases often involves contents of meaningful elements intermingled with useless information, e.g. objects of interest in complex backgrounds. A preprocessing stage via a saliency modeling algorithm could improve the extraction speed for the retrieval results and allow for semantic queries [7]. Finally, it should be noticed that some modern robotic applications can also benefit from the adoption of saliency modeling techniques [8], due to the connection of attention strategies in tasks such as navigation and obstacle avoidance.

@&#MOTIVATION@&#

The proposed scheme for modeling visual saliency is based on the following bio-plausible rationale: assuming that the different parts (patches) of an image are represented sparsely in the visual neural circuitry, then the visual significance of each image part should be analogous to the usage of neural cells that are commonly activated from other image parts. Thus, a simple and intuitive way to quantify the significance of an image part is to measure the degree of co-activation between the elements used for its representation with respect to the other parts of the image. Based on this idea, we construct an algorithm that initially represents the image patches of an image using a sparse coding procedure, and then, we employ the Hamming distance to quantify the degree of co-activation of the patch coding elements.

The suggested algorithm performs the patch representation stage inspired from the theory of efficient coding, previously employed for saliency modeling in several algorithms (e.g. [9,10]). In our approach, however, we follow a saliency-modeling procedure based on the binarization of patch codes and their comparison with the use of the Hamming distance, a fast measure capable to be implemented efficiently in hardware with simple exclusive-OR (XOR) operations. The computational efficacy of the algorithm is further enforced by the adoption of a batch processing algorithm for the implementation of the sparse coding stage with patches of fixed size. The developed scheme results in the construction of maps that merge local and global saliency information, and its general performance matches these of more complicated approaches, as demonstrated in the experimental evaluation part.

Furthermore, our research involves an investigation regarding the effectiveness of different training approaches that can be used for the construction of the overcomplete dictionary needed during the sparse coding stage. We perform experiments on several databases, quantifying the performance of our algorithm with the use of a variety of dictionary construction algorithms. The results demonstrate the optimum performance achieved by the adopted dictionary training algorithm, the Generalized Sparse Coding Neural Gas (GSCNG) [11], and additionally reveal the general superiority of sophisticated dictionary training approaches against more traditional schemes.

@&#RELATED WORK@&#

The research concerning the computational models for visual saliency draws its origins back to the late ‘90s. In the early work of Itti et al. [12], cues from the Feature Integration Theory (FIT) [13] were combined with center-surround calculations applied on intensity, color, and orientation characteristics of images, leading on the construction of saliency maps that were used for the task of scene analysis. Another research effort, presented in [14], modeled saliency via a more advanced computation scheme based on graph operations. This technique focused in the concentration of activation at certain image points for the construction of informative saliency maps. A spectral approach for modeling low-level saliency was adopted in [15], highlighting the importance of the phase of Fourier transform in saliency modeling. Working in the same spirit, a compressed representation was proposed in [16] for capturing information about distinctiveness of image regions. In specific, it was suggested that the phase information encapsulated in the sign of an image’s Discrete Cosine Transform (DCT) can be exploited for the formation of saliency maps. More recently, a method for modeling saliency through scale-space analysis in the frequency domain was presented in [17].

Among the different approaches for modeling visual saliency there is an outstanding category of methods inspired by sparse representation principles. Investigation of sparse representation during visual processing emerged due to its strong correlation with the coding procedures in visual cortex [18], and more general with the neural processes of mammals [19]. Sparse representation has been employed to solve various image processing and computer vision problems, like image compression and denoising, face identification, and object recognition. In the field of saliency modeling, the first attempt to represent image features with the use of sparse coding basis functions was presented in [9]. In that case, bottom-up saliency was regarded as an attempt of the neural circuits to maximize information gathering while visual sampling of a scene. Priors from Shannon’s information theory were combined with a visual dictionary for the construction of a saliency model which was evaluated with real eye movement data. The dataset used in the specific work has been adopted since then as a common benchmark from various scientific papers (e.g. [16,17,10,21,22]). In [20], the usage of Incremental Coding Length (ICL) was proposed for the extraction of saliency-modeling features. The method was tested both for static images, and for more dynamic visual input consisted of video sequences. The method presented in [10], suggested a Bayesian framework as the basis of saliency modeling. Feature representation was performed both with the use of Difference of Gaussian (DoG) filters and with basis functions extracted from natural images. More lately, saliency via image sparsity was confronted as a low-rank and sparsity matrix decomposition problem [21], whereas in [22] a joint sparsity framework was proposed for the combination of different features competing for saliency.

The algorithm that is proposed in this work is based on the efficient comparison of local sparse codes extracted from image patches. The first step involves the preparation of an overcomplete dictionary of basis functions—also known as visual codewords or atoms—that will be used during the sparse coding procedure. The most naïve approach is to employ a pre-specified set of functions, as for example the overcomplete DCT filters or the overcomplete wavelets. However, there are more sophisticated approaches (e.g. [11,24,25]), that propose the construction of a codebook via training with samples drawn from the signal set related to the problem under consideration, in our case from natural images. Thus, the extracted basis functions are expected to be more effective given the utilization of the special characteristics of the given representation problem. Furthermore, studies of the visual system [26] show that the codewords of an overcomplete dictionary trained with natural image patches resemble at a great extend the cortical cell basis functions in the early visual cortex. Motivated by these findings, we opted to construct a visual codebook using patches from natural images with the adoption of a soft-competitive bio-inspired algorithm, the Generalized Sparse Coding Neural Gas (GSCNG) [11]. In our method, the dictionary is created via an offline process and is afterwards used routinely, allowing thus for the adoption of this more sophisticated training technique with no additional cost on the computational speed during map construction.

The procedure for training an overcomplete dictionary via the GSCNG algorithm initiates with the decomposition of a set of natural images into a large number of patches. The goal of the algorithm is to use these patches to create a limited set of visual words representing the characteristics of input images in an optimal way. The GSCNG algorithm implements this procedure under strict sparsity constraints. The mathematical formulation of the problem under consideration can be expressed by Eq. (1):
                           
                              (1)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          
                                             
                                                d
                                             
                                             
                                                1
                                             
                                          
                                          ,
                                          …
                                          ,
                                          
                                             
                                                d
                                             
                                             
                                                M
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          L
                                       
                                    
                                 
                                 ‖
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 -
                                 D
                                 ·
                                 
                                    
                                       a
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                    
                                       2
                                    
                                 
                                 
                                 s
                                 .
                                 t
                                 .
                                 
                                 ‖
                                 
                                    
                                       a
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       0
                                    
                                 
                                 ⩽
                                 k
                                 
                                 and
                                 
                                 ‖
                                 
                                    
                                       d
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                    
                                       2
                                    
                                 
                                 =
                                 1
                              
                           
                        where dj
                         are the M atoms forming a dictionary D that minimizes the representation error for the input visual words xi
                        , using the coding coefficients ai
                        . The GSCNG algorithm solves this optimization problem working alternatively between two steps, the Sparse Coding Step and the Dictionary Update Step, until the desired codebook is obtained. During the first step (Sparse Coding Step) the algorithm pursuits to recover the best representation vector for each sample 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    n
                                 
                              
                           
                        , by solving the optimization problem:
                           
                              (2)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          
                                             
                                                a
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 {
                                 ‖
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 -
                                 
                                    
                                       D
                                    
                                    
                                       t
                                    
                                 
                                 ·
                                 
                                    
                                       a
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                    
                                       2
                                    
                                 
                                 }
                                 ,
                                 
                                 i
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 L
                                 
                                 s
                                 .
                                 t
                                 .
                                 
                                 ‖
                                 
                                    
                                       a
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       0
                                    
                                 
                                 ⩽
                                 k
                              
                           
                        with 
                           
                              
                                 
                                    D
                                 
                                 
                                    t
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    n
                                    ×
                                    M
                                 
                              
                           
                         denoting the temporary dictionary used to encode the current input patch. This is a NP-hard problem in general, but over the past few years various pursuit methods were proposed to reach an approximate solution to the problem, e.g. the Matching Pursuit (MP) [27], the Orthogonal Matching Pursuit (OMP) [28], and the Optimized Orthogonal Matching Pursuit (OOMP) [29]. During the second step (Dictionary Update Step), the algorithm follows some rules in order to update the formed atoms. More specific, the error residuals generated from the current (temporary) coding coefficients are calculated and subsequently sorted with the use of the bio-inspired Oja’s rule [30]. Then, the overcomplete dictionary is dynamically updated based on the sorted residual values, by modifying each time both the ‘winning’ codeword—the atom with the lower residual error—and all the other codewords that can be selected, taking into consideration their residual values. This soft-competitive approach remedies several sub-optimal features of the hard-competitive methods, such as bad quantization, sensitivity to initialization, and slow convergence. The two steps of the algorithm (Sparse Coding/Dictionary Update) are repeated for a predefined number of iterations, leading on the construction of an overcomplete dictionary with optimum characteristics.

In Fig. 1
                        , we present a comparative illustration of the visual words that can be obtained from a dictionary using pre-specified basis functions (overcomplete DCT filters) (Fig. 1a), a dictionary trained on natural images using overcomplete ICA (Fig. 1b), and a dictionary trained on natural images with the GSCNG algorithm (Fig. 1c). It can be verified that the soft-competitive scheme followed from the GSCNG algorithm results in an ensemble of visual words that present greater diversity and less artificial structure. During the experiments, we quantitatively assessed the performance of the adopted algorithm in comparison to other approaches with the results presented in Section 4.

In this section we describe the developed technique for the construction of a saliency map. In what follows we assume that an overcomplete dictionary D is already trained with the process described in Section 3.1. The algorithm’s processing routine is based on three distinct steps:
                           
                              (1)
                              Feature extraction via the calculation of sparse representation coefficients (sparse codes) for the image patches.

Pairwise comparison of the transformed binary sparse codes using of the Hamming distance, and concentration of saliency activation for every patch.

Normalization for the construction of the final map.

Next, the sparse representation vectors ai
                         are pairwise compared with the use of the Hamming distance, for representing the uniqueness of every patch. Prior to the comparison procedure, the coefficient vectors are transformed into binary form:
                           
                              (5)
                              
                                 
                                    
                                       b
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 j
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   1
                                                   ,
                                                
                                                
                                                   if
                                                   
                                                   
                                                      
                                                         a
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   (
                                                   j
                                                   )
                                                   ≠
                                                   0
                                                   ,
                                                   
                                                   the
                                                   
                                                   j
                                                   th
                                                   
                                                   atom
                                                   
                                                   is
                                                   
                                                   active
                                                
                                             
                                             
                                                
                                                   0
                                                   ,
                                                
                                                
                                                   if
                                                   
                                                   
                                                      
                                                         a
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   (
                                                   j
                                                   )
                                                   =
                                                   0
                                                   ,
                                                   
                                                   the
                                                   
                                                   j
                                                   th
                                                   
                                                   atom
                                                   
                                                   is
                                                   
                                                   not
                                                   
                                                   active
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 j
                                 =
                                 1
                                 ,
                                 2
                                 ,
                                 …
                                 ,
                                 M
                              
                           
                        In order to merge the calculated values into a measure of local saliency for every patch Spi
                        , we sum the pairwise Hamming distances of every patch with all the others:
                           
                              (6)
                              
                                 
                                    
                                       Sp
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          x
                                          ≠
                                          i
                                       
                                    
                                 
                                 (
                                 Ham
                                 
                                    min
                                 
                                 g
                                 (
                                 
                                    
                                       b
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       b
                                    
                                    
                                       x
                                    
                                 
                                 )
                                 )
                              
                           
                        
                     

The employment of the Hamming distance for comparing the sparse coding representations can be justified based on the following rationale: during the sparse representation procedure it is expected that the patches belonging in distinctive image regions would employ for their reconstruction specific atoms with rare usage from the other patches of the image. The Hamming distance provides an intuitive—and computationally efficient—way for comparing the binary vectors that are used in our approach to represent the activated atoms for each patch. Thus, the Hamming distance appears as a particularly suitable measure for quantifying the uniqueness of different image areas, in the scope of the proposed method.

In Fig. 2
                        , we present an illustrative example portraying the rationale of the proposed scheme, for the simple case of three patches. Patches 1 and 3 share similar characteristics, as reflected by their sparse representations which share commonly activated atoms (denoted with red/light color). On the other hand, the corresponding sparse representation for Patch 2, which is clearly distinctive relative to Patches 1 and 3, appears to share less common activated atoms with the other two patches. Based on Eq. (6), we can calculate Sp
                        1
                        =
                        Ham
                        12
                        +
                        Ham
                        13
                        =6+6=22, Sp
                        2
                        =
                        Ham
                        12
                        +
                        Ham
                        23
                        =16+16=32, and Sp
                        3
                        =
                        Ham
                        23
                        +
                        Ham
                        13
                        =16+6=22. The saliency value calculated for Patch 2 is the highest, and as a result the algorithm would designate it as the most salient among the three patches.

During the last step of the algorithm, the calculated values Spi
                         are assigned into the corresponding patch locations and the resulting map is normalized to take its final form. The normalization procedure is implemented via a simple non-linear filtering approach, wherein the responses under a specific threshold (Thrsup
                        ) are suppressed, favoring thus the saliency estimates of higher values. The adopted normalization mechanism was inspired by biological data that strongly support the presence of analogous non-linear neural interactions in the visual cortex [31], and practically improves the localization of the saliency map. A global value for the suppression threshold was empirically chosen (trained) based on the characteristics of the natural image datasets that were used during the evaluation experiments (see Section 4.1). Finally, in order to transform the discrete map of salient points into a probabilistic saliency map we apply a Gaussian filter of standard deviation (σ).

The overall suggested scheme for the construction of a saliency map is graphically summarized in Fig. 3
                        .

Since the saliency modeling algorithms often target in real-time or near real-time applications, it is crucial for a method to be able to provide fast results with the minimum employment of computational resources. Several features of the proposed scheme support these particular requirements. First, the overcomplete dictionary is constructed in an off-line procedure, and thus it does not evoke any additional computational burden on the algorithm. Second, contrary to some previous methodologies [21,22] where Inexact ALM [32] is used, the sparse coding stage in our algorithm is implemented via a batch processing approach (batch-OMP algorithm [23]). This technique provides a computationally efficient solution for the many-input/single-dictionary sparse coding problem. A detailed description of the computational complexity of batch-OMP algorithm in relation to the desired degree of sparseness and the size of the dictionary can be found in [23]. Last, the comparison of patches’ representations is accomplished using a simple scheme based on the Hamming distance, a procedure of O(N
                        2) complexity.

In Section 4, we demonstrate that the developed scheme is able to construct a saliency map in under 1s using our general purpose Intel Core 2.4GHz 4GB RAM system. This representative latency may be considered acceptable for a wide variety of applications, and furthermore, the computational cost may decrease even further with a hardware implementation that would also benefit from parallelization.

@&#IMPLEMENTATION DETAILS@&#

In this section, we present the details regarding the parameters used in the current implementation of the algorithm. We decided to adopt the ICOPP (Intensity Color OPPonent) color space for processing image information. Our scheme can be directly applied on RGB (Red–Green–Blue) color features, however, the ICOPP color space was preferred given its proven representation efficiency in attention modeling schemes [15]. The ICOPP values (I, RG, BY) can be computed from the RGB values (R, G, B) of an image as:
                           
                              
                                 
                                    
                                       
                                          I
                                          =
                                          0.2989
                                          ·
                                          R
                                          +
                                          0.5870
                                          ·
                                          G
                                          +
                                          0.1140
                                          ·
                                          B
                                       
                                    
                                    
                                       
                                          RG
                                          =
                                          r
                                          -
                                          g
                                          ,
                                          
                                          r
                                          =
                                          R
                                          -
                                          (
                                          G
                                          +
                                          B
                                          )
                                          /
                                          2
                                          ,
                                          
                                          g
                                          =
                                          G
                                          -
                                          (
                                          R
                                          +
                                          B
                                          )
                                          /
                                          2
                                       
                                    
                                    
                                       
                                          BY
                                          =
                                          b
                                          -
                                          y
                                          ,
                                          
                                          b
                                          =
                                          B
                                          -
                                          (
                                          R
                                          +
                                          G
                                          )
                                          /
                                          2
                                          ,
                                          
                                          y
                                          =
                                          (
                                          R
                                          +
                                          G
                                          )
                                          /
                                          2
                                          -
                                          (
                                          |
                                          R
                                          -
                                          G
                                          |
                                          )
                                          /
                                          2
                                       
                                    
                                 
                              
                           
                        During the dictionary training phase with the GSCNG algorithm we extracted 10,000 patches of dimensionality d
                        =108 (6pixels×6pixels×3colors), from a set of 100 images of various content (landscapes, objects, faces, etc.). It should be emphasized that none of the training images was also a part of the databases used during the evaluation. This was done in order to avoid any possible overfitting effects. The size of the constructed dictionary was set to 300 codewords to ensure a sufficient degree of overcompleteness, given the dimensionality of the input (d
                        =108). Furthermore, during our preliminary experiments we verified that a higher number of dictionary codewords does not have any substantial impact on performance.

During the map construction process each image was first resized into a fixed resolution, and in the sequence it was divided into non-overlapping patches of a fixed dimension d
                        =108. The normalization step of our algorithm involves a non-linear filtering process. The value for the global suppression threshold was set to Thrsup
                           
                        =0.9∗
                        maxSalVal, with maxSalVal denoting the maximum saliency value of the map before suppression.

In this section, we present the effectiveness of the constructed maps on predicting ground truth salient data, and compare the achieved rates with the performance of several state-of-the-art algorithms. Furthermore, we provide a variety of example images that allow for a visual qualitative inspection of constructed maps.

The basic performance metric adopted in this work is the Area Under ROC Curve (AUC) score. The adopted technique for the construction of the ROC curves is the one proposed by Tatler et al. [33]. This technique allows for a fair comparison of the methods since it effectively compensates for the center bias effects which may artificially raise the AUC scores (for a comprehensive discussion about the topic see [10]). The AUC calculation procedure can be summarized as follows: the saliency maps for the images of a database are subjected under different thresholds, translated into binary form, and then assessed on the classification of ground truth data (e.g. human eye fixations). In this way, diagrams of Receiver Operating Characteristic (ROC) curves can be constructed, and the average area under the curve (AUC score) can be used to quantify the overall efficacy of an algorithm to predict visual saliency. Since the calculated AUC scores are affected by the final Gaussian blur filter applied on the maps, we present the performances by varying the standard deviation of the applied Gaussian filter (code was provided by [16]).

We use the divergence of Kullback–Leibler [36] as a secondary measure for evaluating the performance of tested methods. The KL-divergence expresses the overall dissimilarity between two probability density functions by calculating the relative entropy of one of them with respect to the other. If we denote p, q, the probability density functions of two distributions P(x,
                              y) and Q(x,
                              y), then the divergence of Kullback–Leibler can be calculated as:
                                 
                                    (7)
                                    
                                       KLD
                                       (
                                       P
                                       ,
                                       Q
                                       )
                                       =
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                x
                                                =
                                                1
                                             
                                             
                                                N
                                             
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                y
                                                =
                                                1
                                             
                                             
                                                M
                                             
                                          
                                       
                                       q
                                       (
                                       x
                                       ,
                                       y
                                       )
                                       ·
                                       log
                                       
                                          
                                             
                                                
                                                   
                                                      q
                                                      (
                                                      x
                                                      ,
                                                      y
                                                      )
                                                   
                                                   
                                                      p
                                                      (
                                                      x
                                                      ,
                                                      y
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              with 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          x
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          y
                                       
                                    
                                    p
                                    (
                                    x
                                    ,
                                    y
                                    )
                                    =
                                    1
                                 
                              , 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          x
                                       
                                    
                                    
                                       
                                          ∑
                                       
                                       
                                          y
                                       
                                    
                                    q
                                    (
                                    x
                                    ,
                                    y
                                    )
                                    =
                                    1
                                 
                              , and additionally p(x,
                              y)>0 for any x, y such that q(x,
                              y)>0. In our case, the two distributions are represented by a saliency map and the corresponding ground truth map from the real eye tracking data.

The first dataset employed in our experiments is the Toronto database, assembled in [9]. This particular database consists of 120 natural images of resolution 511×681pixels, along with the corresponding eye tracking ground truth data. Upon its presentation it has been extensively used as a common benchmark in several works exploring visual saliency.

This database, presented in [17], enfolds greater variety than the Toronto dataset since it is composed of six subsets designed to allow for an evaluation under different visual conditions. The C1 subset contains 50 images specifically selected for testing Large Scale Saliency. In an analogous manner, C2 subset contains 80 images to test Medium Scale Saliency, C3 subset contains 60 images for exploring Small Scale Saliency, C4 subset offers 15 images for evaluating saliency with Multiple Distracters, C5 subset has 15 images for testing saliency under Cluttered Background and subset C6 contains 15 images for a joint assessment of Large and Small Scale saliency. The original resolution of the images in all datasets is 480×640pixels.

The third dataset employed in our experiments is the MIT database [34]. This database contains a very large ensemble of 1003 images with various contents, e.g. outdoor/indoor scenes, objects, faces, etc. The images in this database vary in their resolution.

We also employed the recently constructed OSIE database [35]. Although the proposed method, by design, constructs saliency maps based on low-level features, we decided to use this database to evaluate the capability of our approach in cases where higher-level cues are also present. The database consists of 700 images with an original resolution of 600×800pixels.

In Fig. 4
                           (a) we present a comparative diagram showing the calculated performances for the Toronto database. The state-of-the-art algorithms used for the evaluation are: the classic Itti and Koch method (Itti) [12], the graph-based approach (GBVS) [14], the frequency domain methods (Im.Sig.) [16] and (HFT) [17], and four schemes based on sparse coding priors (AIM) [9], (ISS) [20], (SUN) [10], and (SP) [21]. In diagrams, our method is denoted as (SSCH) (Saliency based on Sparse Coding and the Hamming distance). It can be observed that the overall performance of the proposed method is high and very close to the levels achieved by the best performing method for the Toronto database, i.e. the Image Signature (Im.Sig.) [16]. Fig. 4(b) presents a sub-diagram showing a comparison exclusively with the techniques that are based on sparse coding priors. The diagram reveals the superiority of our method in comparison to other approaches that are based on sparse representation principles and dictionary training.

In Fig. 5
                           (a)–(f), we present the performances for the six datasets of the McGill ImgSal database. In this case, we may observe that the highest performances are shared among the datasets. This can be attributed to the sensitivity of every method to certain characteristics that exist in every specific dataset (e.g. the size of salient regions). For datasets C1, C5, C6, the top performances are achieved by the (HFT) algorithm [17], whereas the better performance for dataset C3 is achieved by the Image Signature (Im.Sig.) [16], and for dataset C4 by the Incremental Saliency (ISS) algorithm [20]. Our algorithm (SSCH) provides the top performance for dataset C2, which represents Medium Scale Salient Regions. It is worth commenting that the proposed algorithm apart from its superiority on dataset C2, it is always among the highest ranked methods even for the datasets where its performance is lower than the optimum.

In Fig. 6
                           , we show the AUC score diagram for the MIT database. In this case, the optimum performance is achieved by the AIM 
                           [9] method, whereas our approach achieves a moderate performance. It seems thus possible that the specific design approach followed by AIM algorithm provides a more robust representation for a dataset composed of images of different resolutions. This is done at the expense of a higher computational burden, though, as shown during the analysis of computational cost (Table 3).

In Fig. 7
                           , we may observe the AUC performances of the methods for the OSIE database. It can be verified that the AIM method presents the optimum behavior in this case too. As described earlier, our method does not involve any explicit mechanism for modeling higher-level entities contained in the specific database. Nevertheless, it is worth observing that the proposed scheme performs with satisfactory rates (ranked in fourth place) even in such a difficult scenario.

In Table 1
                           , we present a summary of the optimum AUC scores achieved by every method in each database. Hence, it is possible to acquire an easy overall review of the performance characteristics of our method in comparison to the others.

In Table 2
                           , we present the optimum achieved scores for the secondary evaluation measure of KL-divergence. A close inspection of the results reveals that in most cases the ratings for our algorithm, as well as others, are substantially different from the respective ratings in the case of the AUC score. Contrary to the previous results, algorithms GBVS and ISS seem to prevail in most cases for the KL-divergence measure. We will further analyze the possible reasons for the different behavior of KL-divergence measure in the Discussion part (Section 5).

In Section 3, we argued in favor of the employment of a sophisticated dictionary training algorithm instead of more simple approaches, as the pre-specified filter-banks. In this section, we practically evaluate the performances that can be achieved by the proposed method with the employment of dictionaries of different types. We use all the evaluation databases and provide a comprehensive comparative assessment regarding the performance of simple techniques, such as the overcomplete DCT filter banks and ICA trained with natural images, and by more sophisticated approaches for training the dictionary with natural images, i.e. K-SVD [24], Mairal et al. [25]), and the adopted GSCNG [11]. In Fig. 8
                           (a)–(i), we present the calculated AUC score diagrams for the evaluated dictionary training methods. It can be visually validated that the dictionaries designed by training with natural images and the use of more sophisticated approaches provide superior performance compared to the simpler forms of dictionary training, both for the medium sized databases (Torronto, McGill ImgSal) and for the large databases (MIT, OSIE). It is very important to highlight the persistent superior performance achieved by the GSCNG algorithm. On the other hand, the overcomplete DCT and overcomplete ICA dictionaries appear to share the worst performing rates in all cases.

In Section 3.3, we described the computational efficiency characteristics of the proposed scheme, and argued for its advantages with regard to other methods based on sparse coding. In Table 3
                           , we provide a practical demonstration of the run-time computational efficiency for the evaluated algorithms. The reported values refer to the computational delay for the construction of a saliency map for the images of the Toronto database (computational times were averaged over 120 images). We used this database due to its wide adoption as a benchmark. Furthermore, the images in this database are of fixed resolution, which allows for an easier evaluation of the tested methods. The codes for all used algorithms were obtained from the corresponding authors’ sites and were executed on the same system (an Intel Core 2.4GHz 4GB RAM PC). It can be verified that the proposed scheme is the fastest among the methods that rely on sparse representation priors, and it is outperformed only by the spectral methods [16,17], which are known for their computational efficiency. Furthermore, it presents similar computational delay with the classic method of Itti [12].

In this section, we present several visual examples of the saliency maps constructed by the proposed approach. In all cases, the constructed saliency map is superimposed on the original image. The first class of examples involves images of artificial psychophysics stimuli, shown in Fig. 9
                           . With these images we may directly evaluate the ability of the suggested method to point out regions that differ from their surroundings in basic attention-drawing features like color, orientation and density.

In Fig. 10
                           , we use a test image depicting solid shapes (Fig. 10a), and compare the behavior of our algorithm (Fig. 10b) with the corresponding maps extracted by a spectral algorithm [16] (Fig. 10c), and by a method based on sparse representation principles [9] (Fig. 10d). For this visualization, saliency maps are directly extracted from the algorithms without the application of the final Gaussian blur filter. In the specific example it is clear that the map constructed by our method provides a more homogeneous distribution of activation (edges vs. inner regions) relative to the other approaches.

Finally, in Fig. 11
                           , we show some characteristic examples of saliency maps constructed for natural images drawn from the evaluation databases. It can be visually verified that the proposed approach is capable to detect most of the prominent low-level features, and also, larger entities like salient objects.

@&#DISCUSSION@&#

The experimental evaluation procedure unveiled some very interesting characteristics of the proposed method regarding saliency detection accuracy, computational performance, and visual quality of the constructed maps.

An inspection of Figs. 4–7 and the corresponding Table 1, reveals that the developed algorithm achieves satisfactory AUC scores for all databases used during the evaluation process. It is worth commenting that AUC curves for our method present a smooth variation while changing the standard deviation of the finally applied Gaussian kernel, providing thus insights for the visual quality of the constructed maps. The method presents its optimum behavior for salient regions of medium size in relation to the image content, as shown by the highest performance achieved for C2 dataset of the McGill ImgSal database. For the other datasets, the algorithm does not achieve the optimum performance, however, it appears to be in most cases among the top ranked methods. This is a very important characteristic of the developed method compared to other approaches that might perform well for images of specific saliency characteristics but fail in other cases. This observed stability in performance shows the practical robustness of the suggested scheme under different visual conditions. Given the low computational complexity, detailed in Section 3.3 and demonstrated in Table 3, the proposed method presents considerable advantages for an implementation in a real world scenario, especially when compared with other methods based on sparse coding priors and dictionary training.

In the case of KL-divergence measure, shown in Table 2, the performances of methods appear to have differences compared to AUC. This phenomenon can be attributed partially to the special design specifications of the methods that present the best rates, and partially to the absence of any mechanism accommodating for the central bias effects in the case of KL-divergence measure. An additional reason might be the sensitivity of KL-divergence to the blurring levels applied on the maps. Thus, our experimental analysis seems to confirm the findings of [10], and to support the adoption of the AUC calculation proposed in [33] as a more stable and accurate evaluation metric.

During our experiments we opted to implement an extended analysis regarding the employment of different types of dictionary training methods, and evaluate their influence on the final performance of the suggested algorithm. We believe that this analysis could be valuable not only in the case of our method but in a broader scope, including current and future approaches based on dictionary training procedures for modeling salience. As demonstrated in Fig. 8, the performances achieved with the use of dictionaries trained with natural images using sophisticated approaches (GSCNG [11], K-SVD [24], Mairal et al. [25]), appears to be substantially better compared to the pre-specified filter bank of overcomplete DCT, or a trained overcomplete ICA dictionary. The adopted GSCNG algorithm presents the optimum performance across all databases, making it the preferable choice in terms of representation efficiency. This superiority should be attributed to the soft-competitive scheme followed in GSCNG during the training process, which allows for a robust extraction of the dictionary elements from the used training images.

The visual quality assessment of the constructed maps, presented in Figs. 9–11, portrays the efficacy of the proposed method to detect low-level saliency characteristics, and more structured elements of an image. As demonstrated in the compact shapes test-image (Fig. 10), the generated maps represent the salient regions in a compact manner, without over-emphasizing the edges against the inner regions. It should be also noticed that although we utilize non-overlapping patches during map construction, the selection of a small patch size (6×6pixels) in conjunction with the applied Gaussian filter practically mitigates any border effects that might appear.

Finally, it should be mentioned that the proposed approach can be easily generalized in a multi-scale implementation. In such a case, the analysis of the image should be performed in multiple levels, accompanied by the corresponding multi-level dictionaries, i.e. dictionaries with atoms of different dimensionality. Given the additional computational cost of this multi-level framework, this generalization is out of the scope of the current paper. However, since a multi-scale approach is expected to improve the performance even further, it is proposed as a possible extension of our work with application in cases where the computational cost in not of utmost concern.

@&#CONCLUSION@&#

In this work we presented a method for the construction of saliency maps with the use of a bio-inspired scheme for feature representation and comparison. To this purpose, we trained an overcomplete dictionary with the employment of natural image patches, and calculated the sparse coding vectors for patches extracted from different image regions. Using a simple yet robust comparison procedure with the utilization of the Hamming distance, we developed a computationally efficient scheme capable to provide qualitatively informative saliency maps and also exhibiting reasonable predicting performance when assessed against human eye fixations. A future direction of research for our method could be focused on the integration of the proposed saliency map algorithm as a pre-processing module of a complete computer vision system, for optimizing scene analysis and content based image retrieval.

@&#ACKNOWLEDGMENTS@&#

This research has been co-financed by the European Union (European Social Fund – ESF) and Greek national funds through the Operational Program “Education and Lifelong Learning” of the National Strategic Reference Framework (NSRF) – Research Funding Program: Heracleitus II. Investing in knowledge society through the European Social Fund.

@&#REFERENCES@&#

