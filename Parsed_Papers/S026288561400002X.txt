@&#MAIN-TITLE@&#3D human motion analysis framework for shape similarity and retrieval

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new framework is proposed for 3D human video analysis, based on a new descriptor called Extremal Human Curve.


                        
                        
                           
                           Motion analysis is incorporated by employing motion segmentation into clips.


                        
                        
                           
                           Clips are represented by trajectories on shape space.


                        
                        
                           
                           Clip matching is performed using DTW on the manifold.


                        
                        
                           
                           Summarization by clustering is exploited in content-based motion retrieval.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Motion analysis

Shape similarity

3D video retrieval

3D human action

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

While human analysis in 2D image and video has received a great interest during the last two decades, 3D human body is still a little explored field. Relatively few authors have so far reported works on static analysis of 3D human body, but even fewer on 3D human video analysis.

Parallel to this, 3D video sequences of human motion are more and more available. In fact, their acquisition by multiple view reconstruction systems or animation and synthesis approaches [1,2] received a considerable interest over the past decade, following the pioneering work of Kanade [3].

Most of the recent research topics on 3D video focus mainly on performance, quality improvements and compression methods [4,2,5]. Consequently, 3D videos are yet mainly used for display. However, the acquisition of long sequences produces massive amounts of data which necessitates efficient schemes for navigating, browsing, searching, and viewing video data. Hence, we need to develop an efficient and effective descriptor to represent body shape and pose for shape retrieval and video clustering. We also need a motion retrieval system to look for relevant information quickly.

3D Human body shape similarity is an important area, recently attracted more attention in the field of human–computer interface (HCI) and computer graphics, with many related research studies. Among these, research started with 3D features have been applied for body pose estimation and 3D video analysis.

In this paper, a unified framework providing several processing modules is presented. All viewed within a duality pose/motion approach as summarized in Fig. 1
                      below.

We first focus on the analysis of human pose and we propose a novel 3D human curve-based shape descriptor called extremal human curves (EHC). This descriptor, extracted on body surface, is based on extremal features and geodesics between them. Every 3D mesh is represented by a collection of these open curves. The mesh to mesh comparison is then performed in a Riemannian shape space using an elastic metric between each two correspondent human curves.

At this level, our ultimate goal is to be able to perform reliable reduced representation based on geodesic curves for shape and pose similarity metric. Invariant to pose changes, our EHC descriptor allows pose (and motion) comparison of subjects regardless of translation, rotation and scaling. Such descriptor can be employed not only in pose retrieval for video annotation and concatenation but also in motion retrieval, clustering and activity analysis.

Second, we are interested in the task of video segmentation and comparison between motion segments for video retrieval. As a 3D video of human motion consists of a stream of 3D models, we assume that EHC features are extracted from all 3D shape frames of the sequence, which is further segmented. For direct comparison of video sequences, the motion segmentation can play an important role in the dynamic matching by segmenting automatically the continuous 3D video data into small units describing basic movements, called clips.

For the segmentation of these units, an analysis of minima on motion vector is performed using the metric employed to compare EHC representations. Finally, the motion retrieval is achieved thanks to the dynamic time warping (DTW) algorithm in the feature vector space.

The contributions of this paper are:
                        
                           •
                           The proposed surface-based shape descriptor called EHC provides a compact representation of the shape. Thereby, reducing both the required space for storage and the time for comparison. As our descriptor is composed of a collection of local human curves, the EHC can find a number of useful applications lying on body part analysis.

The use of video segmentation allows a semantic analysis of the human motion, within a hierarchical structure of three level “video-clip-pose”.

The modeling of curves in the shape space manifold allows calculating statistics on shape models and motion clips. Thanks to this latter, templates for the pose/clip are computed as average of a collection of poses/clips. The matching with such templates which represents a class, reduces retrieval complexity algorithm from n to log(n).

The development of a unified framework, viewed as a duality pose/motion, for several processing modules on video retrieval and understanding, where all use the same features and similarity metric.

The outline of this paper is as follows: Section 3 discusses related works in the area of static and temporal shape similarity and video retrieval. The extremal curve extraction is presented in Section 4. Section 5 describes the pose modeling in shape space and the elastic metric used for curve comparison. In Section 6, our approach used for motion segmentation and retrieval is presented. Section 7 describes video clustering and summarization for motion understanding. In Section 8, evaluation of our framework and experimental results for shape similarity, video segmentation and retrieval is performed. Finally, we conclude by a discussion of the limitations of the approach in Section 9 and a summarization of our result issues for future works in the Conclusion section.

@&#RELATED WORKS@&#

3D shape representation and similarity have been under investigation for a long time in various research fields (computer vision, computer graphics, robotics) and for various applications (3D object recognition, classification, retrieval). We address below, the most relevant works related to our approach, which only utilize the full-reconstructed 3D data for shape similarity in 3D human video.

Most works which address this problem evaluate a similarity metric on static shape descriptors based on the surface or on the volume. Others propose to extend the static approaches to temporal shape descriptors.

Some of widely used 3D object representation approaches include: spin images, spherical harmonics, shape context and shape distribution. Johnson et al. [6] propose spin image descriptor, encoding the density of mesh vertices into 2D histogram. Osada et al. [7] use a shape distribution, by computing the distance between random points on the surface. Ankerst et al. [8] represent the shape as a volume sampling spherical histogram by partitioning the space containing an object into disjoint cells corresponding to the bins of the histogram. This later is extended with color information by Huang et al. [9]. A similar representation to the shape histogram is presented by Kortgen et al. [10] as 3D extended shape context. Kazhdan et al. [11] apply spherical harmonics to describe an object by a set of spherical basis functions representing the shape histogram in a rotation-invariant manner. These approaches use global features to characterize the overall shape and provide a coarse description, that is insufficient to distinguish similarity in 3D video sequence of an object having the same global properties in the time. A comparison of these shape descriptors combined with self-similarities is made by Huang et al. [12].

Other works on the 3D shape similarity can be found in the literature, where surface-based descriptors are often used with a step of feature detection. The advantage of these features is that their detection is invariant to pose change. The extremities can be considered as the one among the most important features for the 3D objects. They can be used for extracting a topology description of the object like Reeb-graph descriptor [13] or closed surface-based curves [14,15,16]. The extraction and the matching of these features have been widely investigated using different scalar functions from geodesic distances to heat-kernel [17–19]. Tabia et al. [14] propose to extract arbitrarily closed curves amounting from feature points and use a geodesic distance between curves for 3D object classification. Elkhoury et al. [15] extract the same closed curves but they use heat-kernel distance in the 3D object retrieval process.

Since significant progress in multiple view reconstruction techniques has been made, 3D video sequences of human motion are more and more available. However, the need for handling and processing such data led to several approaches using temporal shape representation and matching.

Huang et al. [12] extend the use of static descriptors to temporal ones for frame retrieval, in a 3D human video, using time filtering and shape flows obtained via invariant-rotation shape histograms. Such approaches give a good shape descriptor but usually do not capture any geometrical information about the 3D human body pose and joint positions/orientations. This prevents using them in certain applications that require accurate estimation of the pose (and the joints in some cases) of the body parts.

The temporal similarity in 3D video is addressed also in the case of skeletal motion and is evaluated from difference in joint angle or position together with velocity and acceleration [20]. Huang et al. [21] demonstrate that skeleton-based Reeb-Graph descriptor has a good performance in the task of finding similar poses of the same person in a 3D video. Shape similarity is also used for solving the problem of video retrieval by matching frames and comparing correspondent ones using a specified metric. In Yamasaki et al. [22], the modified shape distribution histogram is employed as feature representation of 3D models. The similar motion retrieval is realized by Dynamic Programming matching using the feature vectors and Euclidean distance. The Dynamic Time Warping algorithm (DTW), based on Dynamic Programming and some restrictions, was also widely used to resolve the problem of temporal alignment. Given two time series with different size, DTW finds an optimal match measuring the similarity between these sequences which may vary in time or speed. Thereby, by a frame descriptor and the temporal alignment using DTW, many authors succeed to perform action recognition or sequence matching for indexing [23–25].

Recently, Tung et al. [13] propose a topology dictionary for video understanding and summarizing, using the Multi-resolution Reeb graph as a relevant descriptor for the shape in video stream for clustering. In this approach, they perform a clustering of the video frames into pose clusters and then they represent the whole sequence with a Markov motion graph in order to model the topology change states.

From the above review, we can identify certain issues in order to consider in our approach. Most of these works have attempted to use global description of the model ignoring the local details. The similarity metric is usually calculated directly on descriptors whereas the notion of motion is incorporated by time convolution of the distance metric itself computed from static poses. The video sequence is considered as a succession of frames in time and not a succession of elementary motions (or gestures).

On one hand, the extremities feature points used in many state-of-the-art algorithms can be considered as an important compact semantic representation of human posture. On the other hand, the shape analysis of curves extracted from human body mesh allows representing the shape variations. Choosing some representative curves of the body surface may provide an efficient and a compact representation of human shape.

Our approach has several benefits: (1) the EHC descriptor can be considered as a surface skeletal based representation, which allows describing surface deformations of the human posture. As it is composed by a collection of local extremal open 3D curves, a body part representation can be performed; (2) the motion analysis is incorporated in two ways, firstly by time convolution of the distance metric vectors for pose retrieval in video sequence, and secondly by employing motion segmentation and the notion of clips; (3) the video segmentation allows the localization of transition states in the video, in order to analyze the local dynamic of the motion, representing an atomic action or gesture; and (4) an original idea is proposed to represent a clip as a trajectory composed of a collection of successive frames viewed as points in shape space. Finally, the video segmentation and clustering are exploited in content-based summarization and motion retrieval.

We aim to represent a body shape as a skeleton based shape representation. This skeleton will be extracted on the surface of the mesh by connecting features located on the extremities of the body. The main idea behind the use of this representation is to analyze pose variation with elastic deformation of the body, using representative curves on the surface.

Feature points refer to the points of the surface located at the extremity of its prominent components. They are useful in many applications, including deformation transfer, mesh retrieval, texture mapping and segmentation. In our approach, feature points are used to represent a new pose descriptor based on curves connecting each two extremities. Several approaches have been proposed in the literature to extract feature points; Mortara et al. [26] select as features points the vertices where Gaussian curvature exceeds a given threshold. Unfortunately, this method can miss feature points because of the threshold parameter and cannot resolve extraction on constant curvature areas. Katz et al. [27] develop an algorithm based on multidimensional scaling, in quadratic execution complexity. Another approach more robust, is proposed by Tierny et al. [28] to detect extremal points, based on geodesic distance evaluation. This approach is used successfully to detect the body extremities, since it is stable and invariant to geometrical transformations and model pose. The extraction process can be summarized as the following.

Let v
                        1 and v
                        2 be the most geodesic distant vertices on a connected triangulated surface S of a human body. These two vertices are the farthest on S, and can be computed using Tree Diameter algorithm (Lazarus et al. [29]).

Now, let f
                        1 and f
                        2 be two scalar functions defined on each vertex v of the surface S as follows:
                           
                              (1)
                              
                                 
                                    
                                       f
                                       1
                                    
                                    
                                       v
                                    
                                    =
                                    g
                                    
                                       v
                                       
                                          v
                                          1
                                       
                                    
                                    /
                                    
                                       f
                                       2
                                    
                                    
                                       v
                                    
                                    =
                                    g
                                    
                                       v
                                       
                                          v
                                          2
                                       
                                    
                                 
                              
                           
                        where g(x, y) is the geodesic distance between points x and y on the surface. Let E
                        1 and E
                        2 be respectively the sets of extrema vertices (minima and maxima) of f
                        1 and f
                        2 on S (calculated in a predefined neighborhood). We define the set of feature points of the surface of human body S as the intersection of E
                        1 and E
                        2. Concretely, we perform a crossed analysis in order to purge non-isolated extrema, as illustrated in Fig. 2
                         (top). The f
                        1 local extrema are displayed in blue color, f
                        2 local extrema are displayed in red color and feature points resulting from their intersection are displayed in mallow color. Fig. 2 (bottom) shows different persons from three different datasets where feature extraction is stable despite change in shape, pose and clothing for each actor.

Let M be a body surface and E={e
                        1,e
                        2,e
                        3,e
                        4,e
                        5} a set of feature points on the body representing the output of the extraction process. Let β denote the open curve on M which joints two feature points of M {ei
                        , ej
                        }. To obtain β, we seek for geodesic path Pij
                        , whose length is shortest while passing through the surface of the mesh, between ei
                         and ej
                        . We repeat this step to extract extremal curves from the body surface ten times so that we do all possible paths between elements of E. As illustrated in the top of Fig. 3
                        , the body posture is approximated by using these extremal curves M
                        ~∪β
                        
                           ij
                        , and we can categorize these curves into 5 categories (Fig. 3 bottom):
                           
                              •
                              Curves connecting hand and foot on the same side: for controlling the movement of the left/right half of the body.

Curves between hands and between feet: for controlling the movement of the upper/lower body.

Curves connecting crossed hand and foot: for controlling the movement of the crossed limbs.

Curves between head and feet: for controlling the movement of right/left foot.

Curves between head and hands: for controlling the movement of right/left hands.

Note that modeling objects with curves is recently carried out for several applications; Abdelkader et al. [30] use closed curves extracted from human silhouettes to characterize human poses in 2D videos for action recognition. Drira et al. [31] use open curves extracted from nose tip and face surface as a surface parameterization for 3D face recognition.

In our approach, we have chosen to represent the body pose by a collection of curves for two reasons. Firstly, these curves connect limbs and give obviously a good representation of the body shape and pose, using a reduced representation of the mesh surface. Secondly, this representation allows studying the shape variation using Riemannian geometry by projecting these curves in the shape space of curves and using its elastic metric introduced by Joshi et al. [32].

In order to compare the similarity between two human body postures, we must quantify the change of shape between correspondent curves. To do this, the metric used to compare shape of curves can be computed inside an open curve shape space.

In the last few years, many approaches have been developed to analyze shapes of 2-D curves. We can cite approaches based on Fourier descriptors, moments or the median axis. More recent works in this area consider a formal definition of shape spaces as a Riemannian manifold of infinite dimension on which they can use the classic tools for statistical analysis. The recent results of Michor et al. [33], Klassen et al. [34] and Yezzi et al. [35] show the efficiency of this approach for 2-D curves. Joshi et al. [32] have recently proposed a generalization of this work to the case of curves defined in ℝ
                        n
                     . We adopt this work to our problem since our 3-D curves are defined in ℝ3.

While human body is an elastic shape, its surface can be simply affected by a stretch (raising hand) or a bind (squatting). In order to analyze human curves independently to this elasticity, an elastic metric is needed within a shape space framework.

Let β:I
                        →ℝ3, for I
                        =[0,1], represent an extremal curve obtained as described above. To analyze its shape, we shall represent it mathematically using a square-root velocity function (SRVF), denoted by 
                           
                              q
                              
                                 t
                              
                              ≐
                              
                                 β
                                 ˙
                              
                              
                                 t
                              
                              /
                              
                                 
                                    
                                       
                                          β
                                          ˙
                                       
                                       
                                          t
                                       
                                    
                                 
                              
                           
                        . q(t) is a special function introduced by Joshi et al. [32] that captures the shape of β and is particularly convenient for shape analysis.

The set of all unit-length curves in ℝ3 is given by 
                           
                              C
                              =
                              
                                 
                                    q
                                    :
                                    I
                                    →
                                    
                                       ℝ
                                       3
                                    
                                    
                                       
                                          q
                                       
                                    
                                    =
                                    1
                                 
                              
                              ⊂
                              
                                 L
                                 2
                              
                              
                                 I
                                 
                                    ℝ
                                    3
                                 
                              
                              0
                           
                        , where using 
                           L
                        
                        2-metric on its tangent spaces, 
                           C
                         becomes a Riemannian manifold.
                           Proposition 1
                           
                              Having two open curves represented by their SRVF, q1 and q2
                              , the shortest geodesic between them in the shape space of open curves is given by: 
                                 
                                    α
                                    
                                       τ
                                    
                                    =
                                    
                                       
                                          1
                                          
                                             sin
                                             
                                                θ
                                             
                                          
                                       
                                    
                                    
                                       
                                          sin
                                          
                                             
                                                
                                                   
                                                      1
                                                      −
                                                      τ
                                                   
                                                
                                                θ
                                             
                                          
                                          
                                             q
                                             1
                                          
                                          +
                                          sin
                                          
                                             θτ
                                          
                                          
                                             q
                                             2
                                             ∗
                                          
                                       
                                    
                                    ,
                                 
                               
                              and the geodesic distance is given by: d
                              
                                 s
                              (q
                              1,q
                              2)≐
                              cos
                              −1(〈q
                              1,q
                              2
                              ∗〉), where q
                              2
                              ∗ 
                              is the optimal element associated with the optimal rotation O⁎ and re-parameterization γ⁎ of the second curve.
                           

This defined distance allows comparing shape curves regardless of isometric and elastic deformation. In Fig. 4
                        , a geodesic path between each corresponding two extremal curves, taken from two human bodies doing different poses, is computed in shape space.

For the left model, the person's arm is down and for the right model it is raised. The geodesic path between each two curves is shown in the shape space. This evolution looks very natural under the elastic matching.

The elastic metric applied on extremal curve-based descriptors can be used to define a similarity measure. Given two 3D meshes x, y and their descriptors x′={q
                        1
                        
                           x
                        ,q
                        2
                        
                           x
                        ,q
                        3
                        
                           x
                        , …,q
                        
                           N
                        
                        
                           x
                        } and y′={q
                        1
                        
                           y
                        ,q
                        2
                        
                           y
                        ,q
                        3
                        
                           y
                        , …,q
                        
                           N
                        
                        
                           y
                        }, the mesh-to-mesh similarity can be represented by the curve pairwise distances and can be defined as follows:
                           
                              (2)
                              
                                 
                                    s
                                    
                                       x
                                       y
                                    
                                    =
                                    d
                                    
                                       
                                          x
                                          ′
                                       
                                       
                                          y
                                          ′
                                       
                                    
                                    ,
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    d
                                    
                                       
                                          x
                                          ′
                                       
                                       
                                          y
                                          ′
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             d
                                             
                                                
                                                   β
                                                   i
                                                   x
                                                
                                                
                                                   β
                                                   i
                                                   y
                                                
                                             
                                          
                                       
                                       N
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             N
                                          
                                          
                                             
                                                d
                                                s
                                             
                                             
                                                
                                                   q
                                                   i
                                                   x
                                                
                                                
                                                   q
                                                   i
                                                   y
                                                
                                             
                                          
                                       
                                       N
                                    
                                 
                              
                           
                        where N is the number of curves used to describe the mesh. The mean of curve distances between two descriptors captures the similarity between their mesh poses. In case of shape change in even one curve, the global distance is affected and it increases indicating that the poses are different. In order to have a global distance, an arithmetic distance can be computed in order to compare human poses.

The use of EHC descriptor to represent the human pose by a collection of 3D open curves allows analyzing the human shape using the geometrical framework. It also allows computing some related statistics like “average” of several extremal human curves. Such an average, called Karcher Mean, is introduced by Srivastava et al. [36]. It can be computed between different poses to represent the intermediate pose, or between similar poses done by several actors to represent a template for similar poses.

We are interested in defining a notion of “mean” for a given set of human postures in the same cluster of poses for the goal of fast pose retrieval.

To compute the average of EHC representation, we need only to know how to compute an average for one extremal human curve. The Riemannian structure defined on the shape space S enables us to perform such statistical analysis for computing average and variance for each 3D open curve on body surface. The intrinsic average or the Karcher Mean utilizes the intrinsic geometry of the manifold to define and compute a mean on that manifold. In order to calculate the Karcher Mean of extremal human curves {q
                        1
                        
                           α
                        , …,q
                        
                           n
                        
                        
                           α
                        } in S, we define the variance function as:
                           
                              (4)
                              
                                 
                                    V
                                    :
                                    S
                                    →
                                    R
                                    ,
                                    V
                                    
                                       μ
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          
                                             d
                                             S
                                          
                                          
                                             
                                                
                                                   q
                                                   i
                                                   α
                                                
                                                
                                                   q
                                                   j
                                                   α
                                                
                                             
                                             2
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The Karcher Mean is then defined by:
                           
                              (5)
                              
                                 
                                    
                                       
                                          q
                                          α
                                       
                                       ¯
                                    
                                    =
                                    arg
                                    
                                       min
                                       
                                          μ
                                          ∈
                                          S
                                       
                                    
                                    V
                                    
                                       μ
                                    
                                    .
                                 
                              
                           
                        
                     

The gradient of V is used in the tangent space Tμ
                        (S) to iteratively update the current mean μ. 
                           
                              
                                 q
                                 α
                              
                              ¯
                           
                         is an element of S that has the smallest geodesic path length from all given extremal human curves for the index α.

An example of using the Karcher Mean to compute average curve for 6 extremal human curves connecting hand and foot from the same side is shown in the top of Fig. 5
                        , and several examples of using the Karcher mean to compute average EHC representation are shown in the bottom of this figure.

Based on our EHC representation of the shape model, it is possible to compare two video sequences by matching all pairwise correspondent extremal curves inside their frames, using the geodesic distance in the shape space. However, a sequence of human action can be composed of several distinct actions, and each one can be repeated several times. Therefore, the motion segmentation can play an important role in the dynamic matching by dividing the whole 3D video data into small, meaningful and manageable elementary actions called clips. EHC descriptor will be employed to segment continuous sequences into clips.

Video segmentation has been studied for various applications, such as gesture recognition, motion synthesis and indexing, browsing and retrieval. A vast amount of works in video segmentation has been performed for 2D video [37], where usually the object segmentation is firstly performed before the movement analysis. In Rui et al. [38], an optical flow of moving objects is used and motion discontinuities in trajectories of basis coefficient over time are detected. However, in Wang et al. [39], break points were considered as local minima in motion and local maxima in direction change.

Motion segmentation is strongly applied in several algorithms using 3D motion capture feature points trackable within the whole sequence, to segment the video. Detected local minima in motion (Shiratori et al. [40]) or extrema (Kahol et al. [41]) are used in motion segmentation for kinematic parameters.

Most of works on the 3D video segmentation use the motion capture data, and very few of them were applied to dynamic 3D mesh. One of them is presented by Xu et al. [42], where a histogram of distance among vertexes on 3D mesh is generated to perform the segmentation through thresholding step defined empirically. In Yamasaki et al. [43], the motion segmentation is automatically conducted by analyzing the degree of motion using modified shape distribution for mainly Japanese dances. These sequences of motion are paused for a moment and then they are considered as segmentation points. Huang et al. [44] propose an automatic key-frame extraction method for 3D video summarization. To do so, they compute the self similarity matrix using volume-sampling spherical shape histogram descriptor. Then, they construct a graph based on this self similarity matrix and define a set of key frames as the shortest path of this graph.

In our work, we propose an approach fully automatic to segment a 3D video efficiently without making neither thresholding step nor assumption on the motion's nature. In motion segmentation, the purpose is to split automatically the continuous sequence into segments which exhibit basic movements, called clips. As we need to extract meaningful clips, the segmentation is overly fine and can be considered as finding the alphabet of motion. For a meaningful segmentation, motion speed is an important factor. In fact, when human changes motion type or direction, the motion speed becomes small and this results in dips in velocity. We exploit this latter by finding the local minima for the change in type of motion and local maxima for the change in direction. The extrema detected on velocity curve should be selected as segment points (see Fig. 6
                        ). We show frames detected as maxima (the actor changes the foot's direction) on the top of the plot, and frames detected as minima (the actor raise the other foot) on the bottom. In this work, we consider only the change in type of motion as a meaningful clip. Thus, clips with slight variations and a small number of frames are avoided.

Note that optimum local minimum, that detects precise break points where the motion changes, is selected in a predefined neighborhood. For this reason, we fix a size of window to test the efficiency of the local minimum in this condition. To calculate the speed variation, distance between each two successive EHC in the sequence is computed. The variations of the sequence are represented in vector of speed and a further smoothing filter is applied to obtain the final degree of motion vector.

To seek for similar clips, we need to encode gestures in a specific representation that we can compare regardless to certain variations. In fact, two motions are considered similar even if there are changes in the shape of the actor and the speed of the action execution. This problem is similar to time-series retrieval where a distance metric is used to look for, in a database, the sequences whose distance to the query is below a threshold value. Each clip is represented as a temporal sequence of human poses, characterized by EHC representation associated to shape model. Then, extremal curves are tracked in each sequence to characterize a trajectory of each curve in the shape space as illustrated in Fig. 7
                         (top). Finally, the trajectories of each curve are matched and a similarity score is obtained. However, due to the variations in execution rates of the same clip, two trajectories do not necessarily have the same length. Therefore, a temporal alignment of these trajectories is crucial before computing the global similarity measure, as shown in Fig. 7 (bottom).

In order to solve the temporal variation problem, we use DTW algorithm (Giorgino et al. [45]). This algorithm is used to find optimal non-linear warping function to match a given time-series with another one, while adhering to certain restrictions such as the monotonicity of the warping in the time domain. The optimization process is usually performed using dynamic programming approaches given a measure of similarity between the features of the two sequences at different time instants. The global accumulated costs along the path define a global distance between the query clip and the motion segments found in the database. Since DTW can operate with any measure of similarity between different temporal features, we adapt it to features that reside on Riemannian manifolds. Hence, we use the geodesic distance between different shape points ds(qi
                        ,qj
                        ) as a distance function between the shape features at different time instants.

In practice, the first step is to follow independently curve variation in time resulting on N trajectories in the shape space. In fact, each frame in the 3D video sequence can be represented by a predetermined number (N) of extremal curves, splitting the sequence into N parts, where each one represents the trajectory of an open curve in the shape space. Then, DTW will be applied in the feature space for each tracked curve index. The distance between two clips is then the average distance given by each comparison between corresponding trajectories.

Based on the two algorithms, Karcher Mean and DTW, we can extend the notion of “mean” of a set of human poses to the “mean” of trajectories of poses in order to compute an “average” of several clips.

Let N be the number of clips represented by N trajectories T
                        1,T
                        2—TN
                        . For a specific human curve index, we look for the mean trajectory that has the minimum distance to the all N trajectories. As shown in Algorithm 1, the mean trajectory is given by computing the non-linear warping functions and setting iteratively the template as the Karcher Mean of the N warped trajectories represented in the Riemannian shape space.
                           Algorithm 1
                           Computing trajectory template


                           
                              
                                 
                                    
                                       Require: N trajectories from N clips T1
                                       ; T2
                                       ⋯TN
                                       
                                       
                                          
                                             
                                                Initialization: chose randomly one of the N input trajectories as an initial guess of the mean trajectory 
                                                   T
                                                   mean
                                                
                                             


                                                repeat
                                                
                                                   
                                                      
                                                         for i=1: N do
                                                         
                                                            
                                                               find optimal path p⁎
                                                                   using DTW to warp Ti
                                                                   to Tmean
                                                                  
                                                               


                                                         end for
                                                      

Update Tmean
                                                          as the Karcher Mean of all N warped trajectories


                                                until Convergence

In order to represent compactly a video sequence, we need to know how to exploit the redundancy of information over time. However, when this information should be extracted from motion and not from frames separately, the challenge is then about complex matching processes required to find geometric relations between consecutive data stream elements. We therefore propose to use EHC to represent a pose and a trajectory as key descriptors characterizing geometric data stream. Based on EHC representation, we develop several processing modules as clustering, summarization and retrieval.

Let V denote a video stream of human sequence containing elements {e
                        
                           i
                        }
                           i
                           =1…
                           k
                        , where e can be a frame or a clip. To cluster V, the data set is recursively split into subsets Ct
                         and Rt
                         as described in the following recursive algorithm:
                           Algorithm 2
                           Data clustering


                           
                              
                                 
                                    
                                       Require: V {ei
                                       }
                                          i
                                          =1…k
                                       ;


                                       Ensure: C
                                       0
                                       =∅; R
                                       0
                                       ={e
                                       1,…,ek
                                       };
                                          
                                             
                                                if (Rt
                                                
                                                ≠∅)&&(t
                                                ≤
                                                k) then
                                                
                                                   
                                                      
                                                         Ct
                                                         
                                                         ={f∈R
                                                         
                                                            t-1: dist(et
                                                         , f)<
                                                         Th};


                                                         Rt
                                                         
                                                         =
                                                         R
                                                         
                                                            t
                                                            −1\Ct
                                                         ;


                                                end if
                                             

The result of clustering is contained in C
                           t
                           =1..k
                         where Ct
                         is a subset of V representing a cluster containing similar elements to et
                        . For each iteration of clustering steps, from t
                        =1—K, the closest matches to et
                         are retrieved and indexed with the same cluster reference as et
                        . Any visited element et
                         already assigned to a cluster in C during iteration step is considered as already classified and is not processed subsequently. We regroup not empty subsets Ct
                         in l clusters {c
                        1,…,cl
                        } (with l≤
                        k). Similarities between elements of V are evaluated using a similarity distance dist allowing to compare the elements of V. The threshold Th is defined experimentally.

If we consider the video V as a long stream of 3D meshes, the clusters that should be obtained must gather models with similar poses. In this case, the EHC feature vector is used as an abstraction for every mesh and the similarity distance is the elastic metric computed between each pair of human poses. Motion can be incorporated in this similarity by applying a simple time filter on static similarity measure with a window size chosen experimentally [46]. The use of temporal filter integrates consecutive frames in a fixed time window, thus allowing the detection of individual poses while taking into account smooth transitions. Note that the pose invariance property of the EHC allows us to compare poses (and motions) of subjects regardless of translation, rotation and scaling.

The video V can also be considered as a stream of clips resulting from the video segmentation approach and clusters here gathers clips with similar repeated atomic actions. In this case: (1) the feature vector used as abstraction for each clip is a trajectory on shape space of extremal human curves; and (2) the similarity distance, used to compare clips, is based on the DTW algorithm.

Our approach for video summarization is based on three steps: First, the whole video is segmented and clustered into several clusters of clips. Second, only the most significant clip (the nearest one to all cluster elements) of each cluster is kept. Third, we construct a subsequence, from the starting video, where these representative clips of each cluster are concatenated. Finally, this new subsequence is clustered into clusters of poses, and only most representative poses are kept to describe the dataset.

This summarization allows a reduction of dimension for the original dataset where we can display only main clips if we stop on third step, or to display key frames if we continue summarization process until pose clustering.

As in a classical retrieval procedure, in response to a given query, an ordered list of responses that the algorithm found nearest to the query is given. Then to evaluate the algorithm, this ranked list is analyzed. Whatever the given query, pose or clip, the crucial point in the retrieval system is the notion of “similarity” employed to compare different objects.

For content-based pose retrieval, thanks to the static shape similarity, we are able to compare human poses using their extremal human curve descriptors and decide if two poses are similar or not. In this scenario, the query consists of a 3D human shape model in a given pose and the response is 3D human bodies more similar in pose to the query. We advocate the usage of the EHC to represent the 3D human shape model in a given pose and then comparison between each pair of models using the elastic metric defined in the Proposition 1. This system can find a number of utilities like pose-based searching and facilitate retrieval of efficient information as subjects in same poses in the database of 3D models scanned in different poses [47,48].

Note also that identifying frames with similar shape and pose can be used potentially for concatenative human motion synthesis. Concatenate existing 3D video sequences allow the construction of a novel character animation. A good descriptor that much correctly correspondent frames allow the synthesis of videos with smooth transitions and finding best frames to summarize the video. However, extension of static shape descriptor to include temporal motion information is required to remove the ambiguities inherent in static shape descriptor for comparing 3D video sequences of similar shape. Therefore, the static shape descriptor can be extended to the time domain by applying a simple time filter with a window size like 2Nt
                        
                        +1. This time filter is a way of incorporating motion in the similarity measure, as so-called temporal similarity, also used by Huang et al. [12]. The temporal similarity is presented in the following equation:
                           
                              (6)
                              
                                 
                                    
                                       s
                                       ij
                                       t
                                    
                                    =
                                    
                                       1
                                       
                                          2
                                          
                                             N
                                             t
                                          
                                          +
                                          1
                                       
                                    
                                    
                                       
                                          ∑
                                          
                                             k
                                             =
                                             −
                                             
                                                N
                                                t
                                             
                                          
                                          
                                             N
                                             t
                                          
                                       
                                       
                                    
                                    
                                    s
                                    
                                       
                                          i
                                          +
                                          k
                                          ,
                                          j
                                          +
                                          j
                                       
                                    
                                 
                              
                           
                        where s is the frame-to-frame similarity matrix and Nt
                         is a time filter with window size 2Nt
                        
                        +1.

For content-based motion retrieval, we advocate the usage of the EHC representation, where a query consists of a trajectories representing a clip on the shape space. As response to this specific query, our approach looks in the sequence for most similar trajectories and returns an ordered list of similar ones using the process of motion clip explained in Section 3.

@&#EXPERIMENTAL RESULTS@&#

To show the practical relevance of our method, we perform an experimental evaluation on several databases (summarized in Table 1
                     ) and compare it to the most efficient descriptors of the state-of-the-art methods. We first evaluate our descriptor for shape similarity application over public static shape database [48] and evaluate the results against spherical harmonic descriptor [11]. Secondly, we measure the efficiency of our descriptor to capture the shape similarity in 3D video sequences of different actors and motions from other public 3D synthetic [12] and real [49,50] video databases. We evaluate this later against temporal shape histogram [12], Multi-resolution Reeb-graph [21] and other classic shape descriptors, using provided ground truth. Motion segmentation into clips and clip matching performance are tested on several video sequences of different people doing different motions. Finally, we evaluate our clustering and summarization approach for pose/clip-based video retrieval.

The extraction and comparison of our curves require the identification of feature end-points as head, right/left hand and right/left foot, which is not affordable in practice. This requirement is important to perform the curve matching separately between models. In order to overcome this problem, our method is based on two benefits from the morphology of the human body. First, we deduce that geodesic path connecting each one of the hand end-points and the head end-point is shortest among all possible geodesics between the five end-points. Second, the geodesic path connecting right hand to left foot end-points or left hand to right foot end-points is the longest. The first observation allows identifying precisely the end-point corresponding to the head, the two end-points connected to this later corresponding to the hands without distinguishing between right and left. The second one allows the identification of the couple of hand/foot as corresponding to same side of the body without distinguishing between right and left. A prior knowledge on the direction of the posture of the human body for static pose and in the starting frame for video sequence has allowed distinguishing between left and right. Once the end-points are correctly detected from the starting frame in the video sequence, a simple algorithm of end-point tracking over time is performed.

The protocol and the dataset used to validate the experiments are firstly presented and then, the results following this protocol are analyzed and compared to those obtained by other approaches.

To assess the performance of the EHC for static shape similarity, several experiments were performed on a statistical shape database [48]. This database, summarized in Table 1 (1st row), is challenging for human body shape and pose retrieval as it is realistic shape database captured with a 3D laser scanner. It contains more than hundred subjects doing more than thirty different poses. We perform our descriptor on a subset of 338 shape models obtained from 144 subjects 59 male and 55 female aged between 17 and 61years. There are 18 consistent poses (p0, p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, p12, p13, p16, p28, p29, p32). Some poses are illustrated in Fig. 8
                           . Each pose represents a class where at least 4 different subjects do the same pose.

For evaluation, we use Recall/Precision plot in addition to the three statistics which indicate the percentage of the top K matches that belong to the same pose class as the query pose:
                              
                                 •
                                 The nearest neighbor statistic (NN): it provides an indication to how well a nearest neighbor classifier would perform (here K
                                    =1).

The first tier statistic (FT): it indicates the recall for the smallest K that could possibly include 100% of the models in the query class.

The second tier statistic (ST): it provides the same type of result, but it is a little less stringent (i.e., K is twice as big).

E-Measures: it is a composite measure of precision and recall for a fixed number of retrieved results.

We note here that these statistics will be used for static and video retrieval evaluations.

From five feature endpoints, we have extracted ten extremal curves representing the human body shape model. According to the human poses, extremal curves exhibit different performance and some curves are more efficient to capture the shape similarity between two poses. Our shape descriptor can be seen as a concatenation of ten curve representations and the similarity between two shape models doing two different poses, is represented by a vector of ten elastic distance values. Before all tests, we analyze the performance of all possible combinations of curves on the shape similarity measurements. A Sequential Forward Selection method, applied on elastic distance values and coupled with ST statistic, has been used to select the best combination of curves among all possible ones (1013 combinations according to Eq. 7):
                              
                                 (7)
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                k
                                                =
                                                2
                                             
                                             n
                                          
                                          
                                       
                                       
                                       
                                          C
                                          n
                                          k
                                       
                                       =
                                       
                                          
                                             ∑
                                             
                                                k
                                                =
                                                2
                                             
                                             n
                                          
                                          
                                       
                                       
                                       
                                          
                                             n
                                             !
                                          
                                          
                                             k
                                             !
                                             
                                                
                                                   n
                                                   −
                                                   k
                                                
                                             
                                             !
                                          
                                       
                                    
                                 
                              
                           where n is equal to 10 and it represent the number of curves.

Experiment of pose-based retrieval on the dataset [48] shows that the best combination is obtained by the five curves: right hand to right foot, left hand to left foot, left hand to right hand, left foot to right foot, and head to the right foot (Fig. 9
                           ).

The selected five curves seem to be the most stable ones and they are sufficient to represent at best the body like a skeleton on the surface. Therefore, the elimination of five curves allows eliminating the ambiguity due to the redundancy of some curves on the body parts.

The self similarity matrix obtained from the mean elastic distance of the five selected curves is shown in Fig. 10
                           .

This matrix demonstrates that similar poses have a small distance (cold color) and that this distance increases with the degree of the change between poses (hot color). This allows pose classification or pose retrieval by comparing models using their extremal curve representation and the elastic metric.

From a quantitative point of view, we present the Recall/Precision plot (see Fig. 11
                           ) obtained by EHC compared to the popular spherical harmonic (SH) descriptor with optimal parameter setting (Ns
                           
                           =32 and Nb
                           
                           =16) [8]. This plot and accuracy rates (NN, FT and ST) reported in Table 2
                            show that our approach provides better retrieval precision. EHC using only the five selected curves outperforms SH and EHC using the 10 curves to retrieve models with the same pose as shown in Table 2 in bold.

Note finally that the accuracies of retrieval ranks for some poses are relatively low. Such ambiguities can be noticed in the case of comparison between neutral pose and a pose where subjects just twist their body to the left, or twist their torso to look around.

We firstly present the protocol and the dataset used in these experiments and then, the results following this protocol are analyzed and compared to the most relevant state-of-the-art approaches.

The recognition performance of the temporal shape descriptor is evaluated using a ground-truth dataset from a synthetic 3D video sequences proposed by Huang et al. [12] and a real captured 3D video sequences of people [49]. As described in Table 1 (2nd raw), the synthetic data is obtained by 14 people (10 men and 4 women) performing 28 motions. Each sequence is composed of 100 frames and the whole dataset contains a total of 39,200 frames.

Given the known correspondences, a temporal ground-truth similarity is computed between each two surfaces. The known correspondence is only used to compute this ground truth similarity. Having two Mesh X and Y with N vertices xi
                            ϵ X and yi
                            ϵ Y, a temporal-ground truth CT
                            is computed by combining a shape similarity Cp
                            and a temporal similarity Cv
                            as follows:
                              
                                 (8)
                                 
                                    
                                       
                                          
                                             
                                                C
                                                T
                                             
                                             
                                                X
                                                Y
                                             
                                             =
                                             
                                                
                                                   1
                                                   −
                                                   α
                                                
                                             
                                             
                                                C
                                                p
                                             
                                             
                                                
                                                   x
                                                   i
                                                
                                                
                                                   y
                                                   j
                                                
                                             
                                             +
                                             α
                                             
                                                C
                                                v
                                             
                                             
                                                
                                                   x
                                                   i
                                                
                                                
                                                   y
                                                   j
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                C
                                                p
                                             
                                             
                                                X
                                                Y
                                             
                                             =
                                             
                                                1
                                                N
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                             
                                             d
                                             
                                                
                                                   x
                                                   i
                                                
                                                
                                                   y
                                                   j
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                C
                                                v
                                             
                                             
                                                X
                                                Y
                                             
                                             =
                                             
                                                1
                                                N
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                             
                                             d
                                             
                                                
                                                   
                                                      
                                                         x
                                                         ˙
                                                      
                                                      i
                                                   
                                                
                                                
                                                   
                                                      
                                                         y
                                                         ˙
                                                      
                                                      j
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where d is an Euclidean distance, 
                              
                                 
                                    
                                       x
                                       ˙
                                    
                                    i
                                 
                              
                            and 
                              
                                 
                                    
                                       y
                                       ˙
                                    
                                    j
                                 
                              
                            are the derivation of x and y between next and current frame. the parameter α is used to balance the equation and it is set to 0.5. In order to identify frames as similar or dissimilar, the temporal ground truth similarity matrix is binarized using a threshold set to 0.3 similarly to Huang et al. [12].

Finally, recognition performance is evaluated using the Receiver–Operator-Characteristic (ROC) curves, created by plotting the fraction of true-positive rate (TPR) against the fraction of false-positive rate (FPR), at various threshold settings. The true and false dissimilarities compare the predicted similarity between two frames, against the ground-truth similarity.

An example of self-similarity matrix computed using temporal ground-truth descriptor, static and temporal descriptors is shown in Fig. 12
                           . This figure illustrates also the effect of time filtering with increasing temporal window size for EHC descriptors on a periodic walking motion.

A comparison is made between our temporal extremal human curve (TEHC) and several descriptors from the state-of-the-art: shape distribution (SD), spin image (SI), spherical harmonics representation (SHR), two shape-flow descriptors, the global/local frame alignment shape histograms (SHvrG/SHvrS) (Huang et al. [12]) and Reeb-graph as skeleton based shape descriptors (aMRG) (Tung et al. [51]). Note that a spectral representation was also evaluated by Huang et al. [21] which is the multi-dimensional scaling (MDS). Huang et al. [12] evaluated the performances of all these descriptors for the purpose of shape similarity.

The effectiveness of our descriptor has been evaluated by varying temporal window and comparing it to the most relevant state-of-the-art descriptors [12] as shown in the plot of ROC curves in Fig. 13
                           .

Several observations can be made on the obtained results:
                              
                                 (i)
                                 Our descriptor outperforms classic shape descriptors (SI, SHR, SD) and shows competitive results with SHvrS and aMRG. We also notice that recognition performance of EHC increases with the increase of the window size of time-filter like any other descriptor. In fact, time-filter reduces the minima in the anti-diagonal direction, resulting from motion in the static descriptor (Fig. 13). Multiframe shape-flow matching required in SHvrS allows the descriptor to be more robust but the computational cost will increase by the size of selected time window.

EHC descriptor by its simple representation, demonstrates a comparable recognition performance to aMRG. It is efficient as the curve extraction is instantaneous and robust as the curve representation is invariant to elastic and geometric changes thanks to the use of the elastic metric.

The result analysis for each motion shows that EHC gives a smooth rates that are stable and not affected by the complexity of the motion. Such complex motions are rockn'roll, vogue dance, faint, shot arm (Fig. 14
                                    ). However, this is not the case for SHvrS where performance recognition falls suddenly with complex motions as illustrated in Fig. 15
                                    .

We also applied the time filtering EHC descriptor on two real captured 3D video sequences of people. The first sequence is extracted from the dataset [49] described in Table 1 (3rd row). The second one is extracted from real data reconstructed by multiple camera video [50] and described in Table 1 (4th row).

Inter-person similarity across two people in a walking motion with an example similarity curve is shown in Fig. 16(a). Our temporal similarity measure identifies correctly similar frames across different people. These similar frames are located in the minima of the similarity curve. In addition, despite the topology change and the reconstruction noise as shown in Fig. 16(b), our algorithm succeed to identify correctly the frame in the sequence similar to the query.

In this section, we evaluate temporal shape similarity descriptor. Details about the computation of the ground truth descriptor are given in addition to the description of the different datasets used for evaluation. The results obtained by our approach, compared to those of different state-of-the-art descriptors, are then discussed.

The two datsets (2) and (3) presented in Table 1 are used in these experiments. From the synthetic dataset [12], we have chosen 14 different motions: walk (slow, fast, circle left/right, cowboy, march, mickey), run (slow, fast, circle left/right), sprint, and rockn'roll. These motions are performed by two actors (a woman and a man) making a total of 28 motions (2800 frames). They are chosen for their interesting challenges as: (i) change in execution rate (slow/fast motions) (ii) change in direction while moving (walking in straight line, moving in circle and turning left and right) (iii) change in shape (a woman and a man). We used these motion sequences for both segmentation and retrieval experiment.

To validate the segmentation step, we segment all these 3D video sequences with the proposed approach and then compare results to manual segmentation ground-truth.

In the retrieval process, each query clip is compared to all other clips obtained by the segmentation of sequences. Finally, the statistics (NN, FT, ST and E-measure) are used for the evaluation.

Plotting the distance between EHC representation of successive frames gives a very noisy curve. The break points from this curve do not define semantic clips and the extracting of minima leads to an over-segmentation of the sequence (see Fig. 17
                            (top)). To obtain more significant local minima, we convolve the curve with a time-filter allowing to take into account the motion variation, not only between two successive frames but also in a time window. The motion degree after convolution is shown in Fig. 17 (bottom). Break points are more precise and delimit significant clips corresponding to step change in the video sequence.

In order to evaluate its efficiency, we apply our segmentation method on the whole dataset (3) described in Table 1 (3rd raw) and then compare the results to a manual segmentation of the base done carefully.

We performed the clip segmentation for all window size values from 1 to 11 over a representative set of clips extracted from the dataset (3) [49]. Compared to manual ground truth, the best segmentation is obtained using a window size of 5. This value is then fixed for the rest of the tests. The segmentation of the dataset (3) gives 83 segmented clips (78 correct clips and 5 incorrect clips). This can be explained by the fact that the 5 failing clips are short. They contain about 6 frames at most and do not describe atomic significant actions. Otherwise, a total of 144 clips have been obtained by the segmentation of the 14 motions taken from the dataset (2) described in Table 1 (2nd raw) performed by two actors.


                           Fig. 18
                            shows some results of motion segmentation on a “slow walk” and a “fast walk” motions. Although the walk speed increases, the motion segmentation remains significant and does not change and corresponds to the step change of the actor. The Rockn'roll dance motion segmentation is also illustrated in Fig. 18 (bottom). Thanks to the selection of local minima in a precise neighborhood, only significant break points are detected.

The motion segmentation method, applied on 14 motion sequences from the dataset (2) and performed by a man and a woman, gives a total of 144 clips. These clips, with an average number of frames per clip equal to 15, are categorized into 14 classes. The motion sequences consist mainly of different styles of walking, running and some dancing sequences. Classes grouped together represent different styles of walking, running and dancing steps. For example, a step change in a walk may represent a class and groups similar clips done with different speed and in different trajectories. We notice that right to left change step is grouped in a different class than left to light change step.

The similarity metric represented by elastic measure values between each pair of clips allows us to generate a confusion matrix for all classes of clips, in order to evaluate the recognition performance by computing dynamic retrieval measures thanks to a manually annotated ground truth. An example of the matrix representing the similarity evaluation score among clips in sequences performed by a female actress against the clips of sequences of motions performed by a male actor is shown in Fig. 19
                           . The more the color is cold the more the clips are similar.

Thanks to the use of DTW, it is noticed that similarity score between same clips done in different speeds is small (see Fig. 19). The matching between the clip representing change in step in a slow walk motion composed of 25 frames and a fast walk motion, composed of 18 frames, is small.

Besides, our approach succeeds to retrieve clips within motions done in different ways. For example, the walk circle clips can be matched with the clips of slow walk motion done in a straight line (see Fig. 19). This explains why the use of an elastic metric, to compare and match trajectories, makes the process independent to rotation. Although the actors performing the motions are different, it is observed that similar clips yield smaller similarity score. Like it is shown in “rockn'roll” dance motion, steps of the dance performed by different actors are correctly retrieved.

It is demonstrated that 79.26% of similar motion clips are included in the first tier and 93% of clips are correctly retrieved in the second tier. It is a rather good performance considering that only such low-level feature as the EHC is utilized in the matching. This can be explained by the fact that geodesics are not completely invariant to the topology changes. Thereby, the extracted sequential curves that represent the trajectory tend to change the path on the models for certain motions and therefore mislead the matching performed by DTW.

We also apply our retrieval approach to a real captured 3D video sequence from the real dataset (3) described in Table 1 (3rd raw). Self similarity example with an actor in a walking motion (walking in circular way) and its similarity curve are shown in Fig. 20
                           . For the query clip presented at the right of the figure, retrieved clips are found correctly in the sequence when the actor is turning.

In this section, we firstly conducted multiple experimental trials by analyzing the video clustering method on two aspects: the pose-based clustering and the clip-based clustering. Secondly, we evaluate the impact of the summarization process on the retrieval system by comparing the results with and without using clustering.

The performance of the content-based summarization approach is evaluated for pose and clip data. To validate the pose-based summarization, we use a composed long sequence of a subject performing walk and squat motions from the dataset (3). For clip-based summarization experiment, the same 28 motions used for video segmentation and the retrieval have been used.

The effectiveness of clustering process is evaluated by the number of clusters found which should allow the identification of eventual redundant patterns. The threshold Th in the Algorithm 2 is set accordingly to the values of the similarity function. The distances computed between descriptors (EHC for pose and trajectory of EHC for clip) are normalized to return values in the range [0 1], and Th was then defined experimentally. An optimal setting of tTh should return a set of clusters similar to what a “hand-made” ground-truth classification would perform. Fig. 21
                            shows the clustering result obtained from the composed long sequence. The number of clusters decreases with the increase of the threshold Th. We obtain the best result for Th
                           =0.5 with 51 clusters partitioned as the bar diagram shown in the right of Fig. 21.

Pose-based clustering process can be improved by increasing the window size of the time filter as shown in Fig. 22
                           .

We notice from this figure that for a Th
                           =0.2, the number of clusters varies from 330 to 440 and a good compromise is obtained for Nt
                           =3.

Furthermore, clustering is applied on 14 motions extracted from the dataset (3) and performed by two actors (a man and a woman) in order to evaluate the efficiency of the clip-based clustering. By decreasing the threshold Th of the clustering algorithm, we obtain more clusters. Experimentally, we set Th to 0.43 and obtain 23 clusters from initially 110 clips for the first actor and 26 clusters for the second one (see Fig. 23
                           ). We notice that clips representing sprint or running steps are clustered together.

The video summarization process can be used efficiently in hierarchical structure, starting by video segmentation into clips, followed by clip-based clustering and then a pose-based clustering performed on the frames of all represented clusters of the clips resulting from the last step. The effectiveness of our summarization process is shown in Fig. 24
                            for the sequence of a real actor performing walking and squatting motion. From 500 frames segmented into 18 clips, the clustering process gives 6 clusters. The new subsequence containing 6 clips (most representative clip in each cluster) and 180 frames is then clustered into 41 clusters where each one represents a class of pose.

For a mesh model of 1MB size, the size of the 3D video sequence grows linearly of 1MB per frame. Hence, the video retrieval becomes very difficult in long sequences. Within our framework, we propose to combine the data clustering approach with the content-based retrieval in order to perform a hierarchical retrieval.

The clustering approach gathers models with similar poses/clips in clusters. If we consider the element of cluster as a pose, clusters are firstly performed over the entire sequence in order to gather frames with similar poses and then a template model is obtained for each cluster by computing its Karcher Mean as described in Section 4. The retrieval system can then be described as an hierarchical structure composed of two levels, the first one containing templates and the second one containing all models of the dataset. In view of this structure, a natural way is to start at the top, compare the query with the template of each cluster and proceed down the branch that leads to the closest shape.

We reconsider the same experiments for pose based retrieval in Section 3 by applying the hierarchical approach to the dataset summarized in Table 1 (1st raw). Each query model is compared to each one of the template models representing the clusters. The elastic measure values are used to generate a confusion matrix for all classes of pose, in order to evaluate the recognition performance by computing statistic retrieval measures thanks to the provided ground truth. The matrix of comparison in the first level (model-template comparison), is shown in Fig. 25
                           .

If we compare this matrix to that already obtained for the same dataset without the use of summarization (Fig. 10), you can easily notice the effectiveness of the summarization. The main advantage of this approach is the reduction of computation time which complexity pass from n to log(n) while keeping relevant information. Retrieval performances obtained from this matrix for FT, ST and E-measure are respectively 84.5%, 88.2% and 43.6%. Comparing these results to those in Table 2, a small improvement is achieved for classic retrieval scenario in term of second tier.

In terms of pose classification, the obtained accuracy is about 90.24%. Models of the class #2 are the most ones affected by misclassification and are assigned to the class #16. Looking at these two classes, we perceive that their poses are close to each other, both representing people with hands outstretched. The only difference is that one does with open legs and the other with closed ones.

Finally, we consider the element of cluster as a clip, where a video segmentation is firstly performed on the whole sequence. In this case, the template model is a “mean” clip obtained for each cluster of clips by computing its Karcher Mean (see Algorithm 1). The retrieval system can then be viewed as above with hierarchical structure. As experimental test, we performed a similar experimentation on the 14 motions performed by two actors as already evaluated in Section 7.3. In this experimentation, each query is a clip compared to each one of the template models representing the clusters of clips. The similarity measure values obtained by DTW algorithm between clips are used to generate a confusion matrix for all classes of clips, in order to evaluate the recognition performance by computing statistic retrieval measures thanks to the provided ground truth. The matrix of comparison in the first level (model-template comparison) is shown in Fig. 26
                           .

Retrieval performances obtained from this matrix for FT, ST and E-Measure are respectively 84.09%, 95.83% and 55.26%. In term of clip classification, obtained accuracy is about 93.75%. The analysis of the result given by the binarized matrix shows that the most misclassified clips are those of “fast run” class. In fact, they are assigned to class template representing “sprint” motion class.

@&#DISCUSSION@&#

The advantages of using EHC to represent human pose and motion in our approach include: (1) invariance to affine transformation (2) possibility to compute mean poses and mean clips (3) the use of well defined measure for pose comparison in Reimannian manifold and (4) the use of well established algorithm of DTW to align sequences taking benefits from the temporal aspect of curves.

However, this representation has some limitations. Firstly, EHC depends on the accuracy of extremities (head and limbs) extraction and on the definition of the path connecting end-points. In fact, the extraction of end-points and extremal curves is based on the definition of geodesic distance between each pair of curves. Thus, geodesic distances play an important role in our geometric representation of the human body shape. However, they are sensitive to significant topology changes as shown in Fig. 27
                     . In this figure, only 4 extremities are successfully detected and the left hand extremity is missed. Thus, information about position of this hand is lost. In the future, other strategies will be investigated for the extremities extraction step and shortest path detection on the mesh by using diffusion or commute time distances as presented by Elkhoury et al. [15] and Sun et al. [52].

Secondly, we note that our curve extraction can be sensitive to loose clothes. For example, the mesh represented in Fig. 27 shows a girl wearing a skirt and the shape of the curve connecting her feet is different from the same curve extracted on her mesh if she were wearing a trouser. This problem will be even more critical if she wears a long skirt.

Thirdly, a prior knowledge on the direction of the posture of the human body for the starting frame in video sequence is used to distinguish between left/right hand and foot. Other feature matching algorithms, like Heat Kernel Signature as proposed by Sun et al. [52] and Zheng et al. [53], could be used in future work to correctly identify the right from the left side.

@&#CONCLUSION@&#

In this work, we have proposed a unified framework able to represent human body shape with a pose descriptor, as well as a sequence of frames with a specific representation. This framework relies on an extremal human curve (EHC) descriptor, based on extremal features and geodesics between each pair of them. This descriptor has the advantage of being a skeletal representation, which is trackable over time. It is also an extremal descriptor of the surface deformation which is composed by a collection of local open-3D-curves. The representation of these curves and the comparison between them are performed in the Riemannian shape space of open curves. By this way, we have chosen to represent the pose of a mesh regardless to its rotation, translation and scale. Convoluted with a time filter to incorporate the motion, it becomes a temporal descriptor for pose retrieval. The degree of motion using feature vector, extracted from this descriptor, is then used for splitting continuous sequence into elementary motion segments called clips. Each clip describing an atomic movement is characterized by EHC representation associated to human mesh. The open curves in 3D space, which are the elements of EHC representation, are viewed as a point in the shape space of open curves and hence each clip is represented by a trajectory on this space. Dynamic time warping is used to align different trajectories and give a similarity score between each two clips.

The quality of our descriptor regarding the recognition performance of shape similarity in 3D video is analyzed and verified also by comparison with other related recent techniques. Moreover, our approach achieves a performance accuracy of 93.44% for video retrieval as second tier, which is encouraging. Finally, we will investigate 3D human action recognition and semantic activity analysis based on this framework.

@&#ACKNOWLEDGMENTS@&#

This research program is supported partially by the Region Nord-Pas de Calais (France).

@&#REFERENCES@&#

