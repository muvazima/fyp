@&#MAIN-TITLE@&#Improved sentence retrieval using local context and sentence length

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We extend the TF–ISF method to use local context.


                        
                        
                           
                           We extend the TF–ISF method to promote retrieval of long sentences.


                        
                        
                           
                           Context and promoting retrieval of long sentences both improves sentence retrieval.


                        
                        
                           
                           We also combine using context and promoting retrieval of long sentences.


                        
                        
                           
                           It is useful to use at the same time context and promoting retrieval of long sentences.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sentence retrieval

TF–ISF

Context

Sentence length

@&#ABSTRACT@&#


               
               
                  In this paper we propose improved variants of the sentence retrieval method TF–ISF (a TF–IDF or Term Frequency–Inverse Document Frequency variant for sentence retrieval). The improvement is achieved by using context consisting of neighboring sentences and at the same time promoting the retrieval of longer sentences. We thoroughly compare new modified TF–ISF methods to the TF–ISF baseline, to an earlier attempt to include context into TF–ISF named tfmix and to a language modeling based method that uses context and promoting retrieval of long sentences named 3MMPDS. Experimental results show that the TF–ISF method can be improved using local context. Results also show that the TF–ISF method can be improved by promoting the retrieval of longer sentences. Finally we show that the best results are achieved when combining both modifications. All new methods (TF–ISF variants) also show statistically significant better results than the other tested methods.
               
            

@&#INTRODUCTION@&#

The task of sentence retrieval is finding relevant sentences from a document base in response to a query. Sentence retrieval is used in tasks like novelty detection, question answering, summarization and opinion mining (Fernandez, Losada, & Azzopardi, 2010; Murdock, 2006). Sentence retrieval can be used as the first step of novelty detection. The possible application scenario is presented in (Harman, 2002). In this scenario the user uses a smart “next” button that allows him to walk down a ranked list of documents by highlighting next relevant and novel sentence. This way the user avoids non-relevant and duplicate information and saves time moving through the document collection. When used for question answering, sentence retrieval is used to find sentences that contain the answer to the user’s question. For example if the question is “How far is it from Earth to Mars” the aim is to find sentences like “The minimum distance from Earth to Mars is about 55 million kilometers”. Giving an answer in the form of a set of sentences is an improvement over the classic document retrieval where the user has to exploit whole documents. We expect that such functionality will be common in future search engines. When using for summarization sentence retrieval is used to find a number of sentences relevant to a query to create a summary of documents. For example sentence retrieval is used to create summaries from Wikipedia articles (Ganguly, Leveling, & Jones, 2012). In (Chen & Verma, 2006) a document summarization system is built specialized for medical domain which retrieves and summarizes up-to-date medical information from trustworthy online sources according users queries. We see that sentence retrieval can be used in various ways to simplify the end user task of finding the right information from document collections.

Methods used for sentence retrieval are usually simple adaptations of document retrieval methods where sentences are treated as documents (Harman, 2002; Soboroff, 2004; Soboroff & Harman, 2003). The state of the art and most successful models for sentence retrieval are the vector space model (Allan, Wade, & Bolivar, 2003; Fernandez et al., 2010; Zhang, Xu, Bai, Wang, & Cheng, 2004) and the language modeling approach. Language modeling based methods were improved by taking into account local context made of surrounding sentences or the whole document, (Fernandez et al., 2010; Murdock, 2006). Attempt of improving the TF–ISF (Term Frequency–Inverse Sentence Frequency) method by taking into account context by Fernandez et al. was unsuccessful and had no statistically significant improvements (Fernandez et al., 2010).

In addition to context usage, there is a new modification called “the importance of the sentence within the topic of document” or p(d|s) that managed to improve different language modeling based methods (Fernandez et al., 2010). The improvement appeared because of promoting the retrieval of long sentences (Fernandez et al., 2010). Our hypothesis is that it would be valuable to also try to apply modifications that would use local context of sentences and would promote retrieval of longer sentences to the TF–ISF method which showed good results in the past (Allan et al., 2003; Fernandez & Losada, 2009; Losada & Fernandez, 2007). The first modification consists of using local context (previous and next sentence) and second one is a component that promotes the retrieval of longer sentences. Related work is presented in Section 2. The corresponding new methods are explained in Sections 3 and 4. Other tested state of the art methods are explained in Section 5. In Section 6. we compare our new methods to other state of the art methods with good results. We conclude this paper with the conclusion in Section 7.

@&#RELATED WORK@&#

Sentence retrieval methods are usually simple adaptations of document retrieval methods where sentences are treated as documents (Harman, 2002; Soboroff, 2004; Soboroff & Harman, 2003). One of the first and most successful methods for sentence retrieval is the TF–ISF or Term Frequency–Inverse Sentence Frequency method (Allan et al., 2003) which is a trivial adaptation of the TF–IDF method for document retrieval. TF–IDF is a numerical statistic descriptor which indicates how important a word is to a document in a collection. It assumes that a word is important to a document if it often appears in the document and at the same time rarely appears in the collection. The sentence retrieval method TF–ISF was shown to outperform other methods like BM25 based methods or language modeling based methods (Allan et al., 2003; Fernandez & Losada, 2009; Losada & Fernandez, 2007). Another typically used method for sentence retrieval is the query likelihood method which is language modeling approach to the document retrieval. That method, invented by Ponte and Croft for document retrieval (Ponte & Croft, 1998) was in a trivial manner adapted and often used for sentence retrieval (Murdock, 2006). The TF–ISF method and the query likelihood method are considered as baseline methods for sentence retrieval in (Fernandez et al., 2010; Murdock, 2006).

In recent works (Fernandez et al., 2010; Murdock, 2006) query likelihood was modified to take into account the local context of the sentence. Due to the sparsity of sentences there is little overlap between the query and the sentence which negatively affects the performance of sentence retrieval (Fernandez et al., 2010). The assumption is that this problem can be partially solved by using the local context of sentences.

The idea that “good” sentences come from “good” documents was proposed by Murdock (2006). So the query likelihood method was improved using local context in form of the document the sentence came from. A mixture model which combines a sentence language model, document language model and collection language model was proposed. The method showed better results when compared to the query likelihood baseline (Murdock, 2006).

The same method as in (Murdock, 2006) was also tested in (Fernandez et al., 2010). In (Fernandez et al., 2010) the method was called Three-mixture model (or 3MM). Two types of context were tested: the document that contains the sentence and surrounding sentences (previous, current and next sentence). The 3MM method, additionally extended with the estimation of the importance of the sentence within the topic of the document (or p(d|s) where d is the document and s is the sentence), showed best results (even better than TF–ISF) along with other similar methods tested in (Fernandez et al., 2010). It was also shown that the p(d|s) component promotes retrieval of longer sentences and that the improvements arise due this effect.

A method similar to 3MM was also used for the CADIAL search engine with the difference that the unit of retrieval was not a sentence but an element of a semi-structured XML document (Mijić, Moens, & Dalbelo Bašić, 2009).

Due to the repeated use (Fernandez et al., 2010; Mijić et al., 2009; Murdock, 2006) and good results when combined with p(d|s) (Fernandez et al., 2010) the 3MM method is included into our tests in Section 6. This method serves as a baseline method that uses both, a context and an element for promoting retrieval of longer documents (p(d|s)).

Fernandez et al. also tried to improve the TF–ISF method by modifying it to take into account the local context (Fernandez et al., 2010). Two types of local context, document that contains the sentence and surrounding sentences (previous, current and next sentence), were tested again. They tried to modify the TF part to take into account the number of occurrences of term in the context and also tried to compute the ISF part at document level rather than at sentence level. The tests did not show consistent and significant improvements. Despite of the negative result we included into our tests a variant of the TF–ISF with context called tfmix from (Fernandez et al., 2010). The tfmix method uses context consisting of surrounding sentences (previous, current and the next sentence) which is similar to context used in our new method. In our tests the tfmix method served as a baseline method that combines TF–ISF with context.

In (Doko, Štula, & Stipaničev, 2013) we showed that the TF–ISF method can be improved using local context of sentences when it comes to the MAP and R-precision measures. The method used in (Doko et al., 2013) is again introduced in Section 3.

All in all in this paper we compared our new TF–ISF based methods with local context and component that promotes retrieval of longer sentences with the following earlier methods:
                        
                           •
                           TF–ISF baseline (Allan et al., 2003; Losada, 2008), (a strong baseline that showed good results in earlier tests (Allan et al., 2003; Fernandez & Losada, 2009; Losada & Fernandez, 2007);


                              tfmix (Fernandez et al., 2010), (TF–ISF with context, could not improve the TF–ISF baseline in tests in (Fernandez et al., 2010));

Variant of the 3MM (Three mixture model) method (Fernandez et al., 2010; Murdock, 2006) that uses context and p(d|s), a component that promotes the retrieval of long sentences (this method is called 3MMPDS in this paper).

It should be noted that in (Fernandez et al., 2010) other methods besides 3MM were also tested with context (methods 2S and 2S-I) and with p(d|s) (methods 2S, 2S-I, JM and Dir) but the comparison to all of these methods is outside the scope of this paper. From the methods tested in (Fernandez et al., 2010) we chose 3MM because it was also used in other papers (Mijić et al., 2009; Murdock, 2006) and because that method showed similar good performance (when used with p(d|s)) as other best methods from (Fernandez et al., 2010).

From the previous examples we saw that some sentence retrieval methods were improved by using local context of sentences. An exception is the TF–ISF method. In (Doko et al., 2013) we showed that it is possible to improve the TF–ISF method by using local context that consists of the two neighboring sentences (previous and next sentence of the current sentence) and using a recursive ranking function. While in (Fernandez et al., 2010) it was tried to include the local context by modifying parts of the TF–ISF ranking function, we extended it with the estimate of relevance of the context to the query. Here we introduce the new method again.

The TF–ISF based ranking function for sentence retrieval is (Allan et al., 2003; Losada, 2008):
                        
                           (1)
                           
                              R
                              (
                              s
                              ∣
                              q
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       t
                                       ∈
                                       q
                                    
                                 
                              
                              log
                              (
                              
                                 
                                    tf
                                 
                                 
                                    t
                                    ,
                                    q
                                 
                              
                              +
                              1
                              )
                              log
                              (
                              
                                 
                                    tf
                                 
                                 
                                    t
                                    ,
                                    s
                                 
                              
                              +
                              1
                              )
                              log
                              
                                 
                                    
                                       
                                          
                                             n
                                             +
                                             1
                                          
                                          
                                             0.5
                                             +
                                             
                                                
                                                   sf
                                                
                                                
                                                   t
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where


                     
                        
                           •
                           
                              tft
                              
                              ,q
                               is the number of occurrences of term t in question q,


                              tft
                              
                              ,s
                               is the number of occurrences of term t in sentence s,


                              sft
                               number of sentences that contain term t,


                              n number of sentences in the collection.

As the local context of each sentence we are using the two neighboring sentences (the previous and next sentence of each sentence). We assume that the relevance of a sentence depends partly on the content of the sentence itself and partly on the content of the two neighboring sentences. The relevance of the neighboring sentences again depends partly on the content of their neighbors. Using these two assumptions we define the new recursive ranking function for sentence retrieval as follows:
                        
                           (2)
                           
                              
                                 
                                    R
                                 
                                 
                                    con
                                 
                              
                              (
                              s
                              ∣
                              q
                              )
                              =
                              (
                              1
                              -
                              μ
                              )
                              ·
                              R
                              (
                              s
                              ∣
                              q
                              )
                              +
                              μ
                              ·
                              [
                              
                                 
                                    R
                                 
                                 
                                    con
                                 
                              
                              (
                              
                                 
                                    s
                                 
                                 
                                    prev
                                 
                              
                              (
                              s
                              )
                              ∣
                              q
                              )
                              +
                              
                                 
                                    R
                                 
                                 
                                    con
                                 
                              
                              (
                              
                                 
                                    s
                                 
                                 
                                    next
                                 
                              
                              (
                              s
                              )
                              ∣
                              q
                              )
                              ]
                           
                        
                     where sprev
                     (s) depicts previous sentence of sentence s and snext
                     (s) next sentence of sentence s. Rcon
                     (sprev
                     (s)|q) and Rcon
                     (snext
                     (s)|q) represent the relevance of the previous and next sentence. Rcon
                     (sprev
                     (s)|q) is by definition 0 if s is first sentence in document and Rcon
                     (snext
                     (s)|q) is by definition 0 if s is last sentence in document. μ is a tuning parameter. In our tests in Section 7. the recursive function calls itself until three previous and three next sentences of the sentence s are involved. In other words three recurrences are used. After that no context is used i.e. Rcon
                     (s|q)=
                     R(s|q) and the recurrence stops. For better understanding and reproducibility purposes we present the non-recursive version of Eq. (2) as follows (Eq. (3)):
                        
                           (3)
                           
                              
                                 
                                    R
                                 
                                 
                                    con
                                 
                              
                              (
                              s
                              ∣
                              q
                              )
                              =
                              (
                              1
                              -
                              μ
                              )
                              S
                              +
                              μ
                              (
                              (
                              1
                              -
                              μ
                              )
                              
                                 
                                    P
                                 
                                 
                                    1
                                 
                              
                              +
                              μ
                              (
                              (
                              1
                              -
                              μ
                              )
                              
                                 
                                    P
                                 
                                 
                                    2
                                 
                              
                              +
                              μ
                              (
                              
                                 
                                    P
                                 
                                 
                                    1
                                 
                              
                              +
                              
                                 
                                    P
                                 
                                 
                                    3
                                 
                              
                              )
                              +
                              (
                              1
                              -
                              μ
                              )
                              S
                              +
                              μ
                              (
                              
                                 
                                    P
                                 
                                 
                                    1
                                 
                              
                              +
                              
                                 
                                    N
                                 
                                 
                                    1
                                 
                              
                              )
                              )
                              +
                              (
                              1
                              -
                              μ
                              )
                              
                                 
                                    N
                                 
                                 
                                    1
                                 
                              
                              +
                              μ
                              (
                              (
                              1
                              -
                              μ
                              )
                              S
                              +
                              μ
                              (
                              
                                 
                                    P
                                 
                                 
                                    1
                                 
                              
                              +
                              
                                 
                                    N
                                 
                                 
                                    1
                                 
                              
                              )
                              +
                              (
                              1
                              -
                              μ
                              )
                              
                                 
                                    N
                                 
                                 
                                    2
                                 
                              
                              +
                              μ
                              (
                              
                                 
                                    N
                                 
                                 
                                    1
                                 
                              
                              +
                              
                                 
                                    N
                                 
                                 
                                    3
                                 
                              
                              )
                              )
                              )
                           
                        
                     where S
                     =
                     R(s|q), Pi
                     
                     =
                     R(pi
                     |q), Ni
                     
                     =
                     R(ni
                     |q). p
                     1, p
                     2, p
                     3 denote the previous three sentences of sentence s and the n
                     1, n
                     2, n
                     3 denote the next tree sentences of sentence s as shown in Fig. 1
                     .

By using the recursive function, it is also possible to take into account all the neighboring sentences of the current sentence in a document. Tests, showing if it can help to include more than three previous and three next sentences into computation of the relevance of a sentence, are left for future work.

One benefit of our new TF–ISF variant in comparison to the tfmix method from (Fernandez et al., 2010) is the explicit modeling of the relevance of the context (the previous and the next sentence of the current sentence) to the query Rcon
                     (sprev
                     (s)|q)+
                     Rcon
                     (snext
                     (s)|q) while in (Fernandez et al., 2010) parts of the TF–ISF function are replaced with components related to the sentence and to the neighbor sentences (tft
                     
                     ,
                     
                        s
                      is replaced with 
                        
                           α
                           ·
                           
                              
                                 tf
                              
                              
                                 t
                                 ,
                                 s
                              
                           
                           +
                           (
                           1
                           -
                           α
                           )
                           
                              
                                 tf
                              
                              
                                 t
                                 ,
                                 context
                              
                           
                        
                      where α is a tuning parameter). Our approach allows better exploring of the influence of context to the sentence relevance. That is important for our future work where we want to automatically generate a document representation for sentence retrieval. Our function (Rcon
                     (s|q)) can be used to automatically create a useful structured representation of a textual document with a representation of context. Our aim is to use such representations in a web environment where structured document representations are stored at web servers along with the plain text documents. That means that a part of the computation needed for sentence retrieval will be converted to a document representation

We already mentioned that in (Fernandez et al., 2010) the probability of generating a document given the sentence (p(d|s)) was used to improve several language modeling based sentence retrieval methods. In (Fernandez et al., 2010) p(d|s) was regarded as a measure of the importance of the sentence within the topic of the document. Multiple methods (3MM,2S,2S-I,DIR,JM) were tested with p(d|s) and all of them showed similar good performance (Fernandez et al., 2010). There were no significant differences among them and at the same time all of them showed better performance than the TF–ISF and BM25 baselines. One important observation was that p(d|s) promotes the retrieval of longer sentences and that the improvements were due to this effect (Fernandez et al., 2010). We assume that the TF–ISF method can also be improved by adding a component that promotes retrieval of longer sentences. We simply assume that the relevance of a sentence is proportional to the ratio between current sentence length and average sentence length in document that contains the sentence. The new ranking function can be defined as follows
                        
                           (4)
                           
                              
                                 
                                    R
                                 
                                 
                                    length
                                 
                              
                              (
                              s
                              ∣
                              q
                              )
                              =
                              
                                 
                                    |
                                    s
                                    |
                                 
                                 
                                    AvgSenLength
                                    (
                                    d
                                    (
                                    s
                                    )
                                    )
                                 
                              
                              R
                              (
                              s
                              ∣
                              q
                              )
                           
                        
                     In the Eq. (4)
                  


                     
                        
                           •
                           |s| denotes the length of sentence s,


                              d(s) denotes the document that contains the sentence s,


                              AvgSenLength(d(s)) denotes the average sentence length in the document that contains the sentence s.

Eq. (4) raises the probability of retrieving long sentences by giving them extra weight. Specifically, the component 
                        
                           
                              
                                 |
                                 s
                                 |
                              
                              
                                 AvgSenLength
                                 (
                                 d
                                 (
                                 s
                                 )
                                 )
                              
                           
                        
                      has a high value for sentences that are long in regard to the average sentence length in the document. We can now combine Eqs. (2) and (4) to get a new ranking function that at the same time uses context of sentences and promotes the retrieval of longer sentences as follows (Eq. (5)):
                        
                           (5)
                           
                              
                                 
                                    R
                                 
                                 
                                    con
                                 
                                 
                                    length
                                 
                              
                              (
                              s
                              ∣
                              q
                              )
                              =
                              
                                 
                                    |
                                    s
                                    |
                                 
                                 
                                    AvgSenLength
                                    (
                                    d
                                    (
                                    s
                                    )
                                    )
                                 
                              
                              
                                 
                                    R
                                 
                                 
                                    con
                                 
                              
                              (
                              s
                              ∣
                              q
                              )
                           
                        
                     
                  

In addition to the already presented methods (Sections 3 and 4) we also included into our tests the tfmix method (Fernandez et al., 2010) and the Three mixture model (3MM) with importance of the sentence within the topic of the document (p(d|s)) (Fernandez et al., 2010; Murdock, 2006).

The tfmix method is defined in (Fernandez et al., 2010):
                        
                           (6)
                           
                              
                                 
                                    R
                                 
                                 
                                    tfmix
                                 
                              
                              (
                              s
                              ∣
                              q
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       t
                                       ∈
                                       s
                                       
                                          
                                             ∩
                                          
                                          
                                             q
                                          
                                       
                                    
                                 
                              
                              log
                              (
                              
                                 
                                    tf
                                 
                                 
                                    t
                                    ,
                                    q
                                 
                              
                              +
                              1
                              )
                              log
                              (
                              α
                              ·
                              
                                 
                                    tf
                                 
                                 
                                    t
                                    ,
                                    s
                                 
                              
                              +
                              (
                              1
                              -
                              α
                              )
                              
                                 
                                    tf
                                 
                                 
                                    t
                                    ,
                                    context
                                 
                              
                              +
                              1
                              )
                              log
                              
                                 
                                    
                                       
                                          
                                             n
                                             +
                                             1
                                          
                                          
                                             0.5
                                             +
                                             
                                                
                                                   sf
                                                
                                                
                                                   t
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where tft
                     
                     ,
                     
                        context
                      denotes the number of occurrences of term t in context. The context consists of the previous, current and next sentence.

The 3MM model with p(d|s) is defined as follows (Fernandez et al., 2010):
                        
                           (7)
                           
                              
                                 
                                    R
                                 
                                 
                                    3
                                    MMPDS
                                 
                              
                              (
                              s
                              ∣
                              q
                              )
                              =
                              p
                              (
                              d
                              ∣
                              s
                              )
                              ·
                              
                                 
                                    
                                       ∏
                                    
                                    
                                       t
                                       ∈
                                       q
                                    
                                 
                              
                              
                                 
                                    {
                                    λ
                                    p
                                    (
                                    t
                                    ∣
                                    s
                                    )
                                    +
                                    γ
                                    p
                                    (
                                    t
                                    ∣
                                    context
                                    (
                                    s
                                    )
                                    )
                                    +
                                    (
                                    1
                                    -
                                    λ
                                    -
                                    γ
                                    )
                                    p
                                    (
                                    t
                                    )
                                    }
                                 
                                 
                                    
                                       
                                          tf
                                       
                                       
                                          t
                                          ,
                                          q
                                       
                                    
                                 
                              
                           
                        
                     The above ranking function (Eq. (7)) is similar to the one presented in (Murdock, 2006) with some differences. Instead of p(t|d), p(t|context(s)) is used where context(s) denotes the previous, current and next sentence of sentence s. p(d|s) is used which is called measure of the importance of the sentence within the topic of the document and is defined as follows (Fernandez et al., 2010):
                        
                           (8)
                           
                              p
                              (
                              d
                              ∣
                              s
                              )
                              =
                              
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          t
                                          ∈
                                          s
                                       
                                    
                                    p
                                    
                                       
                                          (
                                          t
                                          ∣
                                          d
                                          )
                                       
                                       
                                          
                                             
                                                tf
                                             
                                             
                                                t
                                                ,
                                                s
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          t
                                          ∈
                                          s
                                       
                                    
                                    p
                                    
                                       
                                          (
                                          t
                                          )
                                       
                                       
                                          
                                             
                                                tf
                                             
                                             
                                                t
                                                ,
                                                s
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     In Eqs. (7) and (8) 
                     
                        
                           p
                           (
                           t
                           ∣
                           s
                           )
                           =
                           
                              
                                 
                                    
                                       tf
                                    
                                    
                                       t
                                       ,
                                       s
                                    
                                 
                              
                              
                                 |
                                 s
                                 |
                              
                           
                        
                     , 
                        
                           p
                           (
                           t
                           ∣
                           context
                           (
                           s
                           )
                           )
                           =
                           
                              
                                 
                                    
                                       tf
                                    
                                    
                                       t
                                       ,
                                       context
                                       (
                                       s
                                       )
                                    
                                 
                              
                              
                                 |
                                 context
                                 (
                                 s
                                 )
                                 |
                              
                           
                        
                     , 
                        
                           p
                           (
                           t
                           )
                           =
                           
                              
                                 
                                    
                                       tf
                                    
                                    
                                       t
                                       ,
                                       coll
                                    
                                 
                              
                              
                                 |
                                 coll
                                 |
                              
                           
                        
                      where tft
                     
                     ,
                     
                        s
                     , tft
                     
                     ,
                     
                        context
                     
                     (
                     
                        s
                     
                     ) and tft
                     
                     ,
                     
                        coll
                      denotes the number of occurrences of term t in sentence s, context context(s) and collection coll respectively. |s|, |context(s)| and |coll| denotes the length of the sentence s, context context(s) and collection coll respectively.

@&#RESULTS AND DISCUSSION@&#

An overview of all tested methods in this paper is shown in Table 1
                     .

The origin of each method from Table 1 is illustrated in Fig. 2
                     .

We tested all sentence retrieval methods (Table 1) using data from the TREC Novelty tracks which are series of competitions used to test novelty detection systems. There were three TREC Novelty Tracks in the years from 2003 to 2004 (Harman, 2002; Soboroff, 2004; Soboroff & Harman, 2003). The task was novelty detection which consists of two subtasks, finding relevant sentences and finding novel sentences. We are only interested in finding relevant sentences i.e. sentence retrieval. Sentence retrieval is an important part of novelty detection. Allan (Allan et al., 2003) showed that the performance of novelty detection depends on the quality of the performance of sentence retrieval.

In each of the three Novelty Tracks in the years 2002, 2003 and 2004 the task was as follows: given a topic or query (topic is the term used for query at TREC) and an ordered list of documents find relevant and novel sentences. In each Track participants got a set of 50 topics where each topic consisted of titles, descriptions and narratives (Table 2
                     ). They also got a list of mostly relevant documents and a list of sentence level relevance judgments.

In TREC 2002 the topics from ad hoc Tracks (previous TREC tracks that deal with document retrieval) were used. 25 most relevant documents were assigned to each topic. If the topic had 25 or more relevant documents, only 25 relevant documents were used. If the topic had less than 25 documents, non-relevant documents were added to reach the number of 25 documents. Two files with sentence relevance judgments were provided. One with 2% of sentences judged relevant and one with 7% of sentences judged relevant. We used the file with 7% of sentences judged relevant in our tests.

In TREC 2003 topics where constructed specially for the Novelty track. For every topic 25 relevant documents were chosen. 37.56% of sentences were judged relevant.

In TREC 2004 between 25 and 100 documents were chosen with 25 of them relevant. 16.2% of sentences were judged relevant.

An example of a topic from the TREC 2002 Novelty track is shown in Table 2.

To evaluate our three new methods (TF–ISFcon,TF–ISFlength,TF–ISFcon,length) we compare them to the following earlier methods
                        
                           •
                           TF–ISF baseline (Allan et al., 2003; Losada, 2008), a strong baseline that showed good results in earlier tests (Allan et al., 2003; Fernandez & Losada, 2009; Losada & Fernandez, 2007) (Eq. (1)),


                              tfmix (Fernandez et al., 2010), an earlier try to improve TF–ISF with context that could not improve the TF–ISF baseline in tests in (Fernandez et al., 2010) (Eq. (6)),

3MM method with p(d|s) (Fernandez et al., 2010; Murdock, 2006) that uses context and p(d|s), a component that promotes the retrieval of long sentences, that together with some other language modeling based methods showed highest performance in (Fernandez et al., 2010).

In the experiments we partially used Rapidminer,
                        2
                        
                           http://rapid-i.com/content/view/181/196/.
                     
                     
                        2
                      an open-source system for data mining, with Text Extension
                        3
                        
                           http://rapid-i.com/content/view/202/206/.
                     
                     
                        3
                      that allows doing text preprocessing and using the vector space model. With Rapidminer all upper cases were transformed to a lower case, standard stop words were removed. Stemming was not applied. Results from Rapidminer were presented as a web service and further used in a custom program that implemented the sentence retrieval methods.

We used short queries consisting mostly of two or three key words from the title field (Table 2). We measured the performance using the standard precision oriented measure P@10 and recall oriented measures MAP, and R-precision. The same performance measures were used in similar papers (Fernandez et al., 2010, Murdock, 2006). For further reading about the measures please refer to (Manning, Raghavan, & Schuetze, 2008). To compare the difference between two methods we used two tailed paired t-test with significance level α
                     =0.05 identically to related work (Fernandez et al., 2010; Murdock, 2006). The t-test showed highly reliable when testing information retrieval systems (Sanderson & Zobel, 2005) and also showed robust to violations of its normality assumption (Hull, 1993).

Several ranking functions require tuning of the parameters, so we employed a train-test methodology similar to (Fernandez et al., 2010) and (Doko et al., 2013). We experimented with three training–testing configurations using TREC Novelty track data as follows:
                        
                           •
                           Training with TREC 2002 and testing with TREC 2003 and TREC 2004.

Training with TREC 2003 and testing with TREC 2002 and TREC 2004.

Training with TREC 2004 and testing with TREC 2002 and TREC 2003.

Training was performed to find the value of parameters μ, α, λ, γ, for which the corresponding methods show best performance. During each of the tree trainings (TREC 2002, 2003, 2004) we tried values from 0.0 to 1.0 in steps of 0.05 for each of the parameters. The best parameter values were fixed in order to apply it to the two remaining data sets. During training we measured the performance of the system by using Mean average precision (MAP). Table 3
                      shows the optimal values of parameter μ for the corresponding methods (TF–ISFcon, TF–ISFcon,length) and datasets.


                     Table 4
                     . shows optimal parameter values of methods tfmix and 3MMPDS.

The next tables and graphs (Tables 5–7
                     
                     
                      and Figs. 3–5
                     
                     
                     ) show testing of the tuned methods for the three training–testing configurations. In Tables 5–7 statistically significant differences in comparison to the TF–ISF baseline are marked with an asterisk. Statistically significant differences in comparison to the tfmix method are marked with a †. Statistically significant differences in comparison to the 3MMPDS method are marked with an m.

The tfmix method showed similar performance as in (Fernandez et al., 2010) with no statistically significant improvements of the baseline TF–ISF.

The 3MMPDS method showed mostly worse performance than the TF–ISF and tfmix. Precisely, 3MMPDS showed statistically significant worse performance than TF–ISF and tfmix
                     
                        
                           •
                           according P@10 in 18 out of 18 cases,

according MAP in 2 out of 18 cases,

according R-Precision 1 out of 18 cases.

3MMPDS also showed statistically better performance than TF–ISF and tfmix according MAP in 2 out of 18 cases.

The TF–ISFcon method showed mostly statistically significant better results according MAP and R-precision and competitive results according P@10 in comparison to methods TF–ISF and tfmix. The TF–ISFcon method showed always statistically significant better results according to P@10, MAP and R-precision in comparison to 3MMPDS.

The TF–ISFlength method showed mostly statistically significant better results according to all of the measures (P@10, MAP, R-Precision) in comparison to all methods TF–ISF, tfmix and 3MMPDS.

The TF–ISFcon,length method always showed statistically better results according all of the measures (P@10, MAP, R-Precision) in comparison to methods TF–ISF, tfmix and 3MMPDS.

To better analyze the effect of including local context and the effect of promoting longer sentences we additionally compared only the TF–ISF baseline and the three new methods. The results are shown in Tables 8–10
                     
                     
                      and as graphs in Figs. 6–8
                     
                     
                     . In Tables 8–10 statistically significant differences in comparison to the TF–ISF baseline are marked with an asterisk. Statistically significant differences in comparison to the TF–ISFcon method are marked with a †. Statistically significant differences in comparison to the TF–ISFlength method are marked with an L.

According to the presented results (Tables 8–10 and Figs. 6–8) the TF–ISFcon mostly has statistically significant better results according to MAP and R-precision in comparison to the baseline (according to MAP in 6 out of 6 cases and according to R-Precision in 5 out of 6 cases).

We also see from experimental results that the TF–ISFlength mostly has statistically significant better results according to all tested measures (P@10, MAP, R-precision) in comparison to the baseline TF–ISF method (according to P@10 in 4 out of 6 cases, according to MAP in 6 out of 6 cases and according to R-precision in 6 out of 6 cases).

We also see from Tables 8–10 that the TF–ISFcon,length always has statistically significant better results according to all tested measures (P@10, MAP, R-precision) in comparison to the baseline TF–ISF method.

It is important to notice that the TF–ISFcon,length method also has statistically better results according to MAP and R-precision in comparison to the methods TF–ISFcon and TF–ISFlength. What can be concluded from this? Method TF–ISFcon improves the baseline according to MAP and R-precision. Method TF–ISFlength also improves the baseline according to MAP and R-precision. However when we combine the modification of the baseline from TF–ISFcon (i.e. using context) and the modification from TF–ISFlength (i.e. promoting retrieval of longer sentences) into a new method TF–ISFcon,length we get even a statistically significant better result according to MAP and R-precision then each of the two methods that use only one modification. That means that the positive effect of using context and the positive effect of promoting retrieval of longer sentences sum up. The one effect does not overlap with the effect of other. In other words it is useful to use local context and at the same time promote retrieval of longer sentences.

When it comes to the P@10 measure the TF–ISFcon has competitive results (no statistically significant differences) and the TF–ISFlength has statistically significant improved results in comparison to the baseline TF–ISF method. The TF–ISFcon,length method has similar improvements as TF–ISFlength in comparison to the baseline. We see again that the positive effects of using context and promoting retrieval of longer sentences do not overlap, on the contrary they sum up.

So we conclude the following
                        
                           •
                           It can be useful to use context for sentence retrieval.

It can be useful to promote retrieval of longer sentences for sentence retrieval.

It can be useful to combine previous two.

The measures MAP and R-precision by which our new methods (TF–ISFcon, TF–ISFlength, TF–ISFcon,length) show better performance are recall oriented. We saw that the improvement according to MAP and R-precision comes from two different modifications of the baseline TF–ISF method (i.e. including context and promoting retrieval of longer sentences). Including context promotes sentences that do not have many terms in common with the query but the context have some terms in common with the query which increases recall. When it comes to promoting the retrieval of longer sentences the reason for improvements according to MAP and R-precision may lie in the fact that relevant sentences chosen from the assessors are in average longer than non-relevant sentences (Fernandez et al., 2010). Recall is important for the application scenario presented in (Fernandez et al., 2010; Harman, 2002). In the scenario a user uses a smart “next” button which allows him to walk down a ranked list of documents by highlighting only relevant (and novel) sentences. Having access to all relevant documents is also important for multi-document summarization (Fernandez et al., 2010).

When it comes to the precision oriented measure P@10 we have better results when using methods TF–ISFlength and TF–ISFcon,length because of promoting the retrieval of longer sentences. Again (as with MAP and R-precision) it may come from the fact that relevant sentences are on average longer than not relevant sentences. The measure P@10 is important when using sentence retrieval for tasks that require high precision like question answering.

@&#CONCLUSION@&#

In this paper we have implemented two improvements for TF–ISF method for sentence retrieval that were shown useful in methods based on language modeling approach. In our earlier paper (Doko et al., 2013) we successfully improved the TF–ISF method using local context and called the new method TF–ISFcon. We described this method again. Additionally, we introduced a new method named TF–ISFlength that promotes the retrieval of long sentences taking into account current sentence length in comparison to average sentence length in document. A second new method named TF–ISFcon,length was also proposed that combines the modifications of the previous two. All methods were compared against the state of the art methods: TF–ISF baseline, the tfmix method (an earlier attempt to include context into TF–ISF presented in (Fernandez et al., 2010)) and 3MMPDS (Three mixture model with p(d|s)). All new methods showed statistically significant improvements in comparison to the state of the art methods. TF–ISFcon,length showed best results among all tested methods. It was also shown that the method TF–ISFcon,length summed up the positive effects of the individual methods TF–ISFcon and TF–ISFlength. Through this we showed that it can be useful to use both, context of sentences and promoting the retrieval of longer sentences at the same time and that the positive effect of one modification does not overlap with the positive effect of the other which was not clear in (Fernandez et al., 2010).

Taking into account the presented results we can say that the main contributions of this paper are as follows:
                        
                           •
                           Improvement of the TF–ISF baseline by promoting the retrieval of longer sentences.

Finding that the positive effects of two modifications (using context and promoting the retrieval of longer sentence) do not overlap. In fact they sum up and it is useful to use both of them at the same time.

New method TF–ISFcon,length that showed statistically significant better results according to P@10, MAP and R-precision in comparison to state of the art methods TF–ISF, tfmix, 3MMPDS.

@&#REFERENCES@&#

