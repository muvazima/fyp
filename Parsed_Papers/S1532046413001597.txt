@&#MAIN-TITLE@&#Compass: A hybrid method for clinical and biobank data mining

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Traditional hypotheses testing is not ideal for knowledge discovery in large data.


                        
                        
                           
                           We developed an approach for data mining intended as a hypothesis generating tool.


                        
                        
                           
                           Our approach is able to handle incomplete information.


                        
                        
                           
                           The model can handle both categorical type questionnaire data and continuous variables.


                        
                        
                           
                           The model generates variable groups acting as “hotspots” for significant associations.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Data mining

Clinical data

Rule extraction

Self-Organizing Map

Association mining

@&#ABSTRACT@&#


               
               
                  We describe a new method for identification of confident associations within large clinical data sets. The method is a hybrid of two existing methods; Self-Organizing Maps and Association Mining. We utilize Self-Organizing Maps as the initial step to reduce the search space, and then apply Association Mining in order to find association rules. We demonstrate that this procedure has a number of advantages compared to traditional Association Mining; it allows for handling numerical variables without a priori binning and is able to generate variable groups which act as “hotspots” for statistically significant associations. We showcase the method on infertility-related data from Danish military conscripts. The clinical data we analyzed contained both categorical type questionnaire data and continuous variables generated from biological measurements, including missing values. From this data set, we successfully generated a number of interesting association rules, which relate an observation with a specific consequence and the p-value for that finding. Additionally, we demonstrate that the method can be used on non-clinical data containing chemical–disease associations in order to find associations between different phenotypes, such as prostate cancer and breast cancer.
               
            

@&#INTRODUCTION@&#

Due to the existence of large amounts of data, such as biobank data, collected by scientific groups over the years, there is a growing interest in mining such data for the purpose of new knowledge discovery [1,2]. Traditional hypotheses testing approaches are typically not ideal in more comprehensive data mining efforts aiming for new and unexpected patterns due to the immensely large search space, particularly in high-volume data sets [3].

Methods for unsupervised data mining have commonly been employed in market basket analysis and fall under the category of Association Mining (AM). The main goal of AM in market basket analyses is to find interesting associations of the form “{chips}→{beer}” which would indicate that people who buy chips are likely to also buy beer. More complex rules involving more items may also be formed such as “{ham+cheese}→{milk+bread}, i.e. those who buy ham and cheese are likely simultaneously to also buy milk and bread. In these rules items appearing on the left side of the arrow are called antecedent, while items on the right side are called consequent. As market basket data sets tend to be large, and the number of possible combinations between items may be extremely high, the central effort in this type of mining has been to restrict the search space in a sensible way.

The concept of association rules was popularized by Agrawal et al. [4], although the concept may have already been created as far back as 1966 [5]. AM has traditionally consisted of two steps: first to find frequent item sets, and second to generate rules by calculating the confidence, which may give an estimate of the interestingness of the rule. Frequent item sets are collections of items, such as “{bread, milk, butter}”, which appear together (customers often buy these together) more often than a specified threshold called support. The confidence indicates the probability of the consequent given the antecedent in a given rule, say “{bread}→{milk, butter}”, and is often used to restrict which rules are retained. There are other measures of interestingness besides confidence, such as lift, leverage and conviction, which we will not discuss here.

The various types of existing AM methods typically address relatively simple dichotomous data sets containing only 1s and 0s. Applying AM to mine other types of data, such as clinical data, has been done in several, previous studies [6]. These approaches utilized the support measure to control the size and shape of the search space of associations. However, restricting the search space by applying one strict minimum support threshold, thus generating frequent item sets, as has traditionally been done in AM, is not necessarily an optimal strategy when analyzing clinical data. The reason is primarily due to the nature of such data, where some features and associations, may only be present in a subset of samples, for instance certain types of tests that only apply to certain diseases. When generating frequent item sets, if setting the support threshold too high, there is a risk that one will not find associations which contain these non-frequent, but interesting, features. However, if setting the threshold low enough to detect these associations, the combinatorial explosion of the number of rules generated may prevent detailed manual inspection.

In this paper we present Compass, a new hybrid approach using a combination of Self-Organizing Maps (SOM) and AM, which is suitable for mining unexpected patterns in clinical data and other similar types of data. In this approach, SOM is applied as a first step before the application of AM. The reason for applying SOM is that it acts as a navigating pointer to areas in the search space where one is more likely to find statistically significant association rules, hence acting as a Compass to reduce the search space.

SOM, also known as Kohonen neural networks, is an unsupervised neural-network method which clusters the data into a set of interconnected nodes [7,8]. These nodes are typically arranged in two-dimensional maps with rectangular or hexagonal grids, but may also be arranged in multi-dimensional maps with other types of grids. The training process is started by randomly assigning to each node a model vector, which has the same dimensionality as the data samples (i.e. the same number of variables). All samples are then iteratively assigned to the nodes whose’ model vector is most similar (often measured with Euclidian distance), and gradually regressing the model vectors towards each sample that is assigned. At the end of the training process, each node will have a number of samples (clusters) assigned to it as well as a model vector. The patterns of interest are extracted from the model vectors. Since the time of the introduction of SOM in 1982, different variants of this method have found their way into many practical applications in different scientific fields including biology, economics and physics [9]. The idea of combining SOM with association rule mining is not new [10]. However, previous studies did not fully explore the potential of this approach on complex medical data. In particular, they did not explore the ability of SOM to address two important challenges in the context of association mining; handling numerical variables and dealing with a large output of association rules to find the most interesting and unexpected ones.

Our method can handle numerical variables without resorting to a priori binning, i.e. grouping numerical variables into classes before the analysis. Such grouping is common when studying for example income, wherein people are assigned to a number of arbitrary and discrete classes depending on their level of income. While previous studies have explored novel ways to deal with numerical variables without binning, to our knowledge these approaches rely on setting a minimum support cut-off to find frequent item sets [14]. Our approach makes it also much easier by manual inspection to identify new, interesting and unexpected patterns by introducing the concept of Associative Variable Groups (AVGs). AVGs are simply groups of variables which are much more likely to contain statistically significant association rules than any variables randomly chosen from the data, and which arise naturally using SOM as the initial step. Instead of generating a long list of possibly thousands of association rules as the output, our method generates a list of AVGs, which drastically reduces the size of the output and makes it much more comprehensible for subsequent manual inspection.

@&#METHODS@&#

For the analysis we used human fertility related data generated by the Jørgensen group [11], containing questionnaire data as well as biological measurements, such as sperm concentration, from Danish military conscripts. The Danish National Committee on Biomedical Research Ethics, Copenhagen Region, had approved the research on the conscripts (protocol no H-KF-289428), and all young men had given written informed consent. We obtained two, independent, medium sized data sets on the conscripts. We used one data set for training, and the other as a test set to validate associations that were found on the training data. The original training set included 1444 samples, and 163 variables. The original test set included 2532 samples; the corresponding 163 variables also present in the training set were extracted and included in the analysis. The raw data sets were subjected to an extensive cleaning process, which is summarized in Table 1
                        .

Both the training set and test set contained missing values. In the training set, 20% of the total data was missing, both among the numeric and categorical variables. A large number of missing values among the categoric variables tended to occur among a select few variables, whereas among the numeric variables the missing values tended to be more spread out. The test set, which consisted of data collected between 6 and 8years later than the training set, had fewer missing values than the training set. Among numerical variables, roughly half as many values were missing compared to the training set, and among categorical variables one third as many values were missing.

Variables with more than 90% missing values were removed. Moreover, variables that were deemed either irrelevant or highly unreliable by the clinicians who generated the data were also removed.

Variables representing times and dates were converted into scalars, reflecting time in years from a reference time point. Geographical data were provided in the form of zip codes, which was subsequently converted into latitudes and longitudes [16].

New variables were added from performing operations on other existing variables such as the age of each military conscript, which was calculated by taking the difference between “date of questionnaire fill-in” and “birth date”.

Numerical values were scaled to a mean value of zero and standard deviation of one, while categorical variables were converted into binary form with 1s and 0s. As the majority of the categorical variables contained more than one attribute value, each categorical variable had to be “expanded” such that each attribute value was represented by one new variable. As an example, consider the variable “Smoking” with the possible attribute values “Yes” and “No”. From this variable, two new variables are generated; “Smoking-Yes” and “Smoking-No”. Thus, 67 categorical variables were converted into a total of 184 attribute variables. Categorical variables with non-informative attribute values, with less than 20 samples coded as 1’s, or more than 90% samples coded as 1’s, were removed. Moreover, we merged the binary categorical variables that were identical above a cut-off of 95% identity. In our case, we found for example that a low disposition in one testis strongly implied a low disposition in the other testis, thus making one of these variables redundant. Nearly identical variables like these are bound to end up together in many association rules, but do not confer any new or interesting information. The cleaned data set included 95 categorical variables encoded in binary form. Some contradictions and errors in the data were discovered and removed. Examples included people who claim to be non-smokers but then subsequently specify that they smoke more than 0 cigarettes per day. The resulting cleaned data had 145 variables. No samples were removed from either the training or test sets.

We extracted chemical–disease annotations from the Comparative Toxicogenomics Database (CTD) [12]. The CTD contains direct and inferred chemical–disease associations. Direct chemical–disease associations are curated from the published literature. These associations are either demonstrated experimentally in model physiological systems or through epidemiological studies. Inferred relationships are established via CTD–curated chemical–gene interactions (for example chemical A is associated with disease B because chemical A has a curated interaction with gene C, and gene C has a direct relationship with disease B). The data were downloaded from CTD on September 28 2011, and contained 424,266 chemical–disease relationships, consisting of 5915 unique chemicals and 3436 diseases (annotated by unique MeSH terms). We excluded all inferred chemical–disease associations. Furthermore, we only kept diseases which were directly associated to at least 20 chemicals. We converted the data into a binary matrix, representing existing associations as 1, and non-reported associations as 0. The resulting cleaned data set contained 2057 chemicals (samples) and 65 diseases (variables).

The Compass workflow is divided into four steps, where SOM and AM form the backbone. The main idea is that the SOM is employed as a first step to find “hotspots”, i.e. areas in the multi-dimensional search space where we are more likely to find strong associations. AM is then subsequently applied to extract the association rules from the hotspots. The four steps of the Compass are the following: SOM, AVG extraction, AM and post-processing (described in greater detail in Supplementary material). The workflow of the method is shown in Fig. 1
                        .

SOM is performed on the training set. To perform SOM, we used a variant available in the R-package (ver. 2.11.0) “kohonen” [13]. It can handle missing values and has the built-in feature where the user may assign arbitrary weights to different variables in the data, thus controlling the degree of influence these variables have on the training process. This feature turns out to be extremely useful, as it affects what area of the search space is covered, thus granting the user some degree of control over which associations is found. We explored maps of size 3×3, 5×5 and 7×7 nodes. Smaller maps will typically produce a smaller number of association rules, and these will in general relate to larger groups of samples than association rules which are generated from larger maps. The choice of map size may depend on the size of the data set (larger data sets can be analyzed with larger maps), but will also depend on the size of the output that the analyst wants to obtain.

From each node, the variables with greatest weights were extracted to form AVGs as well as the estimated numeric intervals of any corresponding variables that were involved in the node and its associations. This allowed for converting these variables into binary form, which was subsequently used to perform AM in Step 4. Specifically, the interesting categorical variables were primarily those that had high model vector values, above a user specified cut-off, while the interesting numerical variables had high and low model vector values, below or above some specified cut-offs, respectively. In our analysis, the cut-offs for numerical variables were set to the 1st and 3rd quartiles, respectively. For categorical variables, which are binary variables containing only 0’s and 1’s, the cut-off was set to 0.75. These cut-offs reflect what the analyst deems as high or low levels for a certain variable, and is mostly a convenience for the human analyst when interpreting the associations in the final output. The values of these parameters will generally influence the strength of the associations. For categorical variables, setting lower cut-offs will generate a larger number of rules, but which may be represented by fewer samples and thus be reliable.

Intermediate ranges for each numerical variable were deemed potentially interesting, if the standard deviation of all samples in a cluster was below a cut-off. In our analysis we did set this parameter to 0.3, Setting this parameter to lower cut-offs will yield association rules that have smaller numerical ranges but will consequently involve fewer samples from a given dataset. Setting this parameter to higher cut-offs will allow the method to extract a larger amount of rules, but these rules may likely become less interesting if the numerical ranges are so big they span a major part of the numerical spectrum for a given numerical variable.

Each AVG was subjected to a thorough search for association rules, iterating through all combinations of the variables in the group. Within each combination of variables, all possible association rules were in turn generated and subjected to Fisher’s Exact Test [14] to obtain a p-value. The confidence measure was used to assess the direction of the rule. In order to reduce the computational load, limit the complexity of associations found and facilitate easier interpretation in the final result output, we limited the number of variables in each AVG to a maximum of 20, and limited the number of variables in each item set to five. To reduce the number of rules generated, we implemented two types of rule filters in the AM step, which can filter out certain types of rules based on prior knowledge; the notopp (not opposite) and notdom (not dominant) filter.

The notopp filter removes all rules, where certain variables occur on opposite sides of the implication arrow. For instance if we know that the sizes of the left and right testis correlate, we also know that whenever these two variables occur on the opposite sides, along with any other variables on either side, we can be confident that the rule will be uninteresting. The notdom filter removes all rules where certain variables are dominant, i.e. rules that only contain a specified group of variables, but not others. In our data set, there were a number of variables related to smoking and drinking habits. As these are associated, they are likely to be grouped into the same AVGs. However, as our main interest is the effects of smoking or drinking on clinical outcomes, association rules that only contain drinking or smoking are not of primary interest.

The AVGs obtained in Step 2 and their corresponding AM rules obtained in Step 3 were processed into a human readable output. To reduce the output size, the AVGs were clustered into groups based on similarity, i.e. the proportion of variables in common. For each such AVG group, their corresponding AVGs and association rules were made easily accessible. As the output is structured around the AVGs, the amount of information a human scientist has to inspect is vastly reduced, compared to an exhaustive list of all association rules. To further reduce the amount of information in the output, we opted to present the association rules as fuzzy rules, i.e. any numeric ranges in the association rules were coded as either ‘H’ (high), ‘L’ (low) or ‘I’ (intermediate). When presented with a list of many associations at once, it may be less important to specifically know exactly the size and endpoints of any intervals involved in each rule. However, if the association is deemed interesting, the relevant intervals can be easily extracted from the output.

In the type of unsupervised data mining we present here, there is an extreme risk of finding false positives due to the large number of associations tested. Strategies to deal with this problem have been discussed in literature, such as directly adjusting for multiple testing, validating the associations on a test set [15], and data randomization [16]. The data randomization approach typically requires much more computational power than the other two approaches, as a larger number of analyses need to be performed in order to generate empirical distributions. In this study, we therefore explored the use of the direct adjustment approach and validation on test set, which are much simpler and more straightforward, in order to limit the number of false positives (i.e. the statistical type-I error). We briefly comment on these approaches in the discussion section. In the analysis of the military conscript’s data, we used the strategy of validating the newfound associations on a test set. From the results on the training set, we generated 81 hypotheses of interest that we subsequently tested in the test set. These 81 hypotheses were selected manually by browsing the AVGs in the result output. As their interestingness is difficult to quantify ‘objectively’ they can only be evaluated subjectively by a human analyst. This is fine as the Compass is an exploratory approach meant to find interesting associations, and because these associations are subsequently verified on a test set. The p-values obtained from the test set were corrected for multiple testing using the Holms method [17]. In the analysis of the data from the Comparative Toxicogenomics Database, due to the sparse nature and small size of the data, we applied the direct adjustment approach. To correct for multiple testing, Holms test was used, setting the number of hypotheses tested to the size of the total search space, which was equal to 257,108 associations.

@&#RESULTS@&#

The primary objective of the Compass is to allow scientists to find new, unexpected and interesting associations in clinical data, and as such, its usefulness is ultimately measured in subjective terms. However, as a central feature of the Compass is its ability to generate AVGs (for manual inspection) which contain statistically significant association rules, an approach to assess the usefulness is to compare the statistical significance of the rules generated from the AVGs to the rules obtained from randomly generated variable groups (RVG). RVGs are generated by randomly choosing variables from the data to form variable groups, and are then subjected to AM. Thus, in this case SOM is not used as an initial step. In the process of generating RVGs, care was taken to not include more than one categorical variable from the same attribute into each group, as this would artificially cause the RVGs to produce poorer associations.

We also examined the performance of Compass after the non-missing values in each data variable were randomly shuffled. This may assess the tendency of the method to find spurious associations given the margins and incompleteness of the data set at hand. Fig. 2
                         illustrates the performance of the Compass (green lines), the RVGs (blue lines), and on randomly shuffled data (red line). The fraction of significant associations is noticeably higher for Compass than RVGs, thus fully justifying the use of SOM as an initial step in the analysis. We noted that smaller map sizes in the SOM step give rise to statistically more significant associations. We presume this is due to the fact that smaller maps may generally find associations that are represented by a larger number of samples in the data set. The baseline for the Compass performance on randomly shuffled data is shown in red. In our case the method was unable to find any spurious associations with a p-value of 1e−6 or lower. By default, the SOM does not grant the user any control over the area in the search space where association rules are mined. However, there may be many circumstances where one would be interested in finding associations involving certain specific variables, in this case, for example birth weight. It is possible to obtain control over this by employing a weighted search, by assigning higher weights to certain variables of choice during the SOM-step.


                        Fig. 3
                         illustrates the performance of six different analyses, each assigning a higher weight to one variable during the SOM. The performance differs considerably, depending on which variable is weighted higher. Assigning higher weights to certain variables affects which AVGs are created, and thus which associations are found. Our analysis showed that this approach allows for spreading out the search into a greater area of the multidimensional space, thus finding a larger number of diverse rules (data not shown). However, assigning a higher weight to a certain variable does not guarantee that all AVGs, and their corresponding association rules, will contain that particular variable. This is true especially in cases with weakly associated variables, where the Compass method may still generate other highly associated variable groups that do not necessarily contain the variable of interest.

The Compass method generated 111 AVG groups containing a total of 373,745 fuzzy association rules from the training set with a p-value lower than 0.001. Examples of some of the AVGs found are shown in Table S1 in the Supplementary material. By browsing the 111 AVG groups generated from the analysis, we selected 81 interesting (subjectively based on domain knowledge) associations, and subsequently evaluated them on the test set. The p-values obtained from the test set were corrected for multiple testing using Holms test, taking into account that 81 associations were found. Ten of these associations had significant p-values in the test data, and are listed in Table 2
                        .

Note, the confidence given in the table is defined as the conditional probability of the consequent given the antecedent. This definition is commonly employed in AM, and differs from that used in traditional statistics. The statistically most significant association presented in Table 2 demonstrates a correlation in size between the right and left testis. However, importantly associations with the lowest p-values are not necessarily the most interesting; they often tend to be trivial. In fact, in our study, we observed that many associations with higher p-values were actually more interesting from a biological and scientific point of view, as these patterns are usually less visible to scientists in the field and therefore more likely to be unexpected.

Association 7 in Table 2, which indicates that there is a positive correlation between alcohol intake and free androgen index (levels of free unbound testosterone in blood), has a relatively high p-value. However, it is biologically interesting, and has been discussed in the literature previously [18]. Previous studies have indicated that people in their late teens who smoke perform more poorly in school than non-smokers [19,20]. We find in association 5 in Table 2, an indication that smokers are less likely to go to school, which may be correlated to poor performance. We find this to be true for 47% of smokers in our test set. We found the unusual association 9, which indicates that if the mother smoked during pregnancy, the military conscript is less likely to eat organic food. The intuitive explanation for this association is that most of the conscripts we studied are in their late teens and early 20’s and may still live at home. Therefore, they will often eat what their parents serve. A mother who smokes during pregnancy is less likely to be as health conscious as a non-smoking mother, and therefore is less likely to buy organic food for the household. To our knowledge, this association has not been published previously.

On this completely different dataset Compass generated a total of three association rules from the Comparative Toxicogenomics Database (CTD) data with a p-value lower than 0.001. To correct for multiple testing we applied the direct adjustment approach, using the Holms method [17] and taking into account the size of the search space. Table 3
                         lists the rules, along with their p-value and confidence. These rules associate diseases based on the chemical–disease associations present in the data. In other words, diseases or symptoms that share a common set of chemicals, larger than expected by chance, will form association rules together.

As such, these rules may imply that two diseases share a common genetic mechanism, or that they may co-occur together in the general population more often than expected by chance. However, care must be taken when interpreting these results, as they are based on chemical–disease associations reported in literature. As such, they may be heavily biased towards chemicals and diseases that have been studied extensively. Moreover, chemical–disease associations that were not present in CTD, and likely not reported extensively in the literature, were assumed to not exist.

The statistically most significant association in Table 3 relates pain to inflammation, indicating that 43% of chemicals associated with pain are also associated with inflammation. These chemicals mostly include known anti-inflammatory painkillers available on the market, but also natural substances found in plants such as Capsaicins, Mangifer Indica extract or Desmodium Gangeticum extract. This association may reasonably be regarded as a trivial association, as pain is one of five cardinal signs in acute inflammation. Association 2 links lung cancer with prostate cancer via chemicals such as epoxy compounds, polyvinyl chlorides, arsenite and butylated hydroxytoluene. We were unable to find any reports in the scientific literature linking these two cancers specifically, although it is known that metastatic cancers originating from other tissues, not only lung, may spread to the liver. Association 3 in Table 3 links prostatic cancer with breast cancer; both being gender-specific for males and females, respectively. The chemicals in CTD linking these two diseases mainly consist of different estrogens and androgens, which are endogenous sex hormones. Familial co-occurrence between prostate cancer and breast cancer has been reported previously in a number of different studies [21,22].

@&#DISCUSSION@&#

We have developed the Compass method, an unsupervised approach that can successfully mine data for interesting associations in clinical data with missing values and mixed data types. The use of this concept is also demonstrated on non-clinical data containing chemical–disease associations based on text mining from CTD, where relevant associations between different phenotypes where generated. The proposed approach is sufficiently sensitive to find relatively weak associations which are nonetheless interesting, such as the relation between alcohol consumption and levels of free androgen index, which we were able to confirm in the literature.

The Compass method is divided into four steps, where SOM and AM are the two initial components of the pipeline. SOM is applied as a first step in order to reduce the search space covered by AM in a later step. Several benefits of this approach has been discussed, including handling of numerical variables, the possibility for a weighted search (the ability to control which part of the search space is covered), and generation of AVGs which reduces the search space covered to find associations with low p-values and also allows for easier handling of a large number of associations in the result output.

We are able to handle numerical variables without a priori binning, and in particular, without using the support measure to restrict the search space by applying SOM. The resulting clusters obtained from the SOM output can suggest approximate intervals in the numerical variables that are relevant to the associations found in that cluster. Although the method is able to find numeric intervals in any intermediate range that may be associated with a group of variables, the main driving force in the Compass pipeline are the values in the high or low end of the spectrum, as these are more likely to be greater distances apart from other non-similar samples during the learning process of the SOM.

In the first step of the analysis, SOM is employed in order to point towards hotspots in the search space with strong associations. The locations of the hotspots are normally outside the control of the user. However, by performing a weighted search, assigning higher weights to one or more variables in the SOM, the method may be coerced to find associations that include particular variables of interest. This grants the user some control over which associations are found, and may also increase sensitivity. The increase in sensitivity arises because variables that are involved in patterns represented by very few samples will have a higher chance of being found if they are weighted higher. The analysis indicated that iterating through all variables in a dataset, assigning a higher weight to a single variable in each iteration, will essentially “force” the SOM to spread out over a bigger area in the search space, thus finding a greater diversity of associations than in the case where a non-weighted analysis is performed (data not shown).

Typically, one will find thousands of associations, which fulfil certain criteria of reliability, such as low p-value and high replicability. However, the p-value and replicability of an association does not confer any solid information about its interestingness. In our study, we found many associations with extremely low p-values (some as low as 1e−100) which turned out to be either already known or trivial. One such trivial example is the finding that the size of the left testis strongly correlates with the size of the right testis (see Table 2). The only true measure of interestingness for our purposes is the subjective opinion given by an expert in the field, who can decide whether an association is new and/or interesting. Compass is not an artificial intelligence method intended to replace human expertise on its own being able to deem an association interesting and previously unreported.

It is nearly impossible for a human brain to browse through all newfound associations, and this procedure does not give a good overview of the structure of the data either. Therefore the reduction of the amount of information to be inspected manually is important. For this purpose, we propose the approach where a number of AVGs are presented in the result output. Given that the AVGs are “hotspots” for statistically significant associations, experts in the field can with a glance easily identify if any potential association between the variables occurring together in a group would be interesting. Thus the amount of information that a human has to process is strongly reduced, as each AVG can, depending on its size, represent hundreds or even thousands of association rules. While the AVG step does considerably reduce the amount of information to manually process, it may still be large when analyzing big dataset. We have therefore implemented additional measures of rule filters and generation of fuzzy rules (described in the methods section) and merging nearly identical categorical variables (described in the data section).

Another challenge with dealing with large number of associations is, of course, the extreme risk of finding false positives. We briefly discuss two approaches that address this challenge: direct adjustment and validation on test set.

In the case of directly adjusting the p-values for multiple testing, the correction factor is dependent upon the size of the search space, i.e. the theoretical number of associations that can be tested in any given data. As the possible number of rules is large, this strategy has a tendency to filter out many true associations. In simple binary data sets, calculating the size of the theoretical search space is usually straightforward. However, the presence of numerical variables may complicate the matter due to the fact that any given numeric interval may theoretically be involved in any association. As an example, consider two numeric variables N1 and N2 that are linearly correlated. These two variables could produce association rules like “N1 High→N2 High”, and “N1 Low→N2 Low”, as well as “N1 Intermediate→N2 Intermediate”. Due to the fact that these two numerical variables are continuous, there can theoretically be an extremely large number of intermediate intervals in which N1 and N2 are associated. If all these intervals are to be taken into account, the theoretical search space may easily become astronomical, even for relatively small data sets. Only extremely significant associations would then pass as reliable.

Validating associations from a training set on a test set is a common strategy to control for false discoveries. The correction factor for multiple testing is in this case the number of associations that were considered interesting in the training set, and subsequently validated on the test set, and is thus much smaller then the correction factor used in the direct adjustment approach. In many cases, if no test set is available, it is common to divide a given data set into a training and test set. A disadvantage with this approach is that the splitting of the data into two smaller sets reduces the number of samples, and hence reduces the power to detect new associations. Another drawback is that the splitting may not be optimal with regards to missingness and skewed distributions. By chance, variables may be unevenly divided such that more missing values appear in the test set than the train set (and vice versa). Thus, certain associations may fail to be validated, not because of being spurious, but rather due to the fact that the degree of missingness is too high for the variables involved. These problems are more pronounced for higher order associations, but should be less problematic for data sets containing large numbers of samples. Moreover there may be a general issue of similarities between training and test sets, which may limit to what degree newfound associations can be generalized to other parts of the population if the data sets are too similar.

This has not been discussed extensively in the biobank questionnaire analysis field but has often been a topic within molecular level bioinformatics, where the performance of prediction algorithms depends strongly on the similarity between training and test set examples (for example the sequence similarity between two proteins) [23,24].

In general, we would not recommend using the direct adjustment approach, as it is in many cases needlessly strict and sets the p-value cut-off extremely low due to the usually large search space. As a consequence, only associations with extremely low p-values will be retained, and potentially interesting associations with higher p-values will be overlooked. In special cases, it may however be used, as in our analysis of the data from CTD, which was a relatively small data set that generated a small number of very significant rules.

In the field of machine learning, many unsupervised data mining methods aim at extracting common patterns in data sets. These methods include clustering techniques such as hierarchical clustering, k-means clustering, SOM, and methods for dimensionality reduction such as Principal Component Analysis (PCA). The output generated by these methods is qualitatively different compared to the proposed Compass approach. On their own they may produce grouping of either samples or variables, much like the AVGs that are generated in Compass. However, such grouping alone does not inform the analyst of the strength of associations between specific variables.

Traditional association rule mining approaches (such as the Apriori) do not produce clusters or groupings, but a list of rules that can be ranked based on some objective measures such as p-value. Compared to the Apriori algorithm, Compass requires more computational time as it is a multi-steps procedure. Nevertheless, Compass has two major advantages compared to traditional AM:
                           
                              (1)
                              Explicit binning of numerical variables is not performed with Compass. Compass allows flexibility using parameters, which influence quantitative rules found. For example the subjective parameters specifying ”High” and ”Low” values for each variable, and what is considered to be the maximum size of intermediate intervals.

As the Compass performs initial clustering using SOM, the variables are already grouped in a way that facilitates the generation of the final output. The rules will be grouped into AVGs based on which cluster they were generated from. Such grouping also helps facilitate the manual inspection of the association rules, without the analyst having to examine every single rule (which can number in the thousands or hundreds of thousands).

Based on the two advantages listed above, the top associations obtained from Compass will be qualitatively very different compared to the Apriori algorithm [26]. This is illustrated by an analysis with the Apriori setting the support and confidence cut-offs to 0.1 and 0.7, respectively, and binning each numeric variable into three bins. The Apriori generated 739,000 rules. The list of rules was then ranked by p-value, and the top ten rules were then validated on the training set, illustrated in Table 4
                        . Compared to the top associations obtained from Compass (listed in Table 2), the associations in Table 4 have much lower p-values but are less interesting than the rules obtained from Compass (listed in Table 2) from a subjective point of view. Moreover, the list of top ten rules obtained from Apriori are also less diverse than the rules obtained from Compass, as the majority of the rules include variables relating to smoking, and on occasion a variable involving pregnancy. As these rules are very similar, they could all have been potentially grouped into an AVG by Compass, and thus presented in a much more compact and presentable way for manual screening.

As mentioned in the method section, Compass uses SOM to generate AVGs, which are then subject to Association Mining. It is likely that other techniques than SOM may be used in a similar way to generate AVGs. We compared the performance of the Compass workflow, if using k-means clustering [25] instead of SOM as the first step of the procedure. k-Means clustering is similar to a special case of SOM, where the degree of influence on neighbouring nodes is set to 0. Using k-means we found weaker associations than using SOM, i.e. the proportion of association rules with lower p-value was smaller for k-means (Table 5
                        ). The interest for the type of unsupervised analysis presented here has been growing in the life sciences, particularly due to the large amounts of data available today. The Compass method is well suited for data sets with a much higher number of samples than that used in our study (roughly 2000 samples in the CTD data), as the analysis scales close to linearly in the SOM step. However, data sets with a much larger number of variables may create a challenge when using the current Compass approach. From a theoretical point of view, data sets with a very large number of variables (numbering thousands of variables) may be difficult to mine with SOM+ARM approach as the contribution of individual variables decreases. Particularly in the SOM-step, the presence of a large number of variables may prevent the method from finding localized patterns. A future prospect will be to investigate this issue more fully on larger data sets with the aim to improve the performance.

Other existing approaches to mine for quantitative association rules have been discussed in the literature. A method which uses rank-correlation measures to mine for numeric association rules was reported by Calders et al. [27]. This approach is only able to extract rules containing positively correlated numerical variables, and not negatively, which is a major limitation. Moreover, the computational time increases somewhat exponentially with the number of variables, making it unfeasible to run on data sets of the size presented in this study. Other approaches perform a priori binning first, and then merge the bins into optimal intervals, keeping the support for the intervals between user-specified maximum and minimum support 
                        [28]. A promising method based on an information theoretical approach uses this strategy to mine for quantitative association rules [29]. We investigated this method as their research group had easily available windows binaries available for download. Unfortunately, their program was unable to handle the size of our data set. Indeed, in their publication they had tested their method on data sets that had considerably fewer variables (the largest having 50 variables). Other approaches strive to extract quantitative association rules for uninstantiated attributes (without performing a priori binning). These methods have restrictions imposed on the type of rules that can be mined such as only allowing numerical variables in the LHS (left hand side of rule) and qualitative variables in the RHS (right hand side) [30], or rules being restricted to only two quantitative variables [31,32]. None of the association mining approaches that we were able to find solve the problem of the need to manually inspect a large number of rules where interestingness is partially based on a subjective measure. Compass deals with this challenge by generating AVGs, and then grouping the corresponding association rules in order to facilitate speedy inspection by experts in the field.

Comparing the association rules obtained from Compass with those obtained from the Apriori algorithm, we have observed that the overlap is small, i.e. these two methods tend to find different rules. With more stringent parameters, such as higher support and confidence requirements for Apriori, and smaller SOM-map sizes in Compass, the overlap tends to be zero. Nevertheless, as the Compass is aimed at providing an advantage when mining quantitative rules, a comparison in the p-value distribution of quantitative rules can be made. The median of the log p-value distribution of quantitative rules tends to be a few orders of magnitude lower for Compass than for Apriori. Table 6
                         below shows an example with rules containing 2–5 variables generated from the military conscripts data set using both the Apriori-algorithm and Compass. The Apriori-algorithm was set to mine for minimum support of 0.1 and confidence of 0.8, while the Compass was applied using a weighted search (for each variable) on 3×3 maps. With these settings, Compass generated 465,685 quantitative rules, while Apriori generated 434,315 quantitative rules.

Although Compass generates more statistically significant rules than the Apriori-algorithm, the comparison between these methods does not illustrate the flexibility and ability of Compass to find suitable numeric ranges for quantitative rules. This is illustrated in a comparison between quantitative rules from Compass (defined by the numeric ranges that Compass produced) with the same rules measured in a pre-binned data set (where numeric ranges have been obtained from a priori binning of numeric variables). Binning using both equal width and equal counts are common ways to pre-process numeric data. As Table 6 shows, the log p-value distribution of compass is several orders of magnitude lower than rules obtained from the pre-binned data sets. Association rules from data sets subjected to equal width binning were statistically far less significant than rules from data set subjected to equal depth binning. The reason is likely that numerical variables had skewed distributions, which is common in clinical data.

The computational cost of Apriori, in the worst case scenario, is proportional to 
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                D
                                             
                                          
                                          
                                             
                                                L
                                             
                                          
                                       
                                    
                                 
                              
                           
                        , where D is the number of variables in the data set and L is the maximum number of variables that are permitted in any association rule (specified by the user). Usually, the size of the rules of interest, L, is much smaller than D, thus the computational complexity of Apriori can be simplified to O(DL
                        
                        ⋅
                        N) where N is the number of samples in the data. The Apriori algorithm only searches for rules with high enough support and confidence. A post-processing step to calculate p-values is also necessary, and this step is also subject to O(DL
                        
                        ⋅
                        N).

The first step in Compass is the Self-Organising Map algorithm which has a computational cost of O(D
                        ⋅
                        N
                        ⋅
                        M) where where D is the number of variables in the data set, N is the number of samples and M is the number of nodes in the feature map. The output from the SOM is processed to prepare for AM at a cost of O(d
                        max
                        ⋅
                        N
                        ⋅
                        M) where d
                        max is the maximum size of the AVGs (user specified parameter). In the worst case scenario, if d
                        max is set to be equal to D, the clusters associated with each node can become very large. However, this is undesirable and defeats the purpose of generating AVGs that are conveniently screened on a manual basis by an analyst. In our experience, a suitable number for d
                        max is between 10 and 15 variables.

After the processing step, the Compass method proceeds with Apriori which has a similar cost to Apriori alone, although the notation in this case will be 
                           
                              O
                              (
                              
                                 
                                    d
                                 
                                 
                                    max
                                 
                                 
                                    L
                                 
                              
                              ·
                              N
                              ·
                              M
                              )
                           
                        . This computational cost also applies to all post-processing steps, including calculation of p-values and generating the final formatted output.

The SOM generally tends to find clusters that are represented by larger number of samples, and thus the variables associated with each cluster (AVGs) do mimic “frequent item sets” as obtained by traditional AM. However, there is one very crucial difference between the AVGs and item sets generated by traditional AM: The item sets found in traditional AM will only have the number of variables as defined by the sizes of rules that the analyst is interested in. For example, if the analyst wants to find rules of sizes 3–4, all frequent item sets will only consist of 3 or 4 variables. The AVGs on the other hand, can consist of many more variables, such as 10, 15 or 20, even though we are only interested in mining rules containing 3–4 variables. The AVGs generated by Compass can therefore be through of as combining and grouping many smaller item sets as those generated by traditional AM. These AVGs may then be used in the final output to help facilitate manual screening of the thousands of generated rules.

@&#CONCLUSION@&#

The proposed ‘Compass’ approach, allows mining of clinical data without a priori binning of numerical variables. The method can generate a highly condensed and structured output for efficient manual screening of potentially interesting rules through the use of Associative Variable Groups.

@&#ACKNOWLEDGMENTS@&#

The study was funded by the Villum Kann Rasmussen Foundation (www.vkrf.org), the European Comission (FP7, Grant agreement 212844), the Novo Nordisk Foundation, the Danish Agency for Science, Technology and Innovation (09-067180), the Center on Endocrine Disrupters (Denmark) and the GENDINOB project supported by the Danish Council for Strategic Research (Grant: 09-067111).

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2013.10.007.


                     
                        
                           Supplementary data 1
                           
                        
                     
                  

@&#REFERENCES@&#

