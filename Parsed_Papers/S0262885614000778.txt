@&#MAIN-TITLE@&#Exploiting Universum data in AdaBoost using gradient descent

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We address a novel boosting algorithm by taking advantage of Universum data.


                        
                        
                           
                           A greedy, stagewise, functional gradient procedure is taken to derive the method.


                        
                        
                           
                           Explicit weighting schemes for labeled and Universum samples are provided.


                        
                        
                           
                           Practical conditions to verify effectiveness of Universum learning are described.


                        
                        
                           
                           This algorithm obtains superior performances over AdaBoost with Universum data.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

AdaBoost

Gradient boost




                     




                        U




                     AdaBoost

Universum




                     




                        U




                     -SVM

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Conventional machine learning algorithms take labeled data, unlabeled data or both of them for learning. Vapnik [1] proposed the third kind of data: Universum data. The Universum data contains the data that belongs to the same domain as the classification problem but it does not belong to any class of the problem. For example, in handwritten digits recognition problems, if the samples of handwritten digits ‘5’ and ‘8’ are prepared for learning, then other handwritten digit samples can be naturally treated as Universum data since they belong to the same domain but cannot be assigned to any of the two classes.

It is a common case that large labeled training data is included in order to obtain good quality of training. However, it is quite costly or sometime even impossible to have very large training data. To deal with such problem, semi-supervised learning is a common option when unlabeled data is available since unlabeled data helps model data distribution of the whole data. On the other hand, without unlabeled data, Universum data is still able to provide the supports to maintain the training quality with relatively small labeled data set. The reason is Universum data can be generated through a lot of ways from labeled data only [2] (mentioned later). Moreover, Universum data can carry additional valuable prior information from the domain of the problem into the training process. To the best of our knowledge, there is no comparison between semi-supervened learning and Universum based learning. But in our opinion, Universum based learning can better model the whole data set since Universum data stays in the same domain of learning problem with which we are concerned [1] while the unlabeled data may be too general and stay outside of the domain. In terms of data acquisition, Universum data can be obtained more widely.

Vapnik first discussed transductive learning with Universum since transductive learning provides prior information to estimate the upper bound of inductive inference [1]. However, the classifier trained by inductive learning is more practical to classify unknown data. Weston et al. [2] proposed an inductive algorithm, Universum Support Vector Machines (
                        U
                     -SVM). 
                        U
                     -SVM contains an additional regularization term for Universum data in addition to conventional SVM. The regularization is based on this assumption: the decision values on the Universum data should be close to zero. That is Universum data should fall inside the margin of the classifier since it does not belong to any class. The Universum samples which meet such assumption are called contradictions because the goal of learning is putting labeled data outside of the margin. Thus the margin should contain more Universum data to achieve better learning performance. More Universum data means more contradictions. Therefore the learning criterion for Universum based learning is called Maximal Contradiction on Universum (MCU) [1]. Two learning problems: common semi-supervised and training based on Universum are demonstrated in Fig. 1
                     . The unlabeled data should be away from the margin like labeled data while Universum data should fall between margins.

Sinz et al. [3] analyzed the 
                        U
                     -SVM for inference and they showed that 
                        U
                     -SVM would give the hyperplane which had its normal lying in the orthogonal to the principal directions of Universum data. They also discussed the connection of least squared version of 
                        U
                     -SVM with Fisher discriminant analysis and oriented principal component analysis. They showed that 
                        U
                     -SVM outperformed SVM with carefully selected Universum data. In addition to SVM, Universum data has also been extended to other learning problems, such as semi-supervised learning [4], linear discriminant analysis [5], twin support vector machine [6], cost-sensitive learning [7], linear programming [8], and domain adaptation [9]. In terms of application, besides the handwritten digits recognition problem mentioned above, it is also applied into medical imaging [10], document clustering [11], pose recognition [12,13], etc.

Universum data is always obtained from the domain of the classification problem mentioned above. Weston et al. [2] proposed four kinds of Universum data: random noise, the rest of the training data (e.g. the other digits in the handwritten digits recognition problem), artificial data from the same distribution of training data and random average of training data. Bai & Cherkassky [14] applied Universum data into gender classification and they took three kinds of Universum data: random average, empirical distribution and animal faces. In the field of Universum data selection, Sinz et al. [3] suggested that a good Universum set should contain invariant directions and be positioned “in between” the two classes of the classification problem. Chen & Zhang [15] proposed a guided formulation to pick the informative ones, i.e., in-between Universum (IBU) samples.

Boosting family contains a series of well-known algorithms with a large number of applications. Motivated by the success of 
                        U
                     -SVM, Shen et al. [16] proposed 
                        U
                     Boost by adding Universum data to boosting algorithms and showed that they can benefit from Universum data as SVM did. 
                        U
                     Boost is derived from AdaBoost-CG [17] which is another view of boosting. Compared with AdaBoost-CG, AdaBoost is a stagewise method [18] which is more general and popular. The whole training procedure of AdaBoost is also much faster. Although Universum data has shown its power on 
                        U
                     -SVM [2] and 
                        U
                     Boost [16], to our knowledge, its importance on AdaBoost has not been evaluated.

In this paper, we propose a new boosting algorithm called 
                        U
                     AdaBoost to improve the classification performance of AdaBoost with the help of Universum data. The learning is not straight forward since Universum data belongs to neither positive nor negative data. Stagewise AdaBoost keeps the pre-selected weak classifiers unchanged in the following training. It pays more attention on misclassified samples in the next training iteration. The weights for training samples and coefficients for the pre-selected weak classifiers are obtained according to gradient descent. Involving Universum data into AdaBoost framework needs to take these properties of AdaBoost into account. Instead of 
                        U
                     AdaBoost, 
                        U
                     Boost takes Universum as a conventional convex optimization problem and solves it by column generation as AdaBoost-CG does.

To tackle the above challenges, we propose explicit weighting schemes for both labeled data and Universum data which are both involved in AdaBoost training procedure. The rationale of updating weights in common AdaBoost training is to enforce the training to focus on hard samples. In this paper, this rationale is revisited and further extended on Universum data in the proposed 
                        U
                     AdaBoost.

The major contributions of this paper are as follows:
                        
                           1)
                           Given Universum data, a new 
                                 U
                              AdaBoost learning based on the framework of AdaBoost is proposed. We propose 
                                 U
                              AdaBoost using the same functional gradient descent as AdaBoost. By taking advantage of AdaBoost, 
                                 U
                              AdaBoost is much easier and more practical to apply than 
                                 U
                              Boost [16] since 
                                 U
                              AdaBoost only needs one parameter to tune.

The whole training procedure of 
                                 U
                              AdaBoost is efficient. The time cost for 
                                 U
                              AdaBoost is less than 
                                 U
                              Boost. Our experimental results demonstrate such improvement.


                              
                                 U
                              AdaBoost provides a better framework for investigating the benefits of Universum data in boosting approaches. It is known that AdaBoost is a popular algorithm in boosting algorithm family. In recent years, researches have contributed significant efforts to investigate AdaBoost in order to improve its performance. To our best knowledge, 
                                 U
                              AdaBoost is the only framework using the same approach (i.e. stagewise) as AdaBoost and integrating Universum data, so the performance evaluation on integrating Universum data into AdaBoost is more precise and convincing. In contrast, 
                                 U
                              Boost follows column generation approach [17] which is different from AdaBoost so we cannot use 
                                 U
                              Boost framework in evaluating the benefits of Universum data to AdaBoost.

Also, in this paper, we discuss a method for selecting effective and informative Universum data in order to better take the advantages of Universum data in AdaBoost framework. This will benefit several applications in computer vision area.

The paper is structured as follows. In Section 2, we discuss the related work about 
                        U
                     -SVM, 
                        U
                     Boost and the motivation to 
                        U
                     AdaBoost. In Section 3, we propose the novel boosting formulation 
                        U
                     AdaBoost based on the Universum data and compare it with semi-supervised boosting, AdaBoost and 
                        U
                     Boost. In Section 4, we analyze the practical conditions for 
                        U
                     AdaBoost. In Section 5, the performance of our model will be demonstrated with several public data sets. In Section 6, we conclude the paper.

In this paper, our focus is only on binary classification problems, while our method can be extended to the multi-class scenario. Let 
                           
                              
                                 X
                                 L
                              
                              =
                              
                                 
                                    
                                       x
                                       1
                                    
                                    
                                       y
                                       1
                                    
                                 
                                 
                                    
                                       x
                                       2
                                    
                                    
                                       y
                                       2
                                    
                                 
                                 …
                                 
                                    
                                       x
                                       m
                                    
                                    
                                       y
                                       m
                                    
                                 
                              
                           
                         be the set of m labeled examples, where yi
                        ∈{−1,1} is the class label. Let 
                           
                              
                                 X
                                 U
                              
                              =
                              
                                 
                                    x
                                    1
                                    *
                                 
                                 
                                    x
                                    2
                                    *
                                 
                                 …
                                 
                                    x
                                    n
                                    *
                                 
                              
                           
                         represent the Universum data with n samples. wi
                         and w
                        
                           j
                        
                        ∗ represent the weights of the labeled sample (x
                        
                           i
                        ,yi
                        ) and x
                        
                           j
                        
                        ∗ during boost training phase respectively.

Weston et al. [2] proposed 
                           U
                        -SVM and treated it as an inductive learning problem. The Universum examples are considered to be close to the separating hyperplane selected by SVM. The optimization objective should minimize the cumulative loss on the Universum examples. Given the Hinge loss H
                        
                           a
                        [t]=max{0,
                        a
                        −
                        t} for the standard SVM (a
                        =1) and ε-insensitive loss I
                        
                           ε
                        [t]=
                        H
                        −
                           ε
                        [t]+
                        H
                        −
                           ε
                        [−
                        t] for Universum data, the learning problem can be formulated as:
                           
                              (1)
                              
                                 
                                    
                                       min
                                       
                                          w
                                          ,
                                          b
                                       
                                    
                                    
                                       1
                                       2
                                    
                                    ∥
                                    w
                                    
                                       ∥
                                       2
                                    
                                    +
                                    
                                       C
                                       L
                                    
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          m
                                       
                                       
                                          
                                             H
                                             1
                                          
                                          
                                             
                                                
                                                   y
                                                   i
                                                
                                                
                                                   f
                                                   
                                                      w
                                                      ,
                                                      b
                                                   
                                                
                                                
                                                   
                                                      x
                                                      i
                                                   
                                                
                                             
                                          
                                          +
                                          
                                             C
                                             U
                                          
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   I
                                                   ε
                                                
                                                
                                                   
                                                      
                                                         f
                                                         
                                                            w
                                                            ,
                                                            b
                                                         
                                                      
                                                      
                                                         
                                                            x
                                                            j
                                                            *
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The first two terms are for the standard SVM and the last one is for Universum data. 
                           
                              C
                              L
                           
                         and 
                           
                              C
                              U
                           
                         are parameters for regularization. f
                        
                           w,b
                        (x) is the learned classifier. Parameter ε controls the margin of Universum data. Sinz et al. [3] gave the least squares version of 
                           U
                        -SVM and showed that this kind of 
                           U
                        -SVM can also show similar performances with the original 
                           U
                        -SVM with less parameters (there is no ε in Eq. (2)).


                        
                           
                              (2)
                              
                                 
                                    
                                       min
                                       
                                          w
                                          ,
                                          b
                                       
                                    
                                    
                                       1
                                       2
                                    
                                    ∥
                                    w
                                    
                                       ∥
                                       2
                                    
                                    +
                                    
                                       
                                          C
                                          L
                                       
                                       2
                                    
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          m
                                       
                                       
                                          
                                             Q
                                             
                                                y
                                                i
                                             
                                          
                                          
                                             
                                                
                                                   f
                                                   
                                                      w
                                                      ,
                                                      b
                                                   
                                                
                                                
                                                   
                                                      x
                                                      i
                                                   
                                                
                                             
                                          
                                          +
                                          
                                             
                                                C
                                                U
                                             
                                             2
                                          
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   Q
                                                   0
                                                
                                                
                                                   
                                                      
                                                         f
                                                         
                                                            w
                                                            ,
                                                            b
                                                         
                                                      
                                                      
                                                         
                                                            x
                                                            j
                                                            *
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where Q
                        
                           a
                        [t]=∥t
                        −
                        a∥
                        2
                        2 is the quadratic loss for labeled data (a
                        =
                        yi
                        ) and Universum data (a
                        =0).

Shen et al. [16] proposed a boosting algorithm 
                           U
                        Boost using Universum data. They applied squared loss of Universum data in the optimization objective. They formulated this problem with the same framework of AdaBoost-CG [17] by adding the regularization term for Universum data.
                           
                              (3)
                              
                                 
                                    
                                       
                                       
                                          
                                             min
                                             w
                                          
                                          
                                             1
                                             m
                                          
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                m
                                             
                                             
                                          
                                          
                                          exp
                                          
                                             
                                                z
                                                i
                                             
                                          
                                          +
                                          
                                             c
                                             
                                                2
                                                n
                                             
                                          
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                          
                                          
                                          
                                             z
                                             j
                                             
                                                *
                                                2
                                             
                                          
                                          +
                                          D
                                          
                                             1
                                             ⊤
                                          
                                          w
                                       
                                    
                                    
                                       
                                       
                                          
                                          s
                                          .
                                          t
                                          .
                                          
                                          
                                             z
                                             i
                                          
                                          =
                                          −
                                          
                                             y
                                             i
                                          
                                          
                                             H
                                             i
                                          
                                          w
                                          ,
                                          ∀
                                          i
                                          ;
                                          
                                             z
                                             j
                                             ∗
                                          
                                          =
                                          
                                             H
                                             j
                                             *
                                          
                                          w
                                          ,
                                          ∀
                                          j
                                          ;
                                          w
                                          ≽
                                          0
                                          .
                                       
                                    
                                 
                              
                           
                        
                     


                        D controls the regularization of the weighting coefficients of boosting algorithm. c controls the trade-off between the errors of labeled data and unlabeled data. H is the prediction of all available weak classifiers on the training data. H
                        ={h
                        
                           k
                        (x):
                        x
                        →{1,−1}}, H
                        
                           ik
                        
                        =
                        h
                        
                           k
                        (x
                        
                           i
                        ). hk
                         denotes the k-th weak classifier. H
                        
                           i
                         is the i-th row of H which denotes the output of all weak classifiers on example x
                        
                           i
                        . Likewise, H
                        ⁎ is the prediction of Universum data. Two additional variables z and z
                        ⁎ are used for generating the dual problem. Based on this formulation, they obtained its corresponding dual problem and solved it with column generation. Experiments on a variety of handwritten digits recognition problems and computer vision problems showed that 
                           U
                        Boost outperformed AdaBoost and AdaBoost-CG.

@&#MOTIVATION@&#

Although 
                           U
                        Boost has shown the effectiveness of Universum data for boosting algorithms, it still has some drawbacks. First, the selection of the trade-off parameter D of Eq. (3) in AdaBoost-CG influences the final performance significantly and how to tune the parameter efficiently remains an unsolved problem [17]. There are two parameters D and c in 
                           U
                        Boost, resulting in more difficulties to select a pair of appropriate parameters. Second, 
                           U
                        Boost needs to update all the previous trained weighting coefficients w during each iteration, making the training procedure more and more slower. Last, 
                           U
                        Boost is built on AdaBoost-CG, therefore, the direct performance comparison is 
                           U
                        Boost and AdaBoost-CG other than AdaBoost. It is difficult to verify the effectiveness of Universum learning with conventional boosting algorithm.

In this paper, we propose a new boosting algorithm with Universum data, termed 
                           U
                        AdaBoost, to overcome these limitations. First, 
                           U
                        AdaBoost is derived by margin cost function framework of boosting algorithms which gives explicit weighting schemes for labeled and Universum data. 
                           U
                        AdaBoost trains the coefficient of the selected weak classifier at current iteration while 
                           U
                        Boost updates coefficients of both newly selected weak classifier and also the previously determined weight values of existing weak classifiers. Therefore, 
                           U
                        AdaBoost is much easier and faster to train than 
                           U
                        Boost. In addition, only one parameter c for the loss of Universum data in 
                           U
                        AdaBoost makes it more practical to use. Finally, since 
                           U
                        AdaBoost follows the same stagewise way to learn as AdaBoost, it is easier to understand the difference of the AdaBoost with and without Universum data. This is very important since we can predict the performance of boosting algorithm with different kinds of Universum data before really training the final classifier.

Cherkassky et al. [19] proposed practical conditions to verify the effectiveness of Universum data. Histogram is applied as an analytical tool. They showed that the projection of effective Universum data on the norm direction of the standard SVM decision boundary should be symmetric and has a wide distribution between the margins. In addition, they analyzed these conditions and showed that they were closely related to analytic conditions in [3]. Since the proposed 
                           U
                        AdaBoost follows the marginal framework of boosting, we can extend these conditions on 
                           U
                        -SVM to 
                           U
                        AdaBoost. Experiments show that the performance of 
                           U
                        AdaBoost can be further improved with carefully selected Universum data.

We first give an optimization objective for 
                        U
                     AdaBoost and show the proposed algorithm with the same procedure as AdaBoost. Then we compare 
                        U
                     AdaBoost with semi-supervised boosting algorithms, AdaBoost and 
                        U
                     Boost.

Boosting learns a strong classifier consisting of a voted combination of base learners, i.e. weak learners. The strong classifier is sign[f(x)], where f(x) is the linear combination of weak learners: f(x)=∑
                        
                           t
                           =1
                        
                           T
                        
                        α
                        
                           t
                        
                        h
                        
                           t
                        (x). For binary classification, h
                        
                           t
                        (x)→{+1,−1} is weak classifier, α
                        
                           t
                        (≥0) is the weight for linear combination and T is the number of selected weak classifiers.

When Universum data is added into a supervised learning algorithm, it needs additional loss function or regularization term for Universum data. So does AdaBoost algorithm. We propose 
                           U
                        AdaBoost algorithm which contains Universum data to improve the performance of AdaBoost. A loss function L(y,f(x)) is defined for the learning process of 
                           U
                        AdaBoost which contains two components corresponding to the labeled data L
                        1 and Universum data L
                        2 as:
                           
                              (4)
                              
                                 
                                    L
                                    
                                       
                                          y
                                          ,
                                          f
                                          
                                             x
                                          
                                       
                                    
                                    =
                                    
                                       1
                                       m
                                    
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          m
                                       
                                       
                                          
                                             L
                                             1
                                          
                                          
                                             
                                                
                                                   y
                                                   i
                                                
                                                ,
                                                f
                                                
                                                   
                                                      x
                                                      i
                                                   
                                                
                                             
                                          
                                          +
                                          
                                             c
                                             
                                                2
                                                n
                                             
                                          
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   L
                                                   2
                                                
                                                
                                                   
                                                      f
                                                      
                                                         
                                                            x
                                                            j
                                                            *
                                                         
                                                      
                                                   
                                                
                                                .
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        c(≥0) is the trade-off parameter between the loss of labeled data and Universum data. It also can be regarded as the contribution of Universum data. Since we are trying to formulate our model as a boosting method, conventional boosting algorithms can be applied here, e.g. AdaBoost, GentleBoost [18], and LogitBoost [18]. For simplicity, we use AdaBoost with the traditional exponential loss function for the labeled data while squared loss function for Universum data, i.e. L
                        1(y,
                        f(x))=
                        e
                        −
                           yf(x), L
                        2(f(x*))=
                        f(x*)2. Then Eq. (4) can be written as:
                           
                              (5)
                              
                                 
                                    L
                                    
                                       
                                          y
                                          ,
                                          f
                                          
                                             x
                                          
                                       
                                    
                                    =
                                    
                                       1
                                       m
                                    
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          m
                                       
                                       
                                          
                                             e
                                             
                                                −
                                                
                                                   y
                                                   i
                                                
                                                f
                                                
                                                   
                                                      x
                                                      i
                                                   
                                                
                                             
                                          
                                          +
                                          
                                             c
                                             
                                                2
                                                n
                                             
                                          
                                          
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                f
                                                
                                                   
                                                      
                                                         x
                                                         j
                                                         *
                                                      
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

When c equals 0, the above equation becomes the standard AdaBoost.

Within the margin cost functional framework, ρi
                        
                        =
                        yif(x
                        
                           i
                        ) is defined as the margin of a labeled example (x
                        
                           i,
                        yi
                        ) and then L
                        1
                        =
                        e
                        −
                           ρ
                         in AdaBoost. Considering the loss for Universum data, we define the margin for Universum sample x
                        
                           j
                        
                        ⁎ with ρ
                        
                           j
                        
                        ⁎
                        =
                        f(x
                        
                           j
                        
                        ⁎) and the corresponding loss L
                        2
                        =
                        ρ
                        ⁎2.

Since L is a convex function (both L
                        1 and L
                        2 are convex) and the available weak classifiers span a convex set, Eq. (5) is a convex optimization problem. Then the optimization problem will converge to a globally optimal solution. We adopt the functional gradient descent view of boosting to select the weak learner during each iteration of the boosting [18,20]. According to the gradient descent principles, at the training step t, a weak classifier ht
                        (x) will be selected to be added to the current ensemble classifier f
                        
                           t
                           −1(x). This selected ht
                        (x) should improve the objective value based on the loss function. Given the current learned strong classifier at iteration t, ft
                        (x), the descent direction should be the gradient direction. Since the best weak classifier may not be available from the weak classifiers candidates (e.g. function family), the available best weak classifier can be obtained from the following equation. Note that we sometimes omit the argument x throughout this paper.


                        
                           
                              (6)
                              
                                 
                                    
                                       h
                                       *
                                    
                                    =
                                    arg
                                    
                                       max
                                       h
                                    
                                    −
                                    <
                                    
                                       ▽
                                       L
                                       
                                          y
                                          f
                                       
                                       ,
                                       h
                                    
                                    >
                                 
                              
                           
                        where <,> is dot product, <f,
                        h>=∑
                        
                           i
                        
                        
                        f(x
                        
                           i
                        )h(x
                        
                           i
                        ). The gradient of convex function L(y,f) based on Eq. (5) with respect to the current models f(x) can be written as:
                           
                              (7)
                              
                                 
                                    ▽
                                    L
                                    
                                       f
                                    
                                    
                                       x
                                    
                                    =
                                    
                                       
                                          
                                             
                                                l
                                                =
                                                
                                                   1
                                                   m
                                                
                                                y
                                                
                                                   e
                                                   
                                                      −
                                                      yf
                                                      
                                                         x
                                                      
                                                   
                                                
                                                ,
                                                x
                                                =
                                                
                                                   x
                                                   i
                                                
                                             
                                          
                                          
                                             
                                                
                                                   c
                                                   n
                                                
                                                f
                                                
                                                   
                                                      x
                                                      *
                                                   
                                                
                                                ,
                                                x
                                                =
                                                
                                                   x
                                                   j
                                                   *
                                                
                                                ,
                                             
                                          
                                          
                                             
                                                0
                                                ,
                                                otherwise
                                                ,
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

The sample weight is defined as
                           
                              (8)
                              
                                 
                                    
                                       w
                                       i
                                    
                                    =
                                    
                                       
                                          
                                             L
                                             ′
                                          
                                          
                                             
                                                ρ
                                                
                                                   
                                                      f
                                                      
                                                         
                                                            x
                                                            i
                                                         
                                                      
                                                      ,
                                                      
                                                         y
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                x
                                                i
                                             
                                          
                                          
                                             
                                                L
                                                ′
                                             
                                             
                                                
                                                   ρ
                                                   
                                                      
                                                         f
                                                         
                                                            
                                                               x
                                                               i
                                                            
                                                         
                                                         ,
                                                         
                                                            y
                                                            i
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                    with
                                    
                                    
                                       L
                                       1
                                       ′
                                    
                                    =
                                    −
                                    
                                       e
                                       
                                          −
                                          ρ
                                       
                                    
                                    ,
                                    
                                       L
                                       2
                                       ′
                                    
                                    =
                                    2
                                    
                                       ρ
                                       *
                                    
                                    .
                                 
                              
                           
                        
                     

Then the best weak classifier h
                        ⁎ for the current learned strong classifier f(x) should be
                           
                              (9)
                              
                                 
                                    
                                       
                                          
                                             h
                                             ∗
                                          
                                          =
                                          arg
                                          
                                             max
                                             h
                                          
                                          −
                                          <
                                          ▽
                                          L
                                          
                                             y
                                             f
                                          
                                          ,
                                          h
                                          >
                                       
                                    
                                    
                                       
                                          =
                                          −
                                          
                                             1
                                             
                                                m
                                                +
                                                n
                                             
                                          
                                          
                                             
                                                
                                                   1
                                                   m
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      m
                                                   
                                                   
                                                      −
                                                      
                                                         y
                                                         i
                                                      
                                                      h
                                                      
                                                         
                                                            x
                                                            i
                                                         
                                                      
                                                      
                                                         e
                                                         
                                                            −
                                                            
                                                               y
                                                               i
                                                            
                                                            f
                                                            
                                                               
                                                                  x
                                                                  i
                                                               
                                                            
                                                         
                                                      
                                                      +
                                                      
                                                         c
                                                         n
                                                      
                                                      
                                                         
                                                            ∑
                                                            
                                                               j
                                                               =
                                                               1
                                                            
                                                            n
                                                         
                                                         
                                                            f
                                                            
                                                               
                                                                  x
                                                                  j
                                                                  ∗
                                                               
                                                            
                                                            h
                                                            
                                                               
                                                                  x
                                                                  j
                                                                  ∗
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          =
                                          −
                                          
                                             1
                                             
                                                m
                                                +
                                                n
                                             
                                          
                                          
                                             
                                                
                                                   1
                                                   m
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      m
                                                   
                                                   
                                                      −
                                                      
                                                         y
                                                         i
                                                      
                                                      
                                                         w
                                                         i
                                                      
                                                      h
                                                      
                                                         
                                                            x
                                                            i
                                                         
                                                      
                                                      +
                                                      
                                                         c
                                                         n
                                                      
                                                      
                                                         
                                                            ∑
                                                            
                                                               j
                                                               =
                                                               1
                                                            
                                                            n
                                                         
                                                         
                                                            
                                                               w
                                                               j
                                                               ∗
                                                            
                                                            h
                                                            
                                                               
                                                                  x
                                                                  j
                                                                  ∗
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 w
                                 i
                              
                              =
                              
                                 e
                                 
                                    −
                                    
                                       y
                                       i
                                    
                                    f
                                    
                                       
                                          x
                                          i
                                       
                                    
                                 
                              
                           
                         and w
                        
                           j
                        
                        ⁎
                        =
                        f(x
                        
                           j
                        
                        ⁎) are the weights of labeled sample x
                        
                           i
                         and Universum data x
                        
                           j
                        
                        ⁎ based on Eq. (8).

Here we give a brief explanation of the weights for Universum samples. Since the optimal objective value for x
                        
                           j
                        
                        ⁎ is 0, 0 can be treated as its label. This is reasonable because it does not belong to any class of the problem while the optimal values for labeled samples should be their labels. Based on this explanation, Universum examples are misclassified during all the iterations since each weak classifier gives the prediction +1/−1. If its current predicted value h
                        
                           t
                        (x
                        
                           j
                        
                        ⁎) (+1/−1) is the same as the previous iteration h
                        
                           t
                           −1(x
                        
                           j
                        
                        ⁎), meaning that the previous error still exists, its weight increases. This is consistent with AdaBoost which amplifies the importance of those misclassified samples. On the other hand, if h
                        
                           t
                        (x
                        
                           j
                        
                        ⁎)≠
                        h
                        
                           t
                           −1(x
                        
                           j
                        
                        ⁎), its weight decreases since w
                        
                           j
                        
                        ⁎
                        =
                        f(x
                        
                           j
                        
                        ⁎)=∑
                        
                           t
                        
                        
                        α
                        
                           t
                        
                        h
                        
                           t
                        (x
                        
                           j
                        
                        ⁎). This implies that current weak classifier changes the prediction for this sample and decreases the ensemble predicate value (error).

The boosting algorithm should terminate as no h can be found to reduce L(y,f), i.e. −<▽
                        L(y,
                        f),
                        h
                        >≤0. The optional step size, in the direction of h
                        ⁎, is
                           
                              (10)
                              
                                 
                                    
                                       α
                                       
                                          h
                                          *
                                       
                                       *
                                    
                                    =
                                    arg
                                    
                                       min
                                       α
                                    
                                    L
                                    
                                       
                                          f
                                          
                                             x
                                          
                                          +
                                          α
                                          
                                             h
                                             *
                                          
                                          
                                             x
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        By the gradient descent method, the best 
                           
                              α
                              
                                 h
                                 *
                              
                              *
                           
                         happens when the gradient ▽
                        L(f
                        +
                        αh*)=0. That is
                           
                              (11)
                              
                                 
                                    
                                       
                                          
                                             
                                                ∂
                                                L
                                                
                                                   
                                                      f
                                                      +
                                                      α
                                                      
                                                         h
                                                         *
                                                      
                                                   
                                                
                                             
                                             
                                                ∂
                                                α
                                             
                                          
                                          =
                                          
                                             1
                                             m
                                          
                                          
                                             
                                                ∑
                                                i
                                             
                                             
                                                −
                                                y
                                                
                                                   h
                                                   *
                                                
                                                
                                                   e
                                                   
                                                      −
                                                      yf
                                                   
                                                
                                                
                                                   e
                                                   
                                                      −
                                                      yα
                                                      
                                                         h
                                                         *
                                                      
                                                   
                                                
                                                +
                                                
                                                   c
                                                   n
                                                
                                                
                                                   
                                                      ∑
                                                      j
                                                   
                                                   
                                                      
                                                         
                                                            f
                                                            +
                                                            α
                                                            
                                                               h
                                                               *
                                                            
                                                         
                                                      
                                                      
                                                         h
                                                         *
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          =
                                          
                                             1
                                             m
                                          
                                          
                                             
                                                ∑
                                                i
                                             
                                             
                                                −
                                                y
                                                
                                                   w
                                                   i
                                                
                                                
                                                   h
                                                   *
                                                
                                                
                                                   e
                                                   
                                                      −
                                                      yα
                                                      
                                                         h
                                                         *
                                                      
                                                   
                                                
                                                +
                                                
                                                   c
                                                   n
                                                
                                                
                                                   
                                                      ∑
                                                      j
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               w
                                                               j
                                                               *
                                                            
                                                            +
                                                            α
                                                            
                                                               h
                                                               *
                                                            
                                                         
                                                      
                                                      
                                                         h
                                                         *
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          =
                                          
                                             1
                                             m
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                            
                                                               
                                                                  y
                                                                  i
                                                               
                                                               ≠
                                                               
                                                                  h
                                                                  *
                                                               
                                                               
                                                                  
                                                                     x
                                                                     i
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                      
                                                      
                                                         w
                                                         i
                                                      
                                                      
                                                         e
                                                         α
                                                      
                                                      −
                                                      
                                                         
                                                            ∑
                                                            
                                                               
                                                                  y
                                                                  i
                                                               
                                                               =
                                                               
                                                                  h
                                                                  *
                                                               
                                                               
                                                                  
                                                                     x
                                                                     i
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                      
                                                      
                                                         w
                                                         i
                                                      
                                                      
                                                         e
                                                         
                                                            −
                                                            α
                                                         
                                                      
                                                   
                                                   ⏟
                                                
                                                
                                                   AdaBoost
                                                   
                                                   for
                                                   
                                                   labeled
                                                   
                                                   data
                                                
                                             
                                          
                                          +
                                          
                                             c
                                             n
                                          
                                          
                                             
                                                ∑
                                                j
                                             
                                             
                                          
                                          
                                             
                                                
                                                   w
                                                   j
                                                   *
                                                
                                                +
                                                α
                                                
                                                   h
                                                   *
                                                
                                             
                                          
                                          
                                             h
                                             *
                                          
                                       
                                    
                                 
                              
                           
                        when c equals 0, the optimization problem will become the standard AdaBoost. Then we can obtain the closed solution for α:
                           
                              (12)
                              
                                 
                                    
                                       α
                                       
                                          h
                                          *
                                       
                                       *
                                    
                                    =
                                    
                                       1
                                       2
                                    
                                    ln
                                    
                                       
                                          1
                                          −
                                          
                                             
                                                ∑
                                                
                                                   
                                                      y
                                                      i
                                                   
                                                   ≠
                                                   
                                                      h
                                                      *
                                                   
                                                   
                                                      
                                                         x
                                                         i
                                                      
                                                   
                                                
                                             
                                             
                                          
                                          
                                          
                                             w
                                             i
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   
                                                      y
                                                      i
                                                   
                                                   ≠
                                                   
                                                      h
                                                      *
                                                   
                                                   
                                                      
                                                         x
                                                         i
                                                      
                                                   
                                                
                                             
                                             
                                          
                                          
                                          
                                             w
                                             i
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

However, there is no closed solution in Eq. (11), so line search is performed to find the optimal step size. In practice Newton–Raphson method is used for its fast speed. The iteration at n
                        +1-th is
                           
                              (13)
                              
                                 
                                    
                                       α
                                       
                                          n
                                          +
                                          1
                                       
                                    
                                    =
                                    
                                       α
                                       n
                                    
                                    −
                                    
                                       
                                          ▽
                                          L
                                          
                                             
                                                α
                                                n
                                             
                                          
                                       
                                       
                                          
                                             ▽
                                             2
                                          
                                          L
                                          
                                             
                                                α
                                                n
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                        
                           Algorithm 1
                           
                              
                                 
                                    
                                 
                              
                           

The second order gradient is
                           
                              (14)
                              
                                 
                                    
                                       ▽
                                       2
                                    
                                    L
                                    
                                       
                                          f
                                          +
                                          α
                                          
                                             h
                                             *
                                          
                                       
                                    
                                    =
                                    
                                       1
                                       m
                                    
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          m
                                       
                                       
                                          
                                             w
                                             i
                                          
                                          
                                             e
                                             
                                                −
                                                yα
                                                
                                                   h
                                                   *
                                                
                                             
                                          
                                          +
                                          c
                                       
                                    
                                    .
                                 
                              
                           
                        The updated strong classifier is 
                           
                              
                                 f
                                 
                                    t
                                    +
                                    1
                                 
                              
                              =
                              
                                 f
                                 t
                              
                              +
                              
                                 α
                                 
                                    h
                                    *
                                 
                                 *
                              
                              
                                 h
                                 *
                              
                           
                        .

During the iterations, some variables can be pre-saved to speed up the calculation of the first derivative and the second derivative. Experiments show that the iteration stops after only several steps. The framework of 
                           U
                        AdaBoost is presented in Algorithm 1.


                        
                           U
                        AdaBoost is different from semi-supervised boosting algorithms because Universum data does not belong to any class of the problem while unlabeled data can be treated as either positive samples or negative samples. That is to say, there is an explicit label for unlabeled sample that even we won’t know during the learning procedure. The label of Universum sample is explicitly known which can be treated as 0 during the training.

Besides that, the difference of conventional semi-supervised boosting algorithms and 
                           U
                        AdaBoost is clear from the loss function. Specifically, a pseudoclass [21] or a pseudomargin [22] is always introduced to unlabeled data in semi-supervised algorithms. The pseudoclass label of an unlabeled point x is typically defined as 
                           
                              
                                 y
                                 ˜
                              
                              =
                              sign
                              
                                 
                                    f
                                    
                                       x
                                    
                                 
                              
                           
                         and its corresponding pseudomargin is 
                           
                              
                                 y
                                 ˜
                              
                              sign
                              
                                 
                                    f
                                    
                                       x
                                    
                                 
                              
                              =
                              |
                              f
                              
                                 x
                              
                              |
                           
                         
                        [22,21]. With pseudoclass or pseudomargin, semi-supervised boosting algorithms can be treated as a supervised learning problem. In comparison, 
                           U
                        AdaBoost omits the label for Universum data.

Training efficiency (e.g. training time) is an important issue to be considered in Universum based boosting. In order to select effective Universum data and proper parameter for the loss of Universum data, a large number of experiments is needed (e.g. cross validation). Fast training procedure is beneficial to better experimental design. Training speed is also of great concern when a large amount of weak classifier candidates and training iterations are evaluated, e.g. Haar-like feature in face detection application [23]. Here we compare the training speed of AdaBoost, 
                           U
                        Boost and 
                           U
                        AdaBoost. Boosting based algorithms are trained through a number of iterations. Thus training time is determined by the cost of one iteration and the number of iterations.

AdaBoost is the fastest in one iteration training for weak classifier selection since only labeled samples are included. For the Universum based boosting algorithms, 
                           U
                        AdaBoost and 
                           U
                        Boost share the same weak classifier candidates' generation step. 
                           U
                        AdaBoost only needs to update one weak classifier coefficient α in each training iteration while all coefficients for the pre-selected weak classifiers in 
                           U
                        Boost are updated. Thus 
                           U
                        AdaBoost is faster than 
                           U
                        Boost in each iteration.

In terms of number of iterations, both 
                           U
                        Boost and 
                           U
                        AdaBoost have a faster convergence speed than AdaBoost. 
                           U
                        Boost needs the least number of iterations to converge because column generation is used. Fig. 2
                         shows that three algorithms converge at about 100 iterations. Experiment is performed on MNIST data set (see description in Section 5). Although 
                           U
                        Boost obtains the best training performance, the gaps with 
                           U
                        AdaBoost are marginal. Overall, to reach the convergence point, the total time cost by training procedures for 
                           U
                        Boost is much more than 
                           U
                        AdaBoost. Experiments show that the speed of 
                           U
                        AdaBoost is more than 10 times faster than 
                           U
                        Boost. Specific total time for training a classifier is shown in Table 1
                        .

Although Universum data has been applied into a lot of applications and has been analyzed, it needs careful selection since not all the generated Universum samples are effective and informative. Cherkassky et al. [19] took histogram as an analysis tool and gave practical conditions to verify the effectiveness of Universum data under SVM classifier. First, an optimal SVM classifier will be trained with the labeled data. Then they project the training data and Universum data onto the normal direction vector of the trained hyperplane. Finally, the relationships of training data and Universum data are analyzed. The histogram of projections of effective Universum data should be symmetric relative to the SVM decision boundary and has a wide distribution between margin borders (+1/−1).

These conditions are designed for SVM based learning. When they are applied into boosting based learning, two differences happen. The first one is the standard boosting does not need parameters to tune compared to the standard SVM which has one parameter 
                        
                           C
                           L
                        
                      in Eq. (1). When Universum data is added, least square versions of 
                        U
                     -SVM and 
                        U
                     AdaBoost both need another parameter of the trade-off losses between labeled data and Universum data. The original 
                        U
                     -SVM still needs another parameter ε in Eq. (1). That is to say, 
                        U
                     AdaBoost is easier to use than 
                        U
                     -SVM. The second one is that AdaBoost is not an explicit margin controlled algorithm. It only considers the training loss while SVM gives an explicit margin in the optimization problem in Eq. (1). AdaBoost does not maximize the minimum margin but has some relations about the average and division of the margin [24]. In comparison, SVM separates the two classes with a clear margin [−1+1].

Based on above analysis, the same Universum data matching the above conditions will not be appropriate for 
                        U
                     AdaBoost. But we can still analyze Universum data with the tool of histogram. As in [19], the distribution of effective Universum data should be symmetric relative to decision hyperplane w
                     ⊤
                     x
                     +
                     b
                     =0. In boosting, the decision function is f(x)=∑
                     
                        t
                     
                     α
                     
                        t
                     
                     h
                     
                        t
                     (x)+
                     b,with b
                     =0 from Section 3.2. In practice, we can project the available Universum data onto the pre-trained AdaBoost classifier and choose those samples meeting the conditions above. For a separate problem, AdaBoost can also give a clear margin (similar to Fig. 5). The following experiments will show that the performance of 
                        U
                     AdaBoost can be further improved by selecting effective Universum data.

Handwritten digits sets are always used for evaluating the algorithms with Universum data [15,16,3]. The reason is that if we take two digits for classification, the other digits data can be naturally used as Universum data. We experiment 
                           U
                        AdaBoost on two handwritten sets: MNIST and USPS, and a computer vision application: gender recognition. The original feature provided in the data sets (gray value) is used. Decision stump is applied as weak classifiers in all tests and the maximum number of iterations Tmax
                         is limited to 1000. Note that most of the boosting algorithms converge at less than 100 iterations from the experiments. There is only one parameter c for the loss ratio between labeled data and Universum data. c is validated from {2−17, 2−15, 2−13, 2−11, 2−9, 2−7, 2−5}. The best performance with the highest accuracy on validation set will be selected. The experiments are run on all the data sets for 10 times, and average test errors with standard deviations are reported.

MNIST handwritten digits data set contains 10 categories of digits with 1,000 for each digit and 10,000 in total. It has split the total data into training set and test set. Each digit is represented by a 28×28 image. Sample images are shown in Fig. 3
                           .

Since both 
                              U
                           -SVM and 
                              U
                           Boost showed their performance of classifying digits ‘5’ and ‘8’ with digits ‘3’ and ‘6’ for Universum data, we also take this option. The training and validation samples are randomly selected from the original training set. The original test set is used for testing. We use all the training examples of digits ‘3’ and ‘6’ for Universum data (12,094 examples). Four experiments are performed with different sizes of training and validation samples. The size of validation set is the same as training set which is selected from {200, 400, 600, 800}. The classification accuracies and standard deviations are reported in Table 2
                           . In addition, we fix the size of validation set with 400 and change the size for training set. The results are shown in Table 3
                           . Both Tables 2 and 3 show that 
                              U
                           AdaBoost outperformed AdaBoost under different situations. Compared with 
                              U
                           Boost, our method achieves better performances for most of the cases. Still we notice that the performance of 
                              U
                           Boost is unstable. Sometimes, it is even inferior to AdaBoost.

The USPS data set contains 16×16 images for 10 categories of digits. Sample images are shown in Fig. 3. Like the experiment of MNIST data set, we classify digits ‘5’ vs. ‘8’ with ‘3’ and ‘6’ as Universum data. The original test is used for testing. Four experiments are taken with different sizes of training examples which are selected from {100, 200, 400, 600}. The validation set contains 200 examples. All the training examples of digits ‘3’ and ‘6’ are used as Universum data, containing 1,287 examples. The classification results are shown Table 4
                           . Our proposed algorithm achieves better performances over AdaBoost and 
                              U
                           Boost in most cases.

Another experiment shows the effectiveness of different sizes of Universum data. We use 400 examples for training and 400 examples for validation. The Universum data set is selected with different sizes (100, 300, 500, 800). From Table 5
                           , we can see that, the performances of 
                              U
                           AdaBoost are further improved with more Universum data.

We test 
                              U
                           AdaBoost on a gender recognition task [14]. The data set contains 113 males and 20 females (20 images per person). Sample images are shown in Fig. 4
                           . The colored face images are converted to 8-bit gray level images and scaled to 45×50 sizes without extra preprocessing. The feature length for each training example is 2250.

Following the experimental setting of [14], a set of 52 individuals (32 males and 20 females) is randomly selected, containing 13 individuals for training (8 males and 5 females) and the remaining 39 individuals for test. The training and validation sets are randomly selected from 52 individuals (one image for one individual) consisting of 13 training samples, 13 validation samples and 39 test samples. Universum data is generated by random average [14] shown in Fig. 4 (the average value of one male sample and one female sample). Experimental results with different sizes of Universum data are shown in Table 6
                           . Similar to previous results, the performances of the standard AdaBoost are improved by Universum data.

In order to further take advantage of Universum data, we refine the Universum data by ignoring the Universum samples whose final confidence scores are out of the margin of the pre-trained classifier. For example, for the setup of Table 3, the histograms of projections over training data and Universum data are shown in Fig. 5
                        . The margin is about [−0.18 0.18] which is different from SVM based margin. The majority of projections of Universum samples are located in the margin borders. We refine the original Universum data and reject about 3,000 Universum samples. Then we retrain the classifier with this newly generated Universum data. The results are shown in the last row of Table 3. It shows that the performance of 
                           U
                        AdaBoost can be further improved. Similar results can be seen in Table 4.

In order to investigate the effectiveness of the different kinds of Universum data, additional experiments are conducted on three kinds of Universum data. For handwritten digits classification in MNIST data, digits ‘1’, ‘3’ and ‘6’ are chosen as Universum data to classify digits ‘5’ and ‘8’. The training/validation set size is 200 (100 per class); Universum set size is 1,000 and test set size is 1,866. Experimental results in Table 7
                         show that 
                           U
                        AdaBoost outperforms AdaBoost with all the three Universum data, and digits ‘3’ and ‘6’ make it better. The observation is the same as 
                           U
                        -SVM in [19]. Fig. 6
                         shows the histograms of projection for the three kinds of Universum data. This confirms the result of Table 7 that digit ‘1’ is not as good as digits ‘3’ and ‘6’ for Universum data because its histogram of projections is more biased and a large amount of them is out of the decision boundary.


                        
                           U
                        AdaBoost follows the same procedures as AdaBoost while the model of 
                           U
                        Boost is based on AdaBoost-CG. The classification performance of AdaBoost-CG is similar to the standard stage-wise AdaBoost. There is no theoretical evidence to clarify which one is better [17]. Above experiments show such conclusion still remains when they meet Universum data. During the experiments, we find that 
                           U
                        Boost is sensitive to parameters. Inappropriate parameters (i.e. c and D in Eq. (3)) will result in large decreases of performance. Only one parameter in 
                           U
                        AdaBoost (i.e. c in Eq. (5)) makes it more practical to use. Finally, with carefully selected Universum data, it is easy to predict the improved performance of AdaBoost with the help of Universum data.

@&#CONCLUSION@&#

This paper presented a new algorithm 
                        U
                     AdaBoost which improved the performance of AdaBoost by taking advantage of Universum data. Explicit weighting schemes were derived by functional gradient descent which made the speed of 
                        U
                     AdaBoost similar to AdaBoost. By analyzing the distribution of Universum data on AdaBoost classifier, more informative and effective Universum data can be obtained. Experiments showed that the performance of 
                        U
                     AdaBoost can be improved further with the refined Universum data.

@&#REFERENCES@&#

