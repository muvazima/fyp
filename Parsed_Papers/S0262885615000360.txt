@&#MAIN-TITLE@&#Document image binarization using local features and Gaussian mixture modeling

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Background removal technique based on adaptive median filtering and thresholding


                        
                        
                           
                           A Local Co-occurrence Map with local contrast can distinguish between document text and document stains and background.


                        
                        
                           
                           Low complexity approach with fast and accurate binarization results


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Binarization

Handwritten documents

Historic documents

Classification

Background estimation

@&#ABSTRACT@&#


               
               
                  In this paper, we address the document image binarization problem with a three-stage procedure. First, possible stains and general document background information are removed from the image through a background removal stage. The remaining misclassified background and character pixels are then separated using a Local Co-occurrence Mapping, local contrast and a two-state Gaussian Mixture Model. Finally, some isolated misclassified components are removed by a morphology operator. The proposed scheme offers robust and fast performance, especially for both handwritten and printed documents, which compares favorably with other binarization methods.
               
            

@&#INTRODUCTION@&#

Document images commonly arise from historical documents, books or printed documents that are digitized using a scanning device. The advancement of imaging devices, such as scanners and digital cameras, has widely facilitated the digitization of paper-printed material, including historical documents and books. Many libraries throughout the world, such as the British Library in London, UK,
                        1
                     
                     
                        1
                        
                           http://www.bl.uk/aboutus/stratpolprog/digi/digitisation/.
                      have digitized books, manuscripts and other printed material from their collection, which are available online as images. We can extract the text information from these document images using Optical Character Recognition (OCR) techniques. Nevertheless, to enhance the performance of OCR algorithms, a number of preprocessing steps are systematically applied, including page skew detection, artifact and noise removal, document page layout analysis and document image binarization [1–4]. In this paper, we address the problem of background removal and document image binarization.

Scanned documents often contain undesired textual noise, such as specks, dots, black borders, lines, and hole-punch marks. Background estimation and removal is a preparatory step that enhances the quality of the document images and is beneficial for binarization techniques [5–8]. For example, historic document images often suffer from different types of degradation that render document image binarization and character recognition very challenging tasks. In summary, the main objective of background removal techniques is to remove all these degradations from a document image and enhance the discrimination of characters from the page background.

After the original document images have been enhanced, the output of most document processing systems is a bi-level image containing characters and background. Image binarization can then be performed either on a global or a local basis. Conventional binarization techniques of gray-scale documents were initially based on global thresholding algorithms (clustering approaches) [9], which have proved to be efficient in converting simple gray-scale images into a binary form but are inappropriate for complex documents, and degraded documents. For this purpose, the local binarization techniques of Niblack [10], Sauvola [11] and Bernsen [12] have been extensively used by the document image processing community. There are numerous specialized binarization techniques for document images (see [13] for a more detailed review). Here, we will outline several important binarization methods that have appeared so far.

In [1], Papamarkos proposed a neuro-fuzzy technique for binarization and gray-level (or color) reduction of mixed-type documents. Badekas and Papamarkos [13] proposed a binarization technique that combines the results of multiple binarization algorithms using a Kohonen Self-Organizing Map (KSOM) neural network. In [14], the binarization results of many independent techniques were initially produced and then combined with a Kohonen Self-Organizing Map (KSOM). Badekas et al. [15] also introduced a binarization technique, specialized for color documents, where the resulting “binary” image contains the detected text regions with black characters in white background leaving the remaining original color parts of the document intact. In [16], Makridis and Papamarkos introduced a two-stage approach to image binarization. The first stage included a background removal technique that was based on fixed-size median filtering of the document image. Once the background was removed, the second stage aimed at creating 2D clusters of neighboring pixels of similar intensity, i.e., document characters and background. Binarization was then performed by identifying 2 clusters (text-background) using the multithresholding technique of Reddi et al. [17].

Gatos et al. [18] (GPP method) estimated the document background by an adaptive threshold which labels each pixel as either text or background. To estimate the background surface, they used Sauvola's binarization algorithm to roughly extract the text pixels and calculated the background surface from them by interpolation of neighboring background pixels intensities. For the other pixels, background surface is set to the gray level of the original image. Ntirogiannis et al. [19] proposed a modular system for handwritten document binarization. Background is initially estimated via an inpainting procedure starting from the Niblack binarization output. The background estimate is then normalized to smooth great variations and is used as an input to Otsu's global thresholding which removes most unwanted noise but also some faint characters. Therefore, the local binarization algorithm of Niblack is also used, but initialised using the stroke width information, extracted by skeletonization of Otsu's output, window size and contrast information. The two binarization outputs are combined at connected component level.

In [20], Su et al. demonstrated the use of local contrast image thresholding in estimating the text stroke width more accurately. In [6], Lu et al. performed background estimation using a modified version of 1D iterative polynomial smoothing to compensate for several degradation types. Text-stroke edges are then identified via Otsu's global thresholding on L1-norm horizontal and vertical edge detection. Document text pixels are extracted, since they are surrounded by text stoke edges and feature lower intensity levels.

Hedjam et al. [7] used grid-based modeling and impainting techniques to recover text pixels starting from an under-binarization result using Sauvola's technique. The proposed technique featured smooth and continuous strokes, due to its spatially adaptive estimation of the text pixels' statistical features. Moghaddam and Cheriet [8] presented an adaptive form of Otsu's thresholding for binarization. Based on a rough binarization result, they produce an estimated background and a stroke gray level map using a multi-scale framework. This estimated background is further refined using the AdOtsu method, which is an adaptive, parameterless form of Otsu's thresholding, which is generalized to a multiscale setup. Finally, skeleton-based post-processing is employed to remove possible artifacts and sub-strokes.

Valizadeh and Kabir [21] devised a novel feature space consisting of the structural contrast and the intensity value of each pixel. Structural contrast relates text stroke width, pixels' intensities and their relationships with their neighbors at stroke width distances. This results in a 2D image representation where text and background pixels are separable. Clustering is performed by partitioning the feature space into small regions. Then, using the result of another binarization algorithm with at least 50% successful labeling (Niblack), each region is classified either as background or text, according to the prevailing number of text or background pixels in the region. The reverse procedure produces the document binary image.

Howe [2] performed binarization by minimizing a global energy functional inspired by Markov Random Fields, where a) the image Laplacian edge map is employed to distinguish between ink and background in the energy data fidelity term and b) ink discontinuities are enforced in the binarization result by incorporating a Canny edge detector into the smoothness term. Howe also introduced a procedure to automate the optimal parameter selection for his algorithm.

Ramirez-Ortegon et al. [22] introduced the concept of transition pixel, i.e., calculating intensity differences over a small neighborhood, which can then be employed by common gray-level thresholding algorithms to produce a binarization result (transition method). This was further refined in [23], where an unsupervised thresholding was proposed for unimodal histograms, assuming Gaussian priors for the distribution of character and background neighborhoods. In [4], the method was enriched with a mechanism to remove binary artifacts after binarization. An auxiliary image is calculated via minimum-error-rate thresholding. The connected components of the auxiliary and the original binary image are compared in terms of an intersection ratio to remove possible binarization artifacts. In [24], Ramirez-Ortegon et al. explored possible effects of inaccurate estimations of the transition proportion on the estimated thresholds. In [25], Ramirez-Ortegon et al. proposed the use of skewed log-normal, instead of symmetrical Gaussian, priors [23] for the background and character clusters.

Lelore and Bouchara [3] introduced the FAIR binarization algorithm, where they ran the S-FAIR (simplified) algorithm for two threshold values: one giving a noiseless binarization output but with important edges missing and another containing all character edges but with some misclassification noise. The S-FAIR algorithm first performs text localization using the Canny algorithm. A Gaussian Mixture Model is then used to classify pixels around edges to belong either to the text or the background image or to a third class where pixels cannot be attributed with certainty to text or background. The FAIR algorithm merges the two outputs with a “max” rule. Finally, a post-filtering process classifies unknown pixels using a variety of rules. The most important feature is an iterative procedure where the text labeled regions grow into the unknown using morphological dilation and the previous EM algorithm is used to define the final class of these regions. Final unknown areas are connected morphologically and labeled according to neighboring pixels.

In this paper, the authors extend the previous work of Makridis and Papamarkos [16] toward a more automated three-stage document image binarization system. In the first stage, the background removal technique in [16] is enhanced by automating the window size selection for the median filter and improving the threshold selection between the document image and the background estimate. In the second stage, the proposed local neighborhood representation is redesigned to also include local contrast information to enhance the presence of character outlines. Binarization is then performed by separating two clusters of document characters and background artifacts that were not removed in the first stage of background removal. Clustering is performed using Mixtures of Gaussians (MoG). The Gaussian with lowest value mean corresponds to the character cluster. The local neighborhood representation share a similar concept with those introduced by Valizadeh and Kabir [21] and Ramirez-Ortegon et al. [22], however, the proposed multidimensional representation is different to the 1D representations discussed in [21,22]. Contrast information for binarization was also used by Su et al. [20], however, in this work contrast is incorporated into a local intensity representation forming a joint, rather than an isolated feature. Similarly, Gaussian modeling for binarization has been employed before by Hedjam et al. [7] and Ramirez-Ortegon et al. [23], but here it is applied on the novel LCM representation. Moreover, MoG-based clustering is a common clustering technique in pattern recognition, thus it is the application that is novel here. In the final post-processing stage, small-size 8-connected clusters are removed to eliminate possible binarization noise.

The paper is organized as follows: Section 2 sets the essential notation and outlines the system. Section 3 describes the background removal process in detail; Section 4 describes the binarization stage using GMM clustering; Section 5 explains the post-processing step; Section 6 presents the evaluation results of the proposed methodology and finally Section 7 concludes this paper.

Let I(x,
                     y) be the initial color document image of size 3×
                     M
                     ×
                     N, where x,
                     y denote integer samples across the horizontal and vertical axes. The desired output of a document image binarization algorithm is a bi-level M
                     ×
                     N image I
                     
                        BN
                     (x,
                     y) that attributes the value 255 (white) to background pixels and the value 0 (black) to character pixels. It consists of three stages: a) the Background Removal stage, b) the Image Binarization stage and c) the post-processing stage, which are then presented in detail.

Background removal is a preprocessing stage in a document binarization system that can eliminate the presence of artifacts, including stains, paper cuts, paper coloring and opposite-page ink leaks, prior to binarization.

The first step is to map the three-channel RGB image to an one-channel intensity image that detains all the useful information from all color channels. One method is to simply average all three channels to create the intensity image, which has been shown not to be effective in our experiments. Another method is to move to another color space, such as the Hue–Saturation–Luminance (HSL) cylindrical color space, where the color information (HS channels) is isolated from the Luminance (L) channel, which is kept for further processing (as implemented by MATLAB's rgb2gray function). Several techniques have also been proposed that attempt to produce gray-scale images with visual contrast similar to the color contrast [26,27]. In [28], a linear transform is proposed that converts a color image to a gray-scale image in such a way that the variance of the transformation is maximized and at the same time, the gray-scale image preserves the brightness of the color image. Also, Kanan and Cottrell [29] proposed new techniques for general color to gray conversion. Recently, Moghaddam and Cheriet [30] developed a new heuristic technique that is based on a dual transformation, color reduction and interpolation. In order to ensure that all useful information from all color channels is conveyed to the grayscale image, we perform Principal Component Analysis [31] on the multichannel image. The principal component image is then retained as the grayscale image. This methodology for grayscale conversion is pursued in our system. In Fig. 1
                        , we can see an example of a color document image conversion to grayscale using PCA. The final grayscale image appears to have increased contrast compared to a typical grayscale conversion.

The proposed background removal algorithm is based on the observation that the aforementioned artifacts can be isolated from the original image by performing low-pass filtering of long window size [16]. This long-window low-pass filtering can essentially filter out the document characters, as they are generally small-size high-pass details, leaving only an image containing artifacts and the document background that needs to be removed. Median filtering is more preferable to ordinary low-pass filtering, since this will not create new intensity values in the document image, but will simply replace the character intensity values with background or artifact intensity values. Nonetheless, the size of the median filter window needs to be defined. In [16], Makridis and Papamarkos used a fixed window size, which was defined by the user. In this study, we propose to automate the procedure, by starting with a small median filter window of size G
                        =5. After median filtering the input image I(x,
                        y), we measure the standard deviation of every possible 3×3 image patch. If the standard deviation of the majority of image patches (e.g., 98%) is greater than a threshold value S
                        
                           I
                         (e.g., S
                        
                           I
                        
                        =6), this implies that image still contains character information and the median filter window has to increase by 5, i.e., G
                        ←
                        G
                        +5. This procedure is repeated until most 3×3 patches have low standard deviation, i.e., low-order texture, background. The final image I
                        
                           MED
                        (x,
                        y) is an estimate of the document background. The above values of 98% and S
                        
                           I
                        
                        =6 have been determined by experimentation on the DIBCO [32–36] image datasets and remain unchanged. A more detailed study to determine the statistical properties of a background image is presented by Ramirez-Ortegon et al. [25], where similar values for S
                        
                           I
                         are reported. A more extensive investigation of this parameter goes beyond the scope of this paper, since it does not appear to greatly affect performance.

To remove the document background from the document image and form the “No-Background” I
                        
                           NBG
                        (x,
                        y) image, a simple comparison classifies every pixel (x,
                        y) as background or text. If the absolute difference between the original image intensity I(x,
                        y) and the I
                        
                           MED
                        (x,
                        y) is below a selected threshold value T, then this pixel must be part of the document background and is attributed the value white, i.e., I
                        
                           NBG
                        (x,
                        y)=255. In the opposite case, this pixel is very different from the background image and thus must be a character pixel. Therefore, we set I
                        
                           NBG
                        (x,
                        y)=
                        I(x,
                        y).

In Fig. 3
                        
                        , we depict the various stages of the background removal algorithm. An original document image of size 682×690 is depicted in Fig. 3(a). The background image estimate for G
                        =20 is shown in Fig. 3(b) and the final estimate for G
                        =30 in Fig. 3(c). The document image with the estimated background removed for various values of T are shown in Fig. 3(d), (e). Selecting larger values of T, more parts of the document image are classified as background and thus are removed (transformed to 255) from the image. Hence, the value of T can define the background removal strength of the algorithm. However, selecting larger values for T may remove character information apart from unwanted noise.

One can make the selection of T more adaptive, by calculating a histogram of |I(x,
                        y)−
                        I
                        
                           med
                        (x,
                        y)|. Dividing histogram values by the number of image pixels, we get an approximate probability density estimate p
                        
                           i
                         of the previous difference. In most document images we encountered in this study, this density seems to be a decreasing function (see Fig. 2). Thus, an adaptive threshold value of T can be set at the point, where this probability falls below a fraction q of this maximum value, i.e., qmax(p
                        
                           i
                        ) with q
                        ∈[0,1]. This provides a more general threshold which is more adaptive for different images than selecting a specific value for T. Lowering q removes more background information, while increasing q leaves more background information unprocessed. In our system, we tend to keep the background removal stage less strict, so as not to accidentally remove character parts or outlines in the background removal stage. In Fig. 3(f), the background removal result is depicted using a value of q
                        =0.5. Although thresholding is now more adaptive to a variety of document images, the parameter q has to be manually selected. The specification of this parameter remains key to the performance of the binarization stage, as it will be explained in the experimental section.

This image is then presented to the image binarization algorithm, described in the next section. The proposed algorithm is summarized below:
                           
                              Document image background removal algorithm
                                    
                                       1.
                                       Transform the initial M
                                          ×
                                          N color document image I(x,
                                          y) to an 1-channel image I(x,
                                          y), using only the Principal Component.

Set a neighborhood size G
                                          ×
                                          G, where G
                                          =5.

Estimate
                                             
                                                (1)
                                                
                                                   
                                                      I
                                                      
                                                         M
                                                         E
                                                         D
                                                      
                                                   
                                                   
                                                      x
                                                      y
                                                   
                                                   =
                                                   
                                                      median
                                                      G
                                                   
                                                   
                                                      
                                                         I
                                                         
                                                            x
                                                            y
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       

Calculate the standard deviation σ(x,
                                          y) of every 3×3 patch in I
                                          
                                             MED
                                          (x,
                                          y).

If the number of patches that satisfy the condition σ(x,
                                          y)<
                                          S
                                          
                                             I
                                           is less than 0.98MN then set G
                                          ←
                                          G
                                          +5 and repeat steps 3, 4, 5.

Estimate a value for T, where the normalized histogram p
                                          
                                             i
                                           of |I(x,
                                          y)−
                                          I
                                          
                                             med
                                          (x,
                                          y)| falls below qmax(p
                                          
                                             i
                                          ).

The document image without background I
                                          
                                             NBG
                                          (x,
                                          y) is given by:
                                             
                                                (2)
                                                
                                                   
                                                      I
                                                      
                                                         N
                                                         B
                                                         G
                                                      
                                                   
                                                   
                                                      x
                                                      y
                                                   
                                                   =
                                                   
                                                      
                                                         
                                                            
                                                               I
                                                               
                                                                  x
                                                                  y
                                                               
                                                               ,
                                                            
                                                            
                                                               if
                                                               |
                                                               I
                                                               
                                                                  x
                                                                  y
                                                               
                                                               −
                                                               
                                                                  I
                                                                  
                                                                     m
                                                                     e
                                                                     d
                                                                  
                                                               
                                                               
                                                                  x
                                                                  y
                                                               
                                                               |
                                                               >
                                                               T
                                                            
                                                         
                                                         
                                                            
                                                               255
                                                               ,
                                                            
                                                            
                                                               if
                                                               |
                                                               I
                                                               
                                                                  x
                                                                  y
                                                               
                                                               −
                                                               
                                                                  I
                                                                  
                                                                     m
                                                                     e
                                                                     d
                                                                  
                                                               
                                                               
                                                                  x
                                                                  y
                                                               
                                                               |
                                                               ≤
                                                               T
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       

Document Image Binarization is defined as the process where a grayscale document image I(x,
                     y) is transformed into a bi-level image I
                     
                        BN
                     (x,
                     y), where I
                     
                        BN
                     (x,
                     y)=0 for each pixel (x,
                     y) that is attributed to a document character and I
                     
                        BN
                     (x,
                     y)=255 for each pixel (x,
                     y) that is attributed to background. Local thresholding methods seem to offer more stable solutions, exploiting local statistical measurements, including the local mean, standard deviation, entropy and contrast.

Some other local character properties that can be exploited to perform binarization are the following:
                        
                           •
                           Pixels belonging to the same character are geometrically close.

Pixels belonging to the same character should feature similar intensity values.

Any local area (neighborhood) that includes the outline of a character should have increased contrast, compared to areas containing only background or only character pixels.

In Fig. 4
                     , we show some examples of the above principles in a document image. These principles were also discussed in a more mathematical manner in [22].

In this section, we will use these properties to create a Local Co-occurrence Mapping (LCM) that will assist us in discriminating between the character and the background pixels. The first two properties were initially discussed in [16], leading to the introduction of a Symmetrical Frequency Map (SFM) that was used to perform binarization. Here, we extend this framework to use these three properties simultaneously and increase binarization performance.

To emphasize proximity and connectivity between neighboring character pixels, the main concept is to devise a co-occurrence map in the following manner. The image is divided into every distinct Q
                        ×
                        Q patch. This implies that these patches are created with 1-pixel overlap from the original image. Let (x
                        
                           c
                        
                        
                           i
                        ,
                        y
                        
                           c
                        
                        
                           i
                        ) be the center pixel of the i-th patch. Each distinct patch is then transformed to the following (Q
                        2
                        −1) points in the 2D space given by:
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             
                                                I
                                                
                                                   N
                                                   B
                                                   G
                                                
                                             
                                             
                                                
                                                   x
                                                   c
                                                   i
                                                
                                                
                                                   y
                                                   c
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                I
                                                
                                                   N
                                                   B
                                                   G
                                                
                                             
                                             
                                                
                                                   
                                                      x
                                                      c
                                                      i
                                                   
                                                   +
                                                   d
                                                   x
                                                   ,
                                                   
                                                      y
                                                      c
                                                      i
                                                   
                                                   +
                                                   dy
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 ∀
                                 −
                                 
                                    
                                       Q
                                       /
                                       2
                                    
                                 
                                 ≤
                                 d
                                 x
                                 ,
                                 dy
                                 ≤
                                 
                                    
                                       Q
                                       /
                                       2
                                    
                                 
                              
                           
                        
                     

In other words, each pixel in the i-th patch is transformed to a 2D point containing the intensity of the central patch pixel and the pixel's intensity. The whole procedure is visualized in Fig. 5
                        . We note that the combination of the center pixel with itself is not included in the formation of this group of 2D points, since it does not offer any information about connectivity. Thus, each patch produces a set of (Q
                        2
                        −1) 2D points denoted by I
                        
                           W
                        (t
                        
                           i
                        ), where t
                        
                           i
                        
                        =1,…,(M
                        −
                        Q
                        +1)(N
                        −
                        Q
                        +1) is an index that runs through all possible image patches. Repeating the procedure for all possible Q
                        ×
                        Q patches of the image yields the Local Co-occurrence Mapping (LCM), i.e., the new image representation I
                        
                           W
                        (k), where k represents the 2D-point index. The new image representation is of size 2×(M
                        −
                        Q
                        +1)(N
                        −
                        Q
                        +1)(Q
                        2
                        −1). Calculating the 2D histogram of the 2D points I
                        
                           W
                        (k), we acquire the Symmetrical Frequency Map (SFM), as proposed by Makridis and Papamarkos [16]. In Fig. 6(a), a typical SFM histogram is depicted.

One can observe the basic properties of this histogram. First of all, the SFM plot is symmetric over the main diagonal, because in two overlapping patches for i.e., Q
                        =3, one can get the symmetric points [I
                        
                           NBG
                        (x,
                        y)I
                        
                           NBG
                        (x
                        +1,
                        y
                        +1)]
                           T
                         [I
                        
                           NBG
                        (x
                        +1,
                        y
                        +1)I
                        
                           NBG
                        (x,
                        y)]
                           T
                         and are counted twice. The most important property is that there are two main concentrations of points: one where the center pixel takes higher intensity levels along with its neighboring pixels and one where the center pixels and its neighbors take lower intensity levels. The first point-cluster represents background pixels and the second point-cluster represents character pixels. The same trend appears in most printed or handwritten document images in our experiments using the DIBCO [32] image database. The only difference is there might be more visible clusters, due to paper stains or other artifacts of different intensity (see Fig. 7(a)). However, these small clusters can be re-grouped in two main clusters: one of lower intensity denoting characters and one of higher intensity denoting background. This can be achieved during the clustering phase and will be discussed in a later section.

Observing the original SFM histograms in many document images, we made the following observations. Firstly, the character cluster is usually shorter and smaller compared to the background cluster, since characters constitute only a small part of the image, compared to background pixels. This will hinder the task of any clustering attempt to estimate the character cluster, since the background cluster dominates the SFM histogram. In addition, this mapping is usually following the background removal stage, which implies that many pixel values will be set to 255 by the background removal process. This will cause a huge concentration of points around the point (255,255), which will make the character cluster barely visible and thus really difficult to be identified by a clustering algorithm.

The main proposal here is to remove all 2D-points whose central pixel value equals to 255 from I
                        
                           W
                        (t
                        
                           i
                        ). These points have already been classified as background and therefore should not be part of the binarization process. After removing these points, the SFM histograms change considerably. The character clusters are more visible compared to the background cluster. In addition, the actual task that is required to solve here has also changed. After removing the pixels that have been classified as background, this image binarization step aims at discriminating between the character and the misclassified background or artifact pixels. In Figs. 6(b), 7(b), the improvement in the two previous SFM histograms is depicted. The new SFM histograms in either case contain two prominent clusters : the character and the artifacts cluster. The character cluster is now much stronger, compared to previous SFM histograms. After removing the background pixels, the proportion of character and artifact pixels is now comparable. This will improve clustering performance, since the character cluster is more clearly separable than previously.

Another improvement in the LCM framework is to remove 2D points far from the main diagonal. Ideally, character pixels should have similar intensity values with the central pixel, allowing for some slight deviation. Thus, pixels far from the main diagonal should be attributed to local noise and should be removed. We measure the distance d of each 2D point from the main diagonal and if it exceeds a threshold then it is rejected. The choice of threshold d should be carefully selected, as we will see in the experimental section. A narrow choice of d results into character thinning. A rather large choice of d may undermine performance, since it incorporates noise. Optimal values for d will be discussed in the experimental section.

One can also use different neighborhood patterns around each central pixel, such as cross or diamond neighborhoods. This produced inferior results in our experiments. Also, the proposed 2D point representation resembles the 2D point representation proposed by Valizadeh and Kabir [21], with the difference being that their points contain structural contrast and local intensity and they look at neighboring pixels at stroke-width distance.

So far, we have incorporated the first two of the three previously mentioned local character properties in the LCM representation. The third property emphasizes the existence of strong contrast in the Q
                        ×
                        Q neighborhood, which denotes the existence of character outlines. To include this information in the LCM, we will simply calculate local contrast for each Q
                        ×
                        Q image patch and its value will be incorporated in the LCM representation as a third dimension. The contrast of each patch C(I
                        
                           W
                        (t
                        
                           i
                        )) is calculated by the following equation:
                           
                              (4)
                              
                                 C
                                 
                                    
                                       
                                          I
                                          W
                                       
                                       
                                          
                                             t
                                             i
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       max
                                       
                                          
                                             
                                                I
                                                W
                                             
                                             
                                                
                                                   t
                                                   i
                                                
                                             
                                          
                                       
                                       −
                                       min
                                       
                                          
                                             
                                                I
                                                W
                                             
                                             
                                                
                                                   t
                                                   i
                                                
                                             
                                          
                                       
                                    
                                    
                                       max
                                       
                                          
                                             
                                                I
                                                W
                                             
                                             
                                                
                                                   t
                                                   i
                                                
                                             
                                          
                                       
                                       +
                                       min
                                       
                                          
                                             
                                                I
                                                W
                                             
                                             
                                                
                                                   t
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The above definition of contrast is known as the Michelson contrast [37] and is recommended for patterns, where the amount of bright and dark pixels in the examined area is almost equal. The use of contrast to estimate text stroke width was also discussed in [20], using a similar definition of contrast. We also experimented with other textural measures that can identify character outlines (strong edges), including standard deviation and entropy, but the use of contrast seemed to be more stable in our experiments. The value of constrast is greater for patches containing character outlines (the desired patches), whereas is smaller for background patches. To move the desired cluster toward small values, in a similar manner to the previous 2D LCM representation and in order to suppress its range values, we propose the following nonlinear mapping to the original C(⋅) values.
                           
                              (5)
                              
                                 i
                                 C
                                 
                                    u
                                 
                                 =
                                 255
                                 
                                    
                                       1
                                       −
                                       tanh
                                       
                                          
                                             2
                                             C
                                             
                                                u
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The nonlinear function tanh(⋅) serves as a method of increasing separation between the two clusters: character outlines and low-contrast patches. In Fig. 8
                        , we depict the original contrast histogram of a document image and the proposed mapping iC(⋅), which features improved range and the character outlines cluster mapped to lower values. The new iC(⋅) values are used to form the novel 3D LCM representation, as follows:
                           
                              (6)
                              
                                 
                                    I
                                    W
                                 
                                 
                                    k
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                I
                                                
                                                   N
                                                   B
                                                   G
                                                
                                             
                                             
                                                
                                                   x
                                                   c
                                                   i
                                                
                                                
                                                   y
                                                   c
                                                   i
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                I
                                                
                                                   N
                                                   B
                                                   G
                                                
                                             
                                             
                                                
                                                   
                                                      x
                                                      c
                                                      i
                                                   
                                                   +
                                                   d
                                                   x
                                                   ,
                                                   
                                                      y
                                                      c
                                                      i
                                                   
                                                   +
                                                   dy
                                                
                                             
                                          
                                       
                                       
                                          
                                             i
                                             C
                                             
                                                
                                                   
                                                      I
                                                      
                                                         N
                                                         B
                                                         G
                                                      
                                                   
                                                   
                                                      
                                                         x
                                                         c
                                                         i
                                                      
                                                      
                                                         y
                                                         c
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 ∀
                                 −
                                 
                                    
                                       Q
                                       /
                                       2
                                    
                                 
                                 ≤
                                 d
                                 x
                                 ,
                                 dy
                                 ≤
                                 
                                    
                                       Q
                                       /
                                       2
                                    
                                 
                              
                           
                        
                     

As one can observe, the local contrast information of each patch is added as another feature to the previous 2D LCM, creating a novel 3D LCM feature, aiming at enhancing character outline binarization.

Once the LCM representation has been established, image binarization can be achieved by performing clustering on the data points I
                        
                           W
                        (k). There exist numerous methods to perform clustering. In this work, we examine the application of Mixtures of Gaussians (MoG) modeling to address the clustering problem. Mixtures of Gaussians (MoG) is a weighted sum (mixture) of different multidimensional Gaussians that can be used to model any arbitrary probability density function (pdf) that does not follow a particular known distribution.
                           
                              (7)
                              
                                 p
                                 
                                    x
                                 
                                 =
                                 
                                    
                                       ∑
                                       1
                                       K
                                    
                                    
                                 
                                 
                                    a
                                    i
                                 
                                 N
                                 
                                    
                                       m
                                       i
                                    
                                    
                                       Σ
                                       i
                                    
                                 
                              
                           
                        where x is a random vector that is observed from the data, a
                        
                           i
                         are the mixing coefficients, m
                        
                           i
                         is the mean vector and Σ
                        
                           i
                         is the covariance matrix of the i-th multivariate Gaussian 
                           N
                           
                              
                                 m
                                 i
                              
                              
                                 Σ
                                 i
                              
                           
                        . In the special case that the arbitrary data distribution features relatively disjoint clusters of data, MoG can be employed to perform clustering by fitting each individual Gaussian of the mixture to each data cluster. The essentials of general multidimensional MoG were established in [38,39], where the estimation of the MoG's parameters are performed using the Expectation–Maximization (EM) algorithm. The MoG estimation is sensitive to the initialisation of its parameters. To accelarate MoG training, one can initialize the EM using the result of a simple clustering algorithm, including the K-Means, the Fuzzy C-Means and the Harmonic K-Means algorithm.

We will employ the EM algorithm, as described in [39], to perform clustering of the LCM data I
                        
                           w
                        (k). Our clustering problem is very constrained and these constraints should be used in the initialisation of the EM algorithm. First of all, we are looking at identifying 2 clusters (characters-artifacts), thus K
                        =2. This implies that the mixing coefficients should be initialised by a
                        
                           i
                        
                        =0.5. The initialization of the means m
                        
                           i
                         is also very important. As previously observed, the desired clusters are usually centered on the main diagonal. In addition, the character cluster should be centered near the beginning of the main diagonal (dark intensities) whereas the artifacts cluster should be placed in the opposite part of the main diagonal (lighter intensities). Consequently, the character mean can be initialized as e.g., m
                        1
                        =[202020]
                           T
                        , whereas the artifacts mean can be initialised as e.g., m
                        2
                        =[230230230]
                           T
                        . Finally, to simplify calculations, we can assume that the Gaussians' covariance matrices are diagonal and use random initialization for their variances. In the previous section, we mentioned the case of discovering more than 2 concentrations of LCM points, due to significant paper stains, or different text color. In this case, we can initialise the EM using 3 or more Gaussians (as necessary) and equidistant initialisation on the main diagonal. After the convergence of the EM, we can merge the new middle clusters with either the text or the background cluster, depending on the distance between their means.

Once the EM algorithm has converged, we have to use the LCM points that correspond to the character cluster (the cluster with the lowest mean vector) to form the binarised image I
                        
                           BN
                        (x,
                        y). The classification rule is straightforward: “if any LCM data point in each Q
                        ×
                        Q neighborhood is classified to the character cluster, then the corresponding central pixel (x
                        
                           c
                        
                        
                           i
                        ,
                        y
                        
                           c
                        
                        
                           i
                        ) is set to black, i.e., I
                        
                           BN
                        (x
                        
                           c
                        
                        
                           i
                        ,
                        y
                        
                           c
                        
                        
                           i
                        )=0. The remaining pixels are set to white i.e., I
                        
                           BN
                        (x
                        
                           i
                        ,
                        y
                        
                           i
                        )=255.”

The proposed algorithm is summarized, as follows:
                           
                              Document Image Binarization Algorithm
                                    
                                       1.
                                       Use the proposed Background Removal algorithm to create the image I
                                          
                                             NBG
                                          (x,
                                          y).

For every Q
                                          ×
                                          Q neighborhood in the image, create the 3D LCM representation I
                                          
                                             w
                                          (k) using (6). Neighborhoods whose central pixel has been classified as background are not used in the LCM representation.

Identify two clusters on the LCM representation using the MoG-EM algorithm and the initialization discussed earlier.

Initialize the M
                                          ×
                                          N matrix I
                                          
                                             BN
                                          (x,
                                          y)=255.

If any pixel in each Q
                                          ×
                                          Q neighborhood is classified to the character cluster, then the corresponding central pixel (x
                                          
                                             c
                                          
                                          
                                             i
                                          ,
                                          y
                                          
                                             c
                                          
                                          
                                             i
                                          ) is set to zero, i.e., I
                                          
                                             BN
                                          (x
                                          
                                             c
                                          
                                          
                                             i
                                          ,
                                          y
                                          
                                             c
                                          
                                          
                                             i
                                          )=0.

The final stage aims at removing artifacts from the previous binarization stage. Isolated blobs or small misclassified noisy items can be removed using a mathematical morphology step. We use MATLAB's bwlabel command to identify connected objects with 8-connectivity in the binary output of the 3D LCM algorithm. The command returns an annotated image containing all the different connected components with 8-connectivity that exist in the image. If some of these components are small in size, they should be noisy artifacts, as described earlier. Therefore, we remove all those connected components that contain less than 20pixels. Of course, this threshold relates to the image's resolution and has to be adapted accordingly. This choice seemed to work well for the DIBCO image databases that were our main experimental ground. In Fig. 9(f), we can see the result of post-processing on the previous 3D LCM binary image (d). Many of the previous binarization errors have been removed. Of course, there are several more complicated post-processing methods, one can use to improve the binarization output, such as those proposed in [6,4], however, we wanted to keep the computational complexity of our algorithm as low as possible.

@&#EVALUATION@&#

In this section, we evaluate the performance of the proposed image document binarization approach. In the first section of the evaluation process, we investigate several properties of the proposed binarization algorithm. In the second section, we compare its performance with other well-established approaches in the field and several evaluation datasets of historical machine-printed and handwritten document images. For the training and evaluation of MoGs, we used the functions gaussmix and gaussmixp respectively available freely from Voicebox.
                        2
                     
                     
                        2
                        
                           http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html.
                      All experiments were conducted on an Intel Core i5-460M (2.53GHz) PC with 6GB DDR3 SDRAM running Windows 7 Professional 64-bit and MATLAB R2013a. Our MATLAB implementations were not optimized in terms of execution speed.

The document images used in our study, were publicly available by the document image binarization community in previous open competitions, including DIBCO2009, DIBCO2011, H-DIBCO2010, H-DIBCO2012 and DIBCO2013 
                     [32–36]. In these competitions, datasets including both machine-printed (P) and hand-written historical (H) document images were publicly provided, along with their hand-annotated Ground Truth binarization result. All these images have very challenging noise and degradations due to the document's wear.

There are many metrics available for the evaluation of image binarization algorithms [32–35]. Let I
                        
                           BN
                        (x,
                        y) be the binary image result of a binarization algorithm and I
                        
                           GT
                        (x,
                        y) be the hand-annotated ground truth binary result. Some commonly used evaluation measurements for the evaluation of image binarization algorithms, that will be used in our study, are the following:
                           
                              •
                              Mean-Square Error (MSE)
                                    
                                       (8)
                                       
                                          M
                                          S
                                          E
                                          =
                                          
                                             1
                                             
                                                M
                                                N
                                             
                                          
                                          
                                             
                                                ∑
                                                
                                                   x
                                                   =
                                                   1
                                                
                                                M
                                             
                                             
                                          
                                          
                                             
                                                ∑
                                                
                                                   y
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      I
                                                      
                                                         B
                                                         N
                                                      
                                                   
                                                   
                                                      x
                                                      y
                                                   
                                                   −
                                                   
                                                      I
                                                      
                                                         G
                                                         T
                                                      
                                                   
                                                   
                                                      x
                                                      y
                                                   
                                                
                                             
                                             2
                                          
                                       
                                    
                                 
                              

Picture Signal-to-Noise Ratio (PSNR)
                                    
                                       (9)
                                       
                                          PSNR
                                          
                                             dB
                                          
                                          =
                                          10
                                          log
                                          
                                             
                                                255
                                                2
                                             
                                             
                                                M
                                                S
                                                E
                                             
                                          
                                       
                                    
                                 
                              

One can also count the number of True Positive (TP), True Negative (TN), False Positive (FP) and False Negative (FN) matches between the two binary images and calculate the following metrics.

Recall
                                    
                                       (10)
                                       
                                          Recall
                                          =
                                          
                                             
                                                T
                                                P
                                             
                                             
                                                T
                                                P
                                                +
                                                F
                                                N
                                             
                                          
                                       
                                    
                                 
                              

Precision
                                    
                                       (11)
                                       
                                          Precision
                                          =
                                          
                                             
                                                T
                                                P
                                             
                                             
                                                T
                                                P
                                                +
                                                F
                                                P
                                             
                                          
                                       
                                    
                                 
                              

F-Measure (FM)
                                    
                                       (12)
                                       
                                          F
                                          M
                                          =
                                          
                                             
                                                2
                                                ×
                                                Recall
                                                ×
                                                Precision
                                             
                                             
                                                Recall
                                                +
                                                Precision
                                             
                                          
                                       
                                    
                                 
                              

Negative Rate Measurement (NRM)
                                    
                                       (13)
                                       
                                          NRfn
                                          =
                                          
                                             
                                                F
                                                N
                                             
                                             
                                                F
                                                N
                                                +
                                                T
                                                P
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       (14)
                                       
                                          NRfp
                                          =
                                          
                                             
                                                F
                                                P
                                             
                                             
                                                F
                                                P
                                                +
                                                T
                                                N
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       (15)
                                       
                                          N
                                          R
                                          M
                                          =
                                          
                                             
                                                NRfn
                                                +
                                                NRfp
                                             
                                             2
                                          
                                       
                                    
                                 
                              

In this section, we discuss the effect of contrast information in the algorithm's performance, as well as the effect of the parameters d and q in the binarization performance.

Firstly, we demonstrate the positive effect of incorporating the contrast information in the LCM implementation. In Fig. 9, we demonstrate the algorithm's performance on a document image. In Fig. 9(c), we depict the output of the 2D LCM algorithm and in Fig. 9(d), we depict the output of the 3D LCM algorithm incorporating contrast. The difference between the two outputs is depicted in 9(e). It is evident that the inclusion of local contrast information has enhanced the presence of character outlines, which was missing from 2D LCM. To perform objective evaluation of the effect of local contrast, we measured the aforemention binarization performance metrics for the two cases. The results are reported in Table 1
                           . It can be observed that contrast information improves all performance indices compared to 2D LCM. 3D LCM improves Recall but reduces precision; however the FM measurement is improved. Thus, the inclusion of contrast information improves the performance of the proposed algorithm both subjectively and objectively.

Next, we evaluate the effect of rejecting LCM points that are far from the main diagonal. Points that are close to the main diagonal should belong to character pixels. Points far from the main diagonal may belong either to character outlines or background noise. Rejecting points close to the main diagonal usually results into character thinning. Fig. 10
                            shows the algorithm outputs for various values of threshold d. The difference between character outlines can be seen in Fig. 10(a) and (b), whereas in (c), we can see the inclusion of background noise and oversize characters.

In an attempt to find an optimal value for d via experimentation, we evaluated the algorithm's average PSNR and FM for all the available P and H-DIBCO datasets for various values of d. Fig. 11(a) and (b) contains the average PSNR and FM for the printed (P) images and Fig. 11(c) and (d) the same measurements for the handwritten (H) images. It appears that in most cases for low and great values of d, the binarization result is much inferior. The characters in this case appear very thin or too much noise has been incorporated in the binarization result or the character appears much thicker. Unfortunately, we cannot automate the optimal selection of d and thus has to be manually selected. Judging from the results, we can pick a value of d
                           =40, which seems to perform better in most printed and handwritten datasets. This value is not adapted any further in our experiments.

In this section, we evaluate the effect of background removal in the binarization result. This is controlled via the parameter q, which defines the threshold after which some pixels are considered text or background. Lower values of q denote stronger background removal, whereas higher values leave more background pixels classified as text. In a similar effort to the previous section, we evaluated the algorithm's average PSNR and FM for all the available P and H-DIBCO datasets for various values of q. Fig. 12(a) and (b) contains the average PSNR and FM for the printed (P) images and Fig. 12(c) and (d) the same measurements for the handwritten (H) images. Here, we can see some difference between hand-written and printed documents. Hand-written documents seem to give better performance at lower values of q compared to the printed ones. Again, automation of the optimal selection of q seems not possible at this stage and thus has to be hand-picked. Hence, we use a value of q
                           =0.6 for the printed documents and a value of q
                           =0.4 for the handwritten ones.

In this section, we compare the proposed LCM binazation method with other common binarization methods. In our benchmarking exercise, we use Otsu's thresholding method [40] and the local binarization techniques of Sauvola (Sau) [11] and Bernsen (Bern) [12]. For the Sauvola method, we use a value of k
                        =0.4 and a window size of 21×21 to calculate the local statistics. We also use the Adaptive Logical Level Technique (ALLT) and the Improvement of Integrated Function Algorithm (IIFA), as proposed by Badekas and Papamarkos [13]. We use the GPP binarization method, as proposed by Gatos et al. [18], the binarization method of Howe [2]
                        
                           3
                        
                        
                           3
                           Code kindly provided at http://www.cs.smith.edu/~nhowe/research/code/.
                         with automated threshold selection. Finally, we include Ramirez-Ortegon et al. method (Ramir.) [4,24,25]
                        
                           4
                        
                        
                           4
                           Code kindly provided at https://sites.google.com/site/martehomepage/.
                         and Su et al. method (Su) [20].
                           5
                        
                        
                           5
                           Code kindly provided at https://sites.google.com/site/flydreamersu/.
                         For the proposed LCM method, we use a value of d
                        =40, a value of q
                        =0.6 for the machine-printed documents and a value of q
                        =0.4 for the handwritten documents. This implies that stronger background removal was essential for the handwritten documents. It was not our intention to develop the best performing binarization algorithm, however, we can see that the proposed algorithm performs favorably with the tested approaches and those scores reported at image binarization competitions at a reasonable running time. We employed the images from the available DIBCO datasets in our study. The objective evaluation metrics of PSNR, MSE, Recall, Precision, FM and NRM were calculated from all the results.

Firstly, we estimate the algorithm's running time by using MATLAB's commands tic-toc on the aforementioned PC system. We estimate the average running time in s per image for each dataset. Since the algorithm's running time depends also on the image size, we calculated a normalized average running time in ms per pixel, in order to get a clearer performance overview. The results are summarized in Table 2
                           . We can understand that the algorithm's running time is on average 0.0246ms per pixel. This implies that for a 640×480 image the algorithm requires an average 7.7s on an average PC. It was only possible to compare the proposed algorithm's running time with Howe's approach, since they were both implemented in MATLAB, whereas the other methods were implemented in different platforms. Howe's algorithm is the best performing algorithm in our later experiments, therefore it is sensible to compare with the best. We also normalized the two algorithms' running time to the running time of Sauvola's algorithm (implemented in MATLAB as well). The results are shown in Table 3
                           . The LCM algorithm is on average 63.83 times slower than Sauvola's algorithm but is much faster than Howe's approach, which is 254.77 times slower than Sauvola. This implies that LCM is about 4 times faster than Howe's approach.

In Table 3, we present the results of binarization of machine-printed historical document images of DIBCO2009 DIBCO2011 and DIBCO2013. Some typical document images and the respective binarization results are depicted in Figs. 13, 14, and 15
                           
                           
                           . For the printed document images, we used a q
                           =0.6 for the LCM approach. Howe's method seems to give the best performance in P-DIBCO 2009 both in terms of PSNR and FM. The results are summarised in Table 4
                           . For the P-DIBCO2011, Ramirez et al. seems to give the best performance both in terms of PSNR and FM with Su et al. being the winner at P-DIBCO2013. The LCM approach seems to be third best in P-DIBCO 2011 and 2013 in terms of PSNR and second best in terms of FM. This is not the case for the DIBCO2009 dataset where the LCM is fifth best in terms of PSNR and FM, since it contains images with multiple color characters. The LCM approach is not calibrated to work for multi-color documents, as it was described before. However, it can be easily adapted to handle multi-color document images, simply by increasing the number of desired clusters in the MoG clustering module. The lower intensity centered clusted can then be merged to form the text cluster. This justifies the lower performance of the algorithm in some of the images, which undermines the average scores. The result images of all datasets can be downloaded from the following url.
                              6
                           
                           
                              6
                              
                                 http://utopia.duth.gr/~nmitiano/machine.rar.
                           
                        

In Table 5
                           , we depict the results of binarization of handwritten historical document images of DIBCO2009, DIBCO2010, DIBCO2011, DIBCO2012 and DIBCO2013. Typical binarization results are shown in Figs 16, 17, and 18. Here, we used a value of q
                           =0.4 for the document background removal, implying that there was more need for degradation removal in these document images. Howe's method seems to be the winner in all datasets in terms of PSNR. In terms of FM, Howe's method is the winner in H-DIBCO2009, H-DIBCO2010, H-DIBCO2012 with LCM being the winner in H-DIBCO 2011 and H-DIBCO 2013. LCM is fourth in terms of PSNR in H-DIBCO2009, H-DIBCO2010, H-DIBCO2012, H-DIBCO2013 and second in H-DIBCO2011. In terms of FM, LCM ranks 4th in H-DIBCO2009, H-DIBCO2010 and 3rd in H-DIBCO2012. In Figs. 16 and 18
                           
                           
                           , typical examples of the images are shown. These images were heavily contaminated by ink from the opposite page. As it is evident, in this case the LCM approach performs well at removing these contamination artifacts, especially for document images with bleed-through contamination. The result images of all datasets can be downloaded from the following url.
                              7
                           
                           
                              7
                              
                                 http://utopia.duth.gr/~nmitiano/handwritten.rar.
                           
                        

In Table 6
                           , the average score of all available printed and handwritten datasets is presented. Howe's method is the winner, while LCM ranks fourth both in terms of PSNR and FM.

In this section, we compare LCM's scores with those reported in DIBCO competitions. More methods than the ones examined here have taken part in these competitions, therefore, it is important to know LCM's standing compared to a wider range of techniques. Comparing with the results reported in DIBCO2009 [32], the method would rank 3rd in terms of PSNR and 2nd in terms of FM for the combined printed and handwritten dataset. Comparing with the results, reported in H-DIBCO2010 [33], the method would rank 7th in terms of PSNR and 6th in FM. Comparing with the results, reported in DIBCO2011 [34], the method would rank 1st both in terms of PSNR and FM for the printed and 4th in terms of PSNR and 2nd for the FM for the handwritten dataset (no more measurements were provided in the paper). Looking at the H-DIBCO2012 results [35], the method would get the 13th position in terms of PSNR and 8th for the FM, but will be at the top faster methods at this performance on a slighter faster machine. For the H-DIBCO2013 results [36], the method would get the 5th position in terms of PSNR and FM for both handwritten and printed dataset. Finally, the LCM method was submitted to H-DIBCO 2014 [19], getting the 5th position in terms of PSNR and the 4th in terms of FM. These results are summarized in Table 7
                           .

In summary, the method performs relatively well in terms of binarization, of course lacking in performance compared to state-of-the-art methods, such as Howe's method. Nevertheless, the method is not very complicated, compared to the best-performing ones described earlier. Thus, this lower complexity can be an advantage to use this method in an environment where computational power is constrained, without losing much in quality.

It appears that the proposed algorithm performance depends on the choice of the parameter q, which defines the amount of background that needs to be removed from the initial image. Removing much of the background may remove character information, whereas removing less background may leave stains that may not be sorted later by MoG clustering. The next task will be to automate this parameter choice in order to optimize the performance of the algorithm. Our previous study can give rough guidelines for the optimal value of q. Nonetheless, we have observed that every image may benefit from a different value of q during binarization. Thus, it would be very important to automate the choice of this threshold.

@&#CONCLUSIONS@&#

In this paper, the authors propose a novel document image binarization system that can be applied on both machine-printed and handwritten document images. The system consists of three stages. During the background removal stage, an estimate of the background image is calculated via adaptive median filtering. The background is removed by statistical thresholding of the differences between the estimated background and the document image. In the next stage, Local Co-occurance map (LCM) is calculated as described earlier. This representation aims at grouping together pixels of similar intensity value and similar contrast, thus creating two dominant clusters: character and remaining background. Clustering is performed using a Mixture-of-Gaussian (MoG) model of two Gaussians. In the last stage, some isolated binary artifacts are removed by morphological 8-connected object segmentation.

The proposed approach is robust to severe degradation of the document images. The inclusion of contrast seems to improve the inclusion of character outlines in the binarization results. The method performs quite well in our experiments and DIBCO benchmarks. Although, it is not the best performing method, it is a low-complexity good performing method that can be used in environments, where computational power use is important. Nonetheless, the method is very sensitive to the amount of background removal performed in the first stage, which is controlled by the parameter q. In this study, we have used a value of q
                     =0.6 for the printed images and a value of q
                     =0.4 for handwritten images, that seemed to work well in our experiments.

The authors would also like to extend the method to work for multi-color documents. Although it is trivial to extend the number of clusters in the MoG model, it would be preferable if the system could automatically identify the number of colors and configure the number of clusters accordingly. In addition, the authors would like to look into a more automated method to define the value of q in the background removal stage and the cluster size in the post-processing stage. Another extension can be to change the Gaussian distribution assumption for the background and character cluster for skewed distributions, including the log-normal distribution, as observed by Ramirez-Ortegon et al. [24].

@&#REFERENCES@&#

