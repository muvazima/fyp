@&#MAIN-TITLE@&#Dynamic time warping in phoneme modeling for fast pronunciation error detection

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A low-complexity phoneme modeling method based on Dynamic Time Warping is proposed.


                        
                        
                           
                           Four variants of fast mispronunciation detection method based on DTW are presented.


                        
                        
                           
                           The proposed method works on a small speech corpus with no additional linguistic data.


                        
                        
                           
                           The method proves to be faster and more efficient than methods of higher complexity.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Pronunciation error detection

CAPT systems

DTW algorithm

Phoneme modeling

Word structure analysis

@&#ABSTRACT@&#


               
               
                  The presented paper describes a novel approach to the detection of pronunciation errors. It makes use of the modeling of well-pronounced and mispronounced phonemes by means of the Dynamic Time Warping (DTW) algorithm. Four approaches that make use of the DTW phoneme modeling were developed to detect pronunciation errors: Variations of the Word Structure (VoWS), Normalized Phoneme Distances Thresholding (NPDT), Furthest Segment Search (FSS) and Normalized Furthest Segment Search (NFSS). The performance evaluation of each module was carried out using a speech database of correctly and incorrectly pronounced words in the Polish language, with up to 10 patterns of every trained word from a set of 12 words having different phonetic structures. The performance of DTW modeling was compared to Hidden Markov Models (HMM) that were used for the same four approaches (VoWS, NPDT, FSS, NFSS). The average error rate (AER) was the lowest for DTW with NPDT (AER=0.287) and scored better than HMM with FSS (AER=0.473), which was the best result for HMM. The DTW modeling was faster than HMM for all four approaches. This technique can be used for computer-assisted pronunciation training systems that can work with a relatively small training speech corpus (less than 20 patterns per word) to support speech therapy at home.
               
            

@&#INTRODUCTION@&#

The problem of automatic pronunciation error detection has been recently subjected to intensive research studies involving speech analysis. The need to design methods which are able to assess the phonetic correctness of spoken words came from an idea of creating a Computer-Assisted Pronunciation Training (CAPT) system [1–3] for people having articulation problems, mostly non–native speakers and people struggling with speech dysfunctions, e.g. lisp.

Nowadays, computer systems are widely used both in education and speech therapy. The applications designed for logopeadic purposes may supplement the therapy at home, as they usually provide extensive exercise material with the multimedia stimulus. However, only CAPT systems have the possibility of indicating the patients׳ articulation errors. For this reason, for some people (especially children and people with additional mental retardation) the home-based speech rehabilitation will inevitably require the constant presence of a caregiver who will catch the errors early enough. Preparing a tool that will grant the patient more independence – for example an application capable of providing correction and approval – could improve the quality of life of people with speech disorders.

Logopeadic CAPT system could work as follows. During a visit a speech therapist will show the patient which set of words they should practice. Then, at home, the program will ask the patient to repeat words from the previously selected list. The system will indicate mistakenly spoken word fragments and suggest which phonemes need training. As a result, the patient will be able to correct the spoken word. During a next visit the speech therapist will be able to assess whether the home work delivered the desired results. Then they may select a different set of words and, if needed, supplement the database with recordings of additional words and expressions. The computer system works as a therapy tool, and the therapist׳s knowledge is still inevitable, but the time between the visits can be used more efficiently.

Advantages of CAPT for logopeadic purposes are undeniable. Speech improvement requires many repetitions. The possibility of working at home at the patient׳s own pace, independently of the presence of a therapist and the reduction of stress caused by practising in front of a group of other people are the main reasons for creating such systems [1]. Well-developed computer software, extended with a subsystem able to evaluate pronunciation, could be an extremely useful tool in speech therapy.

Pronunciation error detection methods are being developed all over the world and cover mostly segmental errors, which are related to inappropriate realization of individual phonemes or diphones [4–7]. A vast majority of the conducted research concern the Asian tonal languages. Studies include Mandarin Chinese [4,2,3,8], Taiwanese [7,6] and others. There are slightly fewer works concerning European languages (e.g. [9]). At present, the only well-publicized undertaking associated with Polish in terms of pronunciation error detection is the EURONOUNCE project [1,10–12].

Most of the conducted experiments are concentrated on a small number of selected systematic errors – the most common mispronunciations. Limitations imposed on the search range make the classification simpler and they increase the detection efficiency of these errors. In [4], the 8 most common pronunciation mistakes committed by the Japanese people speaking Chinese have been evaluated. Xu et al. [6] present an example of a base of error patterns (EP) where the models of the most problematic phonemes have been enriched with additional phonological information. Another example of the detection method using supplementary linguistic knowledge is the Phonological Modeling of Mispronunciation Gradations (PMMG) described in [13]. The phonological rules occurring in the mispronunciation of certain words have been modeled in advance and next used to assess the pronunciation quality. In most of the studies, supplementing the speech database with appropriate rules resulting from language characteristics proves to increase the detection efficiency of the systematic errors. Unfortunately, at the same time the detection of any errors from outside the set becomes impossible. Therefore, there is a need to develop more general methods as well.

Predefined error detection requires the use of classifiers and pronunciation quality metrics different from robust search algorithms. Goodness of Pronunciation (GOP) [5,4,14,15] or Log-Posterior Probability (LPP) [6] are examples of methods that do not operate on a predefined set of errors; the probabilistic models are calculated for all different phonetic units that are possible to encounter. More complex solutions use the artificial neural networks, e.g. Deep Neural Network (DNN) for modeling [16,8,17] or Multilayer Perceptron (MLP) for deriving additional signal features [18].

The LPP modification, Revised Log-Posterior Probability (RLPP) [6], is used for error detection within a previously created set. LPP calculates the probability of correctness of the segment in the context of all phoneme models in the database, while RLPP considers only phonemes identified as the most problematic. Other methods of detection of the systematic errors are Likelihood Ratio (LR) [4,19,20] or Weigelt׳s algorithm [5].

CAPT approaches often involve the algorithms based on Hidden Markov Models (HMM) [4,21,19,13,8,22] or similar probabilistic models. HMM, widely used in automatic speech recognition systems, prove to be effective also in speech modeling units shorter than sentences or words. Most commonly used signal features are the Mel-Frequency Cepstral Coefficients (MFCC) [23,21,24,20,16,19,25,17]. Other popular parameters are MFCC׳s first and second-order time derivatives (Delta and Delta–Delta MFCC) [16,19,25] and Perceptual Analysis Prediction (PLP) [18]. However, many mispronunciation detection studies focus on testing and development of new feature types [22,18,26].

Another issue worth discussing in the context of pronunciation error detection is the size of a phonetic segment which is chosen as an analysis unit. There can be found examples of studies that operate on individual phonemes [6], diphones [27], triphones [28–30] or even syllables [31]. Every segment of an analyzed word can be classified as correctly or incorrectly pronounced. It is worth noting that using shorter units may allow errors to be identified more precisely. In addition, the number of phonemes in a specific language is respectively smaller than the number of their combinations forming diphones, triphones or syllables. In the study presented in [32] 14917 different Polish triphones have been annotated based on the speech corpus, while there are only 39 phonemes in the Polish language. Therefore, in order to carry out the detection based on triphones in Polish, almost 15,000 models of correct triphones need to be generated. In this case the analytical complexity becomes significantly higher than that for individual phonemes.

Pronunciation error detection is still a fairly new topic. Even in 2002, the concept of an effective CAPT system was questioned [33]. A few years later, reports began to confirm the effectiveness of the conducted works [34,20,35,36]. However, the results are still not in general use and the studies include only specific languages. Projects addressing pronunciation correction in Polish, especially for speech therapy patients, are very rare. All these factors encourage researchers to carry out studies on the pronunciation error detection methods for the Polish language.

The main problem of the currently developed pronunciation error detection methods is that they require a large amount of speech data as a training set. The problem has been effected by using statistical tools, such as HMM, which cannot work properly on a small speech corpus. As a result, the existing methods can hardly be used in CAPT software for domestic use by a patient. Furnishing these systems with speech databases containing more than several patterns for every element of training material requires a lot of storage space and effort to organize speech data. Moreover, error detection based on a relatively small number of patterns would allow teachers or therapists to supplement the database with additional words or word constructions, according to their needs.

The aim of this study is to develop a fast mispronunciation detection algorithm which will work with a relatively small training corpus. With these two assumptions at the forefront, we are going to focus the study on finding methods which are ready to be used in practice, particularly in CAPT systems dedicated to speech therapy patients.

Our main contribution is (1) the extension of a novel approach to pronunciation error detection based on the Dynamic Time Warping (DTW) algorithm, the basic version of which was described in our previous work [37]; (2) comparative studies of the proposed modeling method and popular HMM involving the mispronunciation detection within a speech corpus reflecting the most popular articulation disorders in the Polish language. According to the authors׳ knowledge, DTW [38,39] has not been suggested for use in the mispronunciation detection so far. However, this type of modeling turns out to be faster and more efficient than very common HMM if used for a small training corpus. This paper presents several variants of the pronunciation error detection method using DTW together with the results of comparative tests which have been carried out. Specific improvements made in comparison to [37] include two additional classification variants, extended speech corpus corresponding to two most popular segmental speech disorders, the iterative DTW modelling procedure and the use of the Sakoe–Chiba Band to optimize the DTW modelling. All these changes have been introduced to improve the efficiency of the algorithm and to turn the experiments towards the speech therapy case study.

The paper is organized as follows. Speech corpus, which has been specially recorded for this work, and the created pronunciation error detection methods are described in Section 2. Section 3 provides a presentation of the performance measures that have been used, designed experiments and the evaluation of the obtained results. The conclusions are discussed in Section 4.

For the purpose of this study a speech corpus has been designed and recorded. Speech databases are usually very expensive, as time and effort necessary to develop them generate high costs. In addition, the specifics of our project require a speech corpus that contains recordings of mispronounced words. However, none of the Polish databases satisfies this condition. Therefore, the collection of the recordings was inevitable.

One of the objectives of the presented work is to propose a method operating on the Polish language and articulation problems characteristic for its phonological system. It was taken into account during the dictionary designing stage and database recording. We decided to focus on segmental errors – problems at the level of the sound forms of individual phonemes.

The recorded words come from a fixed set of 12 words of different phonetic structure (Table 1
                        ). The set was consulted with a speech therapist. The selected words contain the phone ‘r’ or Polish dental phones (ś, ź, ć, dź; s, z, c, dz; sz, ż, cz, dż) to reflect the most common segmental pronunciation problems of native and non-native speakers of the Polish language.

The transcription of the dictionary is presented according to the extended SAMPA standard (Speech Assessment Methods Phonetic Alphabet) [40–42].

Speech databases for testing the error detection should include the recordings of people actually committing errors, e.g. speech therapy patients. However, speech corpora constructed for the purpose of testing new methods of analysis often contain recordings of speakers with correct pronunciation who intentionally mispronounced words (e.g. substituted problematic phoneme with a different one) [4,43]. This is because the effort needed to collect the recordings of real patients is very big. They are almost always in minority as compared to the group with correct pronunciation. The time needed to gather such a group and to carry out the recordings is therefore longer, and compiling the speech database is more time-consuming. Databases of artificially mispronounced recordings can be treated as a good model, especially in cases where error detection is not limited to a predefined set. A detection method seeking general abnormalities in pronunciation should cope well with the errors which are either natural or artificially introduced.

In the presented study similar assumptions were made. The data collected during the first phase contained the recordings of the full dictionary of 12 words pronounced correctly by 60 people. Next, 10 most precisely pronounced utterances of each word were drawn from that corpus to create the training set containing 120 recordings. An independent group of 30 speakers were asked to pronounce the same word set, saying the words improperly (introducing mispronunciations). 15 occurrences of each of the 12 words were randomly chosen from this data in order to create the test set. In total, there were 120 utterances in the training set and 180 utterances in the test set. The introduced mispronunciations were meant to reflect typical errors committed by speech therapy patients, e.g. [Z] replaced by [z] or [
                           
                              
                                 z
                              
                              
                                 ′
                              
                           
                        ], [S] replaced by [s] or [
                           
                              
                                 s
                              
                              
                                 ′
                              
                           
                        ], [r] replaced by [l] or [j]. However, different pronunciation errors were permitted as well in order to simulate a speech problems variation.

The registration was conducted during several sessions in a room with low noise and reverberation. The material was recorded at the sampling rate of 44.1kHz and the resolution 16 bit.

Each recording was divided into phoneme segments manually, using a specially created software. The recordings were organized in a database that was subsequently annotated with substitution, deletion and insertion errors, which was necessary for further evaluation of the proposed error detection method. At the same time silence removal at the beginning and at the end of the recordings was carried out. The example of the segmentation of a correctly spoken word is presented in Fig. 1
                        .

@&#METHODS@&#

In the same way as in [4], the proposed methods are based on the structural features of words pronounced correctly and incorrectly. We decided not to focus on the systematic errors, but to allow mispronunciations of various kinds in order to test the flexibility of the detection. The detection algorithm does not use any linguistic knowledge and is not targeted at any specific subset of errors, so the results do not depend on the database. In the experiment we do use a subset of words with certain phonemes problematic from the logopeadic point of view. Nevertheless, this vocabulary can be easily expanded with virtually any word, as none prior assumptions about the analyzed word are required. Phonemes are used as the analysis units, as they allow errors to be detected more precisely than when using diphones or syllables.

The methodology proposed for segmental pronunciation error detection contains the following stages:
                           
                              •
                              signal preprocessing and features extraction,

model training and analysis (searching),

classification.

As mentioned before, phoneme segmentation and labelling as well as silence removal – the main parts of signal preprocessing – were performed manually. Next, preemphasis was performed, and each segment (phoneme) was divided into 10ms long frames selected with the use of the Hamming window shifted by 5ms.

After preprocessing, a feature vector was extracted from each frame of the recorded signal. The proposed method assumes the use of 13 MFCC (including 1st to 13th MFCC) as features which are commonly used in pronunciation error detection methods. Other methods which occur in the literature – Delta and Delta–Delta MFCC as well as PLP coefficients – were tested as a reference.

The following methodology description operates on MFCC. However, the workflow is the same for any other feature vector.

Two models of each spoken phoneme were created for classification purposes. The first one uses the DTW algorithm and the second is based on HMM. The MFCC parameters extracted from every frame within each specific phoneme are placed in a feature matrix of size [number of extracted features × number of frames of the phoneme]; such matrices are subsequently used for phoneme models training and analysis. In both approaches the recordings of the correctly uttered words create the training set, and the mispronounced ones are used as the test set.

Dynamic Time Warping [44,37,45] algorithm is used to find the path of optimal alignment for two temporal sequences and calculate the cost of this path. The corresponding fragments of the series may be identified and matched. The more similar the sequences are, the lower the cost is.

The proposed way of using DTW as a modeling tool is presented in Fig. 2
                           . Each time, the phoneme representation is a matrix containing feature vectors for each 10ms frame. In the beginning, the model is initialized with the first (shortest – having the least frames) phoneme. Then, the DTW algorithm is run iteratively. In each iteration the current phoneme model together with the consecutive phoneme occurrence matrix is used for the DTW calculation. The feature vectors of the aligned frames are then averaged – for each pair of frames the average of their feature vector is taken to create a new matrix of a phoneme model. As a result, a matrix containing a set of MFCCs for every frame of the model is obtained (the number of frames is dependent on the length of the training phonemes).

The basic version of the DTW algorithm compares vectors, not matrices. For this reason, the DTW method was extended to a two-dimensional case, as described in [37]. Each row of a phoneme feature matrix (both model and analyzed) is a feature vector extracted from an individual frame. The distance d
                           
                              ij
                            between the vector in the ith row (frame) of the matrix (phoneme instance) x and the vector in the jth row (frame) of the matrix (phoneme instance) y is evaluated using the Euclidean metric:
                              
                                 (1)
                                 
                                    
                                       
                                          d
                                       
                                       
                                          ij
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                k
                                                =
                                                0
                                             
                                             
                                                N
                                                −
                                                1
                                             
                                          
                                          
                                             
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                (
                                                k
                                                )
                                                −
                                                
                                                   
                                                      y
                                                   
                                                   
                                                      j
                                                   
                                                
                                                (
                                                k
                                                )
                                                )
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           where N is the length of the feature vector. This way of calculating the partial distances and finding the optimal alignment path is presented in Fig. 3
                           . The cost of the DTW match may be considered as a distance between the compared matrices and is further referred to as D or the DTW distance.

The first step of error detection – the comparison of the DTW model and the mispronounced word (model searching) – is performed by calculating the DTW distances between each phoneme of the analyzed mispronounced word and the phoneme models (Fig. 4
                           ).

In the next step, the calculated DTW distances are being placed in a result matrix (Fig. 5
                           ) showing the DTW structure of the tested word and the model comparison.

In order to make the DTW algorithm more efficient, the Sakoe–Chiba Band (SCB) was introduced [46–48].

The second model type applied was the HMM model. The consecutive frames of each phoneme were used as observations. The emission probability distribution of HMM was specified as a set of Gaussian Mixture Models (GMM) [21].

Dimensionality of the observation feature vectors is in this case synonymous to the number of MFCC parameters extracted for each frame. The mean and standard deviation vectors calculated for each class (correctly pronounced phoneme) were used as the initial values of mean vectors and covariance matrices for probability distribution. The models were trained with the Baum–Welch Expectation Maximization algorithm [49].

The model searching was performed in order to evaluate the probabilities with which the word segments from the test set could be emitted by the models of particular phonemes. The resulting matrix contains the probabilities as a measure of similarity between the segments and is equivalent to the DTW result matrix shown in Fig. 5. The Viterbi algorithm was used for searching the HMM [21,49].

In the presented approach, the search of the model yields a matrix of values describing the similarity of individual phonemes of the analyzed mispronounced word and the trained model. The matrices illustrate the structure of the analyzed word׳s compliance with the model and are homogeneous both for the HMM and DTW models, which makes it possible to employ exactly the same classification methods.

Four different classification methods were developed and tested. The methods have specific advantages and drawbacks and should be chosen depending on the desired objective, taking into account the specifics of individual speech therapies. The general idea of the first two approaches was presented in the previous work [37] yet, as mentioned in the introduction, they were improved and better results were obtained. The other two are based on the assumption that there is only one mispronunciation in a given word (as in [4]). Such an approach makes the solution to be less universal, but it also makes the algorithm less time-consuming and is sufficient in some applications. The speech therapy usually concentrates on one disorder at a time, even if more mispronunciations occur simultaneously. The reason is that without a complete focus a patient cannot properly improve the dysfunction they struggle with. Thus a single mispronunciation detection may be sufficient in many cases.

In each method, the detection was performed basing on the similarity between particular phonemes of the model and the analyzed word.

The first proposition involves the analysis of the Variations of the Word Structure (VoWS). In this method (Fig. 6
                           ), the nearest model phoneme, considered as a distance for DTW or a probability for HMM, is searched for the consecutive segments of the analyzed word. If the position of the two segments does not correspond to each other, this part of the recording is considered as an outlier and classified as mispronunciation. All segments are classified independently.

When using VoWS it may happen that a mispronounced phoneme is sufficiently similar to its appropriate version to be wrongly classified as correct. In effect, we can falsely accept the mispronounced phoneme. However, there are many cases when it is more important to encourage the patient to practise than find all the mispronunciations.

The second algorithm (Fig. 7
                           ) contains the Normalized Phoneme Distances Thresholding (NPDT). In the first step all distances in the result matrix are normalized by division by the maximum diagonal element. Then the distance between the corresponding segments is calculated and if it is not below the threshold the segment is considered as being incorrect. The NPDT is the most general from among the proposed approaches, as the change of the threshold alters the strictness of error detection. Thus, this approach can be personalized and used for patients having different needs. In this study the threshold was tested on the same group as in the test set. However, in the future a study on the threshold value impact on the psychological and linguistic condition of different patient groups will be conducted.

In the third approach – the Furthest Segment Search (FSS) algorithm (Fig. 8
                           ) – only the distances between the corresponding segments (matrix diagonal values) are evaluated. The segment which is the most distant from its corresponding model is suspected to be mispronounced, as it resembles the model the least.

The fourth method is called the Normalized Furthest Segment Search (NFSS) algorithm (Fig. 9
                           ) and it is a modification of the previous algorithm. The distances for each segment of the analyzed word are separately normalized by dividing them by the maximum distance between the model and the specific segment. Then, as in the previous method, the segment which is the furthest from its model is marked as incorrectly pronounced.

FSS and NFSS are very strict and in fact will always detect the phoneme which is the most distant from the model. These approaches may be used for patients in the initial stages of the therapy or for patients struggling with more than one type of articulation disorders, when errors are expected and the main question is where they are.

@&#EXPERIMENTS AND RESULTS@&#

@&#EXPERIMENTS@&#

In total, more than 200 performance tests of the proposed methods were carried out. Several parameters were taken into account.

For HMM, the experiments were conducted for 2 to 10-state left-to-right models. The transition matrices were initialized with values 0.6 for staying at current state and 0.4 for transiting. The chosen initial distribution required the process to be started from the first state.

The effectiveness of DTW was studied for different Sakoe–Chiba Band width values within range from 0.1 to 1.0 for the model training and searching.

For both modeling types, a threshold in the distance normalization and thresholding method was chosen experimentally.

In accordance with other studies on pronunciation error detection [43,4,50,5] we decided to use the following parameters of phonemes classification accuracy:
                           
                              1.
                              Correctly Accepted (CA), correct pronunciation classified as correct.

Correctly Rejected (CR), mispronunciation classified as incorrect.

Falsely Accepted (FA), mispronunciation classified as correct.

Falsely Rejected (FR), correct pronunciation classified as incorrect.

Based on these indicators, three measures of the algorithm performance were calculated [4]: False Acceptance Rate (FAR) (2)
                        
                           
                              (2)
                              
                                 FAR
                                 =
                                 
                                    
                                       FA
                                    
                                    
                                       CR
                                       +
                                       FA
                                    
                                 
                                 ,
                              
                           
                        
                     

False Rejection Rate (FRR) (3)
                        
                           
                              (3)
                              
                                 FRR
                                 =
                                 
                                    
                                       FR
                                    
                                    
                                       CA
                                       +
                                       FR
                                    
                                 
                                 ,
                              
                           
                        and Average Error Rate (AER) (4)
                        
                           
                              (4)
                              
                                 AER
                                 =
                                 
                                    
                                       FAR
                                       +
                                       FRR
                                    
                                    
                                       2
                                    
                                 
                                 .
                              
                           
                        
                     

In the case of the mispronunciation detection problem, FAR is a percentage of mispronounced phonemes within all phonemes classified as correct, and FRR expresses a percentage of correctly pronounced phonemes within all phonemes classified as incorrect. AER, as the mean of the other two, presents the best overall performance of the algorithm. Theoretically, in order to optimize the performance of the system, both FAR and FRR should be minimized and balanced. However, in CAPT systems it is more important to keep FRR low, as an unfounded acceptance of a student׳s or patient׳s pronunciation is usually less harmful for them than faulty rejection [43].

All tests were conducted on a computer with Intel Core i5-2410M 2,3GHz, 8GB RAM, running Windows 7 64-bit.

@&#RESULTS AND DISCUSSION@&#


                        Table 2
                         presents the results achieved for the DTW and HMM models for the feature vector of 13 MFCCs for each of the four classification methods.

The highest efficiency, defined as the lowest AER, was obtained for the second method (NPDT) in the case of DTW, and for the third method (FSS) in the case of HMM.

The presented tests were carried out with the parameters aimed at minimizing AER, however it turned out that the maintenance of FRR at low level was possible as well. Parameter sets for both models were selected experimentally.

For each method DTW models proved to yield better results. It is worth noticing that methods based on statistics, such as HMM, require very large amounts of well-designed and described data in the training set. When operating on a small amount of training data, the statistical models may be easily disturbed, making the calculated probability unreliable. In such conditions, simpler algorithms – DTW in this case – would perform better. However, the experiments relating to the minimum corpus size required for DTW were not a part of the described study, but they are meant to be a part of further research.

Other methods of speech parametrization (PLP, Delta and Delta–Delta MFCC) were tested as well. The results are presented in Table 3
                        . Ultimately, the feature set of 13 MFCCs was chosen as optimal for FSS and NFSS methods, and the feature set of 13 MFCCs, 13 Delta MFCCs and 13 Delta–Delta MFCCs were chosen for VoWS and NPDT methods. These feature sets were used in the remaining part of the tests.

The execution time of the mispronunciation detection based on a specific model was checked as well. Fig. 10
                         presents models learning times, depending on the number of states in HMM case and the width of SCB for DTW. Similarly, Fig. 11
                         shows the measured times involving the search for a single test recording.

Learning and searching times measured for DTW are – irrespectively of the chosen parameters – several times shorter than those for HMM. As mentioned before, the speed of error detection using HMM would probably be sufficient for an effective real-time operating CAPT system. However, due to smaller computational complexity, DTW proves to be better.


                        Table 4 presents the results obtained with the use of two, most accurate approaches proposed in this work together with the methods of pronunciation error detection reported in literature.

All works were based on the analysis of individual phonemes. The main distinction between the methods proposed in this paper and in others involves the set of errors to be considered. In the presented paper, as well as in [18], the errors that could be detected were specified in advance, while in other works [50,43,4] the errors were searched for within a predetermined set. Such an approach can improve the algorithm performance, but there is a risk that some cases can be omitted. The research described in [4] showed that the experiment involving actual non-native speakers yielded worse results as compared to the preliminary tests. It was caused by errors not contained in the previously established range. Therefore, to the best of the authors׳ knowledge, it is impossible to definitely state whether by narrowing down the list of detectable errors we exert a positive or negative impact on the algorithm performance.

Another important fact is that the quoted studies were carried out using speech corpora in different languages (Mandarin Chinese [4,50], Dutch [43], English [13,18] and Polish here). The results of other works concerning the Polish language are not included in the comparison because to the best of the authors’ knowledge, the publications describing them are unavailable. The impact of language selection on the effectiveness of error detection has not been thoroughly investigated yet, but there are some examples of methods tested on languages from different language families. Table 4
                         shows the example of one method – Goodness of Pronunciation – conducted for Dutch and Chinese speech databases by different authors. The difference in the obtained AER value is 0.08 (8 percentage points) in favor of the tests taking into account the Dutch language. It is difficult to assess all factors that had impact on such different results, but different mispronunciation characteristics in these languages could have been one of them.

Most of the cited works [4,50,43,13] use the HMM models, and two works [18,50] use the neural networks as an input for the classification layer. The modeling phase is therefore more complex and time consuming and needs more training data than the method proposed in this paper. The complexities of the classification stages of particular methods are more congruent; most of the ideas are based on thresholding or structure analysis (apart from [18], where additional linguistic rules are used).

Despite the differences between the presented studies, the results of our research show the efficiency factors comparable to most of the listed works, especially when we take into account processing time and a smaller training database needed for the proposed solution.

@&#CONCLUSION@&#

The paper presents four variants of a novel pronunciation error detection method, using the correct words models created with the application of DTW. HMM were tested as a reference. The analysis is based on phonemes, and in that approach we use structural features of correctly and incorrectly pronounced words. It is intended to focus not only on the systematic errors in order to cover also atypical or rare mispronunciations.

The performance tests of the proposed methods were conducted taking into account various parameters, including HMM characteristics, Sakoe–Chiba Band width and several signal parametrization methods. The best efficiency was achieved for the method based on thresholding measures of the similarity of the appropriate parts of the analyzed word – Normalized Phoneme Distances Thresholding method. The highest accuracy was obtained using the model based on DTW. The error detection time for a single word in this variant is 30–35ms according to the conducted time tests. The detection performance expressed by AER is 0.287, which makes it comparable to the results of the methods described in literature. Moreover, the study showed that results of the detection conducted without a predefined set of errors can be similar to the results offered by the approaches which limit the types of detected mispronunciations.

Another advantage of the proposed solution is that it does not require any additional linguistic knowledge apart from the correctly pronounced training speech corpus. That means that the method can be extended to address pronunciation error detection in languages other than Polish – the only material required is a training database in particular languages.

In the presented study, the DTW models proved to be more effective and faster than HMM models when used in pronunciation error detection for small training corpus. Expanding the training set will probably increase the effectiveness of the classification based on the HMM models. However, it should be noted that for the applications with speech corpus size limitations, the use of DTW instead of HMM turns out to yield better results. The obtained efficacy is also sufficient in view of the implementation of this method in a real CAPT system for speech therapy patients or non-native speakers.

None declared.

@&#REFERENCES@&#

