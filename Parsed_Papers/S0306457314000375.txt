@&#MAIN-TITLE@&#An architecture for Malay Tweet normalization

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           To observe features of Malay Tweets, three distinct corpus-based analyses are done.


                        
                        
                           
                           A rule-based architecture is developed based on results of the analyses.


                        
                        
                           
                           The architecture consists of seven distinct modules in a pipeline structure.


                        
                        
                           
                           Experimental results indicate high accuracy in term of BLEU score.


                        
                        
                           
                           The architecture outperforms SMT-like normalization approach.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Malay

Twitter

Text normalization

Noisy text

@&#ABSTRACT@&#


               
               
                  Research in natural language processing has increasingly focused on normalizing Twitter messages. Currently, while different well-defined approaches have been proposed for the English language, the problem remains far from being solved for other languages, such as Malay. Thus, in this paper, we propose an approach to normalize the Malay Twitter messages based on corpus-driven analysis. An architecture for Malay Tweet normalization is presented, which comprises seven main modules: (1) enhanced tokenization, (2) In-Vocabulary (IV) detection, (3) specialized dictionary query, (4) repeated letter elimination, (5) abbreviation adjusting, (6) English word translation, and (7) de-tokenization. A parallel Tweet dataset, consisting of 9000 Malay Tweets, is used in the development and testing stages. To measure the performance of the system, an evaluation is carried out. The result is promising whereby we score 0.83 in BLEU against the baseline BLEU, which scores 0.46. To compare the accuracy of the architecture with other statistical approaches, an SMT-like normalization system is implemented, trained, and evaluated with an identical parallel dataset. The experimental results demonstrate that we achieve higher accuracy by the normalization system, which is designed based on the features of Malay Tweets, compared to the SMT-like system.
               
            

@&#INTRODUCTION@&#

Social Network Services (SNS) and Microblogs are an increasingly popular form of communication. The explosion of SNS and Microblogs has drawn the attention of researchers to this topic like opinion mining and sentiment analyzing (Thelwall, Buckley, & Paltoglou, 2011). However, one of the biggest obstacles to applying text-mining methods to User Generated Content (UGC) is the existence of noisy text. Most of the Natural Language Processing (NLP) and text-mining algorithms are designed to apply to clean texts. Along with more and more demand for UGC processing, the research in normalization, the process of cleaning noisy text, has become an increasingly important topic.

This paper proposes an approach to normalize Malay Twitter messages. In 2012, Twitter, the most popular microblogging website in the world, had over 500 million registered users (Mislove, Lehmann, Ahn, Onnela, & Rosenquist, 2011). The reason for targeting the Malay language is the considerable usage of this language on Twitter. The Malay language is the fourth leading language that is practiced over Twitter (Hong, Convertino, & Chi, 2011).

Incomplete or unavailable digital resources for the language (Noor, Sapuan, & Bond, 2011) lead us to put Malay in the less studied language category from a computational point of view. For example, to the best of our knowledge, currently there is no study on the Malay spellchecker and Named Entity Recognition (NER). The limited resources of the Malay language inspired us to follow a specific methodology to build the Malay Tweets normalization system. The methodology articulates three major phases: (1) performing analyses to scrutinize features and characteristics of colloquial Malay, (2) designing and implementing a normalization architecture based on the results of the analyses, and (3) evaluating the system and comparing it with other approaches.

The remainder of this paper is organized as follows: Section 2 presents related works on normalizing noisy text. Section 3 discusses the analysis of Malay Tweets. Then, the proposed architecture is introduced in Section 4. Section 5 discusses the results of the evaluation. Finally, Section 6 concludes this paper with a brief summary and future work.

@&#RELATED WORKS@&#


                     Aw, Zhang, Xiao, and Su (2006) employed the Statistical Machine Translation (SMT) method to normalize English SMS. They considered the texting language as a source language, and the Standard English as a target language. One of the shortcomings of their approach is that the system can only normalize tokens that are seen in the training set. Kobus, Yvon, and Damnati (2008) combined an ASR-like system with the SMT metaphor to boost the accuracy of the results. To have unlimited noisy-regular parallel data for the training of the SMT system, Gadde, Goutam, Shah, Bayyarapu, and Subramaniam (2011) formulated an algorithm that artificially generates parallel text in six controlled ways: phonetic substitution, character deletion, word merging, typing errors, word dropping, and de-capitalization/capitalization. The first work on normalizing Tweets performs preprocessing based on the orthographic and syntactic features of a Tweet (Kaufmann & Kalita, 2010). The main normalization process is to apply SMT to English Tweets. In addition, the SMT approach is considered useful to enhance Text-to-Speech (TTS) systems (Lopez Ludeña, San Segundo, Montero, Barra Chicote, & Lorenzo, 2012).

In 2007, a normalization method was introduced, which mimics the spelling error correction model (Choudhury et al., 2007). Instead of a character level noisy channel, a word error model based on the Hidden Markov Model (HMM) was built. The HMM word error model along with a unigram language model can normalize English SMSs with considerable accuracy. However, unlike the SMT-like system, the model ignores the context around the token. Cook and Stevenson (2009) modified the supervised approach of Choudhury et al. (2007) to have a lightly-supervised method. Three word formation probabilities (stylistic variation, subsequence abbreviation, and prefix clipping) were integrated into the word error model to achieve 59% accuracy.

There are other studies that take completely different approaches from SMT and noisy channel metaphors. An architecture has been proposed to normalize French SMS using the combination of rules and models (Beaufort, Roekhaut, Cougnon, & Fairon, 2010). The architecture is composed of three consecutive components in a pipeline. The first and the last components perform tokenization and de-tokenization, respectively, based on manually written rules. The second component normalizes the tokens based upon the trained phonetic model. The results of the evaluation show encouraging performance in terms of BLEU and WER scores, although SER is too high due to the phonetic similarities and complexities in French SMS.


                     Han and Baldwin (2011) introduced a lexical approach for normalizing Tweets. After identifying Out of Vocabulary (OOV) tokens and generating a set of candidates, the system calculates the dependency between tokens using the Stanford parser. The next step is to exploit the linear SVM classifier to determine ill-formed words. The best candidate is selected using a variety of metrics: phonemic edit distance, lexical edit distance, affix substring, Longest Common Subsequence (LCS), language model, and dependency-based frequency features. Although, the approach achieved a high F-score and BLEU score, the system performs below par in highly noisy Tweets. It has been proven that using the Han and Baldwin (2011) approach in a time sensitive Twitter search produces more appropriate results (Wei, Zhou, Li, Wong, & Gao, 2011).


                     Clark and Araki (2011) built a Trie-type dictionary that can store phrases for normalizing casual English. The dictionary consists of 1043 items and end-users can add more items to it. The Trie data structure enables us to perform a prefix search and context aware loop-up. Our proposed architecture also uses a Trie-type data structure. In 2012, Han, Cook, and Baldwin (2012) proved that dictionary based systems can outperform state-of-the-art approaches. To compile a dictionary, they gathered 10 million English Tweets, and selected OOV tokens, which have a high occurrence frequency. Then, for each OOV, the most similar morphophonemic In Vocabulary (IV) word was chosen from the gathered corpus. This inspired us to develop a dictionary-based module in our architecture.

To the best of our knowledge, there are two only studies on Malay normalization. The first study developed a dictionary-based system, known as NoisyTerm, to normalize the content of Malaysian online media (Samsudin, Puteh, Hamdan, & Nazri, 2012). Our approach solved the ambiguity problem in NoisyTerm by developing a context-aware dictionary. The second study introduced a Malay normalization approach that has not been evaluated by standard metrics (Basri, Alfred, & On, 2012). The similarity between Basri et al. (2012) and our approach is the usage of the characteristics of Malay Internet lingo writing style. We correct the blogger’s writing style in a different manner compared to Basri et al. (2012). They used a Malay stemmer (Kadir, Musa, Azman, & Abdullah, 2011), which was designed to work in standard Malay, causing the loss of the original term, whilst we normalize the original words along with their affixes.

@&#ANALYSIS@&#

The design of the normalization system hinges on the results of the corpus-driven analyses. The data analysis is based on TF–IDF schema (Aizawa, 2003). Before designing the normalization system, we performed four analyzing tasks: three analyses on a Malay Twitter corpus and one analysis on the standard Malay corpus. Several studies adopted the corpus-driven approach because the descriptions of the linguistic features of texting are based on the analysis of the text messages themselves (Ling & Baron, 2007). The corpus-driven analysis relies on frequency in indicating significance, with insights emerging through word-frequency lists, concordancing, keywords, and clusters (Sinclair, 2004).

We constructed the Malay Twitter corpus due to the unavailability of a corpus. We gathered 1 million Twitter messages, consisting of 14,484,384 word instances and 646,807 vocabularies, and named it the Malay Chat-style-text Corpus (MCC). To build the corpus, which represents Malay Twitter lingo, corpus compiling criteria were envisaged: repressiveness, sampling, balance, machine readability, size of data, and definite aim of designing corpus (McEnery & Hardie, 2011).

It has been proven that sampling has the highest priority in achieving representativeness (Smith, 1976). The sampling includes three stages: (1) defining the target population; (2) specifying the sample frame; and (3) gathering data according to the selected sampling technique. The definite population was defined as the Malay Twitting lingo. To cover a variety of language styles, the sampling frame included 4500 Twitter user-Ids, for which the profile’s location is set as Malaysia to cover a variety of language styles. We selected the purposive sampling technique, which makes decisions based on the population of interest (Tongco, 2008). Therefore, a linguistic expert selected 321 users from the sampling frame in order to exclude those who tweet in a language other than Malay and who tweet in formal Malay language, such as posting commercial and political messages. Finally, 3200 messages were fetched from each user by Twitter APIs.

Moreover, the representativeness of the corpus was evaluated from two different perspectives: (1) cartography and (2) automatic language identification. We drew the geolocation (latitude/longitude coordinates) of the Tweets on a world map in order to track the actual place of users while sending messages. The similarity between the distribution pattern of the geolocations of the MCC posts and the population density of Malaysia shows the representativeness of the corpus. We also utilized Langid.py (Lui & Baldwin, 2012), which is a state-of-the-art language identification Python library. The obtained confidence scores show that the boundary of the desired population is well determined.

We analyzed the MCC in order to calculate the distribution frequency of unknown words. Table 1
                         shows the top 20 most frequent words in the MCC. These most frequent words indicates that there are large number of OOV words in Malay Tweets. The number of IV words and English words are five and six, respectively. In addition, there are six abbreviated words and only one interjection.

We defined five categories of word forms: IV words, words with extra repeated letters, English words, OOV words with special characters, and other types of misspelled words. The narrow categorization was chosen because of the difficulty in distinguishing between types of misspelled words. We calculated the number of IV words by seeking them in Bahasa Wordnet (Noor et al., 2011). To discover the size of the fraction of English words, we searched for all words in the BNC corpus. Table 2
                         refers to the percentage of each category in the corpus. Ergo, more than 60% of MCC is composed of OOV words, and a large fraction of them are English words.

To have a more precise understanding of OOV frequencies, we divided special characters into two groups: Thin-group and Thick-group (see Table 3
                        ). The Thin-group represents the most common special characters (8 types). The members of the Thin-group are very common in the ordinary Malay writing system. The Thick-group includes special characters that are usually printed on ordinary computer keyboards but excludes characters in the Thin-group (24 types). Table 2 indicates that the percentage frequency of OOV words that contain one of the special characters from the Thin-group is only 0.5%; in contrast, the percentage frequency of those OOV words that contain one of the special characters from the Thick-group is 8.1%. This shows that those words that contain one of the special characters from the Thin-group have a low probability of being detected as an OOV word.

Most of the frequent words used in MCC are abbreviated words. This shows that most users have the tendency to abbreviate words. By scrutinizing abbreviated words, we discovered eight major types of abbreviation in chat-style Malay, as shown in Table 4
                        . The first two rows in Table 4 imply that two types of abbreviation have solid and predictable features: reduplication and negation. Reduplication is a morphological method for producing new meanings or expressing grammatical functions in Malay and many other languages. In Malay, the grammatical rule for producing a negative sentence is the insertion of the word tidak before the verb or adjective.

One of the significant contributions of this work is that it presents a method for eliminating repeated letters from Malay words. To find the morphological features of the Malay word, we carried out an analysis on the Dewan Bahasa dan Pustaka (DBP) corpus, which consists of 135 million words. The results of the analysis show that the repetition of the same letter does not occur in Malay words except for the nine conditions shown in Table 5
                        . If C stands for the Consonant letter and V for the Vowel letter, the format of Malay morpheme is CVC, where C is optional (Kadir et al., 2011). In contrast to English, we can rely on the simplicity of Malay in terms of repeating letters in a row. Some Arabic loan words have two of the same letters in sequence. We gather and keep more than 450 loan words as a bag of words. The other eight conditions pertain to the word affixation.

The corpus-driven analysis reveals a variety of information about Malay Tweets, such as the distribution frequency of at-signs, number-signs, asterisks, and hyperlinks; however, only periods and capitalization are described here for the sake of brevity. Only 68% of Tweets contain periods, 683,827 periods in one million Tweets. In other words, 4.72% of words contain periods. The number of periods, as shown in Table 6
                        , shows the tendency of users to drop the periods at the end of sentences. The results of the analysis on capitalization show that only 49.4% of Tweets begin with capital letters. Table 7
                         shows the exact figures regarding capitalization in MCC.

A set of 9000 parallel word-aligned Tweets, consisting of raw (un-normalized) messages and reference messages, was manually prepared by two project members with the inter-annotator agreement checked. We named it the Malay Parallel Tweets (MPT), and used it for developing and testing the architecture. The architecture includes seven modules, as depicted in Fig. 1
                     . Each module employs a Finite-state machine to describe most of the relevant local phenomena encountered in the empirical study of Twitter language. Table 8
                      refers to 13 types of labels that we use in the architecture. The labels are assigned to tokens in different modules. We defined three main labels, PN, IW, and NT, referring to proper nouns, In-vocabulary words, and normalized tokens, respectively, and ten specific labels that are used for tagging words, which have special characters.

Conventional tokenization algorithms cannot be applied to the colloquial text due to its unpredictable characteristics. The proposed tokenization module helps to detect proper nouns that contain special characters. The tokenization algorithm, shown in Fig. 2
                        , is designed based on the characteristics of Malay Twitter messages. Proper nouns can be detected by finding special characters and digits in tokens. We have nine labels that are designed for nine types of special character (see Table 8). According to our OOV word analysis, these symbols are very common in standard writing style, while other symbols occur in proper nouns. In addition, the initial letter of sentences and names may not be in uppercase, and letter capitalization might be used to express emphasis and emotion. Therefore, identifying the end of a sentence is an arduous task, although we can detect End-of-Line (EOL) by finding the EOL special characters.

In the first step, we convert all capital letters to lowercase because uppercase does not have orthographic value in Tweets (see Section 3.4). After converting consecutive blanks to a single blank, we attend to period marks in the third step. The results of the analysis indicate that most of the sentences do not end with a period. We assume that if a period does not appear before EOL characters, it will occur in a proper noun. Therefore, if the last word of the line contains a period located at the last character, the period will be eliminated and the word will be tagged with the DC label. The fourth step is to eliminate all EOL characters and tag the EL label to the words that occur before them.

After converting white spaces to a new line, we consider each line as a token. In the sixth step, if the last character of a token is an exclamation mark, colon sign, question mark, right quotation marks, comma mark, or right parenthesis, we will delete the character and tag the appropriate label (EM, CC, QP, RQM, CM, or RP) to it. The de-tokenization module needs the labels to append symbols to the tokens. In addition, if the first character of a token is a left quotation mark or left parenthesis, we will remove the character and tag the appropriate label (LQM or LP) to it. The last step is identifying the proper nouns. If any character, except alphabetic characters and digit 2 (2 has a special usage in Malay lingo), appears in a token, the token will be tagged with the PN label.

Before starting modification of the tokens, we need to distinguish between IV and OOV words. This module takes each token, and searches for it in a Trie data structure. To have fast access to IV words, we inserted all the IV words from the Bahasa Wordnet (Noor et al., 2011) text file to the q-fast Trie data structure. Q-fast uses O (N) space and O (√logM) time of insertion, retrieval, and deletion (Willard, 1984). To have an expeditious and factual search, we only search for tokens that do not have the PN (proper noun) label. If a token is found in the q-Trie, we will tag it with the IW label.

Trie, also known as the prefix tree, enables high speed longest-prefix matching. Since we have access to a limited number of IV words (most of them are root words), and Malay is a highly inflected language, using the Maximum-prefix-length match enhances the coverage, and boosts the recall. Therefore, we traverse through q-fast, using characters of the input token. If a token’s prefix matches a word, the module will store the current length, and look for a longer match, finally, the longest match returned. To obtain an acceptable precision, a threshold element of the prefix length is defined. Different threshold values were tried during the testing phase. To mention an example, suppose Fig. 3
                         is the data structure, where the threshold value is set to 3, and the input token is toes. The module distinguishes the token as an IV word, and tags the IW label onto it.

To build the dictionary, we first used 7500 Tweets from the Malay Parallel Tweets (MPT), including 33,878 ill-formed word instances. To build a context aware dictionary, we first collected ill-formed words – those that have a different translation from the original words – from MPT along with their preceding and following words, and their translation equivalent. We then counted the occurrences of each word group (preceding word, ill-formed word, following word). Finally, word groups, that appeared two or more than two times, were inserted into an XML file.

Due to the small number of entries and simple scheme of the dictionary, the Python dictionary data structure was employed to implement the dictionary. The Python dictionary is mutable, and consists of pairs (called items) of keys and their corresponding values, where the keys are unique in a dictionary. We integrated the ill-formed words groups into phrases and placed them into the keys, and did the same for the translated equivalents to place them into values. Fig. 4
                         refers to an example of the Python dictionary items.

A unique elimination method is proposed to tackle the letter repetition in Malay words. However, this method is not applied to the nine exceptional conditions presented in Table 5. Thus, these letters are converted to word patterns. This module uses a pattern finder to check if a word matches the patterns. Fig. 5
                         shows the algorithm for this module, which comprises three steps. The first step is to check if the token has letter repetition in a row, and send distinguished tokens to the next step. In the second step, if the token is detected as a loan word, the module will tag it with an IW label, and send it to the next module, otherwise the word will be sent to the last step. The last step eliminates redundant letters, and tags the tokens with NT label.

In the third step, Regular Expression (RE) comes to aid in determining if tokens conform to one of the conditions. Extra letters will be removed based on the matched pattern. If a token matches with one of the conditions, we will eliminate repeated letters in a certain way that does not disturb the pattern. For example, the term ‘perasaaann’ will be converted to ‘perasaan’ because it matches Condition 7. In contrast, the term ‘sayyaaaa’ will be converted to ‘saya’ because it does not match with any pattern, thus all the repeated letters are reduced to only one letter.

The fifth module of the architecture concentrates on two common abbreviations in the Malay lingo. This module normalizes tokens based on the analysis of the patterns of abbreviations (Abb. 1 and Abb. 2 in Table 4). The first abbreviation style is the whole reduplication abbreviation. Malay has two types of reduplication form: partial and whole. Whole reduplication is the process of adding a dash to the right side of a word and repeating the whole word after the dash (i.e. Word→Word-Word). In order to abbreviate whole reduplication, the second word and its preceding dash is replaced with the digit ‘2’ (i.e. Word-Word→Word2). The second abbreviation style is the abbreviation of negation. The standard negation marker ‘tidak’ is used when the predicate is verbal or adjectival (i.e. Subject Verb→Subject tidak Verb). In order to abbreviate negation, the ‘tidak’ is eliminated and the character ‘x’ is appended to the beginning of the verb or adjective (i.e. Subject tidak Verb→Subject xVerb).

The input of this module is tokens, which do not have any PN, NW, IW, or NT label. Therefore, the tokens do not contain special characters or digits because it was checked and confirmed in the tokenization module. Consequently, if the last character of a token is the digit 2, the digit will be replaced with the token’s duplication preceded by a hyphen, i.e. converting Word2 to Word-Word (For example: buku2→buku-buku (books)). In addition, if the first character of a token is the letter x, the letter will be replaced with tidak followed by a white space, i.e. converting xWord to tidak Word (For example: xsenang
                        →
                        tidak senang (not free)). Finally, modified tokens are tagged with the NT label.

As aforementioned, code-switching between Malay and English is very frequent in Malay Tweets. Therefore, this module translates English words into Malay. The Smith Malay–English Dictionary (Smith & Padi, 2006) text file is converted to a Python dictionary data structure. Tokens, which do not have PN, NW, IW, or NT labels, are inserted into this module. We search for tokens in the dictionary, and replace them with their meaning.

The de-tokenizing module, which consists of five steps, undoes the impact of the tokenizing module. Firstly, tokens that have LQM or LP are detected and a quotation mark or open parenthesis will be added to their beginning. Secondly, the module distinguishes tokens with EM, CC, QP, RQM, CM, or RP tags, and adds the appropriate special character to the end of the tokens. Thirdly, all carriage returns will be converted to white spaces. Fourthly, a carriage return preceded by a period character is created after tokens that have the EL tag. Lastly, the module removes all tags.

@&#EXPERIMENTAL RESULTS@&#

Aside from studies that choose extrinsic evaluations, such as Wei et al. (2011), and Samsudin et al. (2012), BLEU has become an acceptable evaluation metric in normalization research. We tested the architecture and SMT-like system with the same real data (MPT dataset) to discover their accuracy in terms of the BLEU score (Papineni, Roukos, Ward, & Zhu, 2002).

We built a prototype of the proposed architecture, using the Python programming language. Since the first 7500 Tweets of MPT were used to develop the architecture, we used the last 1500 Tweets for testing the architecture. Since MPT is aligned by word and message, the message-aligned edition was used to test the architecture. We examined the architecture with a variety of thresholds for a prefix search in the second module, as shown in Table 9
                        . The evaluation proves that the architecture can improve the BLEU score by 0.37.

The result shows that the prefix search reduces the accuracy of the system, and that the second module works better without a prefix search. The reason behind that is the limited size of the Bahasa Wordnet, and the alteration at the end of the words. For example, if the threshold is set to 7, and the input token is terbaiknie, and the word terbaik (best) exists in the Trie, the second module will mistakenly distinguish it as an IV word, whilst the correct word is terbaiknya.

We divided the MPT dataset into 6 equal sets (1500 Tweets) for 6-fold cross validation. The experiment was done using Moses (Koehn et al., 2007). Giza++ (Och & Ney, 2003) was employed to perform word alignment. The SMT-like system was tested with both the word-aligned version and the message-aligned version of the MPT dataset; however, higher accuracy was achieved with the gold word alignments. Although Moses allows setting the distortion limit to between 0 and 7 for reordering phrases, our experiment without reordering phrases produced better results because we were not translating a language to another language.

The reference part of MPT, which contains 108,373 words, was fed into SRILM (Stolcke, 2002) in order to build the trigram Language Model (LM) by applying Kneser–Ney smoothing. Kaufmann and Kalita (2010) asserted that using cleaned Twitter messages instead of the conventional corpus for compiling LM could boost the accuracy of the system. From the experiments, it was found that if the LM has a weight of 0.7; the BLEU score would be increased by 0.4. Table 10
                         shows the results of the SMT-like system.

@&#DISCUSSION@&#

The architecture and SMT-like system attained BLEU scores of 0.83 and 0.81, respectively. This result proves that a normalization system, which was constructed based on the results of analyses, can outperform the state-of-the-art systems. However, several limitations of our method were detected by analyzing the output of the system. The most obvious one is that the context-support colloquial dictionary will fail where the text becomes very noisy, that is, the preceding and following tokens are misspelled. Another shortcoming is that the English translation module can only translate the correct English words, but not the words with spelling errors. Table 11
                         shows that our architecture can obtain more than an 80% increase in the BLEU score. The achieved BLEU score par excellence shows that the architecture possesses a reasonable level of competence.

The experimental results in Han and Baldwin (2011) show that normalizing Tweets obtained a higher BLEU score compared to SMS messages. Kaufmann and Kalita (2010) indicated that the initial Tweet BLEU score is higher than SMS, that is, Twitter messages have less OOV tokens compared to SMS. However, Kaufmann and Kalita (2010) asserted that the normalizing Twitter message is more complicated due to its irregular pattern of errors. Nonetheless, our baseline BLEU score and the unknown word analysis prove that, to a great extent, Malay Tweets are noisy.

Recent years have witnessed the explosive growth of online Social Network Services. Twitter, with nearly 400 million Tweets per day, is the most used and well-known worldwide microblogging service (Lehmann, Castillo, Lalmas, & Zuckerman, 2013). The sheer volume of messages on Twitter is causing Tweets to become a valuable resource for researchers. However, most of the NLP and text-mining methods are constructed to apply to normal text. Twitter messages contain huge ill-formed words, which are also known as noisy text. Therefore, to process the Twitter messages, normalizing the noisy text is the initial hurdle to overcome. The objective of this paper is to normalize Malay Tweets, the fourth most used language in Twitter.

We designed a normalization architecture based on the features of colloquial and standard Malay. To extract the characteristics of normal and chat-style Malay, four corpus-driven analyses were undertaken: (1) analyzing the frequency distribution of unknown words indicates that English terms are the most prevalent unknown words, followed by abbreviated words and letter repetition; (2) analyzing abbreviation pattern demonstrates that users follow certain methods for some types of abbreviation, i.e. abbreviating reduplication and negation; (3) analyzing consecutive repetition of letters in Malay morphology reveals that there is no sequence of repeated letters in a row in Malay morphology; and (4) inspecting the status of periods and capital letters in Malay Tweets demonstrates that most Twitter sentences do not end with a period and do not begin with a capital letter.

The normalization architecture includes seven modules in a pipeline workflow. The first one is the enhanced tokenization module, designed based on attributes of Malay Tweets. This module can also detect certain types of proper noun. The second module is concerned with distinguishing IV words to protect them against alteration in the next modules. The third module is translating the tokens using a colloquial dictionary. We compiled a Malay colloquial dictionary including 765 entries. The fourth module is the elimination of extra repeated letters, followed by the abbreviated words correction module. The sixth module is the English word translation module and the final module is de-tokenization.

The architecture was implemented using the Python programming language and its accuracy was measured using the BLEU score. The system was tested over 1500 parallel Malay Tweets. According to the experimental results, the proposed architecture increased the BLEU score from 0.46 (before normalization) to 0.83. For the sake of comparison, the SMT-like normalization system was implemented and evaluated. The SMT-like normalization, which considered the noisy text as a source language and the standard text as a target language, is a state-of-the-art statistical approach. The phrase-translation table was generated by using Moses (Koehn et al., 2007), and a trigram LM was produced using SRILM (Stolcke, 2002). With chosen golden word alignment in GIZA++ (Och & Ney, 2003), when it is trained and tested over the 6-fold cross validation using 9000 parallel Malay Tweets, the .81 BLEU score was achieved. In conclusion, the evaluation of the approach showed that it is capable of normalizing the Malay Tweets with high accuracy. It is widely accepted that a spellchecker can improve the performance and the accuracy of normalization systems (Liu, Weng, & Jiang, 2012), whereas, to the best of our knowledge, there is no study on the Malay spellchecker. For future work, the architecture might be integrated with the Malay spell correction system.

@&#ACKNOWLEDGEMENT@&#

We gratefully acknowledge the University of Malaya for supporting this research through UMRG Grant (RG089/12ICT).

@&#REFERENCES@&#

