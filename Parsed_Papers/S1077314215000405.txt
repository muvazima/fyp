@&#MAIN-TITLE@&#Identifying visual attributes for object recognition from text and taxonomy

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A system to automatically mine attributes for object recognition is proposed.


                        
                        
                           
                           A taxonomy defined over classes is used to mine attributes.


                        
                        
                           
                           We integrate distributional similarity to improve the quality of mined attributes.


                        
                        
                           
                           The mined attributes are used in supervised and zero-shot learning settings.


                        
                        
                           
                           We report improved recognition accuracies compared to state of the art methods.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Object recognition

Zero-shot learning

Attribute mining

Attribute-based classification

@&#ABSTRACT@&#


               
               
                  Attributes of objects such as “square”, “metallic”, and “red” allow a way for humans to explain or discriminate object categories. These attributes also provide a useful intermediate representation for object recognition, including support for zero-shot learning from textual descriptions of object appearance. However, manual selection of relevant attributes among thousands of potential candidates is labor intensive. Hence, there is increasing interest in mining attributes for object recognition. In this paper, we introduce two novel techniques for nominating attributes and a method for assessing the suitability of candidate attributes for object recognition. The first technique for attribute nomination estimates attribute qualities based on their ability to discriminate objects at multiple levels of the taxonomy. The second technique leverages the linguistic concept of distributional similarity to further refine the estimated qualities. Attribute nomination is followed by our attribute assessment procedure, which assesses the quality of the candidate attributes based on their performance in object recognition. Our evaluations demonstrate that both taxonomy and distributional similarity serve as useful sources of information for attribute nomination, and our methods can effectively exploit them. We use the mined attributes in supervised and zero-shot learning settings to show the utility of the selected attributes in object recognition. Our experimental results show that in the supervised case we can improve on a state of the art classifier while in the zero-shot scenario we make accurate predictions outperforming previous automated techniques.
               
            

@&#INTRODUCTION@&#

While much research in object recognition has focused on distinguishing categories, recent work has begun to focus on attributes that generalize across many categories [1–15]. Attributes such as “pointy” and “legged” are semantically meaningful, interpretable by humans, and serve as an intermediate layer between the top-level object categories and the low-level image features. Moreover, attributes are generalizable and allow a way to create compact representations for object categories. This enables a number of useful new capabilities: zero-shot learning where unseen categories are recognized [3,4,7], generation of textual descriptions and part localization [2,5,10], prediction of color or texture types [1], and improving performance of fine-grained recognition tasks (i.e. butterfly and bird species or face recognition) [3,6,12,13,15] where categories are closely related.

However, using attributes for object recognition requires answering a number of challenging technical questions – most crucially, specifying the set of attributes and the category–attribute associations. Most prior work uses a predefined list of attributes specified either by domain experts [3,4,12] or researchers [2,6,11,13], but such lists may be time-consuming to generate for a new task, and the attributes in the generated list may not correspond to the optimal set of attributes for the task at hand. A natural alternative is to identify attributes automatically, for example, from textual descriptions of categories. However, this is challenging because the number of potential attributes is large, and evaluating the quality of each potential attribute is expensive.

In this paper, we present a system that automatically discovers a list of attributes for object recognition. As we approach the problem from a computer vision perspective, we are mainly concerned with “visual” attributes that directly relate to the appearance of objects, such as “red” or “metallic”. However, an attribute that relates with visual qualities in general may not be selected by our system if it does not help the recognition task, e.g. “metallic” is not a useful attribute if the recognition task is to classify car brands. In contrast, the word “fragrant” does not refer to a visual quality, however due to its indirect correlation to visual features (e.g. its link to flowers), it may be selected as a useful attribute for object recognition by the proposed method. In the remainder of this paper we use the term “visual attribute” to refer to any word that may help object recognition from images.

Our main contributions are as follows. Firstly, we introduce two methods to select words in a text corpus that are likely to refer to visual attributes. One of the methods we propose uses a taxonomy defined over categories and promotes words whose occurrence in textual descriptions of categories is coherent with the given taxonomy. The other method builds upon the previous one and integrates distributional similarity of words into the attribute selection process. Secondly, we propose a way to assess the quality of a candidate word as an attribute for object recognition from images. In the experiments, we provide evaluations of the proposed attribute selection strategies for effectively identifying attributes, in the plant and animal domains, and present the efficacy of the proposed techniques at selecting visual attributes. Furthermore, we analyze the mined attributes semantically and then use them for plant and animal identification tasks.

We use three input sources in the proposed methods: textual descriptions and image samples of categories and a taxonomic organization of the object domain. In the plant identification task, the goal is to identify a plant species from the visual appearance of its foliage. We use the plant foliage image dataset provided in ImageCLEF’2012
                        1
                     
                     
                        1
                        
                           http://www.imageclef.org/2012
                        
                      plant identification task [16]. This dataset contains 9356 foliage images of 122 species of which 6689 are for training and 2667 are for testing. For this dataset, we mine a set of text documents containing encyclopedic information on categories from the web, using Wikipedia,
                        2
                     
                     
                        2
                        
                           http://en.wikipedia.org/wiki/Main_Page.
                      Encyclopedia of Life
                        3
                     
                     
                        3
                        
                           http://eol.org/.
                      and the Uconn Plant Database.
                        4
                     
                     
                        4
                        
                           http://www.hort.uconn.edu/plants/.
                      For the animal identification task, we use the animals-with-attributes (AwA) database provided by [4] to evaluate our approach. This is a popular database to test attribute-based recognition and zero-shot learning approaches. This dataset contains 30,475 images of 50 animals where 24,295 images of the selected 40 animals are used for training and 6180 images of the remaining 10 classes are reserved for testing in the zero-shot learning setting. Similar to the plant identification, we mine textual descriptions for each of the 50 animals in that set using Wikipedia and A-Z Animals.
                        5
                     
                     
                        5
                        
                           http://a-z-animals.com/.
                      In both of the recognition tasks, the challenge is to find the words referring to visual attributes in the mined documents. We test the effectiveness of the automatically selected attributes for recognition in both zero-shot learning and in traditional supervised learning settings.
                        6
                     
                     
                        6
                        All the collected textual descriptions are available online at: https://drive.google.com/file/d/0Bx-64dmWqUHIT09JRGZDOGxPNkk/view?usp=sharing.
                     
                  

Our approach consists of two main components. After reviewing related work in 2, we describe a method for assessing the visual quality of a proposed attribute for object recognition in Section 3. The assessment procedure involves training a binary attribute classifier, where the quality of a candidate word depends on the success of the attribute classifier. Classification-based attribute selection is effective but computationally expensive; consequently, in Section 4, we propose a set of techniques for nominating candidate attributes that are likely to be of high visual quality: we leverage multi-level discriminability across a category taxonomy, and distributional similarity of the words in the text corpus. Our nomination process takes feedback from the visual quality assessment of candidate attributes, making increasingly accurate predictions as it learns more about the types of words that are found to be of high quality. Once the set of attributes is determined, in Section 5, we illustrate how the selected attributes can be used for object recognition in two different settings. Finally, in the experiments section (Section 6) we present experiments to compare attribute selection strategies. Then, the selected attributes are used for classification of categories in two challenging recognition tasks.

@&#RELATED WORK@&#

Although most of the literature on attribute-centric recognition focuses on working with a predetermined list of attributes, a few alternatives propose methods to select attributes interactively [14,15] or automatically [8,9]. The interactive methods first identify local image patches that are important for recognition and then use human supervision to check whether or not these patches refer to attributes. Unlike these methods, we would like to take advantage of a text corpus and select attributes automatically. Berg et al. [8] also use a text corpus, but instead of learning which attributes are valuable for recognition from text using textual features, they iteratively test the most frequent words in the corpus to find attributes. We show that an intelligently guided search identifies effective attributes much more rapidly.

There is also previous work in the literature to identify words referring to visual characteristics [17–19]. In [17] Barnard and Yanai fit a Gaussian mixture model to image regions and determine the “visualness” of a word based on the entropy of the distribution. Boiy et al. [18] mine words having visual information using corpus-based association techniques where words appearing more in the texts of visual corpus rather than non-visual corpus are selected. In [19] authors use several strategies to mine visual text from large text corpora. First, they generate a graph between adjectives based on distributional similarity and apply bootstrapping to select visual nouns and adjectives. Second they construct a bipartite graph between visually descriptive words and their arguments and use label propagation to extend the list of visual words. Finally, they integrate visual features to improve performance. In comparison, we propose methods that utilize a taxonomy over categories and distributional similarity of words to automatically discover attributes of categories that are likely to refer to visual characteristics.

Another related work [20] involves finding discriminative codes for individual images rather than for categories. In their work Rastegari et al. create a system to encode each image with a binary code to balance discrimination between categories and learnability of the individual attribute classifiers. Although there is no direct semantic mapping of the discovered codes, they achieve state-of-the-art recognition results on Caltech256 [21] and ImageNet [22] databases. In contrast to their work, we define attributes on the category level and rely on a text corpus to automatically mine semantically meaningful and discriminative attributes.

In [23], Yu et al. design a category–attribute matrix on the known categories where they balance the separation of the categories while also considering the learnability of the attribute classifiers. In order to perform zero-shot learning, they use human supervision to create a similarity matrix between the novel and the known categories while using the trained attribute classifiers. While we also design a category–attribute matrix, we use no human-supervision in the process other than supplying the readily-available taxonomy on the categories. Moreover, since we use a text corpus to mine the attributes, the attributes we discover can be directly mapped to semantically meaningful units.

The most similar prior work to ours is [9], where Rohrbach et al. use state-of-the-art natural language processing techniques and provide experiments for several linguistic knowledge bases for mining attributes. However, there are key differences. Rohrbach et al. consider part-of relations encoded in WordNet [24]. In contrast, we mine attributes using a taxonomy defined on the categories and consider the whole text so our method can discover attributes referring to color or context that cannot be explained with part-of relations. Furthermore, expanding the set of candidate attributes to all words requires accurate nomination heuristics; in our approach, we provide this by leveraging the object category taxonomy and distributional similarity.

Object classification using a taxonomy has also been studied before. In [25] Griffin and Perona describe a way to automatically learn hierarchical relationships between images of categories and use this taxonomy in the recognition task. Deng et al. [26] show that there is a correlation between the structure of the semantic hierarchy of the WordNet and visual confusion between the categories. They present a cost function based on the WordNet hierarchy for classification of 10000 categories and show that it produces more semantically meaningful classification results. By defining a taxonomy over categories, Binder et al. [27] train an ensemble of local SVMs on various levels of the taxonomy and use trained classifiers in the recognition task. In contrast to previous lines of work that utilize a taxonomy to improve speed and recognition accuracy from images, we rely on the readily available taxonomy of life to discover attributes of categories in a text corpus that help object recognition.

Finally, in [28], a unimodal topic model that integrates textual and image features is built for the tasks of computing word association and similarity. More recently, in [29] the authors combine visual attribute classifiers with text-based distributional models for finding word associations. In both of these papers, improved results are obtained when textual and image features are used together. However, these lines of work aim to ground natural language semantics in visual features, rather than using language to improve object recognition from images.

In the textual description of a category such as a plant or animal, only a very small fraction of words refer to attributes – such as “legged”, “green”, “big”, “ugly” or “sharp”. Moreover, some of these words refer to very high level qualities (e.g. “ugly”) that may not be robustly detectable using automatic methods. Also, attributes that are beneficial might change depending on the recognition task. For example, the word “green” can be used as an attribute for various tasks, but it is not a useful attribute for plant identification using images of foliage, as most plant foliage is green.

In this section, we present a method to test whether a given candidate word refers to an attribute that can be recognized using visual features. The method is based on the assumption that a word denoting an attribute of an object category will appear frequently in its description. Furthermore, we require from an attribute classifier trained with a proportion of object categories having the attribute versus others, to do a good job in separating novel categories having the attribute from others. Fig. 1 summarizes our process for assessing whether a candidate word is accepted as an attribute; each step is described in detail in the following subsections.

Given a candidate word as an attribute, all object categories in the image collection are automatically labeled as either having or not having the attribute, based on their textual descriptions. The categories having the attribute are associated with the label positive (P) while the remaining categories are associated with the negative (N) label. This is unlike previous works [20,30,8] that use instance-attribute rather than category–attribute association.

The category–attribute association is based on the occurrence frequency of the candidate word in the description of that category in the text corpus. Specifically, we compute the mean and standard deviation of the word frequency across all categories, and then associate a category with P if the frequency of the word in descriptions of that category is at least one standard deviation above the mean frequency. All other categories are associated with the label N. We have tried various other methods for finding the category–attribute associations but saw that this method works quite well in practice.

The P and N categories identified in Section 3.1 are split into training and evaluation sets, using a 50/50 split. If a category is placed in the training set, then all image instances for that category are used for training, and vice versa for the evaluation set. In this way, we avoid attributes specific to a single category that cannot be shared across categories. To be selected, an attribute must be recognizable in novel categories that are unseen in the training data.

Using the P and N image instances reserved for training, we train a binary attribute classifier that learns to differentiate between them. In other words, given a candidate attribute (e.g. “striped”), the classifier is trained to separate the set of images labeled as having this attribute (zebras, tiger, …) or not (lion, panther, …) based on their textual description. The trained attribute classifier is used to detect the attribute a in a given image x, with its output interpreted as p(a|x).

Our experiments focus on plant and animal identification tasks. In the plant identification experiments, we use a binary attribute classifier that operates on a set of features extracted from images; specifically histogram of gradients [31], shape context [32] and local curvature on the object boundary. Each attribute classifier is trained using these feature descriptors of the images in the P and N sets reserved for training.

For the experiments we perform on the Animals with Attributes (AwA) dataset, we train the attribute classifiers using the pre-computed descriptors (color, local self similarity, oriented gradients, rgSIFT, SIFT and SURF histograms) supplied by the authors of the AwA database [4] as well an additional feature descriptor extracted using a convolutional neural network. The feature descriptors of 40 training categories specified in [4] are used in training of the attribute classifiers. During assessment of a candidate word, we train a binary linear SVM as the attribute classifier for each feature descriptor.

The trained attribute classifier is used to produce p(a|x) for each image instance in the evaluation set. We then analyze the probability distributions obtained by the positive and negative samples in the evaluation set, to determine whether the candidate word is accepted as an attribute. The candidate word is accepted as an attribute if the distribution for the instances of the P categories is significantly greater than the distribution of the instances of the N categories. In order to compare the distributions we perform a t-test at p < .01. While precision of the attribute classifier at separating positive and negative instances has been used to assess visual quality of a candidate before [30,8], we preferred to use a t-test which allows us to a detect statistical difference between the distributions of positive and negative instances.


                        Fig. 2
                         presents the histograms of the probability distributions of the positive and negative evaluation instances for two words that are candidate attributes: “deciduous” and “leaflets”. The candidate word “deciduous” fails the assessment, as the classifier assigns essentially the same distributions to the instances of P and N categories in the evaluation set. In contrast, the distribution for the instances of the P categories for the word “leaflets” is skewed to the right compared to that of the instances of N categories. We take this to mean that the word “leaflets” corresponds to some visual characteristics.

The previous section describes an effective procedure for determining if a word is an attribute, but it requires training a binary classification system. Doing so for several thousand candidate attributes can be very time consuming. Hence, we propose techniques to rank the candidates so that the most promising ones are considered first, allowing us to obtain a good set of attributes without iterating through all possible candidates.

The first technique we propose measures each word’s effectiveness as an attribute based on its ability to discriminate not only individual categories, but also higher-order sets of categories as defined by a taxonomy on categories. For example, a good attribute should distinguish not just cars and trucks, but higher-order categories, like vehicles.

The second method we propose organizes all candidate words into a hierarchy, using distributional similarity, so that words with similar distributional properties are in similar parts of the hierarchy. As we assess candidate words, we obtain firm evidence about the visual quality of individual words. This information is then propagated to the neighbors of the assessed candidate word in the hierarchy: i.e. if a word is found to be a good attribute, then distributionally-similar words will be ranked higher as well, and vice versa. We now discuss these ideas in detail.

The living organisms have a taxonomy where organisms are categorized based on similarities or common characteristics. The lowest four ranks of this taxonomy are displayed in Fig. 3
                        : each species is associated with a genus, family, and order. Two species in the same genus are therefore more similar to each other than to other species in different genera. Likewise, two species in the same family are more similar to each other compared to other species in different families.

In this work we are working on classification of plants and animals from images and their biological taxonomy is readily available.
                           7
                        
                        
                           7
                           The taxonomies of plants and animals we use are available online at: https://drive.google.com/file/d/0Bx-64dmWqUHISHhjbWYwOXUxZFk/view?usp=sharing.
                         Assuming this conceptual taxonomy has some correspondence in the visual appearance of each object category, we would like to choose attributes that match it. Such attributes should help us discriminate between highly disparate object classes.

We motivate the use of taxonomy in finding words that are likely to denote visual characteristics with a toy example. Suppose we have a taxonomy defined over 5 species (the leaf nodes) and 2 genera (the middle nodes), as shown in Fig. 4
                         and we have a dictionary of 2 words. We would like to pick one of the words as a candidate attribute and the task is deciding which one to pick. As described in Section 3 each category (species) is associated with either a positive (gray nodes) or negative (white nodes) label depending on the occurrence frequency of the word in textual descriptions of the category. Now, consider the first word that creates the labeling on the left. This word creates a positive set using categories S
                        1 and S
                        3, but these categories belong to different genera. In contrast, the second word that generates the tree on the right side of the figure, illustrates a candidate that respects the taxonomic organization of categories. We hypothesize that words that conform to the taxonomy, such as the second word, will be more likely to have meaningful visual properties, and the results in Section 6 bear this intuition out. We now describe a ranking procedure that will favor such words.

Formally, suppose we are given a set of categories 
                           
                              S
                              =
                              {
                              
                                 S
                                 1
                              
                              ,
                              
                                 S
                                 2
                              
                              ,
                              ⋯
                              ,
                              
                                 S
                                 M
                              
                              }
                           
                        , where each category is represented by a set of text documents 
                           
                              
                                 S
                                 i
                              
                              =
                              
                                 {
                                 
                                    t
                                    
                                       1
                                    
                                    i
                                 
                                 ,
                                 
                                    t
                                    
                                       2
                                    
                                    i
                                 
                                 ,
                                 ⋯
                                 ,
                                 
                                    t
                                    
                                       
                                          N
                                          i
                                       
                                    
                                    i
                                 
                                 }
                              
                           
                        . Assume further that each document has a vector space representation based on tf-idf (term frequency inverse document frequency [33]), where the length of the representation is the same as the dictionary size. Finally, denote by dij
                         a parametric distance between a pair of text documents ti
                         and tj
                        . We will parametrize the distance using a weight vector on the words; words whose discrimination pattern is consistent with the category taxonomy will get higher weights. We pose a constrained optimization problem for this purpose.

For concreteness, we focus on the recognition tasks where the relevant groups are species (S), Genera (G), and Families (F). We pose the following constraints:
                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            d
                                                            ij
                                                         
                                                         +
                                                         1
                                                         ≤
                                                         
                                                            d
                                                            kj
                                                         
                                                         
                                                            
                                                               1
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         ∀
                                                         
                                                            t
                                                            i
                                                         
                                                         ,
                                                         
                                                            t
                                                            j
                                                         
                                                         ∈
                                                         
                                                            S
                                                            I
                                                         
                                                         
                                                            
                                                               0.35
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         
                                                            and
                                                         
                                                         
                                                            
                                                               0.35
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         ∀
                                                         
                                                            t
                                                            k
                                                         
                                                         
                                                            
                                                               0.25
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         ∉
                                                         
                                                            
                                                               0.25
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         
                                                            S
                                                            I
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            d
                                                            ij
                                                         
                                                         +
                                                         1
                                                         ≤
                                                         
                                                            d
                                                            kj
                                                         
                                                         
                                                            
                                                               1
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         ∀
                                                         
                                                            t
                                                            i
                                                         
                                                         ,
                                                         
                                                            t
                                                            j
                                                         
                                                         ∈
                                                         
                                                            G
                                                            I
                                                         
                                                         
                                                            
                                                               0.35
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         
                                                            and
                                                         
                                                         
                                                            
                                                               0.35
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         ∀
                                                         
                                                            t
                                                            k
                                                         
                                                         
                                                            
                                                               0.25
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         ∉
                                                         
                                                            
                                                               0.25
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         
                                                            G
                                                            I
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            d
                                                            ij
                                                         
                                                         +
                                                         1
                                                         ≤
                                                         
                                                            d
                                                            kj
                                                         
                                                         
                                                            
                                                               1
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         ∀
                                                         
                                                            t
                                                            i
                                                         
                                                         ,
                                                         
                                                            t
                                                            j
                                                         
                                                         ∈
                                                         
                                                            F
                                                            I
                                                         
                                                         
                                                            
                                                               0.35
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         
                                                            and
                                                         
                                                         
                                                            
                                                               0.35
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         ∀
                                                         
                                                            t
                                                            k
                                                         
                                                         
                                                            
                                                               0.25
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         ∉
                                                         
                                                            
                                                               0.25
                                                               e
                                                               m
                                                            
                                                            
                                                               0
                                                               e
                                                               x
                                                            
                                                         
                                                         
                                                            F
                                                            I
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        The first constraint states that two documents of the same species should be closer to each other than to documents of the remaining species. Similarly, the second constraint states that two documents belonging to the species of the same genus should be closer to each other compared to the documents of the remaining genera. The third constraint enforces the same condition at the level of families.

Now suppose ti
                         ∈ SI
                        , we define:
                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             
                                                δ
                                             
                                             ij
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             |
                                             
                                                f
                                                
                                                   tf
                                                   −
                                                   idf
                                                
                                             
                                             
                                                (
                                                
                                                   t
                                                   i
                                                
                                                )
                                             
                                             −
                                             
                                                f
                                                
                                                   tf
                                                   −
                                                   idf
                                                
                                             
                                             
                                                (
                                                
                                                   t
                                                   j
                                                
                                                )
                                             
                                             |
                                          
                                       
                                    
                                    
                                       
                                          
                                             d
                                             ij
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             〈
                                             
                                                w
                                                I
                                             
                                             ,
                                             
                                                
                                                   δ
                                                
                                                ij
                                             
                                             〉
                                          
                                       
                                    
                                 
                              
                           
                        where f
                        tf-idf is a function computing the vector of tf-idf values of the dictionary words for its input; 
                           δ
                        
                        
                           ij
                         is the absolute difference vector between the tf-idf vectors of ti
                         and tj
                        , 
                           w
                        
                        
                           I
                         is the weight vector for the object category SI
                         and 
                           w
                        
                        
                           I
                         is the weight vector for the object category SI
                         and 〈, 〉 denotes dot product.

Our procedure for learning the weights is inspired by the metric learning approach of Frome et al. [34]. Suppose 
                           
                              
                                 t
                                 i
                              
                              ∈
                              
                                 S
                                 I
                              
                              ,
                              
                                 
                                    0.35
                                    e
                                    m
                                 
                                 
                                    0
                                    e
                                    x
                                 
                              
                              
                                 t
                                 k
                              
                              ∈
                              
                                 S
                                 K
                              
                           
                        ; denote by 
                           w
                        
                        
                           IK
                         the concatenation of the weight vectors for SI
                         and SK
                        ; let 
                           
                              
                                 x
                                 ijk
                              
                              =
                              
                                 [
                                 −
                                 
                                    
                                       δ
                                    
                                    ij
                                 
                                 
                                    
                                       δ
                                    
                                    kj
                                 
                                 ]
                              
                           
                        , the concatenation of 
                           δ
                        
                        
                           ij
                         negated and 
                           δ
                        
                        
                           kj
                        . Then, the first constraint in Eq. (1) can be rewritten as 〈
                           w
                        
                        
                           IK
                        , 
                           x
                        
                        
                           ijk
                        〉 ≥ 1.

We denote the next two constraints that are defined on the genus and family levels similarly, and we define a loss function over all constraints and all triplets:
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             
                                                ∑
                                                Constraints
                                             
                                             
                                                ∑
                                                ijk
                                             
                                             
                                                
                                                   ⌊
                                                   1
                                                   −
                                                   
                                                      〈
                                                      
                                                         w
                                                         IK
                                                      
                                                      ,
                                                      
                                                         x
                                                         ijk
                                                      
                                                      〉
                                                   
                                                   ⌋
                                                
                                                +
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 ⌊
                                 z
                                 ⌋
                              
                              +
                           
                         denotes the thresholding function max (0, z). After adding a regularization penalty, the objective function becomes:
                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             
                                                1
                                                2
                                             
                                             
                                                
                                                   ∥
                                                   w
                                                   ∥
                                                
                                                2
                                             
                                             +
                                             C
                                             
                                                ∑
                                                Constraints
                                             
                                             
                                                ∑
                                                ijk
                                             
                                             
                                                
                                                   ⌊
                                                   1
                                                   −
                                                   
                                                      〈
                                                      
                                                         w
                                                         IK
                                                      
                                                      ,
                                                      
                                                         x
                                                         ijk
                                                      
                                                      〉
                                                   
                                                   ⌋
                                                
                                                +
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           w
                         is the concatenation of the weight vectors of all categories and C is the regularization parameter. We select C using cross validation, using the value that minimizes the loss function on held-out data.

Once the regularization parameter is selected, the optimization problem can be solved using a row-action method similar to [34]. The weight vector of each species is updated by iterating over all constraints, and the triplets associated with them as follows:
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             
                                                α
                                                ijk
                                             
                                          
                                       
                                       
                                          ←
                                       
                                       
                                          
                                             
                                                ⌊
                                                
                                                   
                                                      1
                                                      −
                                                      〈
                                                      
                                                         w
                                                         IK
                                                      
                                                      ,
                                                      
                                                         x
                                                         ijk
                                                      
                                                      〉
                                                   
                                                   
                                                      
                                                         
                                                            ∥
                                                         
                                                         
                                                            x
                                                            ijk
                                                         
                                                         
                                                            ∥
                                                         
                                                      
                                                      2
                                                   
                                                
                                                +
                                                
                                                   α
                                                   ijk
                                                
                                                ⌋
                                             
                                             
                                                [
                                                0
                                                ,
                                                C
                                                ]
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                w
                                                I
                                             
                                          
                                       
                                       
                                          ←
                                       
                                       
                                          
                                             
                                                w
                                                I
                                             
                                             −
                                             
                                                ∑
                                                ijk
                                             
                                             
                                                α
                                                ijk
                                             
                                             
                                                
                                                   δ
                                                
                                                ij
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                w
                                                K
                                             
                                          
                                       
                                       
                                          ←
                                       
                                       
                                          
                                             
                                                w
                                                K
                                             
                                             +
                                             
                                                ∑
                                                ijk
                                             
                                             
                                                α
                                                ijk
                                             
                                             
                                                
                                                   δ
                                                
                                                kj
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where ⌊z⌋[0, C] denote the function min (C, max (z, 0)). We continue iterating until the change in weights falls below a threshold. Unlike [34], where the authors define constraints on the instances of categories, we have constraints defined over the taxonomy of categories and we do not enforce non-negative weights since words with negative weights can also indicate potential attributes.

Since solving this constrained optimization problem relies on generation of document triplets, let us elaborate on the number of triplets that will be generated. Consider our first constraint in Eq. (1), denote the number of categories by M, and the number of documents per category by N. Then the number of triplets that will be generated is O(M
                        2
                        N
                        3). Working with that many triplets might be infeasible, so when generating triplets we select only a subset of all possible triplets. While forming triplets of the form 〈ijk〉, we select a document k only if it is in the R nearest neighbors of i based on its tf-idf representation, reducing the number of triplets to O(MN
                        2
                        R). We set 
                           
                              R
                              =
                              50
                           
                         during all experiments. In the experiments, we consider the most frequent 2000 non-stoplist words in the text corpus to create tf-idf representations, thus the weight vector of each category is of length 2000.

Once the weight vectors for all categories are learned, we assign a weight for each word in our vocabulary by computing the mean absolute weight of the word over the categories. Words that are shared among categories and obey the category taxonomy will get higher weights, and therefore will be considered first as potential attributes.

We have implemented the described optimization using C#.
                           8
                        
                        
                           8
                           Available online at: https://drive.google.com/file/d/0Bx-64dmWqUHIVzJ3djlCUDQxRjQ/view?usp=sharing.
                         It takes 31 and 276s for the optimization to be completed on the AwA and the ImageClef datasets respectively using a computer having Intel i7 2.00GHz processor.

While taxonomic discriminability is a powerful feature for predicting whether a word will be a useful attribute in general, it ignores the word’s meaning and considers only co-occurrence with category-labeled documents. We hypothesize that if a word has high value as an attribute, then words with similar meanings should also have high value, and vice versa. Word meanings – known as lexical semantics in linguistics – can be difficult to pin down. This is particularly true in technical domains such as plant/animal biology, where annotated resources such as WordNet may have low coverage. We follow an alternative, data-driven approach to lexical semantics, motivated by the distributional hypothesis, which asserts that words with similar meanings tend to appear in similar linguistic contexts [35]. This bears directly on our problem of identifying words that are attributes. For example, if the word “lobed” is found to be a visual attribute, then words that appear in similar contexts to “lobed” (e.g., “serrated”, “oblong”) are also likely to be attributes and should be prioritized for testing. Conversely, if a word such as “Western” is found not to be a visual attribute, then words that appear in similar contexts to “Western” (e.g., “Eastern”, “Chinese”) are unlikely to be attributes (despite having high taxonomic discriminability), and can be tested later.

We use a hierarchical clustering of words to capture word similarity. Each word is represented by a vector of frequencies, where the vector of a word represents its co-occurrence with neighboring words [36,37]. In order to construct the co-occurrence vector of a word we use a context window of five words on either side of the target word where the vector dimensions are constituted by the most frequent 2000 non-stoplist words in the text corpus. Finally, we apply a graph clustering algorithm [38] to the word representations, obtaining a word dendrogram (Fig. 5).
                           9
                        
                        
                           9
                           The word dendrograms for the Awa and the ImageClef datasets are available online at: https://drive.google.com/file/d/0Bx-64dmWqUHIcTN2eEthQnBSMDA/view?usp=sharing.
                        
                     

To see how word similarity can help, consider the toy example shown in Fig. 5. Weights for each word are estimated to be 
                           
                              
                                 w
                                 1
                              
                              =
                              
                                 w
                                 2
                              
                              =
                              1
                              ,
                              
                                 
                                    0.35
                                    e
                                    m
                                 
                                 
                                    0
                                    e
                                    x
                                 
                              
                              
                                 w
                                 3
                              
                              =
                              
                                 w
                                 4
                              
                              =
                              
                                 w
                                 5
                              
                              =
                              0.8
                           
                        , using the procedure described in Section 4.1. Initially, we pick W
                        1, which is tied for the highest weight. However, suppose that W
                        1 fails the visual quality assessment. The word W
                        2 is tied for the next highest weight, but it is distributionally similar to W
                        1. Since W
                        1 is not selected as an attribute, we downweight our prediction for W
                        2, and try another word instead. Note that this idea applies to any hierarchical clustering of words, so we could use other word features instead of co-occurrence-based features to cluster the words and use the same idea.

The key advantage of the word dendrogram is twofold. Firstly, it allows us to integrate distributional similarity into the candidate selection process. Secondly, by propagating information about visual quality assessment between related words, better candidates can be selected as the candidate selection progresses. Initially, the dendrogram helps to smooth the learned weights using taxonomy computed in Section 4.1. As we assess candidate words (Section 3), these initial estimates are replaced with hard evidence. By propagating this evidence through the word dendrogram, we can avoid wasting effort assessing unpromising words whose near neighbors have already failed assessment.

We operationalize this idea in the framework of belief propagation [39,40], treating the word dendrogram as a large graphical model containing binary random variables xe
                         for both words and word clusters. We treat visual quality assessment result as a latent variable xe
                         for node e, and perform inference on the distribution P(
                           x
                        
                        1: E
                        |
                           w
                        
                        1: E
                        ), where 
                           w
                        
                        1: E
                         is the local evidence for all nodes 1: E. This probability is proportional to P(
                           w
                        
                        1: E
                        |
                           x
                        
                        1: E
                        )P(
                           x
                        
                        1: E
                        ), where the first term indicates the likelihood and the second term indicates the prior. After normalizing the learned weights for words between 0 and 1, the likelihood for each node is set equal to the normalized weight of the respective word. The prior is governed by a compatibility function, and in our experiments for neighboring nodes 
                           
                              
                                 x
                                 
                                    e
                                 
                                 ′
                              
                              ,
                              
                                 x
                                 e
                              
                           
                         we define:
                           
                              (6)
                              
                                 
                                    
                                       
                                          
                                             P
                                             
                                                (
                                                
                                                   x
                                                   
                                                      e
                                                   
                                                   ′
                                                
                                                |
                                                
                                                   x
                                                   e
                                                
                                                )
                                             
                                             =
                                             
                                                1
                                                2
                                             
                                             
                                                [
                                                
                                                   
                                                      
                                                         
                                                            α
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               (
                                                               1
                                                               −
                                                               α
                                                               )
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               (
                                                               1
                                                               −
                                                               α
                                                               )
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            α
                                                         
                                                      
                                                   
                                                
                                                ]
                                             
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        where α is a constant greater than 0.5. We experimented with various values of α and set it 0.6 in all experiments, obtaining the best speed at selecting visual attributes. For words whose visual quality has been assessed, we can clamp xe
                         to the true value. By applying belief propagation, information from these clamped nodes is propagated to neighboring nodes, with diminishing influence as we move across the dendrogram.

We use Alg. 1 for nominating and assessing candidate words. With this strategy, we adaptively search the set of possible attributes, focusing our initial efforts on the most promising words while also taking into account the assessed words during later iterations.
                           Algorithm 1
                           Procedure for nominating and assessing attributes.
                                 
                                    
                                       
                                       
                                          
                                             1: function 
                                                propose attributes
                                                
                                                   
                                                      (
                                                      W
                                                      )
                                                   
                                                
                                             
                                          
                                          
                                             2: Estimate taxonomy-based discriminability for all words (Section 4.1)
                                          
                                          
                                             3: Build a dendrogram based on distributional similarity (Section 4.2)
                                          
                                          
                                             4: 
                                                while there are untested words do
                                             
                                          
                                          
                                             5: 
                                                Apply belief propagation to estimate 
                                                   
                                                      P
                                                      (
                                                      
                                                         x
                                                         e
                                                      
                                                      |
                                                      w
                                                      )
                                                   
                                                 for all words
                                          
                                          
                                             6: 
                                                Assess the visual quality of the top-scoring untested word 
                                                   e
                                                 (Section 3), and clamp 
                                                   
                                                      x
                                                      e
                                                   
                                                
                                             
                                          
                                          
                                             7: 
                                                end while
                                             
                                          
                                          
                                             8: end function
                                             
                                          
                                       
                                    
                                 
                              
                           

After generating a list of attributes as described so far, the discovered attributes can be used for object classification. In order to use the selected attributes in object recognition, we train attribute classifiers (one per attribute), such that each classifier is trained to discriminate between the images of positive and negative classes (see Section 3.1) corresponding to that attribute. We use the attribute classifiers for supervised attribute-based classification of plants and zero-shot learning of animals. During the direct similarity-based classification experiments for zero-shot learning, we utilize the learned weight vectors of categories in Section 4.1. Below, we discuss these approaches.

Using attributes we apply the traditional supervised learning paradigm to recognize test images. In order to extract training features, we apply the attribute classifiers on all the training images of each category. For each image we create a feature vector that is the same length as the number of attributes containing attribute classifier responses. Next, an SVM classifier is trained [41] using the extracted features. During testing, we extract the feature vector from a test instance (image) by applying the attribute classifiers and concatenating the classifier responses. The extracted feature vector is then classified by the trained SVM classifier. Supervised attribute-based classification offers advantages over traditional classifiers, since the feature vector is compact and the underlying representation is interpretable to humans.

In zero-shot learning, the aim is to learn to recognize novel categories using only their textual descriptions. For instance, having trained attribute classifiers has-a-torso and has-a-tail, zero-shot learning enables us to label an image of a centaur correctly as having a torso and tail even though the attribute classifiers have never seen an image of a centaur. We use two methods for zero-shot learning: attribute-based recognition and direct-similarity based recognition. These two approaches differ in the way they describe unseen categories. For instance, an unseen category such as leopard will be described as living in Africa and being a member of the feline family by attribute-based recognition whereas direct similarity-based recognition will describe a leopard as being similar to a lion and a bobcat in appearance.

For attribute-based recognition, we create a classifier per attribute, without using any example images from the testing categories. In order to label images of a category we rely on the text corpus and use the found category-attribute associations based on the attribute as in Section 3. During testing, we apply the attribute classifiers to a test image and obtain the probability of each attribute existing in the given test image, i.e.
                           
                              
                                 p
                                 
                                    (
                                    
                                       a
                                       1
                                    
                                    |
                                    x
                                    )
                                 
                                 ,
                                 p
                                 
                                    (
                                    
                                       a
                                       2
                                    
                                    |
                                    x
                                    )
                                 
                                 ,
                                 ⋯
                              
                            The final classification of a test instance is performed using direct attribute prediction (DAP) proposed by Lampert et al. [4].

Using the DAP method, the posterior of a test category given a test image is calculated using:
                              
                                 (7)
                                 
                                    
                                       
                                          
                                             
                                                p
                                                
                                                   (
                                                   z
                                                   |
                                                   x
                                                   )
                                                
                                                =
                                                
                                                   ∑
                                                   
                                                      a
                                                      ∈
                                                      
                                                         
                                                            {
                                                            0
                                                            ,
                                                            1
                                                            }
                                                         
                                                         M
                                                      
                                                   
                                                
                                                p
                                                
                                                   (
                                                   z
                                                   |
                                                   a
                                                   )
                                                
                                                p
                                                
                                                   (
                                                   a
                                                   |
                                                   x
                                                   )
                                                
                                                =
                                                
                                                   
                                                      p
                                                      (
                                                      z
                                                      )
                                                   
                                                   
                                                      p
                                                      (
                                                      
                                                         
                                                            a
                                                         
                                                         z
                                                      
                                                      )
                                                   
                                                
                                                
                                                   ∏
                                                   
                                                      m
                                                      =
                                                      1
                                                   
                                                   M
                                                
                                                p
                                                
                                                   (
                                                   
                                                      a
                                                      
                                                         m
                                                      
                                                      z
                                                   
                                                   
                                                      |
                                                      x
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where z is the test category, az
                            is the category–attribute associations for the category and M is the number of attributes. During testing, identical category priors p(z) are assumed for each category. p(a) is computed using a factorial distribution, 
                              
                                 p
                                 
                                    (
                                    a
                                    )
                                 
                                 =
                                 
                                    ∏
                                    
                                       m
                                       =
                                       1
                                    
                                    M
                                 
                                 p
                                 
                                    (
                                    
                                       a
                                       m
                                    
                                    )
                                 
                              
                            where the attribute priors are approximated using empirical means over the training categories, 
                              
                                 p
                                 
                                    (
                                    
                                       a
                                       m
                                    
                                    )
                                 
                                 =
                                 
                                    1
                                    K
                                 
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    K
                                 
                                 
                                    a
                                    
                                       m
                                    
                                    
                                       y
                                       k
                                    
                                 
                              
                           . This leads to the following MAP prediction f, that assigns the best output category from all test categories 
                              
                                 
                                    z
                                    1
                                 
                                 ,
                                 ⋯
                                 ,
                                 
                                    z
                                    L
                                 
                              
                            to a test image x:
                              
                                 (8)
                                 
                                    
                                       
                                          
                                             
                                                f
                                                
                                                   (
                                                   x
                                                   )
                                                
                                                =
                                                
                                                   
                                                      
                                                         arg
                                                      
                                                      max
                                                   
                                                   
                                                      l
                                                      =
                                                      1
                                                      ,
                                                      ⋯
                                                      ,
                                                      L
                                                   
                                                
                                                
                                                   ∏
                                                   
                                                      m
                                                      =
                                                      1
                                                   
                                                   M
                                                
                                                
                                                   
                                                      p
                                                      (
                                                      
                                                         a
                                                         
                                                            m
                                                         
                                                         
                                                            z
                                                            l
                                                         
                                                      
                                                      |
                                                      x
                                                      )
                                                   
                                                   
                                                      p
                                                      (
                                                      
                                                         a
                                                         
                                                            m
                                                         
                                                         
                                                            z
                                                            l
                                                         
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

For direct similarity-based recognition[9], we first train classifiers to separate each training category from others. Next, the semantic similarity between each testing and training category is computed. Using direct similarity, the posterior of a test category given a test image is calculated using:
                              
                                 (9)
                                 
                                    
                                       
                                          
                                             
                                                p
                                                
                                                   (
                                                   z
                                                   |
                                                   x
                                                   )
                                                
                                                =
                                                
                                                   ∏
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   K
                                                
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            
                                                               p
                                                               (
                                                               
                                                                  y
                                                                  k
                                                               
                                                               |
                                                               x
                                                               )
                                                            
                                                            
                                                               p
                                                               (
                                                               
                                                                  y
                                                                  k
                                                               
                                                               )
                                                            
                                                         
                                                         )
                                                      
                                                   
                                                   
                                                      y
                                                      
                                                         k
                                                      
                                                      z
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where p(yk
                           |x) is the likelihood of a test image belonging to the training category yk
                           , and 
                              
                                 y
                                 
                                    k
                                 
                                 z
                              
                            is the computed semantic similarity between yk
                            and the testing category z. This is essentially applying the DAP method with 
                              
                                 M
                                 =
                                 K
                              
                            (40 in case of the AwA dataset) attributes where each attribute classifier is trained using the instances of a single training category.

In order to compute the semantic similarity between the training and testing categories, we use the computed weight vectors of categories in Section 4.1 using:
                              
                                 (10)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   y
                                                   
                                                      k
                                                   
                                                   z
                                                
                                                =
                                                
                                                   
                                                      〈
                                                      
                                                         w
                                                         k
                                                      
                                                      ,
                                                      
                                                         w
                                                         z
                                                      
                                                      〉
                                                   
                                                   
                                                      
                                                         ∥
                                                      
                                                      
                                                         w
                                                         k
                                                      
                                                      
                                                         ∥
                                                         ∥
                                                      
                                                      
                                                         w
                                                         z
                                                      
                                                      
                                                         ∥
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

@&#EXPERIMENTS@&#

We performed experiments to analyze the success of the proposed methods at selecting attributes for plant and animal identification tasks in Section 6.1. Next, we use the attributes selected by the best candidate selection method in object recognition tasks in Section 6.2: We then group the selected attributes with respect to their semantics in Section 6.3. We use the selected attributes for supervised attribute-based classification of plants and we perform zero-shot learning of animals. Below, we explain these experiments in detail.

We compare four different strategies for proposing candidate words as visual attributes:
                           
                              •
                              
                                 Method-1: Iteratively selecting most frequently occurring words in the dictionary, as in [8], which is our baseline.


                                 Method-2: Estimating word weights using constraints only on the species (category) level and selecting words with highest weights iteratively. This corresponds to applying the approach from Section 4.1, using only the first constraint in Eq. (1).


                                 Method-3: Estimating word weights using taxonomy constraints and selecting words with highest weights iteratively. This corresponds to applying the approach from Section 4.1, but not applying belief propagation across the word dendrogram.


                                 Method-4: Treating visual quality as a latent variable, and applying belief propagation across the automatically-constructed word dendrogram, as described in Section 4.2.

These methods are evaluated in terms of precision at selecting visual attributes and the resulting recognition accuracy, for the plant and animal identification tasks.

While method-1 and method-2 do not require any information other than textual documents describing categories, method-3 is applicable when a taxonomy defined over the categories is available and method-4 requires a word dendrogram to be constructed over the words in the text corpus. In our experiments, we use the weights learned by method-3 to initialize the belief propagation procedure of method-4. However, method-4 can also be used without a taxonomy, by using method-2 to initialize the belief propagation. In summary, among the compared word selection strategies, methods 
                           
                              1
                              ,
                              
                                 
                                    0.35
                                    e
                                    m
                                 
                                 
                                    0
                                    e
                                    x
                                 
                              
                              2
                           
                         and 4 are viable options in the absence of a taxonomy on categories.

We compare the word selection strategies based on the number of candidates they need to assess in order to acquire a fixed number of visual attributes. In Fig. 6
                            the performance of each word selection strategy at finding visual attributes is illustrated. The x axis in the figure is the number of candidate attributes that are assessed, and the y axis is the number of the visual attributes among the candidates. Various points on this figure are also presented in Table 1
                            for comparison. For instance using method-4, in order to obtain 60 visual attributes, 104 candidates need to be assessed.

We have used the output of the visual quality assessment as visualness ground-truth similar to [20,30,8]. The reason for this is threefold: (i) Humans/experts do not have a clear agreement as to which attributes are visual, (ii) As we are looking for visual attributes that are useful for recognition, the decision becomes even more complicated, and (iii) We found that the output of the proposed visual quality assessment correlates with human labels.

We compare the four candidate word selection methods in terms of the number of required candidates in the animal identification task for each feature descriptor. Specifically, we require each method to select 25 visual attributes and compare total number of assessed candidates for each method. The attribute selection results are presented in Table 2. For instance, using the SIFT descriptors and method-4, 35 candidates are assessed for selecting 25 visual attributes.

Compared to the baseline method of selecting the most occurring words in the text corpus (method-1), using method-2 (category-level constraints for learning the weights of words) results in faster, i.e. more precise, mining of visual attributes. In our experiments, for both animal and plant identification tasks, method-2 is favorable to method-1. Thus, we conclude that using category level constraints is useful for automatic attribute selection.

By integrating taxonomy constraints over method-2, we observe a significant performance gain using method-3. In fact, method-3 performs the best in terms of speed at selecting visual attributes in most of our experiments. These results show that having constraints using the taxonomy is crucial to identify candidate words that are likely to be visual attributes.

Adaptive word selection using belief propagation, method-4, builds upon method-3 and incorporates information about word semantics and visual quality assessment into the word selection procedure, through the dendrogram of word similarity. We observe that, while performing competitively, method-4 fails to improve over method-3 in terms of speed in most of our experiments. This might be due to the fact that the graphical model needs to explore more words before exploiting the information about word semantics and visual quality assessment results.

Assessing a single candidate attribute (cross validation to find SVM parameters, training the classifier and testing on the evaluation set) on the ImageClef dataset takes around a minute while it takes around nine minutes on the AwA dataset, using a computer having Intel i7 2.00GHz processor. Method-4 requires assessment of 216/104 candidates before finding 25(*6)/60 attributes for the animal and plant identification tasks, respectively. On the other hand, method-1 requires assessment of 280/130 candidates for the same task. So, if method-4 is used to mine visual attributes instead of method-1, the total time gained for the plant identification task is 26min whereas it is 576min for the animal identification task. These time gains are significant even for relatively small datasets such as AwA and ImageClef.
                        

After the attributes are selected, we use the visual attributes in classification experiments. In this section we compare the attribute-based recognition accuracies of word selection strategies and in Section 6.2 we compare a word selection strategy with state of the art methods.

The resulting recognition accuracies for the plant and animal identification tasks are presented in Table 3. The plant and animal identification tasks are supervised attribute-based classification (see Section 5.1) and zero-shot learning (see Section 5.2) tasks respectively. We see that the recognition accuracy for both plant and animal identification is the best when using the attributes selected by method-4.

In summary, we conclude that method-4 selects the best attributes in terms of recognition accuracy while also performing competitively in terms of precision. As we note above, we think that the lower precision of method-4 with respect to method-3 might be due to the initial smoothing of the learned weights and the number of assessed candidates required to be able to propagate the evidence acquired from visual quality assessment. However, since method-4 also takes into consideration the distributional similarity of words, the visual attributes mined by method-4 are of higher quality for attribute-based recognition purposes.

Below, we use the attributes discovered by method-4 for attribute-based recognition experiments and the weight vectors leaned by method-3 in direct similarity-based recognition experiments. We compare the proposed methods with the state-of-the-art methods in the two recognition tasks.

We use the selected attributes to perform supervised attribute-based classification of plants. In the plant identification task, retrieval performance of a system is also important, so we present recalls for varying values of rank in Fig. 7
                           . For a specific rank, R, a classification decision is accepted as valid if the correct label is in the first Top-R guesses.

We compared five methods in the plant recognition experiment:
                              
                                 (1)
                                 Supervised attribute-based classification described in Section 5.1.

The system of Yanikoglu et al. [42] which had the best results at the ImageClef’2012 plant identification task.

Combination of the first two methods at feature level.

Taxonomy-based classification where given a plant taxonomic hierarchy with M nodes train M one-versus-all classifiers.

Randomly creating a category-attribute association matrix and using it for attribute-based recognition.

The results of the combined system are produced by an SVM classifier that is trained and tested on the concatenation of the attribute-based features (of length 60) with the features used by Yanikoglu et al. (of length 142) for each instance. The taxonomy-based classification approach creates classifiers to separate each species, genus and family from others in the taxonomic classification of plants to perform recognition. In total we created 235 classifiers for taxonomy-based classification. In our final experiment we randomly create a category-attribute association matrix with 60 attributes and train classifiers for each attribute. We repeat the same experiment 5 times and present the mean accuracy which is our baseline.

According to the results, randomly creating a category–attribute association matrix to perform attribute-based classification yields the lowest accuracies as expected. Using the taxonomic hierarchy of plants directly without using a text corpus improves over the baseline. However, this method requires the training of more classifiers (235 compared to 60), does not create semantically meaningful attributes and fails to generalize as well the proposed method. Attribute-based classification using the attributes and the corresponding category–attribute associations we mine not only improves over the baseline it also generalizes well to the recognition task. For R > 4, the recognition results we obtain using attributes are better than the results Yanikoglu et al. obtain. Another observation is that the combined system performs the best for all R suggesting that attribute-based features complement low-level features for the plant identification task.

We illustrate the recognition performance of our system for animal identification using the AwA dataset. Lampert et al. [4] provide 6 feature descriptors for this database and we computed category–attribute associations for each category and for each descriptor. Next, for each category, we concatenated the category–attribute associations of each descriptor to create an extended representation. Thus, after combining all feature descriptors, each category has a representation of length 
                              
                                 150
                                 (
                                 =
                                 25
                                 
                                    
                                       0.35
                                       e
                                       m
                                    
                                    
                                       0.8
                                       e
                                       x
                                    
                                 
                                 *
                                 
                                    
                                       0.35
                                       e
                                       m
                                    
                                    
                                       0.8
                                       e
                                       x
                                    
                                 
                                 6
                                 )
                              
                            during attribute-based recognition experiments. For direct similarity-based recognition experiments, we use the weight vectors of categories to compute semantic similarity between training and testing categories.

As an extension, in addition to the provided 6 feature descriptors of images, we perform recognition experiments using a new feature descriptor. Specifically, we extract a 4096-dimensional feature vector from each image using the Caffe [43] implementation of the convolutional neural network (CNN) described by Krizhevsky et al. [44]. These features are computed by forward propagating a mean-subtracted 
                              
                                 227
                                 ×
                                 
                                 227
                              
                            RGB image through five convolutional layers and two fully connected layers. Using these state-of-the art features for attribute-based recognition we discover 50 attributes and perform the recognition experiment. During direct similarity-based recognition experiment, we train the classifiers of each category using the new feature descriptors.
                              10
                           
                           
                              10
                              The extracted CNN features for the Awa dataset are available online at: https://drive.google.com/file/d/0Bx-64dmWqUHIVTdwS1QyTXlMT3M/view?usp=sharing.
                           
                        

We compare our results with the results of Lampert et al. [45], Yu et al. [23] and Rohrbach et al. [9] as presented in Table 4
                           . While Lampert et al. use the manually defined 85 attributes and category associations in their experiments, Yu et al. utilize a similarity matrix created with human supervision to design attributes, and Rohrbach et al. present experiments with the manually defined attributes and mined category associations, with 74 mined attributes and corresponding category associations, and using direct similarity on several knowledge bases. We use 25 attributes (per feature descriptor) using the provided image descriptors, and 50 attributes using the features extracted by the CNN for attribute-based recognition experiments. In order to perform the direct similarity based recognition experiment with the provided feature descriptors, we concatenate the individual descriptors while training the classifiers of training categories. The comparison includes the knowledge base for the best performing strategy as well the results of Rohrbach et al. using Wikipedia as the knowledge base. Comparison with Wikipedia as the knowledge base is important since we also rely on encyclopedic information for attribute selection.

The highest recognition accuracies using the provided feature-set for attribute-based recognition are obtained by using manually defined attributes and category associations followed by our approach. Our method automatically discovers the attributes and the category–attribute associations while the accuracy we obtain surpasses the case where the attributes are defined manually and only the category–attribute associations are mined. This shows the superiority of our approach and the importance of using the taxonomy/distributional similarity information for automatic attribute selection. Furthermore, when using the CNN features in the attribute-based recognition experiment, a relative increase of around 50% in the recognition accuracy is achieved with respect to using the provided descriptors resulting in a zero-shot recognition accuracy of 45.7%.

In the direct similarity-based recognition experiment, using the provided features, our method obtains an accuracy of 41.8% which is higher than the accuracy obtained by Rohrbach et al. in their experiments. In this setting, our method gets a comparable performance to the accuracies obtained by Lampert et al. and Yu et al. with 85 attributes. Yu et al. report that the accuracy they obtain can be increased up to 48.3% by using more attributes. Using direct similarity and the CNN features, similar to the case of attribute based-recognition, the recognition accuracy we obtain increases up to 59.4%. To our knowledge, this is the best reported zero-shot recognition accuracy on this dataset.

We present the 60 attributes discovered by method-4 for plant identification task in Table 5. We divide the words into 4 groups based on their semantics. The table contains words that may be grouped as:
                           
                              (1)
                              Groupings of plants that are similar to each other such as “Acer”, “Prunus”, and “Sorbus”.

Context for plants such as “catkin”, “gland”, and “branchlet”.

Visual qualities of plants such as “rounded”, “yellow”, and “lobe”.

False positives such as “female”, “root”, and “centimeter”.

Note that words that are categorized as groupings of plants, parts of plants and visual qualities indeed refer to attributes that are used in object description by humans/experts. As for the words that are labeled as false positives, we included all words that we could not directly relate to visual attributes useful for object recognition, including generic words (e.g. “length”, “minutely”, “centimeter”) and some others that may only be indirectly correlated with visual features (e.g. “fragrant”).

The set of 50 attributes that are selected during our experiments using CNN features on the AwA dataset are presented in Table 6
                        . As before, we divided the visual words into 4 semantic categories that we relate to:
                           
                              (1)
                              Groupings of animals such as “cetacean”, “feline”, and “shark”.

Context for animals such as “Africa”, “agricultural”, and “sea”.

Visual qualities of animals such as “hoof”, “ivory”, and “fancy”.

False positives such as “intelligence”, “favored”, “weightkg”.

We remind that we use the word “visual attribute” to refer to any word that may help in object recognition from images as described in Section 3. Indeed, we demonstrate the effectiveness of the selected visual attributes in object recognition, even though only a portion of them relate to truly visual qualities.

In this paper, we tackle the important problem of automatically mining words referring to visual attributes. Our method assists the laborious attribute selection process and allows us to rapidly apply attribute-centric recognition to various recognition tasks. In order to mine attributes, we use the taxonomy of the domain, sample images and textual descriptions of object categories. We show the utility of our approach in two tasks: plant identification and zero-shot learning of animals.

The contributions of this paper include two novel methods for identifying plausible candidates that can be used as attributes. While the first method uses the taxonomy defined on the categories and promotes candidates that conform to the taxonomy, the second method combines taxonomy with a distributional similarity measure. By providing a way to assess the visual quality of candidate words, we create an automatic system that generates a set of attributes for plant and animal identification tasks.

During experiments, we illustrate the utility of integrating taxonomy constraints for candidate word selection via two cases. In the first case we use only species (category) level constraints while in the second one we include taxonomy (species, genus and family) constraints. We show that taking advantage of the taxonomy yields substantially better results. The experiments also show that taxonomy-based distributional similarity of words can be used as a cue for selecting candidate words. By performing inference using the graphical model built on word dendrogram, we can further improve the recognition accuracies.

Plant/animal identification are both challenging problems and we demonstrate the usefulness of the mined set of attributes by using the trained attribute classifiers in both of these tasks. In the plant identification task the attribute classifiers are used for feature extraction in a supervised learning setting; while in animal identification, they are used for zero-shot learning of unseen animal categories.

During the plant identification experiments, we show that using attribute-based features bring performance improvements compared to using only lower-level features. The classification results also highlight better performance of attribute-based features with increasing ranks, while providing a compact representation. The zero-shot learning of animals demonstrate the quality of our attribute selection procedure: our system that automatically selects both attributes and category–attribute associations, achieves better recognition results than the state-of-the-art results obtained with manually defined attributes and mined category–attribute associations, in the same dataset. Using direct similarity-based recognition, we further improve our results and obtain recognition accuracies comparable to the case where both attributes and category–attribute associations are selected manually. In the recognition experiments, we also present the performance of our system using state-of-the-art CNN features and, to our knowledge, get the best zero-shot recognition accuracies obtained on the AwA dataset.

@&#REFERENCES@&#

