@&#MAIN-TITLE@&#Unsupervised video categorization based on multivariate information bottleneck method

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Novel multivariate IB model is proposed for unsupervised video categorization.


                        
                        
                           
                           Effective solution is designed to integrate multiple features simultaneously.


                        
                        
                           
                           Information-theoretic optimization is constructed to alleviate the semantic gap.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Video categorization

Unsupervised learning

Multivariate information bottleneck

Multiple features

Mutual information

@&#ABSTRACT@&#


               
               
                  The integration of multiple features is important for action categorization and object recognition in videos, because single feature based representation hardly captures imaging variations and individual attributes. In this paper, a novel formulation named Multivariate video Information Bottleneck (MvIB) is defined. It is an extensional type of multivariate information bottleneck and can discover categories from a collection of unlabeled videos automatically. Differing from the original multivariate information bottleneck, the novel approach extracts the video categories from multiple features simultaneously, such as local static and dynamic feature, each type of feature is treated as a relevant variable. Specifically, by preserving the relevant information with respect to these feature variables maximally, the MvIB method is able to integrate various aspects of semantic information into the final video partitioning results, and thus captures the complementary information resided in multiple feature variables. Extensive experimental results on five challenging video data sets show that the proposed approach can consistently and significantly outperform other state-of-the-art unsupervised learning methods.
               
            

@&#INTRODUCTION@&#

With the continuing rapid growth of personal video recordings, online video data and broadcast news, unsupervised action categorization and object recognition in video clips have been an active and challenging research area. However, there are two key issues in the task of discovering categories automatically from a collection of unlabeled videos: (1) Because of the cluttered background, camera motion, occlusion, changes of view point and variances of the geometric distribution in videos, robust feature representation extraction remains a difficult problem. Moreover, single feature based representation can hardly capture imaging variations and individual attributes. (2) Automatical method for differentiating videos is also a quite challenging task due to the lack of the ground-truth label information.

For the first key issue, robust feature representation should be achieved before discovering categories from a collection of unlabeled videos. Recently, researchers have found that the quality of feature representation is of great importance to action categorization and object recognition in videos. Therefore, large amounts of feature extraction techniques were proposed. The well-known feature representations include static based on edges and limbs [1–3], shape or form features [4,5]; interest point based representation [6,7]; motion or optical flow patterns [8,9]; nontensor product wavelet filter banks [10]. However, the capability of single feature is not enough to capture discriminative information, which will make the representations include prejudices caused by single feature. Besides, most of feature extraction approaches described above only consider single aspect of information for the task of video action classification. For instance, certain actions, such as hand clapping, produce a small number of dynamic features since most body parts remain static. While some other actions, such as cycling and horseback riding, are similar in motion features. Therefore, it is difficult to distinguish action categories in the task of unsupervised video categorization only based on single feature. So we strongly feel that various kinds of features are mutually complementary for video action categorization.

A reliable mechanism to learn the action and object categories based on the visual features is the second key issue associated with unsupervised category discovery in videos. However, most current learning approaches are supervised methods, which need the ground-truth label information. Labeling the videos manually is a labor intensive and time consuming process, which often invites subject biases or mistakes by human labelers. So some research efforts have been dedicated to the task of unsupervised object category discovery, such as k-means, PLSA [11], LDA [12], AP [13], SC [14]. These existing unsupervised methods try to learn the object categories from the visual contents in a two-step manner: (1) Building an affinity matrix to reflect the video relations based on visual features. (2) Partitioning the videos into different groups by considering the affinity values. This assumption always limits the performance of aforementioned methods due to the semantic gap between the visual features and their high-level semantic concepts. Besides, most unsupervised approaches used in the domain of video categorization can only cope with single feature.

In this paper, a novel unsupervised video categorization method called Multivariate video Information Bottleneck (MvIB) is proposed, which can partition video clips from multiple cues. Like original multivariate information bottleneck (multivariate IB) model, the novel clustering approach treats pattern structure extraction as data compression. In the video categorization procedure, the proposed method conserves the relevant information from multiple feature cues rather than only one source of feature information. Besides, a information-theoretic optimization is adopted to learn the latent semantic correlations between the videos and their constructive visual words automatically, which can relieve the semantic gap between the visual features and their high-level semantic concepts.

The major contributions of this study are summarized as follows:
                        
                           •
                           A novel and effective multivariate information bottleneck model is proposed, which extends the original multivariate IB to the task of unsupervised video category discovery.

The MvIB method can incorporate multiple information cues into the clustering process, which provides an effective solution to integrate multiple features simultaneously.

An effective information-theoretic optimization method is designed to learn the latent semantic correlations between the videos and their low-level visual features, which alleviates the semantic gap in the current unsupervised learning techniques.

The rest of this paper is organized as follows. In Section 2, the related work is introduced. In Section 3, the basic knowledge is presented about multivariate IB method. In Section 4, details of the MvIB approach are described. In Section 5, extensive experimental results are presented to demonstrate the performance of MvIB. Finally, conclusions are given in Section 6.

@&#RELATED WORK@&#

Several works have been reported on the integration of features for action category discovery in realistic videos. Neibles et al. [15] proposed a generative method to learn a hierarchical model using both static and dynamic features for action recognition, their results verified the combined features were useful. Liu et al. [16] adopted Fiedler Embedding model to combine local dynamic and spin image features, where the spin image features capture the global pose information. Natarajan et al. [17] analyzed and combined a large set of low-level features that captured appearance, color, motion co-occurrence patterns in web videos. Ikizler et al. [18] proposed multiple instance learning (MIL) framework for human action recognition, which integrates multiple feature channels from several entities such as objects, scenes and human. Kim et al. [19] proposed a new indexing method, which has the ability to capture videos with spatiotemporal information such as time, location, and camera direction. Despite the good recognition performance of the above methods, low-level features are incapable of understanding the hidden semantic structure latent in videos. Often, the aforementioned methods are akin to object recognition and require extra training videos. So these methods may not be applicable for realistic videos due to the difficulty in acquiring good features in unconstrained videos.

Some research efforts have been dedicated to the task of unsupervised category discovery in videos. Bettadapura et al. [20] presented data-driven techniques to augment the BoW model, which allowed for more robust modeling and recognition of complex long-term activities. Beyond the BoW, the discriminative topic learning model has achieved excellent performance on action categorization and object recognition tasks, such as LDA and PLSA [15,21]. Moreover, several methods based on spectral clustering have been applied to the domain of unsupervised image and video categorization by considering multiple features recently. Correlational spectral clustering [22] separates similarity measures for each data representation, and allows for projection of previous unseen data that are only observed in one representation. Heterogeneous image feature integration via multi-modal spectral clustering [23] learns a commonly shared graph Laplacian matrix by unifying different models by considering each type of feature as one modal. Affinity aggregation for spectral clustering [24] seeks for an optimal combination of affinity matrices so that it is more immune to ineffective affinities and irrelevant features.

For the information bottleneck (IB) method [25], Winston et al. [26] utilized the IB method for video reranking and achieved good results, which shows IB is a promising method for semantic learning, but this study solely focused on video search reranking task. Lou et al. [27] extended the original information bottleneck method to multiple-feature version, which aimed to extract the data patterns from multiple feature variables. Therefore, the partition results reflected the hidden patterns provided by multiple types of features simultaneously. But the method presented in [27] was based mainly on original information bottleneck. Note that, the proposed method in this work is a multiple-feature extension of multivariate IB theory.

In this study, we focus on the unsupervised video category discovery issue. Different from all the aforementioned approaches, a novel multiple-feature extensional type of the multivariate information bottleneck [28] method is defined. The MvIB method can integrate multiple visual information into the clustering process, which provides an effective solution to integrate multiple features simultaneously. Moreover, the MvIB method can relief the semantic gap problem effectively by exploiting the correlations between the videos and the visual words. There are many achievements in the field of machine learning and computer vision to cope with multiple sources of features, but most of them need supervision, i.e. the class label information. Note that, the MvIB algorithm is an unsupervised learning method.

The original single-sided IB principle involves finding a compressing scheme with a given source variable, X, while preserving the information it maintains about relevant variable Y. This formulation is inherently a-symmetric, only X is compressed while only Y serves as a relevant variable. In order to cope with multiple variables scenario, Slonim et al. presented a general formulation for multivariate extension of the single-sided IB principle, named multivariate information bottleneck (multivariate IB) [28]. The multivariate IB method is an information-theoretic based data analysis method, which treats the pattern extraction as a process of data compression. To define the amount of information that multiple variables contain each other, the concept of multi-information is utilized by the multivariate IB method, which is a natural extension of the concept of mutual information. The multi-information is the KL divergence between the joint distribution and the factored distribution of margins. The multi-information is described in the following definition:
                        
                           (1)
                           
                              I
                              (
                              
                                 
                                    X
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    X
                                 
                                 
                                    n
                                 
                              
                              )
                              =
                              
                                 
                                    D
                                 
                                 
                                    KL
                                 
                              
                              [
                              p
                              (
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    x
                                 
                                 
                                    n
                                 
                              
                              )
                              |
                              |
                              p
                              (
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                              )
                              …
                              p
                              (
                              
                                 
                                    x
                                 
                                 
                                    n
                                 
                              
                              )
                              ]
                              ,
                           
                        
                     
                  

The multivariate IB method further utilizes the theory of probabilistic graphical model Bayesian network for specifying the systems of clusters and which information terms should be maintained. A Bayesian network structure over a set of random variables 
                        
                           X
                           =
                           {
                           
                              
                                 X
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 X
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 X
                              
                              
                                 n
                              
                           
                           }
                        
                      is a Directed A-cyclic Graph (DAG) in which vertices are annotated by names of random variables. For each variable 
                        
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                        
                     , we denote by 
                        
                           
                              
                                 Pa
                              
                              
                                 
                                    
                                       X
                                    
                                    
                                       i
                                    
                                 
                              
                              
                                 G
                              
                           
                        
                      the set of parents of 
                        
                           
                              
                                 X
                              
                              
                                 i
                              
                           
                        
                      in G, and by 
                        
                           
                              
                                 pa
                              
                              
                                 
                                    
                                       X
                                    
                                    
                                       i
                                    
                                 
                              
                              
                                 G
                              
                           
                        
                      a specific assignment to this set of variables. We say that a distribution p is consistent with G, if only if p can be factored in the form:
                        
                           (2)
                           
                              p
                              (
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    x
                                 
                                 
                                    n
                                 
                              
                              )
                              =
                              
                                 
                                    
                                       ∏
                                    
                                    
                                       i
                                    
                                 
                              
                              p
                              (
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                              |
                              
                                 
                                    Pa
                                 
                                 
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    G
                                 
                              
                              )
                              ,
                           
                        
                     
                  

The multi-information in 
                        
                           p
                           (
                           x
                           )
                        
                      with respect to a given Bayesian network structure G is defined as:
                        
                           (3)
                           
                              
                                 
                                    I
                                 
                                 
                                    G
                                 
                              
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       i
                                    
                                 
                              
                              I
                              (
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                              
                              ;
                              
                                 
                                    Pa
                                 
                                 
                                    
                                       
                                          X
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    G
                                 
                              
                              )
                              ,
                           
                        
                     where each of the local mutual information terms is calculated using the marginal distributions of 
                        
                           p
                           (
                           x
                           )
                        
                     .

In the framework of multivariate IB, the unsupervised tasks are presented by specifying a pair of Bayesian network. The work presented in [28] used one Bayesian network, denoted as 
                        
                           
                              
                                 G
                              
                              
                                 in
                              
                           
                        
                     , to specify a set of variables which are compressed versions of the observed variables (each new variable compresses its parents in the network). A second network, 
                        
                           
                              
                                 G
                              
                              
                                 out
                              
                           
                        
                     , specifies the relations that should be maintained or predicted by 
                        
                           
                              
                                 G
                              
                              
                                 in
                              
                           
                        
                      (each variable is predicted by its parents in the network). It minimizes the information maintained by 
                        
                           
                              
                                 G
                              
                              
                                 in
                              
                           
                        
                      and maximizes the information maintained by 
                        
                           
                              
                                 G
                              
                              
                                 out
                              
                           
                        
                     . The information contained in 
                        
                           
                              
                                 G
                              
                              
                                 in
                              
                           
                        
                      and 
                        
                           
                              
                                 G
                              
                              
                                 out
                              
                           
                        
                      is defined as follows:
                        
                           (4)
                           
                              
                                 
                                    I
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          in
                                       
                                    
                                 
                              
                              =
                              [
                              I
                              (
                              
                                 
                                    T
                                 
                                 
                                    1
                                 
                              
                              ;
                              X
                              )
                              +
                              ⋯
                              +
                              I
                              (
                              
                                 
                                    T
                                 
                                 
                                    n
                                 
                              
                              ;
                              X
                              )
                              +
                              I
                              (
                              X
                              ;
                              Y
                              )
                              ]
                              ,
                           
                        
                     
                     
                        
                           (5)
                           
                              
                                 
                                    I
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          out
                                       
                                    
                                 
                              
                              =
                              [
                              I
                              (
                              
                                 
                                    T
                                 
                                 
                                    1
                                 
                              
                              ;
                              Y
                              )
                              +
                              ⋯
                              +
                              I
                              (
                              
                                 
                                    T
                                 
                                 
                                    n
                                 
                              
                              ;
                              Y
                              )
                              ]
                              ,
                           
                        
                     where 
                        
                           I
                           (
                           X
                           ;
                           Y
                           )
                        
                      is the mutual information between variable X and Y. As 
                        
                           I
                           (
                           X
                           ,
                           Y
                           )
                        
                      is constant, we can ignore it. The mutual information [29] between variable X and Y is defined as follows:
                        
                           (6)
                           
                              I
                              (
                              X
                              ;
                              Y
                              )
                              =
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       x
                                       ∈
                                       X
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       y
                                       ∈
                                       Y
                                    
                                 
                              
                              p
                              (
                              x
                              ,
                              y
                              )
                              log
                              
                                 
                                    p
                                    (
                                    x
                                    ,
                                    y
                                    )
                                 
                                 
                                    p
                                    (
                                    x
                                    )
                                    p
                                    (
                                    y
                                    )
                                 
                              
                              ,
                           
                        
                     
                  

Similar to the objective function of single-sided IB, the multivariate IB function is not convex with all of its arguments simultaneously. Hence, different heuristics must be employed to obtain at least locally optimal solutions. Formally, Slonim et al. [28] suggested the following multivariate IB-functional:
                        
                           (7)
                           
                              
                                 
                                    L
                                 
                                 
                                    max
                                 
                              
                              =
                              
                                 
                                    I
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          out
                                       
                                    
                                 
                              
                              -
                              
                                 
                                    β
                                 
                                 
                                    -
                                    1
                                 
                              
                              
                                 
                                    I
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          in
                                       
                                    
                                 
                              
                              ,
                           
                        
                     where β is the Lagrange multiplier controlling the trade-off between the compression from 
                        
                           
                              
                                 I
                              
                              
                                 
                                    
                                       G
                                    
                                    
                                       out
                                    
                                 
                              
                           
                        
                      and the preserved information from 
                        
                           
                              
                                 I
                              
                              
                                 
                                    
                                       G
                                    
                                    
                                       in
                                    
                                 
                              
                           
                        
                     . When the value of β is set to 
                        
                           ∞
                        
                     , the multivariate IB method concentrates on the information preservation between the variables in the structure 
                        
                           
                              
                                 I
                              
                              
                                 
                                    
                                       G
                                    
                                    
                                       out
                                    
                                 
                              
                           
                        
                     , while the value of β is set to 0, the multivariate IB method concentrates on the information compression between the variables in the structure 
                        
                           
                              
                                 I
                              
                              
                                 
                                    
                                       G
                                    
                                    
                                       in
                                    
                                 
                              
                           
                        
                      (see Fig. 2
                     ).

For a better understanding, now we consider a classical type of the multivariate information bottleneck, as shown in Fig. 2. Given a joint distribution 
                        
                           p
                           (
                           X
                           ,
                           Y
                           )
                        
                     , where 
                        
                           X
                           =
                           {
                           
                              
                                 x
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 x
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 x
                              
                              
                                 m
                              
                           
                           }
                        
                      is a set of source data, Y is relevant information of source data X. Instead of one compression variable T, Slonim et al. [28] considered a set 
                        
                           T
                           =
                           {
                           
                              
                                 T
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 T
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 T
                              
                              
                                 n
                              
                           
                           }
                        
                     . More specifically, one network, 
                        
                           
                              
                                 G
                              
                              
                                 in
                              
                           
                        
                      over 
                        
                           X
                           ∪
                           T
                        
                     , indicates the relations between the source data and these new compression variables. Another network, 
                        
                           
                              
                                 G
                              
                              
                                 out
                              
                           
                        
                      over 
                        
                           T
                           ∪
                           Y
                        
                     , represents which relations should be maintained or predicted. The multivariate IB method formulates the general principle as a tradeoff between the information each network carries. So the multivariate IB-functional can be externalized as follows (see Fig. 2):
                        
                           (8)
                           
                              
                                 
                                    L
                                 
                                 
                                    max
                                 
                              
                              [
                              p
                              (
                              t
                              |
                              x
                              )
                              ]
                              =
                              
                                 
                                    I
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          out
                                       
                                    
                                 
                              
                              -
                              
                                 
                                    β
                                 
                                 
                                    -
                                    1
                                 
                              
                              
                                 
                                    I
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          in
                                       
                                    
                                 
                              
                              =
                              [
                              I
                              (
                              
                                 
                                    T
                                 
                                 
                                    1
                                 
                              
                              ;
                              Y
                              )
                              +
                              ⋯
                              +
                              I
                              (
                              
                                 
                                    T
                                 
                                 
                                    n
                                 
                              
                              ;
                              Y
                              )
                              ]
                              -
                              β
                              [
                              I
                              (
                              
                                 
                                    T
                                 
                                 
                                    1
                                 
                              
                              ;
                              X
                              )
                              +
                              ⋯
                              +
                              I
                              (
                              
                                 
                                    T
                                 
                                 
                                    n
                                 
                              
                              ;
                              X
                              )
                              ]
                              ,
                           
                        
                     similar to Eq. (7), β is the Lagrange multiplier controlling the trade-off between the information compression from 
                        
                           
                              
                                 I
                              
                              
                                 
                                    
                                       G
                                    
                                    
                                       out
                                    
                                 
                              
                           
                        
                      and preservation from 
                        
                           
                              
                                 I
                              
                              
                                 
                                    
                                       G
                                    
                                    
                                       in
                                    
                                 
                              
                           
                        
                     .

Given a collection of unlabeled videos, our goal is to learn different categories automatically. The original multivariate IB concentrates on compressing source variable X into multiple compression models 
                        
                           T
                           =
                           {
                           
                              
                                 T
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 T
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 T
                              
                              
                                 n
                              
                           
                           }
                        
                     , while preserving the information it maintains about relevant variable Y. Therefore, the original multivariate IB model cannot discover categories by considering multiple features. To overcome this problem, we proposed a novel unsupervised video category discovery method called Multivariate video Information Bottleneck method (MvIB), which is an extensional type of multivariate IB method, to discover categories remained in video collections automatically by multiple features. Now we introduce the MvIB method in detail.

In this section, a novel and effective method is presented to discover the hidden patterns of object by considering multiple cues, which is called Multivariate video Information Bottleneck (MvIB). In the novel approach, multiple relevant variables are defined to represent multiple information sources of the videos. For clarity, the definition of the task of MvIB method is described as follows.

Given a discrete random variable X, taking values from an video collection 
                           
                              X
                              =
                              {
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    x
                                 
                                 
                                    m
                                 
                              
                              }
                           
                        , there are k 
                        
                           
                              (
                              k
                              ⩾
                              1
                              )
                           
                         discrete random variables 
                           
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                           
                         on behalf of different feature types, the corresponding joint distributions are 
                           
                              p
                              (
                              X
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              )
                              ,
                              …
                              ,
                              p
                              (
                              X
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                              )
                           
                        . Each variable 
                           
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                              (
                              1
                              ⩽
                              i
                              ⩽
                              k
                              )
                           
                         takes values from one feature source 
                           
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                              =
                              {
                              
                                 
                                    y
                                 
                                 
                                    1
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    y
                                 
                                 
                                    2
                                 
                                 
                                    i
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    y
                                 
                                 
                                    
                                       
                                          n
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    i
                                 
                              
                              }
                           
                        , which characterizes the samples of X from one cue. By considering the original multivariate IB method, the task of MvIB is to learn a good compressing representation 
                           
                              p
                              (
                              t
                              |
                              x
                              )
                           
                         of X to T from multiple feature variables 
                           
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                           
                         (see Fig. 1).

Suppose there is a video collection contained various action categories 
                           
                              X
                              =
                              {
                              
                                 
                                    x
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    x
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    x
                                 
                                 
                                    m
                                 
                              
                              }
                           
                        , where m is the total number of videos in data collection. We obtain k visual vocabulary corresponding to multiple feature variables 
                           
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                           
                        . For example, 
                           
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                           
                         is one feature type, 
                           
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                              =
                              {
                              
                                 
                                    y
                                 
                                 
                                    1
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    y
                                 
                                 
                                    2
                                 
                                 
                                    i
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    y
                                 
                                 
                                    
                                       
                                          n
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    i
                                 
                              
                              }
                              ,
                              (
                              1
                              ⩽
                              i
                              ⩽
                              k
                              )
                           
                        , where n is the size of vocabulary constructed by k-means method. Based on the BoW model, each video sequence is transformed to a feature vector, which contains the occurrence number of each visual video word in the visual vocabulary. The conditional distribution of the visual words is defined as follows:
                           
                              (9)
                              
                                 p
                                 (
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 x
                                 )
                                 =
                                 
                                    
                                       n
                                       (
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       |
                                       x
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   y
                                                
                                                
                                                   ′
                                                
                                             
                                             ∈
                                             
                                                
                                                   Y
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                       n
                                       (
                                       
                                          
                                             y
                                          
                                          
                                             ′
                                          
                                       
                                       |
                                       x
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              n
                              (
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                              |
                              x
                              )
                           
                         denotes the number of occurrences of the visual word 
                           
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                           
                         in the video x. The prior distribution of 
                           
                              p
                              (
                              x
                              )
                           
                         for each video is fitted as a uniform distribution, i.e., 
                           
                              p
                              (
                              x
                              )
                              =
                              
                                 
                                    1
                                 
                                 
                                    m
                                 
                              
                           
                        . Using the above definitions, the joint distribution between video variable X and the feature variable 
                           
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                           
                         can be obtained by 
                           
                              p
                              (
                              X
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                              )
                              =
                              p
                              (
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                              |
                              x
                              )
                              p
                              (
                              x
                              )
                           
                        . Similarly, we obtain the joint distribution of all relevant feature variable 
                           
                              p
                              (
                              X
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              )
                              ,
                              …
                              ,
                              p
                              (
                              X
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                              )
                           
                         according to the equations described above.

Based on the joint distribution 
                           
                              p
                              (
                              X
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                              )
                           
                        , the ideology of multivariate IB is employed to discover the action categories in an unlabeled video collection. Similar to the original multivariate IB method, The MvIB method is a probabilistic distributional clustering method. It aims to extract a meaningful representation by compressing the video data space X into a “bottleneck” variable T, while preserving the relevant information maximally with respect to multiple feature variables 
                           
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                           
                        . In order to specify the systems of clusters and which information terms should be maintained, the theory of probabilistic graphical models Bayesian network is utilized. We use one network 
                           
                              
                                 
                                    I
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          in
                                       
                                    
                                 
                              
                           
                         to denote the compression of videos X into clusters T, another network 
                           
                              
                                 
                                    I
                                 
                                 
                                    
                                       
                                          G
                                       
                                       
                                          out
                                       
                                    
                                 
                              
                           
                         to denote the preserved information of T with respect to multiple visual feature variables 
                           
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                           
                        . Then, there is a tradeoff between these two Bayesian networks, which can be mathematically expressed as follows (see Fig. 3):
                           
                              (10)
                              
                                 
                                    
                                       L
                                    
                                    
                                       max
                                    
                                 
                                 [
                                 p
                                 (
                                 t
                                 |
                                 x
                                 )
                                 ]
                                 =
                                 
                                    
                                       I
                                    
                                    
                                       
                                          
                                             G
                                          
                                          
                                             out
                                          
                                       
                                    
                                 
                                 -
                                 
                                    
                                       β
                                    
                                    
                                       -
                                       1
                                    
                                 
                                 
                                    
                                       I
                                    
                                    
                                       
                                          
                                             G
                                          
                                          
                                             in
                                          
                                       
                                    
                                 
                                 =
                                 [
                                 
                                    
                                       ω
                                    
                                    
                                       1
                                    
                                 
                                 I
                                 (
                                 T
                                 ;
                                 
                                    
                                       Y
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 +
                                 ⋯
                                 +
                                 
                                    
                                       ω
                                    
                                    
                                       k
                                    
                                 
                                 I
                                 (
                                 T
                                 ;
                                 
                                    
                                       Y
                                    
                                    
                                       k
                                    
                                 
                                 )
                                 ]
                                 -
                                 
                                    
                                       β
                                    
                                    
                                       -
                                       1
                                    
                                 
                                 I
                                 (
                                 T
                                 ;
                                 X
                                 )
                                 ,
                              
                           
                        where 
                           
                              I
                              (
                              T
                              ;
                              X
                              )
                           
                         measures the compactness of video source variable X into the new representation T, and 
                           
                              
                                 
                                    ω
                                 
                                 
                                    1
                                 
                              
                              ·
                              I
                              (
                              T
                              ;
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              )
                              +
                              ⋯
                              +
                              
                                 
                                    ω
                                 
                                 
                                    k
                                 
                              
                              ·
                              I
                              (
                              T
                              ;
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                              )
                           
                         measures the preserved relevant information. β is the balance parameter controlling the trade-off between information compression and preservation. 
                           
                              
                                 
                                    ω
                                 
                                 
                                    i
                                 
                              
                           
                         (
                           
                              1
                              ⩽
                              i
                              ⩽
                              k
                           
                        ) are trade-off parameters to balance the influence among different feature variables.

The formal solution to 
                           
                              
                                 
                                    L
                                 
                                 
                                    max
                                 
                              
                           
                         in Eq. (10) can be characterized by three distributions: 
                           
                              p
                              (
                              t
                              |
                              x
                              )
                           
                         is the membership probability of video x belonging to cluster 
                           
                              t
                              ;
                              p
                              (
                              y
                              |
                              t
                              )
                           
                         is the distribution of cluster t over the feature variable 
                           
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                              ;
                              p
                              (
                              t
                              )
                           
                         is the probability of the video cluster t. The formulation can be characterized as follows:
                           
                              (11)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   p
                                                   (
                                                   t
                                                   )
                                                   =
                                                   
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            x
                                                            ,
                                                            y
                                                         
                                                      
                                                   
                                                   p
                                                   (
                                                   x
                                                   ,
                                                   y
                                                   ,
                                                   t
                                                   )
                                                   =
                                                   
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            x
                                                         
                                                      
                                                   
                                                   p
                                                   (
                                                   x
                                                   )
                                                   p
                                                   (
                                                   t
                                                   |
                                                   x
                                                   )
                                                
                                             
                                             
                                                
                                                   p
                                                   (
                                                   t
                                                   |
                                                   x
                                                   )
                                                   =
                                                   
                                                      
                                                         p
                                                         (
                                                         t
                                                         )
                                                      
                                                      
                                                         Z
                                                         (
                                                         x
                                                         ,
                                                         β
                                                         )
                                                      
                                                   
                                                   
                                                      
                                                         e
                                                      
                                                      
                                                         -
                                                         β
                                                         
                                                            
                                                               D
                                                            
                                                            
                                                               KL
                                                            
                                                         
                                                         (
                                                         p
                                                         (
                                                         y
                                                         |
                                                         x
                                                         )
                                                         |
                                                         |
                                                         p
                                                         (
                                                         y
                                                         |
                                                         t
                                                         )
                                                         )
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   p
                                                   (
                                                   y
                                                   |
                                                   t
                                                   )
                                                   =
                                                   
                                                      
                                                         1
                                                      
                                                      
                                                         p
                                                         (
                                                         t
                                                         )
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            x
                                                         
                                                      
                                                   
                                                   p
                                                   (
                                                   x
                                                   ,
                                                   y
                                                   ,
                                                   t
                                                   )
                                                   =
                                                   
                                                      
                                                         1
                                                      
                                                      
                                                         p
                                                         (
                                                         t
                                                         )
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            x
                                                         
                                                      
                                                   
                                                   p
                                                   (
                                                   x
                                                   ,
                                                   y
                                                   )
                                                   p
                                                   (
                                                   t
                                                   |
                                                   x
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              Z
                              (
                              x
                              ,
                              β
                              )
                           
                         is a normalization function, 
                           
                              
                                 
                                    D
                                 
                                 
                                    KL
                                 
                              
                              (
                              ·
                              |
                              |
                              ·
                              )
                           
                         is the Kullback–Leibler divergence.

By embedding multiple features into the multivariate IB framework, the MvIB extracts the underlying patterns hidden in the data X from multiple cues simultaneously. In the real applications, the number of clusters M is much less than the size of source video data set 
                           
                              |
                              X
                              |
                           
                        , which implies a significant compression between the source variable X and the “bottleneck” variable T. Besides, the relevant feature variables 
                           
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                           
                         are key issues in clustering task, which reveals the hidden pattern information in the videos. Therefore, we concentrate on preserving the relevant feature information maximally. To achieve this goal, the value of β is fitted to 
                           
                              ∞
                           
                        , so 
                           
                              I
                              (
                              T
                              ;
                              X
                              )
                           
                         is eliminated. Now, the task of MvIB method is to maximize the equation 
                           
                              
                                 
                                    ω
                                 
                                 
                                    1
                                 
                              
                              ·
                              I
                              (
                              T
                              ;
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              )
                              +
                              ⋯
                              +
                              
                                 
                                    ω
                                 
                                 
                                    k
                                 
                              
                              ·
                              I
                              (
                              T
                              ;
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                              )
                           
                        . 
                           
                              (12)
                              
                                 
                                    
                                       L
                                    
                                    
                                       max
                                    
                                 
                                 [
                                 p
                                 (
                                 t
                                 |
                                 x
                                 )
                                 ]
                                 =
                                 
                                    
                                       I
                                    
                                    
                                       
                                          
                                             G
                                          
                                          
                                             out
                                          
                                       
                                    
                                 
                                 -
                                 
                                    
                                       β
                                    
                                    
                                       -
                                       1
                                    
                                 
                                 
                                    
                                       I
                                    
                                    
                                       
                                          
                                             G
                                          
                                          
                                             in
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       ω
                                    
                                    
                                       1
                                    
                                 
                                 I
                                 (
                                 T
                                 ;
                                 
                                    
                                       Y
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 +
                                 ⋯
                                 +
                                 
                                    
                                       ω
                                    
                                    
                                       k
                                    
                                 
                                 I
                                 (
                                 T
                                 ;
                                 
                                    
                                       Y
                                    
                                    
                                       k
                                    
                                 
                                 )
                              
                           
                        where 
                           
                              
                                 
                                    ω
                                 
                                 
                                    1
                                 
                              
                              ·
                              I
                              (
                              T
                              ;
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              )
                              +
                              ⋯
                              +
                              
                                 
                                    ω
                                 
                                 
                                    k
                                 
                              
                              ·
                              I
                              (
                              T
                              ;
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                              )
                           
                         measures the preserved relevant information. 
                           
                              
                                 
                                    ω
                                 
                                 
                                    i
                                 
                              
                           
                         (
                           
                              1
                              ⩽
                              i
                              ⩽
                              k
                           
                        ) are trade-off parameters to balance the influence among different feature variables.

However, Eq. (12) is non-convex and there is no method to solve this objective function currently. In [30], Slonim et al. described a simple framework for casting this problem into a “sequential k-means like” algorithm, we call this algorithm as “draw-and-merge” strategy. In particular, the resulting strategy is guaranteed to converge to a stable solution in time and space, and its performance is significantly better than those of agglomerative procedure. This study focuses on the hard clustering, where the value of 
                           
                              p
                              (
                              t
                              |
                              x
                              )
                           
                         is either 0 or 1. Thus, the task of MvIB turns into finding an optimal partition of video collection X, which maximally preserves the information in objective function (12). Next, the details of applying draw-and-merge strategy are given.

To find an optimal partition of source videos X, a sequential “draw-and-merge” optimization method is adopted. The sequential procedure maintains a flat partition in T with exactly M clusters. The initialization of T is based upon a random partition of X into M clusters. Then in the iterative phase, we “draw” potential 
                           
                              x
                              ∈
                              X
                           
                         from its current cluster 
                           
                              
                                 
                                    t
                                 
                                 
                                    old
                                 
                              
                           
                         and represent it as a new singleton cluster x. Next, the new x should be merged into cluster 
                           
                              
                                 
                                    t
                                 
                                 
                                    new
                                 
                              
                           
                        . To maximize the objective function (12), each “draw-and-merge” procedure should increase the value of objective function (12).

Now, the key issue of the MvIB method is to find a measure to decide which video x should be merged at each step. Let 
                           
                              
                                 
                                    L
                                 
                                 
                                    bef
                                 
                              
                           
                         and 
                           
                              
                                 
                                    L
                                 
                                 
                                    aft
                                 
                              
                           
                         denote the value of objective function (12) before and after the draw step of x. Let 
                           
                              
                                 
                                    L
                                 
                                 
                                    new
                                 
                              
                           
                         denote the value of (12) after x is merged into some cluster t. The measure of deciding “merge” procedure is named as “merge cost” 
                           
                              
                                 
                                    d
                                 
                                 
                                    L
                                 
                              
                           
                        . 
                           
                              
                                 
                                    
                                       d
                                    
                                    
                                       L
                                    
                                 
                                 =
                                 Δ
                                 L
                                 =
                                 
                                    
                                       L
                                    
                                    
                                       aft
                                    
                                 
                                 -
                                 
                                    
                                       L
                                    
                                    
                                       new
                                    
                                 
                                 =
                                 [
                                 
                                    
                                       ω
                                    
                                    
                                       1
                                    
                                 
                                 ·
                                 I
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       aft
                                    
                                 
                                 ;
                                 
                                    
                                       Y
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 +
                                 ⋯
                                 +
                                 
                                    
                                       ω
                                    
                                    
                                       k
                                    
                                 
                                 ·
                                 I
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       aft
                                    
                                 
                                 ;
                                 
                                    
                                       Y
                                    
                                    
                                       k
                                    
                                 
                                 )
                                 ]
                                 -
                                 [
                                 
                                    
                                       ω
                                    
                                    
                                       1
                                    
                                 
                                 ·
                                 I
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       new
                                    
                                 
                                 ;
                                 
                                    
                                       Y
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 +
                                 ⋯
                                 +
                                 
                                    
                                       ω
                                    
                                    
                                       k
                                    
                                 
                                 ·
                                 I
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       new
                                    
                                 
                                 ;
                                 
                                    
                                       Y
                                    
                                    
                                       k
                                    
                                 
                                 )
                                 ]
                                 =
                                 
                                    
                                       ω
                                    
                                    
                                       1
                                    
                                 
                                 ·
                                 [
                                 I
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       aft
                                    
                                 
                                 ;
                                 
                                    
                                       Y
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 -
                                 I
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       aft
                                    
                                 
                                 ;
                                 
                                    
                                       Y
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 ]
                                 +
                                 ⋯
                                 +
                                 
                                    
                                       ω
                                    
                                    
                                       k
                                    
                                 
                                 ·
                                 [
                                 I
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       aft
                                    
                                 
                                 ;
                                 
                                    
                                       Y
                                    
                                    
                                       k
                                    
                                 
                                 )
                                 -
                                 I
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       new
                                    
                                 
                                 ;
                                 
                                    
                                       Y
                                    
                                    
                                       k
                                    
                                 
                                 )
                                 ]
                                 =
                                 
                                    
                                       ω
                                    
                                    
                                       1
                                    
                                 
                                 ·
                                 Δ
                                 
                                    
                                       I
                                    
                                    
                                       1
                                    
                                 
                                 +
                                 ⋯
                                 +
                                 
                                    
                                       ω
                                    
                                    
                                       k
                                    
                                 
                                 ·
                                 Δ
                                 
                                    
                                       I
                                    
                                    
                                       k
                                    
                                 
                                 ,
                              
                           
                        where
                           
                              
                                 Δ
                                 
                                    
                                       I
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 p
                                 (
                                 x
                                 )
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          
                                             
                                                Y
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 p
                                 (
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 x
                                 )
                                 log
                                 
                                    
                                       p
                                       (
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       |
                                       x
                                       )
                                    
                                    
                                       p
                                       (
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       |
                                       
                                          
                                             t
                                          
                                          
                                             ̃
                                          
                                       
                                       )
                                    
                                 
                                 +
                                 p
                                 (
                                 t
                                 )
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                y
                                             
                                             
                                                i
                                             
                                          
                                          ∈
                                          
                                             
                                                Y
                                             
                                             
                                                i
                                             
                                          
                                       
                                    
                                 
                                 p
                                 (
                                 
                                    
                                       y
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 t
                                 )
                                 log
                                 
                                    
                                       p
                                       (
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       |
                                       t
                                       )
                                    
                                    
                                       p
                                       (
                                       
                                          
                                             y
                                          
                                          
                                             i
                                          
                                       
                                       |
                                       
                                          
                                             t
                                          
                                          
                                             ̃
                                          
                                       
                                       )
                                    
                                 
                                 =
                                 p
                                 (
                                 x
                                 )
                                 
                                    
                                       D
                                    
                                    
                                       KL
                                    
                                 
                                 [
                                 p
                                 (
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 x
                                 )
                                 |
                                 |
                                 p
                                 (
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 
                                    
                                       t
                                    
                                    
                                       ̃
                                    
                                 
                                 )
                                 ]
                                 +
                                 p
                                 (
                                 t
                                 )
                                 
                                    
                                       D
                                    
                                    
                                       KL
                                    
                                 
                                 [
                                 p
                                 (
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 t
                                 )
                                 |
                                 |
                                 p
                                 (
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 
                                    
                                       t
                                    
                                    
                                       ̃
                                    
                                 
                                 )
                                 ]
                                 =
                                 [
                                 p
                                 (
                                 x
                                 )
                                 +
                                 p
                                 (
                                 t
                                 )
                                 ]
                                 ·
                                 
                                    
                                       JS
                                    
                                    
                                       Π
                                    
                                 
                                 [
                                 p
                                 (
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 x
                                 )
                                 ,
                                 p
                                 (
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 t
                                 )
                                 ]
                                 ,
                              
                           
                        
                        
                           
                              
                                 
                                    JS
                                 
                                 
                                    Π
                                 
                              
                           
                         is the Jensen–Shannon divergence, 
                           
                              Π
                              =
                              {
                              
                                 
                                    π
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    π
                                 
                                 
                                    2
                                 
                              
                              }
                              =
                              
                                 
                                    
                                       
                                          
                                             p
                                             (
                                             x
                                             )
                                          
                                          
                                             p
                                             (
                                             x
                                             )
                                             +
                                             p
                                             (
                                             t
                                             )
                                          
                                       
                                       ,
                                       
                                          
                                             p
                                             (
                                             t
                                             )
                                          
                                          
                                             p
                                             (
                                             x
                                             )
                                             +
                                             p
                                             (
                                             t
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        . Since 
                           
                              
                                 
                                    JS
                                 
                                 
                                    i
                                 
                              
                              ⩾
                              0
                              ,
                              
                                 
                                    d
                                 
                                 
                                    L
                                 
                              
                              ⩾
                              0
                           
                        , when some x is merged into one of the clusters, there must be some information loss. To preserve information maximally, the cluster which makes the minimal loss of information is chosen. That is x will be merged into the cluster 
                           
                              
                                 
                                    t
                                 
                                 
                                    new
                                 
                              
                           
                         such that 
                           
                              
                                 
                                    t
                                 
                                 
                                    new
                                 
                              
                              =
                              arg
                              
                                 
                                    min
                                 
                                 
                                    t
                                    ∈
                                    T
                                 
                              
                              
                                 
                                    d
                                 
                                 
                                    L
                                 
                              
                           
                        . Continue the draw-and-merge procedure until the information remains unchanged.

In each “draw-and-merge” iteration, x is merged into the cluster 
                           
                              
                                 
                                    t
                                 
                                 
                                    new
                                 
                              
                           
                         such that 
                           
                              
                                 
                                    t
                                 
                                 
                                    new
                                 
                              
                              =
                              arg
                              
                                 
                                    min
                                 
                                 
                                    t
                                    ∈
                                    T
                                 
                              
                              
                                 
                                    d
                                 
                                 
                                    L
                                 
                              
                              (
                              {
                              x
                              }
                              ,
                              t
                              )
                           
                        . Since 
                           
                              
                                 
                                    L
                                 
                                 
                                    bef
                                 
                              
                              =
                              
                                 
                                    L
                                 
                                 
                                    aft
                                 
                              
                              -
                              
                                 
                                    d
                                 
                                 
                                    L
                                 
                              
                              (
                              {
                              x
                              }
                              ,
                              
                                 
                                    t
                                 
                                 
                                    bef
                                 
                              
                              )
                           
                        , 
                           
                              
                                 
                                    L
                                 
                                 
                                    new
                                 
                              
                              =
                              
                                 
                                    L
                                 
                                 
                                    aft
                                 
                              
                              -
                              
                                 
                                    d
                                 
                                 
                                    L
                                 
                              
                              (
                              {
                              x
                              }
                              ,
                              
                                 
                                    t
                                 
                                 
                                    new
                                 
                              
                              )
                           
                         and 
                           
                              
                                 
                                    d
                                 
                                 
                                    L
                                 
                              
                              (
                              {
                              x
                              }
                              ,
                              
                                 
                                    t
                                 
                                 
                                    bef
                                 
                              
                              )
                              ⩾
                              
                                 
                                    d
                                 
                                 
                                    L
                                 
                              
                              (
                              {
                              x
                              }
                              ,
                              
                                 
                                    t
                                 
                                 
                                    new
                                 
                              
                              )
                           
                        , then 
                           
                              
                                 
                                    L
                                 
                                 
                                    new
                                 
                              
                              ⩾
                              
                                 
                                    L
                                 
                                 
                                    bef
                                 
                              
                           
                        . 
                           Algorithm 1
                           The Multivariate video Information Bottleneck Algorithm: MvIB 
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                1:
                                             
                                                Input: (
                                                   
                                                      p
                                                      (
                                                      X
                                                      ,
                                                      
                                                         
                                                            Y
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                      )
                                                      ,
                                                      ⋯
                                                      ,
                                                      p
                                                      (
                                                      X
                                                      ,
                                                      
                                                         
                                                            Y
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      )
                                                      ,
                                                      
                                                         
                                                            ω
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                      ,
                                                      ⋯
                                                      ,
                                                      
                                                         
                                                            ω
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      ,
                                                      M
                                                   
                                                ).
                                          
                                          
                                             
                                                2:
                                             
                                                Initialize: 
                                                
                                                   
                                                      T
                                                      ←
                                                   
                                                 Random partition of X into M clusters;
                                          
                                          
                                             
                                                3:
                                             
                                                repeat
                                             
                                          
                                          
                                             
                                                4:
                                             
                                                
                                                for For every 
                                                   
                                                      x
                                                      ∈
                                                      X
                                                   
                                                 
                                                do
                                             
                                          
                                          
                                             
                                                5:
                                             
                                                
                                                Draw: Remove x from current cluster 
                                                   
                                                      t
                                                      (
                                                      x
                                                      )
                                                   
                                                ;
                                          
                                          
                                             
                                                6:
                                             
                                                
                                                Computing merge cost:
                                             
                                          
                                          
                                             
                                             
                                                
                                                For data point x, calculate merge costs 
                                                   
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            L
                                                         
                                                      
                                                   
                                                 of all possible reassignments of x to different clusters based on Eq. (12);
                                          
                                          
                                             
                                                7:
                                             
                                                
                                                Merge: Merge x into cluster 
                                                   
                                                      
                                                         
                                                            t
                                                         
                                                         
                                                            new
                                                         
                                                      
                                                   
                                                 such that 
                                                   
                                                      
                                                         
                                                            t
                                                         
                                                         
                                                            new
                                                         
                                                      
                                                      =
                                                      arg
                                                      
                                                         
                                                            min
                                                         
                                                         
                                                            t
                                                            ∈
                                                            T
                                                         
                                                      
                                                      
                                                         
                                                            d
                                                         
                                                         
                                                            L
                                                         
                                                      
                                                   
                                                ;
                                          
                                          
                                             
                                                8:
                                             
                                                
                                                end for
                                             
                                          
                                          
                                             
                                                9:
                                             
                                                until Convergence
                                          
                                          
                                             10:
                                             
                                                Output: A partition T of X into M clusters.
                                          
                                       
                                    
                                 
                              
                           

The value of the objective function (12) is non-monotonic decreasing in Algorithm 1. Note that, although MvIB algorithm is able to increase the value of objective function (12), it is only able to converge to a local maximum of the information in Eq. (12). Finding the global optimal solution is NP-hard. The convergence of MvIB algorithm is emphasized in the following Theorem 1.
                           Theorem 1
                           
                              MvIB algorithm converges in a finite number of iterations.
                           

In the compressing process of MvIB algorithm, there is some information lost and 
                                 
                                    I
                                    (
                                    T
                                    ;
                                    
                                       
                                          Y
                                       
                                       
                                          i
                                       
                                    
                                    )
                                    ⩽
                                    I
                                    (
                                    X
                                    ;
                                    
                                       
                                          Y
                                       
                                       
                                          i
                                       
                                    
                                    )
                                 
                              . Consequently, we have 
                                 
                                    
                                       
                                          ω
                                       
                                       
                                          1
                                       
                                    
                                    ·
                                    I
                                    (
                                    T
                                    ;
                                    
                                       
                                          Y
                                       
                                       
                                          1
                                       
                                    
                                    )
                                    +
                                    ⋯
                                    +
                                    
                                       
                                          ω
                                       
                                       
                                          l
                                       
                                    
                                    ·
                                    I
                                    (
                                    T
                                    ;
                                    
                                       
                                          Y
                                       
                                       
                                          l
                                       
                                    
                                    )
                                    ⩽
                                    
                                       
                                          ω
                                       
                                       
                                          1
                                       
                                    
                                    ·
                                    I
                                    (
                                    X
                                    ;
                                    
                                       
                                          Y
                                       
                                       
                                          1
                                       
                                    
                                    )
                                    +
                                    ⋯
                                    +
                                    
                                       
                                          ω
                                       
                                       
                                          l
                                       
                                    
                                    ·
                                    I
                                    (
                                    X
                                    ;
                                    
                                       
                                          Y
                                       
                                       
                                          l
                                       
                                    
                                    )
                                 
                              . So, the value of objection function (12) is upper bounded. Each “draw-and-merge” step improves the value of objection function (12), so Algorithm 1 converges in a finite number of iterations. □

As the videos are modeled by the BoW model, the visual words carry the semantic clues about the underlying concepts. So the obtained latent semantic relations provide a more reliable clue for the video categories discovery.

In general, determining the trade-off parameters acting on multiple features is a non-trivial model selection task and is beyond the scope of this paper. In experimental section, we evaluate the impact of the trade-off parameters on five video collections.

The computational costs of the MvIB algorithm are given in this section. At step 6, the merge cost 
                           
                              
                                 
                                    d
                                 
                                 
                                    L
                                 
                              
                           
                         for every t should be calculated, which takes 
                           
                              O
                              (
                              M
                              (
                              |
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              |
                              +
                              ⋯
                              +
                              |
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                              |
                              )
                              )
                           
                        . So, the time complexity of our algorithm is 
                           
                              O
                              (
                              LM
                              |
                              X
                              |
                              (
                              |
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              |
                              +
                              ⋯
                              +
                              |
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                              |
                              )
                              )
                           
                        , where L is the number of repetitions that should be performed over X until convergence is attained. From the results of the next experiment section, the MvIB algorithm takes a few repetitions to converge a local optimal value of objective function (12). Usually, the number of clusters M can be considered as constant. So the time complexity of MvIB is 
                           
                              O
                              (
                              |
                              X
                              |
                              (
                              |
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              |
                              +
                              ⋯
                              +
                              |
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                              |
                              )
                           
                        . Considering space complexity, the MvIB algorithm needs to store all the joint distributions 
                           
                              p
                              (
                              X
                              ,
                              
                                 
                                    Y
                                 
                                 
                                    i
                                 
                              
                              )
                           
                        . Thus, the space complexity is 
                           
                              O
                              (
                              |
                              X
                              |
                              |
                              
                                 
                                    Y
                                 
                                 
                                    1
                                 
                              
                              |
                              +
                              ⋯
                              +
                              |
                              X
                              |
                              |
                              
                                 
                                    Y
                                 
                                 
                                    k
                                 
                              
                              |
                              )
                           
                        . This indicates that the time and space complexity of MvIB algorithm are liner to the input.

@&#EXPERIMENTS@&#

In this section, five benchmark video data sets are adopted to evaluate the performance of the MvIB method. To alleviate the influence caused by random initialization, all algorithms are executed 10 times. For the representation of experiment results, we report the average evaluation with the standard deviation. The number of clusters is taken to be identical with the number of real categories on each video data sets.

In this section, five benchmark video data sets are adopted to evaluate the MvIB method as described in Table 1
                           . The KTH and Weizmann data sets are the subject of lots of research, each video clip is sampled at 25Hz and lasts between 10 and 15s with image frame size of 
                              
                                 160
                                 ×
                                 120
                              
                            and 
                              
                                 180
                                 ×
                                 144
                              
                            respectively. For the other three data sets, we resized each video clip to a resolution of 
                              
                                 320
                                 ×
                                 240
                              
                           . It should be noted that the category size of the data sets used in this paper varies from 4 to 50, the size of video collections varies from 90 to 6000, so the task of categorizing them automatically is challenging.

To incorporate the multiple aspects of feature information, we adopted the following three local patch extraction and description techniques: SIFT, ST and STIP. Then the low-level static and motion interest feature extraction methods from the videos are discussed in detail.


                           SIFT 
                           [4]: the SIFT feature is employed as static information in this study. The SIFT features are based on the appearance of the object at particular interest points, and invariant to image scale and rotation. Moreover, it is also robust to changes in illumination, noise, and viewpoint. In addition to these properties, they are highly distinctive, relatively easy to extract and allow for correct object identification with low probability of mismatch. For every sample frame, we extract local SIFT features, then each feature point is described by its location 
                              
                                 (
                                 x
                                 ,
                                 y
                                 )
                              
                           , scale σ, and a 128-dimensional SIFT descriptor.


                           ST 
                           [31]: This detector produces dense feature points and performs reasonably well on video categorization task. Assuming a stationary camera or a process that can account for camera motion, separable linear filters are applied to the videos to obtain the response function as follows:
                              
                                 (13)
                                 
                                    R
                                    =
                                    
                                       
                                          (
                                          I
                                          ∗
                                          g
                                          ∗
                                          
                                             
                                                h
                                             
                                             
                                                ev
                                             
                                          
                                          )
                                       
                                       
                                          2
                                       
                                    
                                    +
                                    
                                       
                                          (
                                          I
                                          ∗
                                          g
                                          ∗
                                          
                                             
                                                h
                                             
                                             
                                                od
                                             
                                          
                                          )
                                       
                                       
                                          2
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 g
                                 (
                                 x
                                 ;
                                 y
                                 ;
                                 
                                    
                                       h
                                    
                                    
                                       ev
                                    
                                 
                                 )
                              
                            is the 2D Gaussian smoothing kernel, applied only along the spatial dimensions. 
                              
                                 
                                    
                                       h
                                    
                                    
                                       ev
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       h
                                    
                                    
                                       od
                                    
                                 
                              
                            are a quadrature pair of 1D Gabor filters applied temporally. We extract ST feature as suggested by the authors.


                           SIIP 
                           [32]: The HOG/HOF descriptor is adopted to compute the STIP features. To characterize local motion and appearance, the authors computed histograms of spatial gradient and optical flow accumulated in space–time neighborhoods of detected interest points. For the combination of HOG/HOF descriptors with interest point detectors, the descriptor size is defined by 
                              
                                 
                                    
                                       Δ
                                    
                                    
                                       x
                                    
                                 
                                 (
                                 σ
                                 )
                                 =
                                 
                                    
                                       Δ
                                    
                                    
                                       y
                                    
                                 
                                 (
                                 σ
                                 )
                                 ,
                                 
                                    
                                       Δ
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 τ
                                 )
                                 =
                                 8
                                 τ
                              
                           . Each volume is subdivided into a 
                              
                                 
                                    
                                       n
                                    
                                    
                                       x
                                    
                                 
                                 ×
                                 
                                    
                                       n
                                    
                                    
                                       y
                                    
                                 
                                 ×
                                 
                                    
                                       n
                                    
                                    
                                       t
                                    
                                 
                              
                            grid of cells. Normalized histograms are concatenated into HOG, HOF as well as HOG/HOF descriptor vectors. In our evaluation, we use the grid parameters 
                              
                                 
                                    
                                       n
                                    
                                    
                                       x
                                    
                                 
                                 =
                                 
                                    
                                       n
                                    
                                    
                                       y
                                    
                                 
                                 =
                                 3
                                 ,
                                 
                                    
                                       n
                                    
                                    
                                       t
                                    
                                 
                                 =
                                 2
                              
                           , as suggested by the authors.

In this section, the details of BoW model are given to characterize the unlabeled videos. Note that the videos in the same category take on similar visual appearances while the videos in the different categories may have ambiguous visual pattern. Our goal is to find the common visual pattern for each action or object category, even though no prior knowledge is provided. Intuitively, the global video features that describe the holistic video content are not appropriate for this given task due to their sensitivity to the scale or orientation of the object appearances. Instead, we look for low-level features that are invariant to the types of degradation. For video representation, the three types of local features (SIFT, ST and STIP) are utilized, then the bag of words (BoW) model is adopted to represent videos. The BoW model has been widely employed in action, object, scene categorization and recognition due to its simplicity and surprisingly good performance. Next, the construction steps of BoW model are presented briefly.
                              
                                 •
                                 Extracting local patches/cuboids from each video and representing them by SIFT, ST, STIP descriptors.

Building a visual vocabulary by vector quantization via k-means algorithm, in which each cluster centroid is treated as a visual video word. Note that each type of feature has a independent visual vocabulary.

Mapping the descriptors into the vocabulary so that the descriptors can be described by the video words index.

Counting the occurrence number of the visual words in each video and using a histogram to represent it.

At the end of the BoW model, each video is transformed to a feature vector, which contains the occurrence number of the individual visual video word. It is worth noting that such representation is similar to document representation in the text analysis domain, where videos can be treated as documents and the visual words are regarded as the key words.

To evaluate the effectiveness of MvIB on the task of video categorization, the following four unsupervised learning techniques are employed to compare with the MvIB algorithm: k-means, PLSA [21], NCuts [33] and the original IB [25] method. K-means and NCuts are two classical unsupervised learning techniques, while PLSA and IB methods are two state-of-the-art unsupervised categorization methods. The works presented in [21,27] have demonstrated the effectiveness of PLSA and IB algorithm on the task of unsupervised categorization respectively.

Besides, to evaluate the effectiveness of dealing with multiple features, several state-of-the-art multi-feature clustering methods are compared with the MvIB algorithm. Recently, several methods based on spectral clustering have been applied to the domain of unsupervised image and video categorization by considering multiple features. In this section, we compare the MvIB method with Correlational spectral clustering (CSC) [22], Heterogeneous image feature integration via multi-modal spectral clustering (MMSC) [23] and Affinity aggregation for spectral clustering (AASC) [24].

In this section, the clustering accuracy (AC) [34] and normalized mutual information (NMI) are utilized to measure the results of unsupervised video categorization method.

AC is the standard measure widely used for unsupervised learning. It evaluates the clustering performance by comparing the obtained clustering assignment of each data point with the ground truth label of that point. The AC is defined as:
                              
                                 (14)
                                 
                                    AC
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                i
                                                =
                                                1
                                             
                                             
                                                n
                                             
                                          
                                          δ
                                          (
                                          
                                             
                                                l
                                             
                                             
                                                i
                                             
                                          
                                          ,map
                                          (
                                          
                                             
                                                t
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          )
                                       
                                       
                                          n
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                              
                            denotes the cluster assignment of 
                              
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       l
                                    
                                    
                                       i
                                    
                                 
                              
                            is the ground truth label of 
                              
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                              
                           , and n is the size of the data. The delta function 
                              
                                 δ
                                 (
                                 x
                                 ,
                                 y
                                 )
                              
                            equals 1 if 
                              
                                 x
                                 =
                                 y
                              
                            and equals 0 otherwise. The permutation function 
                              
                                 map
                                 (
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                            maps each cluster assignment 
                              
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                              
                            to the equivalent label provided by the data corpus.

Following [35], normalized mutual information (NMI) is adopted to evaluate the performance of the aforementioned methods, the corresponding sample estimate is:
                              
                                 (15)
                                 
                                    NMI
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                h
                                                ,
                                                l
                                             
                                          
                                          
                                             
                                                n
                                             
                                             
                                                h
                                                ,
                                                l
                                             
                                          
                                          log
                                          
                                             
                                                
                                                   
                                                      
                                                         n
                                                         ·
                                                         
                                                            
                                                               n
                                                            
                                                            
                                                               h
                                                               ,
                                                               l
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               n
                                                            
                                                            
                                                               h
                                                            
                                                         
                                                         
                                                            
                                                               n
                                                            
                                                            
                                                               l
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               ∑
                                                            
                                                            
                                                               h
                                                            
                                                         
                                                         
                                                            
                                                               n
                                                            
                                                            
                                                               h
                                                            
                                                         
                                                         log
                                                         
                                                            
                                                               
                                                                  
                                                                     n
                                                                  
                                                                  
                                                                     h
                                                                  
                                                               
                                                            
                                                            
                                                               n
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            
                                                               ∑
                                                            
                                                            
                                                               l
                                                            
                                                         
                                                         
                                                            
                                                               n
                                                            
                                                            
                                                               l
                                                            
                                                         
                                                         log
                                                         
                                                            
                                                               
                                                                  
                                                                     n
                                                                  
                                                                  
                                                                     l
                                                                  
                                                               
                                                            
                                                            
                                                               n
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 
                                    
                                       n
                                    
                                    
                                       h
                                    
                                 
                              
                            is the number of data objects in class 
                              
                                 h
                                 ,
                                 
                                    
                                       n
                                    
                                    
                                       l
                                    
                                 
                              
                            is the number of objects in cluster l and 
                              
                                 
                                    
                                       n
                                    
                                    
                                       h
                                       ,
                                       l
                                    
                                 
                              
                            is the number of objects in class h as well as in cluster l. The NMI value is 1 when clustering results perfectly match the external category labels and close to 0 for a random partitioning.

The MvIB method is a multiple-feature extension of multivariate IB, which can process multiple feature variables simultaneously. In this section, the performance of MvIB algorithm is compared with original IB algorithm on three features.

First, the performance of original IB method is verified on three features respectively. As illustrated in Table 2
                        , the performances of original IB method on three cues are different. The static SIFT feature gets best AC value on Highway Traffic and UCF50 (62.1%, 34.1% respectively); The dynamic ST feature attains best AC value (69.6%) on KTH; While the other dynamic feature STIP performs best on WEIZMANN and UCF Sports (66.5%, 50.4% respectively). The fluctuation of the normalized mutual information (NMI) in Fig. 4
                         also verifies this observation. One explanation is that there is no feature can perform consistently well for different categorization tasks. So it is not wise just using single type of features for the video categorization task (see Fig. 5
                        ).

Next, the experiments on concatenating three features are conducted. As shown in the IB (connect) column from Table 2, the average performance of concatenating three types of features simply is less than satisfactory. Apart from Highway Traffic, the categorization accuracy on the concatenated features drops 17.5%, 5.7%, 3.1% and 1.7% for the other four data sets respectively. The same observation can be obtained from the NMI in Fig. 4. So, simply concatenating features cannot consistently attain improved results compared with just one type of feature. Note that, the connect feature of the original IB method is acquired by a late feature fusion method and the combined features are treated as the instances of one discrete variable in the original IB method. It is different from the MvIB method.

Finally, the benefits of the MvIB method are verified. By considering three types of features simultaneously, the MvIB method can clearly improve the performance compared with the original IB on all five video data sets. As shown in Table 3
                        , the MvIB algorithm obtains 2.4%, 3.0%, 0.5%, 4.2% and 2.7% improvement respectively on the five data sets. Of which the clustering accuracy on Highway Traffic data set is not improved significantly. The NMI value of MvIB algorithm is shown in Fig. 4 compared with the original IB method on the five challenging video data sets. As we can see from Fig. 4, the NMI of MvIB is higher than the others apparently, even though the performances of MvIB algorithm and IB algorithm on the combined features are comparatively the same on Highway Traffic data set. So we can get the conclusion that the MvIB algorithm can consistently improve the performance compared with the best results of original IB on three individual features.

Due to the space limitation, only the confusion matrices of KTH and UCF Sports are shown in Table 4
                         and Fig. 6
                         respectively. From Table 4, the learned categories T1, T2, T4, T5, T6 are rather pure, and each can be highly associated with one true object category. The confusion matrix in Fig. 6 demonstrates the encouraging result for UCF Sports data set. The categories Dive, Ride and Kick are all distributed to true categories correctly. The results of these two data sets also show that the MvIB method is able to reveal the meaningful patterns from the video collection without any supervision, so the MvIB algorithm is an effective action and object category discovery method.

As shown in Table 3 and Fig. 7
                        , the MvIB can get more promising results than the other three state-of-the-art unsupervised categorization methods. Apart from NCuts algorithm on UCF Sports data set, the average results of original IB method on the combined variables by three features are always better than the best evaluation results of the other three methods. Moreover, the results of MvIB are consistently better than original IB. The reason for this phenomenon is that the IB-based method can relieve the semantic gap problem by exploiting the semantic correlations between the videos and visual words, while the other methods ignore these correlations.

In this section, the experiments to verify the effectiveness of the MvIB method are conducted on the five video data sets in the task of unsupervised video categorization by considering multiple features.

Recently, several methods based on spectral clustering have been applied to the multi-feature domain of unsupervised image and video categorization. Correlational spectral clustering [22] separates similarity measures for each data representation, and allows for projection of previously unseen data that observed only in one representation. Heterogeneous image feature integration via multi-modal spectral clustering [23] learns a commonly shared graph Laplacian matrix by unifying different models by considering each type of feature as one modal. Affinity aggregation for spectral clustering [24] seeks for an optimal combination of affinity matrices so that it is more immune to ineffective affinities and irrelevant features.

As shown Fig. 7, the NMI value obtained by MvIB method is consistently higher than the other three unsupervised multi-feature methods based on spectral clustering. So we can get the conclusion that the MvIB method can effectively cope with multiple features. This is mainly because the proposed information-theoretical approach can effectively learn the latent semantic correlations between the videos and their low-level features, which alleviates the semantic gap in the existing unsupervised learning techniques.

In this section, the experiments to evaluate the impact of trade-off parameters are conducted on the five video data sets. Since there are three types of features: SIFT, ST and STIP, we conducted two groups of experiments: (1) The impact of two trade-off parameters. (2) The impact of three trade-off parameters.

In this study, three types of features: SIFT, ST and STIP are adopted to represent the five video collections. So we divided these three features in pairs for each video data set: (SIFT, ST), (SIFT, STIP) and (ST, STIP). The experimental setup of feature pairs is same to each other. Here, we take the feature pair (SIFT, ST) as an example.

For feature group SIFT and ST, there are two trade-off parameters 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       2
                                    
                                 
                              
                           , and 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                                 +
                                 
                                    
                                       λ
                                    
                                    
                                       2
                                    
                                 
                                 =
                                 1
                              
                           , where 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                              
                            acts on SIFT feature variable, while 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       2
                                    
                                 
                              
                            acts on ST feature variable. In this experiment, the values of 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                              
                            vary from 0 to 1, with 0.1 as the change gap between the two adjacent values. Note that, when 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                                 =
                                 0
                                 ,
                                 
                                    
                                       λ
                                    
                                    
                                       2
                                    
                                 
                                 =
                                 1
                              
                           , the acting proportion of the SIFT feature information is zero, so the results of MvIB algorithm only reflect the clustering accuracy gained from ST feature, which is same to the results of original IB algorithm on ST feature. Similarly, when 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                                 =
                                 1
                              
                           , the MvIB algorithm only acts on SIFT feature. The NMI of MvIB with varied 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                              
                            on the five video data sets shown in Fig. 8
                           . In this figure, the NMI is reported with the trade-off parameter values. Since standard deviation is minimal, we ignore it in Fig. 8. we observed that:
                              
                                 •
                                 When 
                                       
                                          
                                             
                                                λ
                                             
                                             
                                                1
                                             
                                          
                                          =
                                          0
                                       
                                     or 
                                       
                                          
                                             
                                                λ
                                             
                                             
                                                1
                                             
                                          
                                          =
                                          1
                                       
                                    , the MvIB only works on ST or SIFT feature. When 
                                       
                                          
                                             
                                                λ
                                             
                                             
                                                1
                                             
                                          
                                       
                                     varies from 0 to 1, 
                                       
                                          
                                             
                                                λ
                                             
                                             
                                                2
                                             
                                          
                                          =
                                          1
                                          -
                                          
                                             
                                                λ
                                             
                                             
                                                1
                                             
                                          
                                       
                                    , the value 
                                       
                                          
                                             
                                                λ
                                             
                                             
                                                2
                                             
                                          
                                       
                                     varies from 1 to 0 correspondingly. These two types of features have different powers to discriminate the video categories, and just using single feature limits the performance of the MvIB algorithm.

After integrating two aspects of feature information, the MvIB algorithm can clearly improve the clustering accuracy compared to the results on one type of features, and most of results are better than the results of original IB method obtained on the more powerful feature type. This phenomenon demonstrates that it is relatively easy to select the trade-off parameters for balancing the multiple feature types. That is to say the proposed method does not depend on the trade-off parameters.

In this section, the experiments to evaluate the impact of three trade-off parameters are conducted on the five video data sets.

There are three trade-off parameters, 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                                 ,
                                 
                                    
                                       λ
                                    
                                    
                                       2
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       3
                                    
                                 
                              
                            (s.t. 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                                 +
                                 
                                    
                                       λ
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       λ
                                    
                                    
                                       3
                                    
                                 
                                 =
                                 1
                              
                           ) which impact on SIFT, ST and STIP respectively. In this experiment, the values of 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                              
                            vary from 0 to 1, with 0.1 as the change gap between the two adjacent values. Then, the values of 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       2
                                    
                                 
                              
                            are set to be 
                              
                                 k
                                 ·
                                 (
                                 1
                                 -
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                                 )
                              
                           , where 
                              
                                 k
                                 =
                                 0
                                 ,
                                 0.1
                                 ,
                                 0.2
                                 ,
                                 …
                                 ,
                                 1
                              
                           . Finally, 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       3
                                    
                                 
                                 =
                                 1
                                 -
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                                 -
                                 
                                    
                                       λ
                                    
                                    
                                       2
                                    
                                 
                              
                           . The normalized mutual information (NMI) on the five challenging video data sets is shown in Fig. 9
                           .

For the scenario of integrating three types of features, the proposed automatical category discovery method is also effective to find a set of appropriate action categories. After integrating three types of feature information, the MvIB algorithm can clearly improve the NMI compared with the results on single type of features.

To further verify the effectiveness of the MvIB algorithm when dealing with more features, five types of popular visual features are applied to the MvIB method, including SIFT [4], ST [31], STIP [32], HOG [7], HOF [32]. Also, this experiment is conducted on the five video data sets mentioned above.

As shown in Table 5
                        , the following observations are obtained: (1) The performances of original IB method on the five feature cues are quite different, so it is wise to integrate different types of features for the video categorization task. (2) By integrating five features, the MvIB method can clearly improve the results compared with the original IB method. As shown in the Table 5, it obtains 3.8% improvement compared with the best average results of original IB on five individual features. (3) The performances of MvIB method on five different features are consistently better than on three features, it obtains 1.7% improvement compared with the average clustering accuracy on three features. So the MvIB algorithm is effective to integrate more features.

In this study, the bag of words model is adopted to construct visual representation for each video. So the performance of MvIB approach is influenced by the size of the vocabulary and the choice of the descriptor. Fig. 10
                         shows the influence of the vocabulary size and the results of different individual features obtained by the MvIB method.

A visual vocabulary is generated by clustering the detected key-points in their feature space, each cluster is treated as a unique visual word in the vocabulary. Similar to the text vocabulary in information retrieval, the size of visual vocabulary in computer vision is determined by the number of key-point clusters. However, a small vocabulary may lack the discriminative power, since two key-points may be assigned into the same cluster even if they are not similar to each other. A large vocabulary, on the other hand, is less generalizable, less forgiving to noises, and incurs extra processing overhead.

The trade-off between discrimination and generalization motivates the studies of visual vocabulary size. In this experiment, the size of vocabulary varies from 200 to 2200 on four types of features (SIFT, ST, STIP and Integration), with 400 as the change gap. The results of three single features are obtained by original IB method, while the results of the integration type of these three features are acquired by the proposed method. In line with earlier studies on bag of words model in image and video classification, a suitable vocabulary size is beneficial in our application. From Fig. 10, we observed that:
                           
                              •
                              With the gradual increasing of the vocabulary size from 200, the NMI of original IB method on three single type of feature has a slightly raise correspondingly. This observation is consistent with the MvIB method on integrating three features. However, with further increasing of the vocabulary size, the accuracy decreases to some extent. The reason of this phenomenon is that small vocabulary may lack the discriminative power, since two key-points may be assigned into the same cluster even if they are not similar to each other. A large vocabulary, on the other hand, is less generalizable, less forgiving to noises, and incurs extra processing overhead.

From Fig. 10, we can get the conclusion that the MvIB algorithm can consistently improve the performance compared with the results of original IB method on three individual features, no matter what value of vocabulary size we select.

To further verify the effectiveness of the proposed algorithm, the executing time of MvIB algorithm is compared with other single feature and multi-feature methods on the two typical video data sets (KTH and UCF Sports). In this experiment, the executing time of these algorithms is the average value by running 10 times. From the results demonstrated in Table 6
                        , we observed that: (1) The executing time of MvIB algorithm is almost equal to the original IB method on concatenated feature by three different cues. In line with expectations, the running time of the proposed method is longer than these methods dealing with single feature. The reason of this phenomenon is that the dimensionality of multiple features is significantly higher than the single one. (2) Since k-means, PLSA, NCuts and IB can only cope with single feature, the executing time of these methods are better than the MvIB algorithm. Note that, the AC results of MvIB algorithm are better than these methods significantly. (3) Compared with other multi-feature methods (MMSC, AASC and CSC), the executing time of MvIB algorithm is much better. In particularly, the running time of MvIB is almost equal to the original IB method on the concatenated feature as shown in IB (connect) column. So we can get the conclusion that the proposed algorithm is effective to analyse video data sets from multiple cues.

The iterative repetitions of MvIB algorithm are shown in Fig. 11
                         on the five video data sets. Note that, the values of objective function (12) increase monotonically with each repetition. And 14 iterations are enough for convergence on all our data sets.

@&#CONCLUSIONS@&#

In this paper, we proposed a novel formulation named Multivariate video Information Bottleneck (MvIB) method for unsupervised video categorization, which is an extensional type of original multivariate IB. Differing from the original multivariate information bottleneck, the MvIB method extracts the video categories from multiple feature variables simultaneously, while source variable X is compressed to compression variable T. Therefore, the compression results capture the complementary information resided in multiple feature variables. Besides, the information-theoretic optimization is adopted to learn the latent semantic correlations between the videos and their constructive visual words automatically, which can relieve the semantic gap between the visual features and their high-level semantic concepts. The experiments on five challenging benchmark video data sets confirm the effectiveness of the MvIB algorithm. In the future, we will try to incorporate more features into the MvIB approach and apply it to more difficult tasks such as action and scene recognition on large-scale data.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.knosys.2015.03.028.


                     
                        
                           
                        
                     
                  


                     
                        
                           
                        
                     
                  


                     
                        
                           
                        
                     
                  


                     
                        
                           
                        
                     
                  


                     
                        
                           
                        
                     
                  


                     
                        
                           
                        
                     
                  


                     
                        
                           
                        
                     
                  

@&#REFERENCES@&#

