@&#MAIN-TITLE@&#Predicting protein–RNA interaction amino acids using random forest based on submodularity subset selection

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We proposed a computational method for protein–RNA binding sites prediction by combining local features and global features from protein sequence based on submodularity subset selection.


                        
                        
                           
                           It achieved better performance than other state-of-the-art methods.


                        
                        
                           
                           It indicated that extracted global features have very strong discriminate ability for identifying interaction sites.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Protein–RNA interaction site

Sample imbalance

Evolution information

Submodularity subset selection

Random forest

@&#ABSTRACT@&#


               
               
                  Protein–RNA interaction plays a very crucial role in many biological processes, such as protein synthesis, transcription and post-transcription of gene expression and pathogenesis of disease. Especially RNAs always function through binding to proteins. Identification of binding interface region is especially useful for cellular pathways analysis and drug design. In this study, we proposed a novel approach for binding sites identification in proteins, which not only integrates local features and global features from protein sequence directly, but also constructed a balanced training dataset using sub-sampling based on submodularity subset selection. Firstly we extracted local features and global features from protein sequence, such as evolution information and molecule weight. Secondly, the number of non-interaction sites is much more than interaction sites, which leads to a sample imbalance problem, and hence biased machine learning model with preference to non-interaction sites. To better resolve this problem, instead of previous randomly sub-sampling over-represented non-interaction sites, a novel sampling approach based on submodularity subset selection was employed, which can select more representative data subset. Finally random forest were trained on optimally selected training subsets to predict interaction sites. Our result showed that our proposed method is very promising for predicting protein–RNA interaction residues, it achieved an accuracy of 0.863, which is better than other state-of-the-art methods. Furthermore, it also indicated the extracted global features have very strong discriminate ability for identifying interaction residues from random forest feature importance analysis.
               
            

@&#INTRODUCTION@&#

Protein–RNA interaction plays a key role in many biological cellular processes, such as regulation and post-transcription of gene expression (Bartel, 2009; Tuschl, 2003). And most RNAs function through interacting with proteins (Lunde et al., 2007), which bind together to form RNA-binding proteins (RBPs) complex. For example, Argonaute (AGO) family of RBPs, crucial components of the RNA-induced silencing complex (RISC), which incorporates microRNAs (miRNAs) to bind to their target genes, thereby negatively regulate gene expression. So identifying the key interaction residues involved in RNA binding can broaden our understanding of mechanism of those processes, and therefore guide for the mutant design and drug design.

With the recent rapid advances in high-throughput technologies for identifying protein–RNA interaction sites, such as cross-linking and immunoprecipitation protocols (CLIP) (Ule et al., 2005; Kishore et al., 2011) and electrophoretic mobility shift assay (EMSA) (Hellman and Fried, 2007), RNA–protein binding residues data are increasingly collected from experiments, such as PRIDB (Lewis et al., 2011). However, high-throughput experiments to identify RNA–protein interaction sites are very time-consuming and lab investment, especially in the post-genomic era. And there are a lot of studies (Hormoz, 2013; Ray et al., 2009; Kim et al., 2006) indicating that the compositions in RBPs are closely related to interaction residues. Hence, to complement experimental methods, a reliable computational method from sequences is considered as one of the most important and efficient methods to predict protein–RNA interacting interface based on accumulated experimentally verified protein–RNA binding sites.

Currently there are a lot of excellent work developed to predict RNA–protein interaction residues, in general it can be categorized into two subgroups based on its source of data: one is protein sequence and structure based approaches, the other is CLIP-seq based approaches. For the first group, one is features extracted from sequences directly, which is inputted to machine learning algorithms, such as physiochemical properties, amino acid compositions, position specific scoring matrix (PSSM) and sequence based predicted features(predicted protein structure or relative solvent accessibilities). RNABindR (Terribilini et al., 2007) predicted amino acid sites from simple sequence composition features using Naive Bayes classifier and Wang et al. (2013) applied an extended Naive Bayes classifier after feature selection. BindN+ (Wang et al., 2010) designed SVM based method to discriminate interaction sites using PSSM information. Based on BindN, RNAProB Cheng et al. (2008) incorporated context information from nearby residues and smoothed PSSM to improve prediction performance. Liu et al. (2010) presented Random Forest to classify binding sites combining different features with interaction propensities (defined as mutual interaction propensity between nucleotide and residue triplet) from sequences, which achieved very promising performance. The other one is information extracted from protein structure. For example, Chen and Lim (2008) firstly inferred surface geometry from structure information, then classified as interface sites if they have higher relative solvent accessibilities. OPRA (Pérez-Cano and Fernández-Recio, 2010) predicted interface residues with higher local patch energy scores and residue's atom distance from any RNA atom smaller than 10Å. For the second subgroup, information is extracted from high-throughput CLIP-seq data to identify binding sites. PARalyzer (Skalsky et al., 2011) proposed kernel density estimate classifier to map the high resolution binding sites from PAR-CLIP. CapR (Fukunaga et al., 2014) calculated the probability of binding position within each secondary structural context from CLIP-seq. Recently MiClip (Wang et al., 2014) was proposed to identify highly reliable protein–RNA binding sites from CLIP-seq using two-stage HMM algorithm, which outperformed state-of-the-art method by 17–300%.

In this study, we focused on prediction from sequences and proposed a novel method based on submodularity sample subset selection to predict the interaction sites, which integrates global and local features, due to the following reasons: (1) different proteins have different percent of interaction sites and interaction patterns, as shown in Fig. 1
                     , which is taken into consideration during model training via introducing global features, such as molecular weight, protein length; (2) number of non-interaction sites dominate the training and testing data, which leads to a sample imbalance problem, and hence bias model with preference to overwhelmed non-interaction sites. Meanwhile, as indicated in (Wei and Dunbrack, 2013), classifiers achieves the best performance on balanced training sets. To this end, here we proposed a novel sampling strategy based on submodularity subset selection to select more informative and representative data subset, which is like batch active learning (Guo et al., 2013), instead of randomly undersampling the majority class or oversampling minority class. To the best of our knowledge, our method is the first work that incorporates submodularity optimization for selecting good training subset from unbalanced data, which can be extended to other studies with imbalanced data problem. Our experiment achieved better performance with accuracy of 0.863, precision of 0.847, sensitivity of 0.885, specificity of 0.840 and Matthews correlation coefficient of 0.726, which outperformed other state-of-the-art methods by 2% respectively.

The benchmark dataset was taken from Liu et al. (2010), whose protein–RNA complexes were downloaded from RsiteDB (Shulman-Peleg et al., 2008). which contains 205 non-redundant protein–RNA chains in 164 complexes with structure resolution smaller than 3Å, none of protein and RNA chains in the whole datasets has above 25% and 60% sequence identity respectively. We extracted the interaction sites between protein and RNA chains defined by ENTANGLE (Allers and Shamoo, 2001). In total 53,315 residues from protein chains, 5261 (9.87%) were interaction residues, and the rest 48,054 (90.13%) were non-binding protein residues, the number of non-interaction residues is about 9 times more than interaction residues.

Different proteins have different percent of interaction sites, as shown in Fig. 1, the minimum is only 0.003, while the maximum is 0.848, they vary very differently. So global features were extracted to take it into consideration in this study. Firstly we used secondary structure prediction tool PSIPRED (Jones, 1999) to obtain structure information from protein sequence, it can be represented by a matrix of 3*L given protein with L amino acids, then we got the global secondary structure through calculating the summation of every column and normalize them, so we can get a 3-dimension feature. It was also found that protein molecular weight is closely correlated with protein–RNA interaction (Rhode et al., 2003), which is calculated from protein sequence using Compute pI/Mw tool (Gasteiger et al., 2005). Other extracted global features are surface accessibility, which is calculated using DSSP Kabsch and Sander (1983), protein length, and amino acid composition of Lysine, Arginine, Aspartic and Glutamic.

Considering the effect of nearby amino acids, a sequence segment of windows size M with centring on residue i was used to extract local features. For making a trade-off between accuracy and computational efficiency, here window size M
                           =5 was applied in this study like Liu et al. (2010). First of all, the local secondary structure information within windows was extracted, which got a 3*5=15 features. Secondly, the PSSM, which shows the evolutionary information of proteins, was used. The L
                           ×20 PSSM scores were generated by using PSI-BLAST (Altschul et al., 1997) to search sequence against the Swissprot database (Bairoch and Apweiler, 2000) through three iterations with 0.001 as the E-value cut-off for multiple sequence alignment, whose diagonal value is extracted as conservation feature. Finally previous study (Liu et al., 2010) also has shown physiochemical property, hydrophobicity, relative accessible surface area, side-chain environment pK
                           a and statistical interaction propensity have high correlations with binding residues, they also were extracted in our study.

In total, we got 85 feature components, i.e. 10 global features and 15*5=75 local features based on window size 5.

Sample imbalance is very ubiquitous in many practical classification problems, i.e., at least one of the classes overwhelms other classes. In this study, the number of non-interaction residues is about 9 times more than interaction residues on whole dataset. The sample imbalance also exists in other binding sites problem, such as protein–ATP binding sites (Zhang et al., 2012). The unbalanced training data has very huge impact on final model performance, it is the reason that classification algorithms, such as support vector machine (Vapnik, 1995) and random forest (Breiman, 2001), usually focus on minimizing the overall error rate, which cannot obtain good model generalization for imbalanced data. To achieve better model, currently most methods used random subsampling of over-represented class to create more balanced training data, while it cannot select a more representative data for model training.

It is well known that how to select a representative subset is very important in constructing machine learning model from redundant data, here we presented a novel method based on submodularity framework to address this problem. Submodularity (Krause et al., 2008) has an intuitive diminishing returns feature, adding an element to an input set rewards more than adding it to another set. Submodularity has been used in data subset selection problem in speech recognition and natural language processing (Lin and Bilmes, 2009; Moore and Lewis, 2010), which encourages small vocabulary subsets and large acoustic spans.

We want to sample a good data subset S that maximizes some objective function with size K from original data V, as shown in Eq. (1):


                        
                           
                              (1)
                              
                                 
                                    max
                                    
                                       S
                                       ⊆
                                       V
                                    
                                 
                                 {
                                 f
                                 (
                                 S
                                 )
                                 :
                                 |
                                 S
                                 |
                                 ≤
                                 K
                                 }
                              
                           
                        
                     

This optimization is NP-hard, so we approximately achieve near-optimal solution using a simple greedy algorithm starting with S
                        =∅, and iteratively adds the element that maximize the objective function until obtain desirable data subset.

Assume we have a graph 
                           G
                           =
                           (
                           V
                           ,
                           E
                           ,
                           w
                           )
                        , V is the nodes (amino acid), and E is the edge between nodes, an edge e
                        =(i, j) connects two nodes i and j, 
                           w
                         denotes pairwise similarity between nodes. Our goal is to select most representative subset S from the whole negative set V via optimizing the following objectives:


                        
                           
                              (2)
                              
                                 
                                    max
                                    k
                                 
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                ∈
                                                V
                                                ∖
                                                S
                                             
                                          
                                          w
                                          (
                                          i
                                          ,
                                          k
                                          )
                                          −
                                          
                                             ∑
                                             
                                                j
                                                ∈
                                                S
                                                ∪
                                                {
                                                k
                                                }
                                             
                                          
                                          w
                                          (
                                          j
                                          ,
                                          k
                                          )
                                       
                                    
                                 
                              
                           
                        
                     

The above objective can be optimized nearoptimally using greedy algorithm, for more details and proof, please refer to Lin and Bilmes (2009). Here we firstly select the desired data subset from whole dataset based on the above objective, after obtaining the same number of majority class with minority class, the selected balanced dataset is applied in following model training. In this study, we use submodularity subset selection to select the balanced data subset with the same number of non-interaction (majority class) and interaction sites (minority class), it also works for data set that the proteins have large interfaces.

Random forest (RF) (Breiman, 2001) is an ensemble of multiple unpruned decision trees grown from separate bootstrap samples of the training data, and a feature subset sampled independently from the original feature space. Compared to other algorithms, such as support vector machine (Vapnik, 1995), it has very few parameters to tune. In our experiment, Scikit-learn (Pedregosa et al., 2011) is used to construct RF model, we only set parameter number of trees as 500, and all other parameters are default values. It can be used for classification and regression with high prediction accuracy, robustness to noise, and high-dimensional big data. Recently it has been widely used in many areas, such as computer vision (Shotton et al., 2011), bioinformatics (Pan et al., 2010; Song et al., 2012).

Random forest also can be used to evaluate the importance of input features. During the RF training process, bootstrap process keeps about 1/3 training data as the out-of-bag data points, whose averaged error is calculated over the forest. Then the out-of-bag error is calculated again after the values of the each feature are exchanged among the 2/3 training data points. The importance score for each feature is the mean value of difference of out-of-bag error before and after the permutation over forest.

In this study, we train machine learning model to predict whether amino acids in protein are RNA binding residues or not. In order to compare with previous proposed methods, 5-fold cross-validation testing was used to evaluate model performance. We follow their evaluation measure by means of the classification accuracy, precision, sensitivity, specificity and the Matthews correlation coefficient (MCC) as defined respectively by:


                        
                           
                              (3)
                              
                                 Accuracy
                                 =
                                 
                                    
                                       TP
                                       +
                                       TN
                                    
                                    
                                       TP
                                       +
                                       TN
                                       +
                                       FP
                                       +
                                       FN
                                    
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 Sensitivity
                                 =
                                 
                                    TP
                                    
                                       TP
                                       +
                                       FN
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 Specificity
                                 =
                                 
                                    TN
                                    
                                       TN
                                       +
                                       FP
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 Precision
                                 =
                                 
                                    TP
                                    
                                       TP
                                       +
                                       FP
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 MCC
                                 =
                                 
                                    
                                       TP
                                       ×
                                       TN
                                       −
                                       FP
                                       ×
                                       FN
                                    
                                    
                                       
                                          
                                             (
                                             TP
                                             +
                                             FP
                                             )
                                             (
                                             TP
                                             +
                                             FN
                                             )
                                             (
                                             TN
                                             +
                                             FP
                                             )
                                             (
                                             TN
                                             +
                                             FN
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        where TP, TN, FP, and FN represents true positive, true negative, false positive, and false negative, respectively. We also exploit Receiver Operating Characteristic (ROC) curve and calculated the area under the ROC curve (AUC).

@&#RESULTS AND DISCUSSION@&#

For verifying the importance of global features in identifying protein–RNA sites, we applied random forest feature selection to rank the importance of all extracted 85 features, which also signify their contribution to model performance for predicting protein–RNA binding sites. As shown in Fig. 2
                        , in the top 5 features with highest importance score, 3 of them are global features, they are molecular weight, protein length and lysine composition respectively. On the other hand, all 10 global features are ranked in top 30 important features, which indicated our extracted global features have very strong discriminate ability between interaction sites and non-interaction sites. For local features, the ranked 1st feature is relative accessible surface area (RSA) of central residue under computation, indicating RSA correlates closely with protein–RNA conformation binding (Marsh and Teichmann, 2011), meanwhile the closer the residue is from centring one, the more important the residue is for predicting binding sites. Similarly, it was also found true for most other types of local features. The same result was also discovered in other window-based prediction for protein B-factor (Pan and Shen, 2009). Our feature importance analysis also indicated that local feature interaction propensity (IP) is useful for sites classification with high feature importance score, which is because of different residues with different preference to different nucleotides.

In protein–RNA binding sites data, the number of non-interaction residues dominates the number of interaction residues, thereby leading to sample imbanlance. Sample imbalance has very huge impact on final performance of machine learning model for predicting RNA binding residues. To demonstrate its effect, we performed on new randomly subsampling constructed datasets based on the ratio of interaction residues (all interaction residues were selected) and non-interaction residues, which is 2:1, 1:1, 1:2,…, and 1:9. As shown in Fig. 3
                        , with the number of non-interaction residues increasing, Sensitivity (TP/(TP+FN)) is hugely reduced from 0.968 (2:1) to 0.320 (1:9), which means more and more interaction residues are misclassified as non-interaction residues, and biased model training in favour of non-interaction residues, where training typically converges to a solution that most of data points from the minority class are classified as the majority class. The result indicated that model performance is significantly reduced by the increasingly imbalanced class, especially for under-representative class. Although the accuracy is higher with non-interaction sites increasing, the model is less and less generalized for interaction sites and dominated by non-interaction sites. It's because unbalanced training data can make model be preferred to majority class (non-interaction sites), and most of binding sites are misclassified as non-interaction sites, whose impact on final model accuracy are drowned by dominate non-interaction sites.

To predict protein–RNA binding residues using machine learning, most of studies used random subsampling method to create a balanced training set consisting of the same number of non-interaction sites and interaction sites. Here we create a 1:1 balanced dataset using sampling approach based on submodularity framework. To demonstrate the effectiveness of our novel sampling strategy, we compared it with random subsampling of majority non-interaction sites. The ROC curve is shown in Fig. 4
                        , where the uppermost curve with the largest area under the curve has the best prediction performance. It indicated our proposed submodularity subset selection method achieved better performance than random sub-sampling, demonstrating the subsampling method based on submodularity subset selection is a very promising way for selecting more informative and representative data subset for model training from unbalanced data.

We compared our method with (Liu et al., 2010) in the same dataset, which combined different source of local features from protein sequence to predict RNA binding residues using random forest. As shown in Table 1
                        , we can achieve accuracy of 0.863, precision of 0.847, sensitivity of 0.885, specificity of 0.840, Matthews correlation coefficient of 0.726 and AUC of 0.932, which improved by more than 2% compared to state-of-the-art method on the same dataset in all measurements. Similar to Liu et al. (2010), the result also showed that RF is much better than SVC model (We grid search to get the optimum parameters with C
                        =1 and γ
                        =0.5) in this dataset. It is indicated that our sampling method based on submodularity framework can select more representative and informative data subset from over-represented class, and hence obtain more robust and powerful model. Meanwhile, molecular weight and sequence length features are highly correlated, here we remove them and evaluate their impact on RF model performance. We obtained accuracy of 0.855 and 0.857 after removing molecular weight or sequence length respectively, and accuracy of 0.851 after removing both of them. To further verify whether feature redundancy impact on our model performance, we firstly applied minimum redundancy maximum relevance (mRMR) (Peng et al., 2005) feature selection to reduce feature redundancy, then feed them into RF and SVC model to evaluate their performance. As shown in Table 1, after selecting the top 70 and 40 features from mRMR, RF model accuracy reduces from 0.863 to 0.844 and 0.822 respectively, SVC model accuracy reduces from 0.747 to 0.742 and 0.729 respectively, indicating feature selection method will remove some informative features for subsequent classification. Although there exists some feature redundancy, our model is robust to small number of redundant features. On the other hand, correlated features has smaller impact on RF than SVM, RF accuracy reduce much faster than SVM after removing some correlated feature using mPMR from 85 to 70 and 40. It is because of following reasons: (1) decision tree itself also is significantly better than SVM when there are redundant features (Gabrilovich and Markovitch, 2004). (2) Every decision tree is constructed on randomly selected feature subset instead of all features, and those correlated features are not involved in every tree construction, so the correlated features can be equally important. (3) For SVM (linear model or non-linear model), all features are involved in objective optimization, it may cause model over-fitting, and those correlated features will not contribute to model performance.


                        Fig. 5
                         illustrate an example of predicting RNA binding sites for protein 1R3E:A. Fig. 5a shows actual RNA binding residues in protein 1R3E:A. Fig. 5b shows predicted binding residues are shown on the original structure where different colors represent TP, TN, FP, FN respectively. It indicated that our method can predict most of binding sites accurately, but there are still some predicted FP (also exist in (Liu et al., 2010)). In this study we used submodularity subset selection to reduce FN caused by imbalanced non-interaction sites, in future work, we will study the approach of extracting additional structural features from protein sequence to reduce FP of predicting protein-RNA binding residues by using graph matching algorithms such as Yan et al. (2013, 2014), which have been shown promising in the work (Zaslavskiy et al., 2009) for gloabal protein–protein interaction network alignment.

@&#CONCLUSION@&#

In this paper, a novel approach for binding sites identification in proteins was presented, which extracted local features and global features from protein sequence, meanwhile construct a balanced training data based on submodularity subset selection. The method achieved high performance compared to other approaches. Experimental results indicated extracted global features have very strong discriminate ability for identifying interaction sites and our subsampling strategy based on submodularity framework is promising for solving the trouble of how to select representative data subset from redundant and unbalanced dataset. It is anticipated that our presented method can be generalized to many other studies with the need of sub-sampling majority class in unbalanced data.

@&#ACKNOWLEDGMENT@&#

This research is partly supported by Fellowship from Faculty of Health and Medical Sciences, University of Copenhagen, the National Natural Science Foundation of China (Grant no. 61462018, 61403247, 61272437, 61373152, and 61305094), YangFan Project (Grant no. 14YF1411000) of Shanghai Municipal Science and Technology Commission, the Innovation Program of Shanghai Municipal Education Commission (Grant no. 14YZ131) and the Scientific Research Foundation of Shanghai University of Electric Power, the Science Research Foundation of Shanghai University of Electric Power (Grant no. K2014-032).

@&#REFERENCES@&#

