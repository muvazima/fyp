@&#MAIN-TITLE@&#Non-localized and localized data storage in large-scale communicating materials: Probabilistic and hop-counter approaches

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This work focuses on the definition of new communicating materials (e.g. concrete and wood brick) using Wireless Sensor Networks.


                        
                        
                           
                           In this article, thousands of micro-sensors nodes are embedded into the material. Thus, the data could be stored in the memory of sensor nodes.


                        
                        
                           
                           New unstructured proactive data dissemination algorithms are developed to store the information in the communicating materials.


                        
                        
                           
                           A simulation study is developed using Castalia/OMNeT++ Tools.


                        
                        
                           
                           The solutions are compared with the most known unstructured proactive data dissemination protocols for Wireless Sensor Networks.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Internet of Things

Communicating materials

Wireless sensor networks

Data storage

Dissemination protocols

@&#ABSTRACT@&#


               
               
                  The rapid development of Internet of Things has triggered the multiplication of communication nodes based on Radio-Frequency Identification (RFID) and Wireless Sensor Networks (WSNs) in various domains such as building, city, industry, and transport. These communication nodes are attached to a thing or directly included in the material of the thing to form a communicating material. In communicating material, one of the desired objectives is to merge the logical data with its physical material, thus simplifying the monitoring of its life cycle, the maintenance operations, and the recycling process. In this context, the initial form of the communicating material can evolve during its lifecycle. It can be split, aggregated with other materials, or partially damaged. However, the entire information in the material should always be accessible after each change. Thus, the objective of this research is to develop specific algorithms for efficient dissemination of information in the material in order to limit information losses. Two dissemination algorithms hop-counter-based and probabilistic-based are proposed for storing data by using WSNs, and non-localized and localized storage is considered. Non-localized storage ensures that information can be retrieved from each piece of the material by using a uniform data replication process. Localized storage ensures that the information is stored in a limited region of the material. Castalia/OMNeT++ simulator is used to compare the performance of the proposed algorithms with other similar protocols such as DEEP, Supple, and RaWMS.
               
            

@&#INTRODUCTION@&#

Communicating material, a new paradigm of industrial information systems, was presented and discussed for the first time in [1]. It enhances a classic material by providing the following capabilities: (a) storage of data, (b) communication of information at any point of its surface, and (c) retention of properties (a) and (b) after physical modifications. This concept leads to an important change in the Internet of Things. During product manufacturing, thousands of ultra-small electronic devices are distributed in its material. Thus, the product does not communicate using certain tags or nodes at specific points, but communicates intrinsically and continuously.

The initial studies focusing on communicating materials are presented in [1–6]. In these studies, a communicating material called e-textile is obtained by scattering a large number of Radio Frequency Identification (RFID) μtags (1500tags/m2) in a manufactured textile. The system consists of an RFID reader/writer connected to a relational database that contains the entire product lifecycle information. For each writing operation, the database is examined to select the relevant data items (fragments of the database tables) that must be stored in the material. In order to achieve this objective, each data item is assigned an importance level between 0 and 1 that is computed via a multi-criteria decision-making algorithm [5]. For example, the importance value 1 indicates a highly critical data item and the value 0 indicates an ordinary one. Then, data items with the highest importance levels are stored in the μtags when the textile passes under the writer module during the manufacturing phase. The RFIDs are memory-constrained, and hence, the data item is split and stored over several tags by using a specific protocol header which can be used to rebuild the initial information. This division process is called segmentation and the resulting parts are segments.
                        1
                     
                     
                        1
                        In our article, the term segmentation is not employed in its classic sense which is typically used for the transport layer of the OSI model.
                     
                  

In such a system, data storage in the communicating material requires a reader/writer connection with each tag. If a tag is not connected during the dissemination phase, it will be isolated and left empty, thus limiting the use of RFID technology in solid and large-scale materials such as concrete in smart buildings, plane wings, and wood panels. Therefore, this study proposes the use of Wireless Sensor Networks (WSNs) in such products by dispersing micro sensor nodes in the material, as shown in Fig. 1
                     . The insertion of WSNs into materials has been proposed by [7]; however, the objective of the insertion was self-measurement purposes. In our study focusing on data management, a dissemination algorithm for WSNs is used to store relevant data items in the material.

During the manufacturing process of a final product and its future use, the communicating material passes through various lifecycle steps, as shown in Fig. 2
                     . In steps 1 and 2, the material undergoes shape transformation (e.g. cutting, sawing, and drilling) to construct the product. In the intermediate stage of the lifecycle (steps 3 and 4), the material could also undergo possible physical transformations (e.g. breakage, piece addition/elimination) which lead to information loss if data is not replicated in the entire material. Therefore, the information should be stored in a uniform manner, and it should be present in each piece of the material. Further, by ensuring uniform distribution, the data item can be retrieved later by the user without visiting many nodes in the material, thus conserving its energy resources.

WSNs in communicating material exhibit certain specific characteristics: high node density and ultra-small nodes (micro-nodes) with extremely limited memory, energy, and computation abilities. Further, replacement of nodes or batteries is impossible because the nodes are embedded in the material. Hence, the data dissemination in such networks and environment must be judicious. The amount of data transmitted in the network should be reduced as much as possible to maximize the material lifetime (i.e. communication is the main activity responsible for energy consumption in WSNs [8–10]). Thus, any data dissemination mechanism developed for nodes with such limited resources must be simple and incur low communication overhead.

Another factor in the storage in communicating materials is the data importance level, as discussed in [5]. In order to disseminate information, the user sends a data item coupled with an importance level having a value between 0 and 1, which affects the storage density (i.e. the data replication rate). The number of storage nodes should depend on this level. For instance, the number of storage nodes for an importance level of 1 should be larger than that for lower levels of importance. Thus, the key research challenge of this study is to determine a mechanism for storage of the data item among limited-resource WSNs; thus, the data item will be present in each piece of the material (uniform storage), irrespective of the importance level. A failure in this process may result in data loss during material transformation in its lifecycle.

Considerable research has been conducted in data dissemination in WSNs [11]; however, few studies have considered the challenges mentioned earlier in this manuscript. In general, dissemination algorithms can be categorized as reactive or proactive. In the reactive approach [12–15], nodes react to an event by sending data towards the nodes that are located close to the event positions. However, in the proactive approach, nodes anticipate future events and distribute their data towards all the nodes or a subset of nodes that have the role of a storage unit. This proactive approach is composed of structured and unstructured algorithms. In structured dissemination [16–18], the storage nodes typically form a virtual structure (e.g. grid, line, and rail) within the WSN, which makes the data available for retrieval at a later point of time (e.g. by sinks visiting the storage nodes to collect data). However, in unstructured dissemination [19,20], the data is replicated throughout the entire network. In this case, the manner in which data dissemination is performed will determine whether the information is uniformly stored. In this study, we focus on unstructured proactive data dissemination strategies and selection of uniform distributed storage nodes in WSNs.

In this study, we have developed two unstructured proactive dissemination algorithms for the storage of data items in large-scale communicating materials. Both algorithms use a counter-based broadcasting scheme for spreading the message to all nodes of the material. However, the algorithms have different storage strategies. The first algorithm uses probabilistic-based storage and the second algorithm uses hop-counter-based storage.
                        
                           –
                           
                              Probabilistic-based storage: Every node that receives data stores it with a probability P which is equal to the importance level.


                              Hop-counter-based storage: The message is broadcast from one node to all its neighbours, and, at each hop, the counter is decremented. When the counter is zero, the node must store the data, reset the counter to its initial value, and rebroadcast the message.

As shown in Fig. 3
                     , various storage modes are proposed for these dissemination algorithms: non-localized storage and localized storage. In the non-localized mode, the data is replicated in a uniform manner, and thus, it will be readable everywhere on the material, even after a shape transformation. The storage density varies in the material according to the data importance level. However, in the localized mode, the storage region size should depend on this importance. The storage region is larger for higher values of importance than for lower values.

These algorithms are simulated with Castalia/OMNeT++ using a realistic collision model and compared with other unstructured proactive data dissemination schemes such as DEEP, Supple, and RaWMS. They are evaluated by studying the uniformity performance for various importance levels, the communication overhead ratio, and the efficiency of the localized storage.

The remainder of the manuscript is organized as follows: In Section 2, related work is discussed. The design of the dissemination algorithms for data storage in communicating materials is described in Section 3. Simulation results and performance evaluation are discussed in Section 4. Finally, the conclusion is presented in Section 5.

@&#RELATED WORK@&#

Unstructured proactive dissemination protocols have been proposed in literature in order to overcome the challenge of node failures and mobile collector sink management. The goal of replication is to copy data at other nodes within the WSN in order to increase resilience and improve mobile sink information gathering without the necessity of visiting all nodes.

Unstructured proactive dissemination protocols in WSNs include broadcast mechanism and storage strategy, which are described below:

Broadcast algorithms are generally referred to as flooding. Flooding is an important algorithm in WSNs, and is applied when a source node must send information to all nodes or a subset of nodes in the network. It is achieved by broadcasting a message to the entire neighbourhood. Each node that receives the message rebroadcasts the message if it has not been forwarded earlier. Thus, the information traverses the entire network and reaches all the nodes which then decide whether to store it based on the storage strategy. Although flooding is an extremely simple and efficient mechanism for data dissemination, it has certain limitations. The main challenges are: duplication (i.e. a node is forced to receive information twice from two different nodes), collision (i.e. the broadcast increases contention), and resource blindness (i.e. nodes do not adopt energy-saving mechanisms) [21]. As a consequence, various schemes, including probabilistic-based schemes, have been proposed for controlled flooding. These schemes are also referred to as gossiping [22,23], counter-based [24,25], distance-based, and location-based [26] schemes.

Distance-based and location-based schemes utilize information related to the distance between nodes and their position. Therefore, nodes must be equipped with a Received Signal Strength Indicator (RSSI) or a Global Positioning System (GPS). Such additional functions could not be applied in communicating material owing to the high density with which the nodes are embedded in the product.

In [27,28], the authors show that the counter-based scheme outperforms the probabilistic one with respect to reliability (i.e. the average percentage of nodes in WSN that are reached) and efficiency (i.e. the average amount of resources required for broadcasting a message in the entire network). The counter-based mechanism can reduce the number of retransmitting nodes having a high arrival rate. Further, it does not require specific hardware such as that required in distance-based and location-based schemes. Hence, the counter-based strategy is adopted as the broadcast mechanism in our dissemination algorithms.

In literature, storage strategies in unstructured proactive dissemination protocols have been developed based on the target application. In [19,20], the information is replicated in each node for network reprogramming. If a node does not receive the new version of binary code, its software is not updated and is isolated. Authors in [29,30] propose storage strategies to improve network resilience against the risk of node failures. The storage nodes are selected based on some critical parameters such as connectivity, available memory, and remaining energy. DEEP [31] adopts another storage strategy for effective data collection by a mobile sink with uncontrolled trajectory. Every node that receives data stores it with a probability ps based on the amount of information that must be replicated in each node. Supple [32] is another probabilistic approach which uses the tree topology. The root node is responsible for receiving and disseminating the data in the network. This node transmits the data a certain number of times using the tree infrastructure, and each node i stores it based on its storage probability W(i). In RaWMS [33], the authors use a random walk mechanism to replicate data for mobile sink collection. This approach is based on a reverse maximum degree random walk (RW) sampling technique. Each source node starts an RW many times until the data is replicated in the entire network. Each RW traverses the network for a predefined number of hops; the last node is responsible for storing the data.

The main broadcast and storage approaches for unstructured proactive data dissemination algorithms in WSNs can be classified into two categories, as shown in Table 1
                        .

The existing storage strategies use the physical node parameters (e.g. memory and energy), network topology, and neighbourhood awareness to replicate data throughout the WSN. They do not assume the properties and the characteristics of the disseminated data. This limitation is the key focus of our study in which dynamic information-based dissemination algorithms are proposed and applied to the communicating material.

In this study, the data importance level is processed. The item with a high level of importance must be replicated to a greater extent than that with a lower level of importance. The dissemination algorithm must ensure the uniformity of replication for each level.

In literature, few studies attempt to compare data dissemination protocols in the entire WSN, and, from our knowledge, replication uniformity has not been studied. Hence, probabilistic and hop-counter approaches coupled with counter-based flooding are developed in this study, and the results are compared from a uniformity perspective. The remainder of the manuscript describes the proposed data dissemination algorithms in detail, and then, presents simulation results obtained for various data importance levels.

This section presents our strategies and algorithms for data item dissemination in communicating material. In this section, first, the concept of master node and the node operations for data dissemination are defined. Then, the counter-based forwarding scheme is described, and finally, the storage strategies are presented.

In order to disseminate information, the user chooses a node in his transmission range to connect to the communicating material, as shown in Fig. 4
                        . In this manuscript, this chosen node is called “master node”. Any node in the material could be a master; it depends on the choice of the user during the process of connection to the material. Certain constraints such as the highest residual energy level could be used in the selection of the master node. The master initiates the data dissemination towards the rest of the network: after receiving the message from the user, the master begins to broadcast the message towards its neighbour nodes.

Each node that executes the proposed algorithms assumes different states during data dissemination, as shown in Fig. 5
                        :
                           
                              ➀
                              Idle state: The node is non-active and its radio module is in standby mode to conserve its energy.

Reception mode: The node receives the message from one of its neighbours in the WSN. The received message contains the data and its associated importance level.

Storage decision: The node decides whether to store the data by using a function that takes the importance level as parameter. The storage decision also depends on the available memory in the node.

Broadcast mode: The node broadcasts the message within its neighbourhood to continue the data dissemination towards the neighbours that have not received this message earlier.

The switch from one state to another is described in the UML sequence diagram depicted in Fig. 6
                        . When a message is broadcast towards the node, the node state changes from the idle state to the reception mode. In the reception mode, the node receives the data and its associated importance level. Then, it commences the storage decision. Based on the result of the storage function, the node determines whether to store the information. The node memory state is also considered; in the absence of sufficient memory space, the data storage is cancelled. Irrespective of the outcome of the storage decision, the node enters the broadcast mode, in which it transmits the message to its neighbours. In the broadcast mode, the dissemination process continues until the message reaches all the nodes in the material. In the last step, the node returns to the idle state.

Our dissemination algorithms adopt the counter-based broadcasting scheme to transmit the message efficiently through the entire material. A detailed description of this scheme is provided below.

In simple flooding, as the number of duplicate broadcasted messages received by a node increases, the effectiveness of its rebroadcasting decreases [27]. The reason for this behaviour is that duplicate messages are likely to have been received by the neighbouring nodes. This concept is used in counter-based flooding.

In this scheme, when the number of times that a node has received a redundant message is greater than a predefined threshold Cth, the node cancels the rebroadcasting. The counter-based algorithm is described below:
                           
                              1.
                              When a node receives a broadcasted message for the first time, the node initializes a counter N to 1, and sets a random assessment delay (RAD) to a value between 0 and Tmax
                                 .

If the node receives the same message during the RAD, it increments the counter N (N
                                 ←
                                 N
                                 +1). If the counter reaches a pre-set threshold Cth, the node cancels the rebroadcasting.

After the RAD expires, the node rebroadcasts the message to its neighbours.


                        Fig. 7
                         summarizes the algorithm of this scheme for each node.

Several studies investigate counter-based flooding in order to optimize its parameters, Cth and Tmax
                        . Various methods such as those described in [33,34] could be used for selection of the threshold Cth. In [34], a node sets the value of Cth based on the number of its neighbour nodes. Similarly, in [35], a node sets the value of Cth based on the distance between the node and the broadcasting node. In [36], the authors demonstrate that a threshold value (Cth) between 4 and 6 is preferable from the perspective of the trade-off between reliability and efficiency. Further, simulation results in [28,37] show that if the threshold is 4 or greater than 4, the reliability is greater than 99.5%. Owing to the existence of high-density embedded sensor nodes in the communicating material, the optimal values of the counter-based parameters for communicating material applications was studied in [38]. These values were set as Cth
                        =4 and Tmax
                        
                        =200ms based on the trade-off between high reliability and number of retransmitting nodes. These parameter values are consistent with previously optimized interval values for this broadcasting scheme.

This section presents two approaches for data item replication in the entire material that use the importance level as a parameter. These approaches are hop-counter-based and probabilistic-based.

Each data item is coupled with an importance level I. Thus, one aspect of the storage strategy consists of converting this level to a dissemination level that is computed once by the master node and sent to the network of the material. Thus, for each approach, a conversion function F is defined, as shown in Table 2
                        .

In the case of a localized storage, the storage zone must be reduced to an area located around the master node. Thus, for each approach, a limitation function G is defined, which adapts the dissemination level (previously computed by the master node using F) to the storage zone. This function is executed on each node that receives the data item (Table 2).

The master node begins to broadcast the message by using a hop counter value equal to H
                           0, which is computed with the conversion function F
                           1 and the importance level I (Table 2). F
                           1 provides the number of hops H
                           0 for each interval subset of I, as shown in Eq. (1). For example, F
                           1(I)=
                           H
                           0
                           =1 if 0.85<
                           I
                           ≤0.9.
                              
                                 (1)
                                 
                                    ∀
                                    I
                                    ∈
                                    
                                       
                                          0
                                          ,
                                          1
                                       
                                    
                                    ,
                                    
                                       H
                                       0
                                    
                                    =
                                    
                                       
                                          
                                             
                                                0
                                                
                                                if
                                                
                                                0.9
                                                <
                                                I
                                                ≤
                                                1
                                             
                                          
                                          
                                             
                                                1
                                                
                                                if
                                                
                                                0.85
                                                <
                                                I
                                                ≤
                                                0.9
                                             
                                          
                                          
                                             
                                                2
                                                
                                                if
                                                
                                                0.8
                                                <
                                                I
                                                ≤
                                                0.85
                                             
                                          
                                          
                                             
                                                
                                                ⋮
                                             
                                          
                                          
                                             
                                                20
                                                
                                                if
                                                
                                                0
                                                <
                                                I
                                                ≤
                                                0.1
                                                .
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The message is broadcast from one node to all its neighbours, and at each hop, the counter H
                           0 is decremented (H
                           0
                           ←
                           H
                           0
                           −1) until it reaches zero. When the value is zero, the node stores the data, resets H
                           0 to its initial value, and rebroadcasts the message within its neighbourhood. This process continues until the edges of the material are reached.

Our dissemination algorithm is broadcast-based. In this communication model, the message is further processed by the nodes that have already received it. Further, the user can disseminate many segments or other data items. Therefore, each message is identified in the network layer (MessageID), and then, the nodes are restricted to accept the same message only once. If a node receives a message with the same ID again, it will drop the message. Fig. 8
                            shows the flowchart of this process executed at each node.

In this solution, the messages are flooded through the entire material and they reach all the sensor nodes by the counter-based flooding process. Each node decides to store data locally with the probability P that is determined by the conversion function F
                           2. As an example, let P
                           =
                           I
                           =0.5. On receiving a message, a node selects a random value rand within the interval [0,1]. The node stores the data item only if rand
                           ≤
                           P(P
                           =0.5). Fig. 9
                            shows the flowchart that describes this process executed at each node. The counter-based flooding process is similar to that shown in Fig. 8, and hence, only the storage process is illustrated in Fig. 9.

The two proposed algorithms can be completed with limitation functions that are defined in order to restrict the storage zone around the master node. For the hop-counter-based approach, G
                        1 is defined as a piece-wise linear function that influences the value of H
                        0 (Table 2). Fig. 10
                         represents the effect of the limitation function on H
                        0. First, the message is sent from the master node to the network. When the number of replications NR (number of times that the data item has been replicated) is higher than a threshold SNR
                        , the dissemination process is stopped.

For the probability-based approach, the limitation function G
                        2 is the division of the initial probability P by a constant K, as shown in Table 2. As we pass from one node to another, the probability P decreases, moving closer to zero, which prevents nodes from storing the data item, and hence, results in termination of the dissemination process.


                        Figs. 11 and 12
                        
                         show the flowcharts describing the localized dissemination algorithm for the hop-counter and the probabilistic approaches, respectively. The counter-based flooding that is used has already been depicted earlier in Fig. 8, and hence, only the storage mechanism is illustrated.

In this section, we evaluate and discuss the performance of our algorithms through computer simulation. First, the simulation settings are described in detail, and then, the performance results are presented and discussed. Two performance metrics are used to evaluate our algorithms:
                        
                           –
                           Uniformity of data replication: The existence of the disseminated data in each piece of the material. This metric is studied as a function of the data importance level.

Communication overhead: The number of message transmissions in the material during the dissemination process. This metric is a strong indicator of the energy consumption for sensor nodes because communication is the main activity responsible for energy consumption in WSNs [8–10].

The proposed algorithms were implemented with Castalia/OMNeT++ Tools which is an event-driven simulation framework written in C++ and widely used in the WSN community. Although many WSN simulators such as COOJA and TOSSIM are available, Castalia provides the most realistic wireless channel, radio models, and sensor node behaviour [39,40].

Castalia is used by researchers to evaluate the characteristics of different platforms for specific applications because it is highly parametric and can simulate a wide range of platforms. In this study, the Tyndall 10mm sensor node [41,42] is simulated. It is one of the smallest nodes used in the industry and in research (10mm×10mm). Thus, thousands of scattered nodes in a communicating material can be simulated. Table 3
                         and Fig. 13
                         show the characteristics of Tyndall 10mm node.

The simulated communicating material consists of 2500 nodes. All the node positions are uniformly distributed within a 50m×50m square (1node/m2). All the nodes are stationary. T-MAC [43], a contention-based medium access control protocol is used as the MAC protocol. Wireless radio channel characteristics such as signal noise, interference ratio, and average path loss are chosen to simulate the realistic modelled radio wireless channel in Castalia based on lognormal shadowing and the additive interference models. A segmented data item is also simulated in this study because a micro-node with limited memory space is used for communicating materials. Each segment is disseminated by a message and the maximum size of the message is fixed as 128bytes. The parameters used in this simulation study are shown in Table 4
                        .

In order to evaluate the storage and distribution performance of our proposed algorithms, first, a non-segmented data item (i.e. one message dissemination) is disseminated in the simulated WSN. Figs. 14 and 15
                           
                            show the simulation results of the broadcast and storage mechanisms of the hop-counter and probabilistic data replications for various values of hop H
                           0 and various values of probability P, respectively.

From the results obtained, the following observations can be made: Figs. 14 and 15 demonstrate the high reliability of the counter-based flooding scheme because all nodes store the information for H
                           0
                           =0 and for P
                           =1. Further, for the hop-counter and probabilistic storage strategies, the data is replicated in the communicating material based on the importance level. When the importance is low, for example, I
                           =0.4, the data item is stored in 1000 or fewer nodes in the communicating material. The density of storage decreases with a decrease in the importance level for the two algorithms, as shown in Fig. 15. However, the probabilistic algorithm has a higher replication rate than the hop-counter algorithm.

In order to study the distribution performance, the simulated material is divided into small cells of 50cm×50cm, as shown in Fig. 16
                           . Each piece has 25 sensor nodes because a uniform 50×50 grid distribution is used. In each cell, the number of nodes that have stored the data item is counted.


                           Fig. 17
                            shows the storage density difference between cells, which is obtained by using the standard deviation tool. The standard deviation represents the variable distribution around the mean, and hence, the figure shows that the probabilistic algorithm has a better resulting distribution than the hop-counter approach. When low importance is used (≤0.5), the standard deviation decreases for the hop-counter algorithm (i.e. the distribution is worst with a large value of counter H
                           0). In this case, many pieces are completely empty and others are poorly filled. However, there is no significant decrease in standard deviation for the probabilistic-based strategy because the data item is uniformly replicated and many pieces are filled for all the importance levels. Therefore, the hop-counter strategy produces more empty areas in the communicating material than the probabilistic-based strategy for low importance levels, which could lead to data loss during material transformation.

In this sub-section, the performance of segmented data item dissemination is evaluated and compared with the performance of other existing algorithms in literature with respect to the uniformity and communication overhead.


                           Fig. 18
                            shows the simulation results for a segmented data item dissemination with the hop-counter and probabilistic strategies. Three segments are disseminated with the same hop-counter value (H
                           0) for the first algorithm and the same probability P for the second one. The colours black, green, and blue represent the nodes that have stored the first, second, and third segments, respectively.

In order to study the uniformity of replication and compare the performance of the proposed algorithms with that of DEEP, Supple, and RaWMS, the division of the simulated material into small cells that is shown in Fig. 16 is used. The storage parameters of the other algorithms in literature are varied for each importance level I although they were not designed to support this type of application. W(inn) for Supple and ps for DEEP are equal to I. For RaWMS, the master node starts RW r times based on the importance I. A simple function is adopted; it provides the r value for each interval subset of I, as shown in Eq. (2) (e.g. r
                              =2300 if 0.85<
                              I
                              ≤0.9).
                                 
                                    (2)
                                    
                                       ∀
                                       I
                                       ∈
                                       
                                          
                                             0
                                             ,
                                             1
                                          
                                       
                                       ,
                                       r
                                       =
                                       
                                          
                                             
                                                
                                                   2400
                                                   
                                                   if
                                                   
                                                   0.9
                                                   <
                                                   I
                                                   ≤
                                                   1
                                                
                                             
                                             
                                                
                                                   2300
                                                   
                                                   if
                                                   
                                                   0.85
                                                   <
                                                   I
                                                   ≤
                                                   0.9
                                                
                                             
                                             
                                                
                                                   2200
                                                   
                                                   if
                                                   
                                                   0.8
                                                   <
                                                   I
                                                   ≤
                                                   0.85
                                                
                                             
                                             
                                                
                                                   
                                                   ⋮
                                                
                                             
                                             
                                                
                                                   100
                                                   
                                                   if
                                                   
                                                   0
                                                   <
                                                   I
                                                   ≤
                                                   0.1
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           

This variation has no negative effect on the existing protocols that are evaluated. On the contrary, it leads to a fair comparison with our algorithms for uniformity analysis and communication overhead.

This experiment consists of determining the number of cells of the material that contain the segmented data item. The information exists in a cell if the three segments are present. This experiment provides us information about the extent to which the dissemination algorithms distribute the information over the entire communicating material. The results of this experiment are shown in Figs. 19 and 20
                              
                              . Fig. 19 presents the existence ratio of the information in the cells for the evaluated algorithms (the existence ratio is the number of cells that contain the segmented data divided by the total number of cells). Fig. 20 shows the storage density of the information in the entire network. This storage density is an indicator of the memory overhead.


                              Fig. 19 shows that the resulting dissemination of our probabilistic-based algorithm and DEEP is more uniform than that of other algorithms. In the case of our probabilistic-based algorithm and DEEP, for importance levels greater than or equal to 0.3, the information exists in all cells of the material. For these two algorithms, the number of cells in which data is distributed is equal and corresponds to a good data distribution (i.e. greater than 71% and 83% of cells for I
                              =0.1 and I
                              =0.2, respectively, and 100% for all other importance levels). In the case of Supple, the data distribution is 100% for I
                              ≥0.4, and the uniformity decreases for lower importance values (0.3, 0.2, and 0.1), thus yielding more empty data cells.

In the case of our hop-counter-based algorithm, the segments are replicated depending on the H
                              0 value. When a high H
                              0 value is used, the segments are stored in a small number of nodes. For instance, when low importance is used (0.4 or less), the segmented data exists in fewer than 60% of the cells (i.e. the uniformity is worst when the H
                              0 value is high). In this case, many pieces are completely empty and others are poorly filled. However, the information exists in 90% of the cells for I
                              ≥0.6, and a small value of H
                              0 is required to attain this uniformity. RaWMS shows similar performance; it attains high uniformity for I
                              ≥0.8. Thus, the counter-based replication strategy does not provide a good solution to our problem because it produces many empty areas in the communicating material for low importance values, which could lead to data loss during material transformation. However, the probabilistic-based strategy ensures that a data item is uniformly replicated and many cells are filled for many importance levels; thus, the probabilistic-based storage seems to be the best solution for our problem.


                              Fig. 20 shows that our probabilistic-based algorithm and DEEP have the highest replication rate (i.e. highest memory overhead) compared to the other algorithms. For instance, the density values are 415, 1112, and 2500 (all nodes) for I
                              =0.1, I
                              =0.5, and I
                              =1, respectively. The data segments are replicated in a large number of nodes, especially for high importance levels, thus resulting in saturation of the memory of the material nodes. Hence, nodes will be unable to respond to any subsequent dissemination. These problems could be resolved by relying on the application of data compression techniques such as those described in [44,45]. Compression algorithms are suited for the reduced storage and limited resources in ultra-small nodes. For example, the compression ratio on environmental datasets can reach 70% [44].


                              Fig. 21
                               shows the total number of message transmissions in the network for all evaluated algorithms and for each importance level. It can be observed that the proposed solutions and Supple are the algorithms with the lowest overhead for an importance level greater than 0.6. For instance, our solutions transmit approximately 10% fewer messages than DEEP and 14% fewer messages than RaWMS.

When an importance less than 0.6 is used, RaWMS has the lowest overhead because it executes the hop-counter process a few times, thus resulting in low uniformity, as described in this section.

Further, our proposed algorithms have less communication overhead than DEEP although they use the flooding scheme to transmit the message in the entire network. The reason for this result is that we used counter-based flooding which generates fewer broadcasting messages than probabilistic flooding used by DEEP, and still ensures high reliability.

Assuming that communication is the main activity responsible for energy consumption in WSNs [10], these results are a strong indication that our solutions will result in low energy consumption for sensor nodes and for all the data importance levels.

An overview and comparison of the above results is summarized in Table 5
                              , where the storage and transmission strategies are provided with the corresponding uniformity, replication rate, and communication overhead performances. It is evident that the probabilistic storage strategy provides the best uniform distribution. Using this strategy, we can replicate the information in each piece of the material. However, it yields the highest replication rate, which could result in saturation of the memory of nodes.

On the other hand, the hop-counter storage exhibits bad uniformity performance. Many pieces of the material could be empty, which could result in data loss during physical transformations, although it has the lowest and best replication rate. It is evident that a low replication rate leads to bad uniformity performance. The data should be replicated many times in the WSN in order to attain the best uniformity, as observed in the probabilistic strategy.

In case of the communication overhead performance, the counter-based flooding and the tree transmission mechanism provide the best and the lowest communication overhead. These broadcast mechanisms reduce the number of transmissions in each neighbourhood, and hence, optimize the energy consumption for data dissemination.

Simulation results for localized data item dissemination are shown in Fig. 22
                            for the hop-counter (R
                           =16) and probabilistic approaches (K
                           =2.5).

From Fig. 22, it can be concluded that the hop-by-hop replication mechanism guarantees efficient data item storage in the target area of the material. The storage density increases based on the hop counter H
                           0. For example, for H
                           0
                           =6, the data item is replicated in 195 nodes. However, for H
                           0
                           =15, it is stored in 122 nodes.

In case of the probabilistic approach, the data is also efficiently replicated in the desired area. The density of storage is high in nodes situated around the master; the probability P is high in these nodes and it decreases in nodes located further from the master.

Simulation results for localized segmented data item dissemination are shown in Fig. 23
                            for the hop-counter and probabilistic approaches. The colours black, green, and blue represent the nodes that have stored the first (S1), second (S2), and third (S3) segments, respectively.

Initially, the master node disseminates the first segment. If the node memory is full, the node retransmits the message without decrementing the counter. Hence, in the hop-counter algorithm, the second segment is replicated outside the first one and similar behaviour is observed for the third segment. The last broadcasted item is replicated in a larger number of nodes than the other items. For example, for H
                           0
                           =15, the first, second, and third segments are stored in 125, 179, and 267 nodes, respectively.

However, in the probabilistic algorithm, the segments are stored in the target area with the same replication rate. All the segments have the same density. Therefore, for localized dissemination, the probabilistic approach is more efficient than the hop-counter approach.

@&#CONCLUSION@&#

In this study, we have investigated methods to disseminate data in communicating material using a wireless sensor network. We focus on the uniformity of dissemination because the material could undergo physical transformations during its entire lifecycle. Further, in this study, we take the data importance level into consideration: highly important data should be replicated in a greater number of nodes than other data. Hence, data should be uniformly disseminated and should be present in each piece of the material, irrespective of the importance level. In addition, we focused on storing the data in a limited region of the material. The region size should be fixed based on the importance level; the region size is larger for higher levels than for lower ones.

A counter-based flooding approach coupled with storage strategies is used to disseminate the data and to control the replication rate based on the importance level of the user information. Non-localized and localized storage algorithms are studied. For each storage mode, hop-counter and probabilistic replication strategies have been proposed. Previous dissemination algorithms (RaWMS, Supple, and DEEP), known to ensure uniform distribution, were used as a reference point for comparison. These algorithms were evaluated in terms of replication uniformity and communication overhead by using Castalia/OMNeT++ simulations. The simulation results showed that our probabilistic-based storage strategy, DEEP, and Supple provide better uniformity performance than our hop-counter-based algorithm and RaWMS. However, our probabilistic-based storage strategy, DEEP, and Supple incur the highest memory overhead.

Further, our algorithms and Supple have the lowest communication overhead. They result in low energy consumption of sensor nodes for all the data importance levels.

For localized dissemination, the simulation results showed an efficient localized storage for non-segmented data when the hop-counter and the probabilistic approaches were used. However, our probabilistic algorithm outperforms the other algorithms in the case of segmented items because all the segments are replicated in the target area with the same density.

Our future work will focus on testing the proposed solutions in actual concrete brick applications. The radio signal propagation inside such real material will be evaluated. Various performance metrics such as lost message ratio and average dissemination delay will be studied to evaluate their impact on replication uniformity and energy consumption.

@&#REFERENCES@&#

