@&#MAIN-TITLE@&#HPS: High precision stemmer

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           New unsupervised stemming algorithm is introduced in this article.


                        
                        
                           
                           The algorithm exploits lexical as well as semantic information of words.


                        
                        
                           
                           Performance of stemming is measured on several languages (Czech, Slovak, Polish, Hungarian, Spanish and English).


                        
                        
                           
                           We outperform competing stemmers in inflection removal test, information retrieval task and language modeling task.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Stemming

Morphology

Maximum entropy

Maximum mutual information

Language modeling

Information retrieval

@&#ABSTRACT@&#


               
               
                  Research into unsupervised ways of stemming has resulted, in the past few years, in the development of methods that are reliable and perform well. Our approach further shifts the boundaries of the state of the art by providing more accurate stemming results. The idea of the approach consists in building a stemmer in two stages. In the first stage, a stemming algorithm based upon clustering, which exploits the lexical and semantic information of words, is used to prepare large-scale training data for the second-stage algorithm. The second-stage algorithm uses a maximum entropy classifier. The stemming-specific features help the classifier decide when and how to stem a particular word.
                  In our research, we have pursued the goal of creating a multi-purpose stemming tool. Its design opens up possibilities of solving non-traditional tasks such as approximating lemmas or improving language modeling. However, we still aim at very good results in the traditional task of information retrieval. The conducted tests reveal exceptional performance in all the above mentioned tasks. Our stemming method is compared with three state-of-the-art statistical algorithms and one rule-based algorithm. We used corpora in the Czech, Slovak, Polish, Hungarian, Spanish and English languages. In the tests, our algorithm excels in stemming previously unseen words (the words that are not present in the training set). Moreover, it was discovered that our approach demands very little text data for training when compared with competing unsupervised algorithms.
               
            

@&#INTRODUCTION@&#

Word stemming tasks are among the basic preprocessing techniques in NLP (Natural Language Processing). IR (Information Retrieval) tasks, MT (Machine Translation) systems, LM (Language Modeling), and many other applications in NLP benefit from reducing the number of word forms by applying a stemming method. Current word stemming methods Goldsmith (2001), Majumder et al. (2007), Paik, Mitra, Parui, and Järvelin (2011a) are usually more task oriented and do not necessarily respect linguistic notations. They try to find different stems for words with different semantics and the same stems for words with the same semantics but a different function in the sentence. The distinction between the same and different semantics is given by the particular task to which the stemmer is being applied. For example, the words friend and friendly may be considered semantically equal for information retrieval but not for machine translation. The stemming results are often arbitrary parts of the input words (e.g., dur from durable) rather than linguistically correct morphological units – e.g., morphemes (such as friend from friendship). Creating correct morphological units would involve extra effort and might introduce errors, but having them is not always necessary. For the above mentioned tasks, it is sufficient to have a stem represented by a sequence of characters extracted from an input word that distinguishes meanings.

A large class of languages (in the linguistic typology, these languages are called synthetic languages) tends to modify the basic word form by adding prefixes and suffixes according to the function of the word in a sentence. These word forms usually share the same basic meaning. Many stemming methods strip off these affixes. However, such a task is rather complicated in many cases, as illustrated by the following examples. The pairs of English words (A) shin and shining, (B) spar and sparing and (C) speak and speaking are lexically similar and differ in having/not having the suffix ing. The first and second pairs (A, B) consist of semantically different words, whereas words from the third pair (C) differ only in their verb tense. Stripping off the suffixes in A and B would be a stemming mistake, whereas in C it is a correct action. The same example can be made of the word pairs blue and blues or word and words. Again, the suffix s can mean two completely different words or just a different grammatical number. These examples illustrate that stemming algorithms cannot just strip off a known affix. It is instead necessary to decide in which case the affix can or cannot be stripped off.

In this article we describe a novel approach to stemming. The distinguishing property of the approach is the ability to provide very accurate stems (high precision) at the cost of a small decrease in the recall rate. This property constitutes the basis for the name of our stemmer: the High Precision Stemmer (HPS), where the word precision comes from preferring precision over recall. Our method works in a fully unsupervised manner (it does not require labeled data or any knowledge about the language itself) and is multilingual. In order to prove the multilingual property, we experiment with four different language families: Slavic, Uralic, Romance and Germanic. The Slavic languages are represented by Czech, Slovak, and Polish; the Uralic languages by Hungarian, the Romance languages by Spanish, and the Germanic languages are represented by English.

The rest of this article is organized as follows. In Section 2, we clarify some terms that are used throughout the article. Section 3 introduces state-of-the-art methods for stemming. Then we describe our algorithm in Section 4. We explain our motivation and the principles of the algorithm. In Section 5, we show the results of detailed performance tests of our method by comparing it with the morphologically annotated data and testing it on the IR and in language modeling tasks. The last sections, Section 6 and Section 7, are devoted to discussing, introducing open issues, describing avenues for further work, and drawing conclusions.


                     Lexeme, lemma, stem: Throughout the article, we employ the terms lexeme, lemma and stem. To clarify these terms, we provide short definitions. Lexeme is a virtual dictionary entry of a given word. All inflectional variants of each word share the same lexeme. Lemma is one selected inflectional variant that is used to designate the lexeme. Lemmas have standardized morphological properties: it usually means that lemmas are words in the singular (nouns), masculine (nouns, adjectives), nominative (nouns), or infinitive (verbs). For example, the words speak, speaks, speaking share the same lexeme, which is designated by the lemma to speak.

The term stem has different meanings in linguistic sources. In some of them, a stem is defined as a part of a word with meaning that can create new words through different linguistic processes. According to Huddleston (1988), stems can be combined together by a process called compounding (e.g., black-bird or day-dream) or affixes can be attached by a process called affixation (e.g., dur-able). The stems black or bird are called free stems because they are words by them self. The stem dur is called a bound stem since it needs an affix to form a word. In other sources (Kroeger, 2005), a stem is the common part of the word that stays the same for all the inflectional variants (e.g., daydream-s, daydream-ing).

In this article, we use a third definition that was outlined in the introduction. We are interested in a common part of a word that carries the same meaning for all its lexical variants.
                        1
                        Lexical variants of words or lexically related words are the words that lexically resemble or lexically overlap one another: e.g., dur-able, dur-ation.
                     
                     
                        1
                      Our definition of meaning is task dependent (e.g., for information retrieval it is the part of the word that defines what a user looks for).


                     Stemming errors: We distinguish two basic types of stemming errors: understemming and overstemming. Understemming means that the word is not shortened enough and the resulting stem does not cover all variants of the word. Overstemming has the opposite meaning: the word is shortened too much and the resulting stem covers more lexemes.


                     Light and aggressive stemmers: Stemmers can be divided into light and aggressive stemmers. The light stemmers prefer precision over recall and are likely to understem the words. In disputable examples, the word is rather left intact instead of creating too short a stem (for example, reducing the words durable and duration to dura). The aggressive stemmers work the other way round. In disputable examples, their stemming is performed even at the risk of creating too short a stem (overstemming).


                     Inflectional morphology vs. derivation (linguistic process): As we noted in the Introduction, stemming tools usually work with affixes. We distinguish two main types of affixes, given their effect on words: inflectional and derivational affixes. An example for English is the following: -s, -ed, -ing forming the words work-s, work-ed, work-ing are inflectional affixes and -able, -less, -ful, -ly, -ness forming blame-able, blame-less, blame-ful blame-less-ly, blame-less-ness are derivational affixes. The example clearly illustrates the different roles of each affix. Inflectional affixes form morphological variants of a given word with the lemma staying the same. Derivational affixes create new words with more or less related meaning. We can also clearly see that removing derivational affixes can be sometimes risky. The words blame-less and blame-ful share the meaning, however, they are antonyms. The question of whether stemmers should or should not remove derivational affixes is difficult and we will address it in our experiments (see Section 5.4) and in the discussion (Section 6).


                     Stemming vs. lemmatization: Stemming and lemmatization are two related fields. In NLP, both the methods are often used for similar purposes: to reduce the number of word forms in a text. The fundamental difference is the different kind of results. The product of lemmatization is a lemma which is a valid linguistic unit. In contrast, the stem, as defined in the Introduction, is mostly task-oriented in NLP. Moreover, some stemmers also remove derivational affixes, whereas lemmatizers are restricted to inflections only. However, both stems and lemmas are intended for reducing the size of the dictionary. Stemming and lemmatization thus can replace one another in some cases. Stemming cannot be used if the output is requested to be a valid word form of a language, just as lemmatization can be too weak for some tasks (e.g., vague and vaguely have different lemmas – in this case, the lemmas are the same as the words: vague, vaguely – which may be a problem for IR). Another difference is that there are currently no means for training a lemmatizer in an unsupervised way: a labeled training corpus or set of manually created rules is needed. Moreover, stemmers are usually more semantically oriented: aggressive stemmers tend to join together semantically related lexemes. For example, runner and running may have one stem, run, but these would have two lemmas and two lexemes. A different example is familiar and unfamiliar. These would have one lemma (one lexeme) but usually two stems since the words have contradictory meaning.

The current state-of-the-art stemming algorithms usually belong to one of two basic categories: the rule-based stemmers and the statistical ones. Rule-based stemmers attempt to transform the word form to its base form by using a set of language-specific rules created manually by linguists. The statistical stemmers usually use unsupervised training to estimate the parameters of a stemming model. The basic qualitative difference is that the rule-based stemmers tend to be better at applying rather complex linguistic rules. They are not limited to stripping off affixes, but they can also change the entire word when necessary. Creating such rules is, however, very time demanding
                        2
                        There are some languages (artificial or very regular) where a short list of simple rules is sufficient. This is, however, not the case for all tested languages.
                     
                     
                        2
                      and preferably requires a linguistic expert or at least a speaker of that particular language. On the other hand, statistical stemmers benefit from a large database of automatically learned rules or parameters. Due to their principle of processing large quantities of texts, they can capture less frequent and less obvious cases. Introducing a new language or a new dialect of a language is straightforward provided that the new language meets the assumptions
                        3
                        In every statistical stemmer some assumptions about the language are made. For example, the assumption that new word forms are derived from a basic form by adding affixes. Some languages may not conform to such an assumption, and then a different stemming approach must be used.
                     
                     
                        3
                      that were made for the given statistical stemmer. However, they fail when the particular linguistic process is outside the scope of the statistical model (e.g., statistical stemmers would fail for words such as sing and sang, and foot and feet, although they are quite frequent in English).

The first published stemming algorithm ever is Lovin’s stemmer Lovins (1968), which was designed for stemming English. It needs only two steps for stemming a word according to predefined endings and transformation rules. This makes the algorithm very simple and very fast.

Another popular algorithm called Porter’s stemmer Porter (1980) evolved into a whole stemming framework called Snowball. Snowball is a string-handling programming language developed by M. F. Porter. Stemming algorithms can be easily defined in this language. In addition, ANSI C or Java programs can be automatically generated. The framework is briefly described at http://snowball.tartarus.org, together with stemmers for several languages.

In Dolamic and Savoy (2009), two rule-based stemmers (light and aggressive) for the Czech language are introduced.
                           4
                           Available at http://members.unine.ch/jacques.savoy/clef/index.html.
                        
                        
                           4
                         The aggressive stemmer exhibits slightly better results in IR than the light one. The authors present a MAP (mean average precision) improvement of about 46% by using the aggressive stemmer, and 42% by using the light stemmer in IR systems, compared with no stemming.

In Savoy (2008), the investigation of information retrieval in Hungarian is presented. The Hungarian language is characterized by a complex morphology, thus two rule-based stemmers (light and aggressive) are used to improve IR. When compared to an IR scheme without stemming, the light stemmer was able to improve MAP by about 53% on average, and the aggressive stemmer, by about 67% on average.

Many studies of the unsupervised learning of the morphology of a language have been published. An outstanding and exhaustive survey can be found in Hammarström and Borin (2011), which provides a description and comparison of the different approaches that deal with morphology at different levels of detail. In terms of that article, our approach belongs to the same-stem decisions level, which is defined as follows: Given two words, decide if they are affixations of the same lexeme.

The authors in Xu and Croft (1998) present a method that uses the word form co-occurrences in a corpus to upgrade or create a stemmer. Their work is based on the assumption that word variants (inflected forms of the same word) should occur close to each other (perhaps within a 100 word text window). To model this fact, a variant of expected mutual information is used. The initial distribution of equivalence classes given by some aggressive stemmer (such as Porter’s) is refined using the co-occurrence statistics. According to experiments, the authors show that this additional information enhances the quality of a stemming algorithm.

An interesting method for unsupervised stemming was described in Goldsmith (2001). This method is based on the principle of MDL (Minimum Description Length). The algorithm tries to find the optimal breakpoint for each word. Each instance of a given word in a corpus uses the same breakpoint, which splits this word into stem and suffix. The model for the optimal distribution of breakpoints minimizes the number of bits to encode the whole collection of words (this is mathematically equal to minimizing the entropy of this collection). The MDL criterion causes breakpoints to segment the words into relatively common stems as well as common suffixes. This method is implemented as a framework called Linguistica
                           5
                           Available at http://linguistica.uchicago.edu.
                        
                        
                           5
                         
                        Goldsmith (2006).

Automatic suffix discovery is investigated in Oard, Levow, and Cabezas (2001). At first, the frequencies of each n-gram character suffix (for n
                        =1, 2, 3, 4) are counted from each word in the collection. The frequency of each n-gram suffix is subtracted from the frequency of the adequate suffix n-gram of the lower order 
                           
                              n
                              -
                              1
                           
                         (for example, the frequency of ing is subtracted from the frequency of ng). The altered frequencies are consequently sorted and a threshold for the optimal number of suffixes for each length is chosen. It is computed by plotting the frequency rank ratios and finding the local extreme. The suffixes with a frequency higher than the threshold are then stored so as to be stripped off during the stemming process. The suffixes are processed starting from the longest ones.

In Bacchin, Ferro, and Melucci (2005) a new probabilistic model for word stemming is presented. The mutual relation between stems and suffixes is investigated. Two sets of substrings (prefixes and suffixes) are generated from the word lexicon by splitting the words at all possible positions. From these sets, the probabilities of prefixes and suffixes are estimated using the MLE (Maximum Likelihood Estimation) method. Three models for combinations of prefix and suffix probability estimations are defined. The stemmer selects the most probable split between stem and suffix given a chosen model. The authors experiment with several languages and measure retrieval performance in an IR system. The proposed algorithm produces results just as good as those produced by Porter’s stemmer for these languages.

In Majumder et al. (2007), YASS
                           6
                           YASS (Yet Another Suffix Stripper) available at http://www.isical.ac.in/clia/resources.html.
                        
                        
                           6
                         stemmer was introduced. It is a simple approach based on word clustering. All the information needed is again taken entirely from the word lexicon. The set of string distance measures between word pairs is defined. These measures should approximate the morphological similarity between words. The lexicon is then clustered to discover morphologically related words (the equivalence classes). The authors present comparable results with rule-based stemmers (Porter’s or Lovin’s stemmers for English) in terms of retrieval effectiveness. Also for the French and Bengali languages, this approach improves results when compared with no stemming.

Another unsupervised approach to stemming was introduced in Paik, Pal, and Parui (2011b). The method uses simple co-occurrence statistics reflecting how often word variants (sharing a common prefix of a given length) occur in the same document. A graph-based algorithm for merging morphologically similar words is then presented. The authors evaluate their stemmer on several languages, including European languages (Czech, Bulgarian, Hungarian, English) and Asian languages (Marathi, Bengali) in the context of IR. Stemmer outperforms YASS, XU stemmer Xu and Croft (1998), and rule-based stemmers.

The novel graph-based stemmer GRAS (GRAph-based Stemmer) was introduced in Paik et al. (2011a). Similarly to the approach of YASS, GRAS is focused only on lexical information about words. The stemmer also works only with the collection of distinct words (given by the text collection). The morphological relation is represented by a graph, where the words are treated as nodes and potentially related word pairs are connected by edges. Then the pivot nodes are identified. The idea is that pivots having many neighbors are likely to be potential roots. The authors perform retrieval experiments on seven languages. According to the presented results, GRAS outperforms YASS, Linguistica, and stemmer by Oard et al. (2001) as well as the rule-based stemmer in all seven languages in the information retrieval task. For some languages, GRAS provides a more than 50% performance improvement in the IR task when compared with no stemming.

Stemmers are usually evaluated indirectly via a target application, e.g., measuring the improvement in IR with and without stemming. However, there have been some attempts how to measure a stemmer’s quality directly (without a target application).

One approach to direct measurement is described in Paice (1994). In the article, stemming is compared with manually created groups of morphologically and semantically related word forms. They measure overstemming and understemming errors, using indices denoted by OI and UI. The indices are defined as the ratios between the number of incorrectly merged words and the total merges, and incorrectly not-merged words and the total merges. The test is designed not to take into account the frequencies of words. An error in a word with frequency 1 has the same impact on the indices as that involving a word with, e.g., frequency 100. They also consider only distinct words and stems, discarding context information.

Some of these attributes of the test may be perceived as problematic. Firstly, errors in highly frequent words have surely a higher impact on the performance of a target application than some infrequently occurring words. Secondly, decisions about the stems of words may be context dependent (i.e., the group of morphologically and semantically related word forms may differ for different contexts). Finally, the results of Paice’s test are hard to interpret as it is difficult to compare the stemmers with one another (the OI and UI indices are not in the same order).

Due the above described reasons, we designed a novel approach to direct stemming evaluation, introduced in Section 5.4.

Our approach consists of two main stages. The first one is based upon the idea that stemming should preserve the semantic information and remove the morphosyntactic information contained in words. The semantics should be an important clue to successful stemming. To model the semantic information, we use the findings from Charles (2000), Rubenstein and Goodenough (1965), which claim that word meaning can be determined from its context. It is expected that the more similar two words are in meaning, the more similar contexts they usually share. This assumption was confirmed in these articles by empirical tests carried out on human test groups. The implication of the studies is that it is possible to compute the semantic similarity of words by a statistical comparison of their contexts.

In our work we use this finding by clustering together words occurring in similar contexts and sharing enough long common prefix (they are semantically and lexically similar). The output of this method can be directly used as the stemming result by reducing all the words in a given cluster to their longest common prefix. However, this method alone does not yield the best results. Instead, we use it to automatically generate the training data for the second stage.

The second stage of our stemmer is motivated by the assumption that stemming is subject to some rules. Given a word, these rules can decide whether and how to stem the word, depending on some conditions. These conditions can model certain properties of the given word, for example, an occurrence of particular characters in the word, the presence of a certain suffix, etc. This motivation led us to treat the second stage as a classification problem, which naturally encodes the above-mentioned rules into features. We used the maximum entropy classifier that outputs stemming decisions for given words. As a training data, we employed the stemming examples generated from all clusters at the first stage. We also relied on two expectations. First, although the clusters from the first stage (the training data) may contain incorrect stems, we assume that from the statistical point of view, these errors are not significant. Second, we expect the learned rules to be general and thus our approach should work on previously unseen data. The results in Section 5 verify both expectations. Our approach is very successful for both known (seen) and unknown (unseen) words.

The architecture of our system is depicted in Fig. 1
                     . The first stage (clustering) is used only for generating the training data for the second stage. It is thus no longer required when the trained stemmer is used for a particular task. In the architecture of our system it is possible to replace the first-phase clustering algorithm with a different way of preparing the training data. A small set of manually prepared training data, another stemming algorithm, or a lemmatizer are viable ways of preparing the training data for the maximum entropy classifier. However, we believe that clustering based on semantic assumptions is the best approach, especially when no manually prepared training data are available.

Our approach to clustering is motivated by the MMI (Maximum Mutual Information) clustering algorithm described in Brown, deSouza, Mercer, Pietra, and Lai (1992). The algorithm was originally developed to improve the language modeling task. In that task the clusters were created using the minimal mutual information loss scenario. After we manually observed the resulting clusters, it became apparent that they are semantically related. We believe that the semantic information comes from the principle of the algorithm to minimize the mutual information loss. As will be shown later, there is a direct connection between the similarity of neighboring words and the mutual information loss. The more similar are the neighboring words, the less mutual information is lost. A clustering method based on the similarity of neighboring words satisfies the conditions presented in Rubenstein and Goodenough (1965), Charles (2000). In these studies, the words occurring in similar contexts (having similar neighbors) are observed to be semantically similar.

In our approach we take advantage of the MMI algorithm’s ability to find semantically related classes. At the same time we successfully reduce the computational costs by processing only words with a minimal (higher than a preset threshold) lexical similarity score. It is defined in the following subsection.

We define the lexical similarity between two words as the length of their longest common prefix normalized by the maximum of their lengths:
                              
                                 (1)
                                 
                                    S
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   a
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   b
                                                
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   LCP
                                                   (
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         a
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         b
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          
                                             max
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  w
                                                               
                                                               
                                                                  a
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  w
                                                               
                                                               
                                                                  b
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 LCP
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       a
                                    
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       b
                                    
                                 
                                 )
                              
                            is the longest common prefix of words 
                              
                                 
                                    
                                       w
                                    
                                    
                                       a
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       w
                                    
                                    
                                       b
                                    
                                 
                              
                           .

It is expected that a word stem is related to the initial part of the word. Therefore, if two words are supposed to share a stem, it is expected that they share a significantly long initial part. After normalization, 
                              
                                 S
                                 
                                    
                                       
                                          
                                             
                                                w
                                             
                                             
                                                a
                                             
                                          
                                          ,
                                          
                                             
                                                w
                                             
                                             
                                                b
                                             
                                          
                                       
                                    
                                 
                              
                            as a similarity metric for words 
                              
                                 
                                    
                                       w
                                    
                                    
                                       a
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       w
                                    
                                    
                                       b
                                    
                                 
                              
                            is supposed to measure a certainty that 
                              
                                 LCP
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       a
                                    
                                 
                                 ,
                                 
                                    
                                       w
                                    
                                    
                                       b
                                    
                                 
                                 )
                              
                            is the stem of the words (from a lexical point of view).

In later stages of the clustering algorithm, the words are already members of some clusters. To compare two different clusters, we use the complete linkage algorithm:
                              
                                 (2)
                                 
                                    S
                                    
                                       
                                          
                                             
                                                
                                                   c
                                                
                                                
                                                   a
                                                
                                             
                                             ,
                                             
                                                
                                                   c
                                                
                                                
                                                   b
                                                
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             min
                                          
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                
                                             
                                             ∈
                                             
                                                
                                                   c
                                                
                                                
                                                   a
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   j
                                                
                                             
                                             ∈
                                             
                                                
                                                   c
                                                
                                                
                                                   b
                                                
                                             
                                          
                                       
                                    
                                    S
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                
                                             
                                             ,
                                             
                                                
                                                   w
                                                
                                                
                                                   j
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           where the resulting similarity is calculated as the minimum similarity between any member of first cluster and any member of the second.

Let W denote the set of possible words (word vocabulary) and C denote the set of word clusters (class vocabulary). Note that in the following we make no distinction between class and cluster. Let m be a mapping function 
                              
                                 m
                                 :
                                 W
                                 →
                                 C
                              
                           , which maps words 
                              
                                 w
                                 ∈
                                 W
                              
                            to a class 
                              
                                 c
                                 ∈
                                 C
                              
                            (
                              
                                 c
                                 =
                                 m
                                 (
                                 w
                                 )
                              
                           ). The goal of our modified MMI clustering is to find the optimal mapping m for the stemming problem.

The original MMI clustering Brown et al. (1992) is based on maximizing the average mutual information of adjacent classes 
                              
                                 I
                                 (
                                 
                                    
                                       C
                                    
                                    
                                       L
                                    
                                 
                                 ;
                                 
                                    
                                       C
                                    
                                    
                                       R
                                    
                                 
                                 )
                              
                           
                           
                              
                                 (3)
                                 
                                    
                                       
                                          m
                                       
                                       
                                          ∗
                                       
                                    
                                    =
                                    
                                       
                                          argmax
                                       
                                       
                                          m
                                       
                                    
                                    
                                    I
                                    
                                       
                                          
                                             
                                                
                                                   C
                                                
                                                
                                                   L
                                                
                                             
                                             ;
                                             
                                                
                                                   C
                                                
                                                
                                                   R
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           where mutual information is defined as
                              
                                 (4)
                                 
                                    I
                                    
                                       
                                          
                                             
                                                
                                                   C
                                                
                                                
                                                   L
                                                
                                             
                                             ;
                                             
                                                
                                                   C
                                                
                                                
                                                   R
                                                
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   c
                                                
                                                
                                                   L
                                                
                                             
                                             
                                                
                                                   c
                                                
                                                
                                                   R
                                                
                                             
                                          
                                       
                                    
                                    P
                                    
                                       
                                          
                                             
                                                
                                                   c
                                                
                                                
                                                   L
                                                
                                             
                                             
                                                
                                                   c
                                                
                                                
                                                   R
                                                
                                             
                                          
                                       
                                    
                                    log
                                    
                                       
                                          P
                                          
                                             
                                                
                                                   
                                                      
                                                         c
                                                      
                                                      
                                                         L
                                                      
                                                   
                                                   
                                                      
                                                         c
                                                      
                                                      
                                                         R
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          P
                                          
                                             
                                                
                                                   
                                                      
                                                         c
                                                      
                                                      
                                                         L
                                                      
                                                   
                                                
                                             
                                          
                                          P
                                          
                                             
                                                
                                                   
                                                      
                                                         c
                                                      
                                                      
                                                         R
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    
                                       
                                          c
                                       
                                       
                                          L
                                       
                                    
                                    ∈
                                    
                                       
                                          C
                                       
                                       
                                          L
                                       
                                    
                                    ,
                                    
                                       
                                          c
                                       
                                       
                                          R
                                       
                                    
                                    ∈
                                    
                                       
                                          C
                                       
                                       
                                          R
                                       
                                    
                                    .
                                 
                              
                           
                        

The symbol 
                              
                                 
                                    
                                       c
                                    
                                    
                                       L
                                    
                                 
                                 
                                    
                                       c
                                    
                                    
                                       R
                                    
                                 
                              
                            denotes any two consecutive classes (class bigram) in the training data. The probabilities 
                              
                                 P
                                 (
                                 
                                    
                                       c
                                    
                                    
                                       L
                                    
                                 
                                 
                                    
                                       c
                                    
                                    
                                       R
                                    
                                 
                                 )
                                 ,
                                 P
                                 (
                                 
                                    
                                       c
                                    
                                    
                                       L
                                    
                                 
                                 )
                              
                            and 
                              
                                 P
                                 (
                                 
                                    
                                       c
                                    
                                    
                                       R
                                    
                                 
                                 )
                              
                            are determined using MLE (Maximum Likelihood Estimation). The superscripts L and R always denote the left side and right side word classes in a bigram, respectively.

However, there is no way to find such a partitioning 
                              
                                 
                                    
                                       m
                                    
                                    
                                       ∗
                                    
                                 
                              
                            that maximizes the average mutual information over so many possibilities (
                              
                                 
                                    
                                       
                                          
                                             W
                                          
                                       
                                    
                                    
                                       
                                          
                                             W
                                          
                                       
                                    
                                 
                              
                           ). In the original paper, the problem is approximated by a greedy algorithm which is further tuned in order to decrease the complexity to the order of 
                              
                                 
                                    
                                       
                                          
                                             W
                                          
                                       
                                    
                                    
                                       3
                                    
                                 
                              
                           . Such a complexity is however still very problematic. The iterative greedy algorithm merges two clusters into one cluster while maintaining a minimal mutual information loss. This means that in each step, it must find two clusters (clusters consist of already merged words or a single word) whose connection has a minimal impact on the mutual information of the whole training data. This can be formally described as follows: in each iteration i, let 
                              
                                 
                                    
                                       m
                                    
                                    
                                       i
                                    
                                 
                                 :
                                 W
                                 →
                                 
                                    
                                       C
                                    
                                    
                                       i
                                    
                                 
                              
                            denote the mapping function we are trying to optimize, where the set of word clusters 
                              
                                 
                                    
                                       C
                                    
                                    
                                       i
                                    
                                 
                              
                            in the ith step is derived from merging the two particular clusters 
                              
                                 
                                    
                                       c
                                    
                                    
                                       a
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       c
                                    
                                    
                                       b
                                    
                                 
                              
                            from the preceding step (the other clusters remaining unchanged) into a new cluster 
                              
                                 
                                    
                                       c
                                    
                                    
                                       ab
                                    
                                 
                              
                           : 
                              
                                 (5)
                                 
                                    
                                       
                                          C
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    (
                                    (
                                    
                                       
                                          C
                                       
                                       
                                          i
                                          -
                                          1
                                       
                                    
                                    ⧹
                                    {
                                    
                                       
                                          c
                                       
                                       
                                          a
                                       
                                    
                                    }
                                    )
                                    ⧹
                                    {
                                    
                                       
                                          c
                                       
                                       
                                          b
                                       
                                    
                                    }
                                    )
                                    ∪
                                    {
                                    
                                       
                                          c
                                       
                                       
                                          ab
                                       
                                    
                                    }
                                    ,
                                    
                                    a
                                    
                                    ≠
                                    
                                    b
                                    ,
                                    
                                    
                                       
                                          c
                                       
                                       
                                          a
                                       
                                    
                                    ,
                                    
                                       
                                          c
                                       
                                       
                                          b
                                       
                                    
                                    ∈
                                    
                                       
                                          C
                                       
                                       
                                          i
                                          -
                                          1
                                       
                                    
                                    ,
                                    
                                    
                                       
                                          c
                                       
                                       
                                          ab
                                       
                                    
                                    =
                                    
                                       
                                          c
                                       
                                       
                                          a
                                       
                                    
                                    ∪
                                    
                                       
                                          c
                                       
                                       
                                          b
                                       
                                    
                                    .
                                 
                              
                           Note that the operator 
                              
                                 ⧹
                              
                            denotes the set difference. The mapping 
                              
                                 
                                    
                                       m
                                    
                                    
                                       i
                                    
                                 
                              
                            that minimizes the mutual information loss compared to the preceding step can be expressed by the following formula:
                              
                                 (6)
                                 
                                    
                                       
                                          m
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          argmin
                                       
                                       
                                          
                                             
                                                c
                                             
                                             
                                                a
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                b
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             I
                                             
                                                
                                                   
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            i
                                                            -
                                                            1
                                                         
                                                         
                                                            L
                                                         
                                                      
                                                      ;
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            i
                                                            -
                                                            1
                                                         
                                                         
                                                            R
                                                         
                                                      
                                                   
                                                
                                             
                                             -
                                             I
                                             
                                                
                                                   
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            i
                                                         
                                                         
                                                            L
                                                         
                                                      
                                                      ;
                                                      
                                                         
                                                            C
                                                         
                                                         
                                                            i
                                                         
                                                         
                                                            R
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    
                                       
                                          c
                                       
                                       
                                          a
                                       
                                    
                                    
                                    ≠
                                    
                                    
                                       
                                          c
                                       
                                       
                                          b
                                       
                                    
                                    ,
                                    
                                    
                                       
                                          c
                                       
                                       
                                          a
                                       
                                    
                                    ,
                                    
                                       
                                          c
                                       
                                       
                                          b
                                       
                                    
                                    ∈
                                    
                                       
                                          C
                                       
                                       
                                          i
                                          -
                                          1
                                       
                                    
                                    ,
                                 
                              
                           where the clusters 
                              
                                 
                                    
                                       C
                                    
                                    
                                       i
                                    
                                 
                              
                            are given by formula (5). This step is repeated until the desired final number of clusters is achieved.

In our modification of the original MMI algorithm, not all possible pairs are allowed to be merged. The merge candidates are instead limited to those which fulfill a minimal lexical similarity score.

Factoring the mutual information loss and the lexical similarity into formula (6) gives
                              
                                 (7)
                                 
                                    
                                       
                                          m
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          argmax
                                       
                                       
                                          
                                             
                                                c
                                             
                                             
                                                a
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                b
                                             
                                          
                                       
                                    
                                    
                                       
                                          S
                                          
                                             
                                                
                                                   
                                                      
                                                         c
                                                      
                                                      
                                                         a
                                                      
                                                   
                                                   ,
                                                   
                                                      
                                                         c
                                                      
                                                      
                                                         b
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          I
                                          
                                             
                                                
                                                   
                                                      
                                                         C
                                                      
                                                      
                                                         i
                                                         -
                                                         1
                                                      
                                                      
                                                         L
                                                      
                                                   
                                                   ;
                                                   
                                                      
                                                         C
                                                      
                                                      
                                                         i
                                                         -
                                                         1
                                                      
                                                      
                                                         R
                                                      
                                                   
                                                
                                             
                                          
                                          -
                                          I
                                          
                                             
                                                
                                                   
                                                      
                                                         C
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         L
                                                      
                                                   
                                                   ;
                                                   
                                                      
                                                         C
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         R
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    
                                       
                                          c
                                       
                                       
                                          a
                                       
                                    
                                    
                                    ≠
                                    
                                    
                                       
                                          c
                                       
                                       
                                          b
                                       
                                    
                                    ,
                                    
                                    
                                       
                                          c
                                       
                                       
                                          a
                                       
                                    
                                    ,
                                    
                                       
                                          c
                                       
                                       
                                          b
                                       
                                    
                                    ∈
                                    
                                       
                                          C
                                       
                                       
                                          i
                                          -
                                          1
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 S
                                 
                                    
                                       
                                          
                                             
                                                c
                                             
                                             
                                                a
                                             
                                          
                                          ,
                                          
                                             
                                                c
                                             
                                             
                                                b
                                             
                                          
                                       
                                    
                                 
                              
                            is the lexical similarity between clusters 
                              
                                 
                                    
                                       c
                                    
                                    
                                       a
                                    
                                 
                              
                            and 
                              
                                 
                                    
                                       c
                                    
                                    
                                       b
                                    
                                 
                              
                            (see Section 4.1.1).

Using formula (7), the distribution of clusters heads toward maximizing the lexical similarity and minimizing the mutual information loss.

The last issue of the algorithm is the selection of the termination criterion. The optimal number of clusters depends on the morphology of the analyzed language and it is not known in advance. Our solution is to repeat the process of merging clusters while there are still two clusters with lexical similarity 
                              
                                 S
                                 (
                                 
                                    
                                       c
                                    
                                    
                                       a
                                    
                                 
                                 ,
                                 
                                    
                                       c
                                    
                                    
                                       b
                                    
                                 
                                 )
                                 ⩾
                                 δ
                              
                           . The threshold 
                              
                                 δ
                              
                            is chosen empirically (see Section 5). The complete clustering process is shown by the following simplified algorithm transcript 1.
                              Algorithm 1
                              Find a word mapping m into morphologically related clusters 
                                    
                                       
                                          
                                          
                                             
                                                
                                                   1: 
                                                      
                                                         δ
                                                         ⇐
                                                      
                                                    minimal lexical similarity between clusters
                                             
                                             
                                                
                                                   2: 
                                                      
                                                         
                                                            
                                                               C
                                                            
                                                            
                                                               0
                                                            
                                                         
                                                         ⇐
                                                         W
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   3: 
                                                      
                                                         
                                                            
                                                               m
                                                            
                                                            
                                                               0
                                                            
                                                         
                                                         ⇐
                                                         W
                                                         →
                                                         
                                                            
                                                               C
                                                            
                                                            
                                                               0
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   4: 
                                                      
                                                         i
                                                         ⇐
                                                         0
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   5: While 
                                                   
                                                      
                                                         ∃
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               a
                                                            
                                                         
                                                         ,
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               b
                                                            
                                                         
                                                         :
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               a
                                                            
                                                         
                                                         
                                                         ≠
                                                         
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               b
                                                            
                                                         
                                                         ,
                                                         S
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        c
                                                                     
                                                                     
                                                                        a
                                                                     
                                                                  
                                                                  ,
                                                                  
                                                                     
                                                                        c
                                                                     
                                                                     
                                                                        b
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         ⩾
                                                         δ
                                                      
                                                    
                                                   do
                                                   ▷ Repeat while there are still lexically similar clusters.
                                             
                                             
                                                
                                                   6: 
                                                   
                                                      
                                                         i
                                                         ⇐
                                                         i
                                                         +
                                                         1
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   7: 
                                                   
                                                      
                                                         
                                                            
                                                               m
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         ⇐
                                                         
                                                            
                                                               
                                                                  argmax
                                                               
                                                               
                                                                  
                                                                     
                                                                        c
                                                                     
                                                                     
                                                                        a
                                                                     
                                                                  
                                                                  ,
                                                                  
                                                                     
                                                                        c
                                                                     
                                                                     
                                                                        b
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               S
                                                               
                                                                  
                                                                     
                                                                        
                                                                           
                                                                              c
                                                                           
                                                                           
                                                                              a
                                                                           
                                                                        
                                                                        ,
                                                                        
                                                                           
                                                                              c
                                                                           
                                                                           
                                                                              b
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                            
                                                               I
                                                               
                                                                  
                                                                     
                                                                        
                                                                           
                                                                              C
                                                                           
                                                                           
                                                                              i
                                                                              -
                                                                              1
                                                                           
                                                                           
                                                                              L
                                                                           
                                                                        
                                                                        ;
                                                                        
                                                                           
                                                                              C
                                                                           
                                                                           
                                                                              i
                                                                              -
                                                                              1
                                                                           
                                                                           
                                                                              R
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                               -
                                                               I
                                                               
                                                                  
                                                                     
                                                                        
                                                                           
                                                                              C
                                                                           
                                                                           
                                                                              i
                                                                           
                                                                           
                                                                              L
                                                                           
                                                                        
                                                                        ;
                                                                        
                                                                           
                                                                              C
                                                                           
                                                                           
                                                                              i
                                                                           
                                                                           
                                                                              R
                                                                           
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                         ,
                                                         
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               a
                                                            
                                                         
                                                         
                                                         ≠
                                                         
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               b
                                                            
                                                         
                                                         ,
                                                         
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               a
                                                            
                                                         
                                                         ,
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               b
                                                            
                                                         
                                                         ∈
                                                         
                                                            
                                                               C
                                                            
                                                            
                                                               i
                                                               -
                                                               1
                                                            
                                                         
                                                      
                                                   
                                                   ▷ Find the mapping.
                                             
                                             
                                                
                                                   8: 
                                                   
                                                      
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               ab
                                                            
                                                         
                                                         ⇐
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               a
                                                            
                                                         
                                                         ∪
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               b
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   9: 
                                                   
                                                      
                                                         
                                                            
                                                               C
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         ⇐
                                                         (
                                                         (
                                                         
                                                            
                                                               C
                                                            
                                                            
                                                               i
                                                               -
                                                               1
                                                            
                                                         
                                                         ⧹
                                                         {
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               a
                                                            
                                                         
                                                         }
                                                         )
                                                         ⧹
                                                         {
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               b
                                                            
                                                         
                                                         }
                                                         )
                                                         ∪
                                                         {
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               ab
                                                            
                                                         
                                                         }
                                                      
                                                    
                                                   ▷ Merge the clusters 
                                                      
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               a
                                                            
                                                         
                                                      
                                                    and 
                                                      
                                                         
                                                            
                                                               c
                                                            
                                                            
                                                               b
                                                            
                                                         
                                                      
                                                    into one cluster.
                                             
                                             
                                                10: end while
                                                
                                             
                                             
                                                11: return 
                                                   
                                                      
                                                         
                                                            
                                                               m
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                    
                                                   ▷ The resulting mapping maps lexically and semantically similar words into clusters.
                                             
                                          
                                       
                                    
                                 
                              

By introducing the lexical similarity constraint, we managed to reduce the complexity of the algorithm from 
                              
                                 O
                                 (
                                 
                                    
                                       
                                          
                                             W
                                          
                                       
                                    
                                    
                                       3
                                    
                                 
                                 )
                              
                           , to 
                              
                                 O
                                 (
                                 |
                                 W
                                 
                                    
                                       |
                                    
                                    
                                       2
                                    
                                 
                                 g
                                 )
                              
                            where 
                              
                                 g
                                 ≪
                                 |
                                 W
                                 |
                              
                            is the average size of a group of lexically similar words. In greedy clustering, it is no longer required to compare all clusters (words) to each other, but only to compare those pairs that satisfy the minimal lexical similarity constraint. It is apparent that g is very likely to be much smaller than 
                              
                                 
                                    
                                       
                                          W
                                       
                                    
                                 
                              
                           , by several magnitudes.

This section describes the second stage of our approach: the maximum entropy classifier. The stages are linked together by the clusters created in the first stage. In the second stage, they are taken as the training data for the classifier. In this way, we can use a supervised classifier while still the whole system remains unsupervised.

The principle of the classifier consists in estimating the conditional probability 
                           
                              p
                              (
                              y
                              |
                              x
                              )
                           
                         of the random variable y, which is the observation on the output of a process given by the knowledge x about y. y is a member of a finite set of all possible outputs Y and x is a member of a finite set of all possible pieces of knowledge X.

The training data are used to set constraints for the conditional distribution. Each constraint expresses a characteristic (knowledge) about the training data that is requested to be present in the final probability distribution. The facts (the knowledge) about the training data are captured by n real-valued feature functions 
                           
                              
                                 
                                    f
                                 
                                 
                                    i
                                 
                              
                              (
                              x
                              ,
                              y
                              )
                              ∈
                              
                                 
                                    
                                       0
                                       ,
                                       1
                                    
                                 
                              
                           
                        .

The final model distribution is restricted in such a way that it has the same expected values for all features as seen in the training data. This can be formalized as
                           
                              (8)
                              
                                 E
                                 
                                    
                                       
                                          
                                             
                                                f
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                
                                                   x
                                                   ,
                                                   y
                                                
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       E
                                    
                                    
                                       ̃
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                f
                                             
                                             
                                                i
                                             
                                          
                                          
                                             
                                                
                                                   x
                                                   ,
                                                   y
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 1
                                 ⩽
                                 i
                                 ⩽
                                 n
                                 ,
                              
                           
                        where 
                           
                              
                                 
                                    E
                                 
                                 
                                    ̃
                                 
                              
                              
                                 
                                    
                                       
                                          
                                             f
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             
                                                x
                                                ,
                                                y
                                             
                                          
                                       
                                    
                                 
                              
                           
                         is the expected value of a feature 
                           
                              
                                 
                                    f
                                 
                                 
                                    i
                                 
                              
                              
                                 
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                              
                           
                         estimated from the training data and 
                           
                              E
                              
                                 
                                    
                                       
                                          
                                             f
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             
                                                x
                                                ,
                                                y
                                             
                                          
                                       
                                    
                                 
                              
                           
                         is the expected value of this feature given by the final model.

It was shown in Berger, Pietra, and Pietra (1996) that the requested conditional probabilities 
                           
                              p
                              (
                              y
                              |
                              x
                              )
                           
                         given the model from formula (8) have exponential form and can be estimated as follows:
                           
                              (9)
                              
                                 p
                                 (
                                 y
                                 |
                                 x
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       Z
                                       (
                                       x
                                       )
                                    
                                 
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       e
                                    
                                    
                                       
                                          
                                             λ
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             f
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             
                                                x
                                                ,
                                                y
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              Z
                              (
                              x
                              )
                              =
                              
                                 
                                    ∑
                                 
                                 
                                    y
                                    ∈
                                    Y
                                 
                              
                              
                                 
                                    ∏
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 
                                    n
                                 
                              
                              
                                 
                                    e
                                 
                                 
                                    
                                       
                                          λ
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          f
                                       
                                       
                                          i
                                       
                                    
                                    
                                       
                                          
                                             x
                                             ,
                                             y
                                          
                                       
                                    
                                 
                              
                           
                         is a normalization function. The parameters 
                           
                              
                                 
                                    λ
                                 
                                 
                                    i
                                 
                              
                           
                         of the maximum entropy model can be estimated by some algorithm for finding the global maximum of a function, such as IIS
                           7
                           IIS (Improved Iterative Scaling) is a hill-climbing algorithm for finding optimal parameters in log-likelihood space. The algorithm is described for example in Berger et al. (1996).
                        
                        
                           7
                         or by some more sophisticated method, for example by OWL-GN.
                           8
                           OWL-GN (Orthant-Wise Limited-memory Quasi-Newton) described in Andrew and Gao (2007) is an algorithm for the efficient optimization of larger numbers of parameters in log-linear models. It is based upon the L-BFGS (Limited-memory variation of the Broyden–Fletcher–Goldfarb–Shanno) algorithm. However, the authors show that it is much faster than other algorithms.
                        
                        
                           8
                        
                     

In order to apply the maximum entropy classifier to the word stemming task (suffix stripping), we need to solve a few issues. Firstly, we define y as the length of a suffix of a given word (it is the suffix that is being stripped off) and 
                           
                              Y
                              =
                              {
                              0
                              ,
                              1
                              ,
                              …
                              ,
                              M
                              }
                           
                        , where M is the maximum of all possible lengths of all suffixes. The x is the word itself. Secondly, we need to define a set of features which add constraints to the final model. We use four types of features, which are described in detail in the following sections. Finally, we use the clusters given in the previous Section 4.1 as the training data for the maximum entropy classifier.

Before we describe the various features for the maximum entropy approach, we need to define a few variables. Firstly, let
                              
                                 (10)
                                 
                                    w
                                    =
                                    
                                       
                                          l
                                       
                                       
                                          1
                                       
                                    
                                    
                                       
                                          l
                                       
                                       
                                          2
                                       
                                    
                                    ⋯
                                    
                                       
                                          l
                                       
                                       
                                          L
                                       
                                    
                                    =
                                    
                                       
                                          l
                                       
                                       
                                          1
                                       
                                       
                                          L
                                       
                                    
                                    ,
                                    
                                    L
                                    =
                                    |
                                    w
                                    |
                                    ,
                                 
                              
                           denote a character string of the word w, where L is the length of the word. Then 
                              
                                 
                                    
                                       l
                                    
                                    
                                       a
                                    
                                    
                                       b
                                    
                                 
                              
                            denotes the substring of the word from position a to position b.

Let the stem be the longest common prefix of a word group c (note that the groups are provided by stage 1 of our algorithm and may contain errors):
                              
                                 (11)
                                 
                                    stem
                                    (
                                    w
                                    )
                                    =
                                    
                                       
                                          l
                                       
                                       
                                          1
                                       
                                       
                                          min
                                          |
                                          LCP
                                          
                                             
                                                
                                                   w
                                                   ,
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                          |
                                       
                                    
                                    ,
                                    
                                    w
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    ∈
                                    c
                                    ,
                                 
                              
                           where w is a given word for which we need to find a stem and 
                              
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                              
                            are other words belonging to the same cluster c.

Then the suffix of a given word is the remaining part following the stem:
                              
                                 (12)
                                 
                                    suff
                                    (
                                    w
                                    )
                                    =
                                    
                                       
                                          l
                                       
                                       
                                          min
                                          |
                                          LCP
                                          (
                                          w
                                          ,
                                          
                                             
                                                w
                                             
                                             
                                                i
                                             
                                          
                                          )
                                          |
                                          +
                                          1
                                       
                                       
                                          L
                                       
                                    
                                    ,
                                    
                                    L
                                    =
                                    |
                                    w
                                    |
                                    ,
                                    
                                    w
                                    ,
                                    
                                       
                                          w
                                       
                                       
                                          i
                                       
                                    
                                    ∈
                                    c
                                    .
                                 
                              
                           
                        

Now, we define an arbitrary ending of a word. It is simply a K character long ending of a word w:
                              
                                 (13)
                                 
                                    end
                                    
                                       
                                          
                                             w
                                             ,
                                             K
                                          
                                       
                                    
                                    =
                                    
                                       
                                          l
                                       
                                       
                                          L
                                          -
                                          K
                                          +
                                          1
                                       
                                       
                                          L
                                       
                                    
                                    ,
                                    
                                    L
                                    =
                                    |
                                    w
                                    |
                                    .
                                 
                              
                           
                        

This feature represents the global distribution of suffixes according to the word length. The probability that an L character long word contains a suffix of length m is estimated using MLE:
                              
                                 (14)
                                 
                                    
                                       
                                          P
                                       
                                       
                                          stats
                                       
                                    
                                    
                                       
                                          
                                             L
                                             ,
                                             m
                                          
                                       
                                    
                                    =
                                    
                                       
                                          #
                                          
                                             
                                                
                                                   w
                                                   ∈
                                                   W
                                                   :
                                                   
                                                      
                                                         
                                                            w
                                                         
                                                      
                                                   
                                                   =
                                                   L
                                                   ,
                                                   |
                                                   suff
                                                   
                                                      
                                                         
                                                            w
                                                         
                                                      
                                                   
                                                   |
                                                   =
                                                   m
                                                
                                             
                                          
                                       
                                       
                                          #
                                          
                                             
                                                
                                                   w
                                                   ∈
                                                   W
                                                   :
                                                   
                                                      
                                                         
                                                            w
                                                         
                                                      
                                                   
                                                   =
                                                   L
                                                
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    0
                                    ⩽
                                    m
                                    ⩽
                                    M
                                    .
                                 
                              
                           This is simply the number of times that the L character long word contains an m character long suffix, normalized by the total number of words with length L. The function 
                              
                                 #
                              
                            denotes the number of elements in a set. M is the maximum length of suffix to be stripped off.

The feature function is
                              
                                 (15)
                                 
                                    
                                       
                                          f
                                       
                                       
                                          stats
                                       
                                    
                                    
                                       
                                          
                                             w
                                             ,
                                             m
                                          
                                       
                                    
                                    =
                                    
                                       
                                          P
                                       
                                       
                                          stats
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      w
                                                   
                                                
                                             
                                             ,
                                             m
                                          
                                       
                                    
                                    ,
                                    
                                    0
                                    ⩽
                                    m
                                    ⩽
                                    M
                                    ,
                                 
                              
                           where m is the position in the word w where the word should be split between stem and suffix.

The motivation for this feature is the assumption that the length of the suffixes depends on the length of the stems. It adds 
                              
                                 M
                                 +
                                 1
                              
                            features to the maximum entropy classifier (one for every possible length of a suffix).

The most important feature (our experiments show that removing this feature from the feature set causes the highest performance drop) for the classifier is the probability of being a suffix. It is defined as the probability that the word ending is the correct suffix. This probability is based on assessing the training data created in stage 1. For example, if the word ending ing is observed, it need not be the correct suffix (king, ring, sparing), but it can be (drinking, swimming, sleeping). This probability represents the amount of certainty that the observed word ending is a suffix causing the inflective form of the word (it is not a part of the stem) and therefore it needs to be stripped off. We can estimate the probability as follows:
                              
                                 (16)
                                 
                                    
                                       
                                          P
                                       
                                       
                                          suff
                                       
                                    
                                    
                                       
                                          
                                             suff
                                             (
                                             w
                                             )
                                          
                                       
                                    
                                    =
                                    
                                       
                                          #
                                          
                                             
                                                
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   ∈
                                                   W
                                                   :
                                                   suff
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  w
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   =
                                                   suff
                                                   (
                                                   w
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          #
                                          
                                             
                                                
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   ∈
                                                   W
                                                   :
                                                   end
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  w
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                            ,
                                                            
                                                               
                                                                  
                                                                     suff
                                                                     (
                                                                     w
                                                                     )
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   =
                                                   suff
                                                   (
                                                   w
                                                   )
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           which is essentially the number of times where 
                              
                                 suff
                                 (
                                 w
                                 )
                              
                            follows the stem of word 
                              
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                              
                            (for each word in each cluster), divided by the number of all times where the word 
                              
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                              
                            ends with 
                              
                                 suff
                                 (
                                 w
                                 )
                              
                           .

The corresponding feature for the classifier has the following form:
                              
                                 (17)
                                 
                                    
                                       
                                          f
                                       
                                       
                                          suff
                                       
                                    
                                    
                                       
                                          
                                             w
                                             ,
                                             m
                                          
                                       
                                    
                                    =
                                    
                                       
                                          P
                                       
                                       
                                          suff
                                       
                                    
                                    
                                       
                                          
                                             end
                                             
                                                
                                                   
                                                      w
                                                      ,
                                                      m
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    0
                                    ⩽
                                    m
                                    ⩽
                                    M
                                    .
                                 
                              
                           The function adds 
                              
                                 M
                                 +
                                 1
                              
                            features to the final classifier.

As shown earlier, the word ending that resembles a correct suffix (e.g., ing) does not always means that stripping it off is a correct action: e.g., drinking vs. king. In order to disambiguate such cases we introduce a feature that captures the context of the characters that precede the suffix.

Let
                              
                                 (18)
                                 
                                    ngram
                                    
                                       
                                          
                                             w
                                             ,
                                             N
                                             ,
                                             K
                                          
                                       
                                    
                                    =
                                    
                                       
                                          l
                                       
                                       
                                          L
                                          -
                                          N
                                          -
                                          K
                                          +
                                          1
                                       
                                       
                                          L
                                          -
                                          K
                                       
                                    
                                 
                              
                           denote an N-character substring (N-gram) of the word w, which ends K characters before the end of the word. This means that this substring starts at the position 
                              
                                 L
                                 -
                                 N
                                 -
                                 K
                                 +
                                 1
                              
                            and ends at the position 
                              
                                 L
                                 -
                                 K
                              
                           , where 
                              
                                 L
                                 =
                                 |
                                 W
                                 |
                              
                           . Then we define the probability
                              
                                 (19)
                                 
                                    
                                       
                                          P
                                       
                                       
                                          ngram
                                       
                                    
                                    
                                       
                                          
                                             ngram
                                             
                                                
                                                   
                                                      w
                                                      ,
                                                      N
                                                      ,
                                                      K
                                                   
                                                
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          #
                                          
                                             
                                                
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   ∈
                                                   W
                                                   :
                                                   end
                                                   
                                                      
                                                         
                                                            stem
                                                            
                                                               
                                                                  
                                                                     
                                                                        
                                                                           w
                                                                        
                                                                        
                                                                           i
                                                                        
                                                                     
                                                                  
                                                               
                                                            
                                                            ,
                                                            N
                                                         
                                                      
                                                   
                                                   =
                                                   ngram
                                                   
                                                      
                                                         
                                                            w
                                                            ,
                                                            N
                                                            ,
                                                            K
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                m
                                                =
                                                0
                                             
                                             
                                                M
                                             
                                          
                                          #
                                          
                                             
                                                
                                                   
                                                      
                                                         w
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   ∈
                                                   W
                                                   :
                                                   ngram
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  w
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                            ,
                                                            N
                                                            ,
                                                            m
                                                         
                                                      
                                                   
                                                   =
                                                   ngram
                                                   
                                                      
                                                         
                                                            w
                                                            ,
                                                            N
                                                            ,
                                                            K
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           as the probability of stripping off a suffix after the observation of the N-gram 
                              
                                 ngram
                                 (
                                 w
                                 ,
                                 N
                                 ,
                                 K
                                 )
                              
                            in the word w. This probability is calculated using MLE as the number of times where the N-gram is observed at the end of the stem, divided by the total number of times where the N-gram is observed in a word.

The feature function is then defined as
                              
                                 (20)
                                 
                                    
                                       
                                          f
                                       
                                       
                                          ngram
                                       
                                    
                                    
                                       
                                          
                                             w
                                             ,
                                             m
                                          
                                       
                                    
                                    =
                                    
                                       
                                          P
                                       
                                       
                                          ngram
                                       
                                    
                                    
                                       
                                          
                                             ngram
                                             
                                                
                                                   
                                                      w
                                                      ,
                                                      N
                                                      ,
                                                      m
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    0
                                    ⩽
                                    m
                                    ⩽
                                    M
                                    .
                                 
                              
                           We have experimentally discovered that the best results are achieved by using N-grams of lengths 1, 2 and 3 (N is set to 1, 2, and 3). This means that this feature produces 
                              
                                 3
                                 (
                                 M
                                 +
                                 1
                                 )
                              
                            features for the maximum entropy classifier.

We also assume that decisions about stemming depend on the length of the words. Therefore, we introduce the last type of feature function:
                              
                                 (21)
                                 
                                    
                                       
                                          f
                                       
                                       
                                          length
                                       
                                    
                                    
                                       
                                          
                                             w
                                             ,
                                             m
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      1
                                                   
                                                   
                                                      if
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     
                                                                        w
                                                                     
                                                                  
                                                               
                                                               =
                                                               L
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      0
                                                   
                                                   
                                                      otherwise
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    0
                                    ⩽
                                    m
                                    ⩽
                                    M
                                    ,
                                 
                              
                           where L ranges from 1 to 
                              
                                 
                                    
                                       L
                                    
                                    
                                       max
                                    
                                 
                              
                           . Thus, 
                              
                                 
                                    
                                       L
                                    
                                    
                                       max
                                    
                                 
                                 (
                                 M
                                 +
                                 1
                                 )
                              
                            features are added to the maximum entropy classifier.

The total number of all features for all possible splitting positions is then 
                              
                                 M
                                 +
                                 1
                                 +
                                 M
                                 +
                                 1
                                 +
                                 3
                                 (
                                 M
                                 +
                                 1
                                 )
                                 +
                                 
                                    
                                       L
                                    
                                    
                                       max
                                    
                                 
                                 (
                                 M
                                 +
                                 1
                                 )
                                 =
                                 (
                                 5
                                 +
                                 
                                    
                                       L
                                    
                                    
                                       max
                                    
                                 
                                 )
                                 (
                                 M
                                 +
                                 1
                                 )
                              
                           , where M is the maximum length of suffix to be stripped off.

@&#EXPERIMENTAL RESULTS@&#

In this section we provide the results of experiments from three different perspectives. Firstly, we look at the stemmer from the inflection removal point of view (Section 5.4). This experiment indicates how well the stemmer removes the inflection of the word forms. The second perspective is the retrieval performance, which is the most frequently used way of measuring the performance of a stemmer (Section 5.5). Finally, stemmers are used to improve language modeling (Section 5.6). Although the first and third experiments are not considered as traditional testing environments, they may uncover the degree of versatility of each tested stemmer. A versatile stemmer should cope well in variety of tasks. Also, successes and failures in different tasks reveal the properties of particular stemmer methods. For example, if a stemmer performs well in IR but fails in inflection removal tasks, it indicates that it produces stems that do not resemble lemmas. Such information may be useful when a similar task (that is know to work well with lemmas) is to be solved.

We also measure the performance of other competitive stemmers (namely, GRAS, YASS, Linguistica as well as rule-based stemmers) on the same data and with the same constraints and conditions. Experiments are conducted for several languages, namely Czech, Slovak, Polish, Hungarian, Spanish, and English. The settings of each stemmer are described in the following Section 5.1.

This section gives an overview of the settings of all tested stemmers that were used in our experiments. Information about stop words was not taken into account in our experiments, in order to make our approach completely unsupervised. The settings of stemmers are as follows:
                           
                              •
                              
                                 HPS: The max length of suffix M was set to 3 for all languages, which means we classify into 4 classes (4 possible lengths of suffix, i.e., from 0 to 3 characters). The minimum similarity between two clusters (the stopping condition for clustering) was set to 
                                    
                                       δ
                                       =
                                       0.7
                                    
                                  for Czech, Slovak, English, and Spanish. For Polish and Hungarian, 
                                    
                                       δ
                                       =
                                       0.6
                                    
                                 . Stemming is performed in two iterations for all languages (see Section 5.2).

We implemented HPS on the Java™ platform and we used Konkol (2014) implementation of maximum entropy classifier.


                                 GRAS: The suffix frequency cut-off coefficient (which is used to prune invalid suffix pairs) was set to 4. The cohesion threshold used to measure whether two nodes are morphologically related or not was set to 0.8. Both parameters are recommended by the authors of GRAS.


                                 YASS: The clustering threshold value was set to 
                                    
                                       1.5
                                    
                                  for all languages. This setting is recommended by the authors of this stemmer.


                                 Linguistica: This does not require any special settings. However, the number of tokens used for training is limited in the only available implementation of this stemmer. The maximal amount of tokens is set to 5,000,000.


                                 Rule-based stemmers: We used Porter’s stemmers for languages available via the Snowball framework. For Czech, we chose to use the light stemmer presented in Dolamic and Savoy (2009). Despite the fact that, according to the authors, the aggressive stemmer performs slightly better in IR (our experiments agree with this), the results of the light stemmer on inflection removal are much better than the results of the aggressive one. For Polish and Slovak, we have not found any rule-based stemmers.

Note that the 
                           
                              δ
                           
                         parameter for HPS is set empirically. By changing 
                           
                              δ
                           
                         it is possible to tune the aggressivity of the stemmer. The lower 
                           
                              δ
                           
                         is, the more aggressive a stemmer is created, because more words are grouped by a clustering. The parameter 
                           
                              δ
                           
                         can be perceived as the minimal ratio between the length of the stem and the length of the word. We recommend setting lower values of 
                           
                              δ
                           
                         for languages that tend to have long suffixes (e.g., Polish and Hungarian) and higher values of 
                           
                              δ
                           
                         for other languages. We recommend 0.6 as the lower value and 0.7 as the higher value. However, it is possible to tune the stemmer using 
                           
                              δ
                           
                         for a specific task and a language. We did not do so, in order to have results that were not tuned for the test data or the task (our aim is to design a multi-purpose stemmer). In fact, 
                           
                              δ
                           
                         is the only information about the language that needs to be determined for training HPS.

We also created three new stemmers by extending GRAS, YASS, and Linguistica by our second stage of HPS (maximum entropy classifier). These stemmers are denoted by GRAS
                        
                        +
                        
                        HPS, YASS
                        
                        +
                        
                        HPS, and Linguistica
                        
                        +
                        
                        HPS, respectively. The original stemmers are used only for generating the training data for the classifier (in the same way as our modified MMI clustering). All other settings remain unchanged.

In our initial experiments, it turned out that some words with more complicated morphology remain understemmed. Repeated or iterative stemming proved to be beneficial for such words. The stage 2 algorithm is repeated more than once. During such a process, the understemmed words are shortened. However, there is a risk of overstemming (creating too short a stem). This process is efficient especially for languages with more complex morphology and it leads to increasing the recall rate in particular.

Consider the following example, which deals with the complex inflectional morphology of Czech. The words mlad-ší (younger – 1st case) and mlad-ší-mi (younger – 7th case) share the same stem mlad with suffixes ší and mi. In the second word, the suffix mi follows the suffix ší. In such a case, the second suffix mi may be removed during the first iteration and the first suffix ší in the second one. The result of the second iteration is the correct stem mlad. Another example concerns derivation in English: fear-less-ly. The first iteration produces fear-less (the correct stem) and the second one fear (overstemmig).

The above mentioned examples show the advantages and drawbacks of iterative stemming. When applying this method, we should be particularly careful with derivational suffixes since the method may easily produce overstemming errors (see the example above). On the other hand, if we deal with inflectional suffixes, iterative stemming can only be beneficial. No inflectional suffix can change the lemma and thus it cannot change the meaning.

In our tests we experimented with the setting of the number of iterations. We found that the optimal value is two iterations for all languages and all tests. Higher values have little impact on the quality of the results. We discovered that when we use iterations, the stemmer shortens the understemmed words but the well-stemmed words are left intact.

We experimented with six languages, namely Czech (CZ), Slovak (SK), Polish (PL), Hungarian (HU), Spanish (ES), and English (EN). The stemmers were trained on the unlabeled corpora mentioned below. Since the implementation of the Linguistica stemmer limits the training size to 5,000,000 tokens, we used this limit for all stemmers. The exceptions are the tests with variable training data sizes. We used up to 15,000,000 tokens there.

Statistics for the corpora are presented in Table 1
                        . We distinguish between (word) tokens and words. Token refers to a single occurrence of a word in the text. By word, we mean one particular word that can occur many times in a text. In the table, we count the total number of words and the number of words that occur in texts at least five times.

The training corpora are:
                           
                              •
                              
                                 CZ: This corpus contains news in Czech on various topics, such as political, business, sports, international, and other news gathered from one year. The data in this corpus are provided by the Czech News Agency.


                                 SK: A huge number of texts from the Slovak National Corpus
                                    9
                                    The prim-5.0-public-all subcorpus of the Slovak National Corpus available at http://korpus.juls.savba.sk.
                                 
                                 
                                    9
                                  oriented towards the arts, journalism, and the professions.


                                 PL and HU: The corpora for Polish and Hungarian which we use in this work are part of a multilingual parallel corpus available through the Joint Research Center (JRC).
                                    10
                                    Available at http://langtech.jrc.it/.
                                 
                                 
                                    10
                                 
                              


                                 ES: The Spanish texts are taken from the Reuters Multilingual Corpus (RCV2). The data are gathered from 1996 and 1997. The corpus is available through the NIST (National Institute of Standards and Technology) Standard Reference Data Products.
                                    11
                                    Available at http://trec.nist.gov/data/reuters/reuters.html.
                                 
                                 
                                    11
                                 
                              


                                 EN
                                 
                                    12
                                    Available from NIST Standard Reference Data Products at http://www.nist.gov/srd/nistsd23.cfm.
                                 
                                 
                                    12
                                 : The data represent a sampling of approximately 40% of the articles published by the Los Angeles Times in the two-year period from January 1, 1989 to December 31, 1990.

We start our evaluation with a direct test. Our motivation is the same as in Paice (1994). We desire a deeper insight into a stemmer’s quality without the effect of a target application. However, we decided to design our test differently. The reason is the presence of the two problematic attributes of the test mentioned in the state-of-the-art section, and mainly due to the lack of manually annotated data for languages other than English. Instead of groups of words that should have same stem, we use readily available groups of words sharing the same lemma. And instead of overstemming and understemming indices, we use precision, recall, and F-measure. The precision directly relates to overstemming errors (the higher the precision, the lower the frequency of errors) and recall to understemming errors. Our values respect the frequencies of words and are computed directly from the texts (not from dictionaries), so they reflect the contexts.

Our test is based upon measuring the ability of a stemmer to remove an inflection from an inflected word form, by comparing groups of words with the same stem with groups of words with the same lemma. Ideally, these should be equivalent. Naturally, such a test is focused solely on inflective morphology and omits the derivation linguistic process. In fact, the test measures the ability of the stemmer to approximate lemmas. The score from the test thus should indicate the ability of the stemmer to replace a lemmatizer in applications where lemmatizers are working well, e.g., language models Brychcín and Konopík (2011), machine translation Koehn and Hoang (2007), etc. Naturally, the score will be less related to applications which require aggressive stemming and working with derivational linguistic processes, e.g., information retrieval. Therefore, the test does not cover all aspect of stemming. However, we believe that a universal test of stemmers cannot be constructed, simply because stemming is always to some extent task dependent.

In our test, we go through the test corpus and for each position in the text we analyze two word groups. The first one consists of all words sharing the same stem with the word at its actual position (the result of the tested stemmer). The second group contains all words sharing the same lemma with the actual word (given by the lemmatized data). We calculate precision (P), recall (R) and their harmonic mean, the F-measure (
                           
                              
                                 
                                    F
                                 
                                 
                                    m
                                 
                              
                           
                        ):
                           
                              (22)
                              
                                 P
                                 =
                                 
                                    
                                       tp
                                    
                                    
                                       tp
                                       +
                                       fp
                                    
                                 
                                 ,
                                 
                                 R
                                 =
                                 
                                    
                                       tp
                                    
                                    
                                       tp
                                       +
                                       fn
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       F
                                    
                                    
                                       m
                                    
                                 
                                 =
                                 
                                    
                                       2
                                       PR
                                    
                                    
                                       P
                                       +
                                       R
                                    
                                 
                                 .
                              
                           
                        Here, tp denotes the number of times the word in the stemmer group matched a word in the lemma group and fp denotes the number of times the stemmer group contained the wrong word. Finally, fn denotes the number of times the stemmer group missed the correct word. The higher the precision rate, the fewer times the stemmer produces an overstemming error. On the other hand, the higher the recall rate, the fewer the understemming results. The F-measure takes both these errors into account and thus can be used as a final evaluation of the stemming quality.

The test corpora contain manual morphological annotations with lemmas. The list of the corpora follows:
                              
                                 •
                                 
                                    CZ: The data are part of the Prague Dependency Treebank 2.0.
                                       13
                                       More information at http://ufal.mff.cuni.cz/pdt2.0/.
                                    
                                    
                                       13
                                     The texts in the corpus consist of manually annotated articles from several newspapers and journals in Czech.


                                    SK: The manually annotated part of the Slovak National Corpus
                                       14
                                       The r-mak-3.0 subcorpus of Slovak National Corpus available at http://korpus.juls.savba.sk.
                                    
                                    
                                       14
                                     mainly consists of artistic, journalistic, and professional texts.


                                    PL: The manually annotated part of the National Corpus of Polish.
                                       15
                                       Available at http://nkjp.pl/.
                                    
                                    
                                       15
                                    
                                 


                                    HU: The manually annotated Hungarian texts of the Szeged Corpus 2.0.
                                       16
                                       Available at http://www.inf.u-szeged.hu/projectdirs/hlt/index_en.html.
                                    
                                    
                                       16
                                    
                                 


                                    ES: The Spanish texts are taken from the Ancora 2.0
                                       17
                                       Available at http://clic.ub.edu/corpus/en/ancora.
                                    
                                    
                                       17
                                     
                                    Taulé, Martí, and Recasens (2008) corpus, which is mainly oriented to journalism.


                                    EN: These texts are part of the Open American National Corpus (OANC).
                                       18
                                       Available at http://www.anc.org/data/oanc.
                                    
                                    
                                       18
                                    
                                 

Some statistics of the corpora are shown in Table 2
                           .

@&#RESULTS@&#

This section presents the inflection removal results. Each stemmer was trained on the first 5,000,000 tokens from the appropriate corpus (Section 5.3) and then evaluated on the corresponding annotated corpus (Section 5.4.1). The results for our novel test are shown in Table 3
                           .

Before analyzing the results for the tested stemmers, we should note the following. As we explained in the Introduction, the goal of a stemming algorithm depends on the particular task at hand. Some aggressive stemmers usually remove derivational suffixes as well as inflectional suffixes. In this testing scenario, removing derivational suffixes is penalized since such an action incorrectly creates the same stem for different lemmas. Thus, we can interpret the results of this test only in relation to the inflection removal task. Nevertheless, we also pointed out that removing derivational suffixes is risky whereas removing inflectional suffixes is safe. Thus, we can expect that a stemmer successful in this test should produce a low number of overstemming results.

If we use the F-measure (
                              
                                 
                                    
                                       F
                                    
                                    
                                       m
                                    
                                 
                              
                           ) as a quality measure for the inflection removal experiments, we can conclude the following. On all languages except Hungarian and Spanish, HPS yields the best results of all unsupervised stemmers. For Hungarian and Spanish, the results of HPS are similar to those of GRAS.

On Czech and Slovak, the results of GRAS and YASS are similar. On other languages, GRAS performs much better than YASS. Linguistica performs the worst in this testing scenario. Furthermore, we can see that HPS always achieves one of the best precision rates. In contrast, GRAS always has one of the best recall rates.

In the tables we also show the results for the stand-alone first phase of HPS to see how large an improvement the second phase of HPS (maximum entropy classifier) produces. The first phase of HPS does not lead to the best results, but has the best precision (except on English).

Our maximum entropy extension also improves the other stemmers (GRAS+HPS, YASS+HPS and Linguistica+HPS). However, the combination of maximum entropy and the first phase of HPS is generally the best one. We assume this to be due to the high precision of the first phase of HPS (with little overstemming errors) which leads to creating better training data for the maximum entropy classifier.

In this section, we study how the quality of the stemming changes depending on the amount of training data available. The stemmers are evaluated on the same data as in the previous section, but different numbers of tokens being used for training. We start with a very small amount of training data (50,000 tokens) for each language, and continue up to 15,000,000 tokens, which we believe is a sufficient amount for all methods. The results are shown in Fig. 2
                           .

From the figures presented above, we can conclude that our stemmer excels in experiments for all sizes of training data and all tested languages. The F-measure results are better for Slavic languages (Czech, Slovak, Polish) in particular.

A very interesting and also important fact is that our stemmer gives very promising results even for an amount of training data as small as 50,000 tokens for each language. In addition, it is possible to say that after 1,000,000 tokens for training, the results of our stemmer do not improve significantly. Thus, we can state that 1,000,000 tokens are optimal for satisfactory results. This property is caused by the second stage of HPS, as we observe large improvements even for the other stemmers when they use this extension (GRAS+HPS, YASS+HPS and Linguistica+HPS).

As we expected, the quality of other stemmers (without the second stage of HPS) rises significantly with an increasing amount of training data. After a certain point (5M or 10M tokens) the results are not improved any more. The exception is the English (our less inflected language), where the performance of the stemmers decreases or stagnates. We believe this is due to focusing on the purely lexical level of the words, where with a rising amount of training data, the larger number of word forms can yield an increase in the recall rate but a very significant drop in precision (some stemmers start to overstem the words). The outcome is that the F-measure drops. Note that this problem is specific to the inflection removal experiments. However, in the information retrieval experiments (Section 5.5), this does not mean a definite retrieval drop.

In this section, we experiment with using different stemmers for the information retrieval task. We compare the results of our stemmer with those of other competitive stemmers in four languages. We use the open source search engine Terrier,
                           19
                           Available at http://terrier.org.
                        
                        
                           19
                         which implements state-of-the-art indexing and retrieval functionalities. Terrier is written in the Java™ platform and was developed by the School of Computing Science at the University of Glasgow.

In our experiments, we used the I(F)B2 model for term weighting. I(F)B2 denotes the model I(F) (tf-itf model: term frequency – inverse collection term frequency), with the normalization factor B (Bernoulli after-effect given by the ratio of two Bernoulli processes) and with the assumption of the hypothesis H2 (the term frequency density is inversely related to the length of document). The derivation and definition of this function can be found in Amati and Van Rijsbergen (2002). The authors of the article also show a comparison of several models for IR. The I(F)B2 is suggested to be one of the best performing models.

The evaluation presented in this section is based upon the collection built during the CLEF
                              20
                              Available at http://www.clef-campaign.org/.
                           
                           
                              20
                            evaluation campaign (CLEF Evaluation Package AdHoc News 2004–2008). Statistics of the corpora are shown in Table 4
                           .

The collection comprises queries which follow the guidelines of the TREC ad hoc task. Each query is structured into three sections: title (reflects the queries that users send to search engines), description (one sentence description of the requested data), and narrative part (a few sentences describing the criteria for the requested data). A set of relevant documents (correct answers) is provided for every query in the collection. In our experiments, we used the title and description parts only.

@&#RESULTS@&#

In this section we use the stemmers listed in Section 5.1 to improve retrieval performance for four languages. As described in Majumder et al. (2007), Paik et al. (2011a), the unsupervised stemmers YASS and GRAS should give as good results in the retrieval context as rule-based stemmers. Our experiments confirm this. However, we also present the retrieval experiments from a slightly different point of view.

In the real world, the indexing performed by an IR system is an iterative process. New data arise continually, and they need to be indexed. However, this fact is usually not taken into account when testing stemmers (YASS and GRAS). During the tests, it is expected that a stemmer is trained on all the data that are indexed. However, in reality, the new data are unseen by the stemmers. Retraining the stemmer is computationally expensive, although possible. However, this process has one big problem. The retrained stemmers are likely to stem some already seen words differently because in many stemmers the stemming decisions are learned from all the data. The direct implication is that the complete data set needs to be reindexed. Reindexing is a process that takes a lot of computer time, especially for large data sets. It also introduces scalability problems when distributing the index.
                              21
                              An input query should be preprocessed with the same stemmer as the index. When the index is distributed, the stemmers need to be synchronized for all parts of the index.
                           
                           
                              21
                            We, however, believe that the retraining and reindexing steps can be done much less frequently or even completely avoided. In such a scenario, the stemmer that is being used needs to be able to handle unseen data (data not seen during training). Taking these facts into account, we have decided to perform the retrieval experiments using stemmers trained on both types of data:
                              
                                 •
                                 
                                    Seen: The stemmers are trained on the same data as they are used for indexing, which causes all words that are indexed to be known by the stemmers. This experiment allows comparisons to be made with the results in the original papers about competitive stemmers. Unfortunately, this test does not include results for Linguistica, since its implementation limits the training data size to 5,000,000 tokens only.


                                    Unseen: The stemmers are trained on the data used for the inflection removal experiments described in Section 5.4.1, which means that the indexed data are previously unseen by the stemmers. This way of testing corresponds to the scenario we introduced above. We used a completely different corpus for training the stemmers because we want to emphasize the ability of a stemmer to work with unseen data. However, we must note that in the scenario where the newly indexed data (the unseen data) are from the same domain, the difference between seen and unseen data probably will not be so big.

To calculate all performance scores, we used the TREC-EVAL program, which is the standard tool for the evaluation of TREC results using the standard NIST evaluation procedures.

Retrieval performance was measured using several measures. In the tables below, MAP denotes the Mean Average Precision. R-prec is an R-precision measure that represents the precision at the Rth position in retrieved documents for a query that has R relevant documents. The symbols 
                              
                                 P
                                 @
                                 5
                              
                            and 
                              
                                 P
                                 @
                                 10
                              
                            denote the precisions at fixed low levels of retrieved results (5 and 10 documents). The number of retrieved relevant documents is denoted by Rel-ret. The results are shown in Table 5
                           .

In the preceding section, the stemmers were tested in the information retrieval task. There was, however, only a limited number of queries provided for each language. It is therefore crucial to evaluate the differences in results using a statistical significance test. For the evaluation, the paired t-test at a confidence level of 0.95 is used (see Hull (1993)). The hypotheses are defined as follows. The preliminary assumption (the null hypothesis 
                              
                                 
                                    
                                       H
                                    
                                    
                                       0
                                    
                                 
                              
                           ) is that there is no difference between these two stemmers in terms of their stemming quality. The alternative hypothesis 
                              
                                 
                                    
                                       H
                                    
                                    
                                       1
                                    
                                 
                              
                            means that one stemmer is significantly better than the other one.

The results of the significance testing are presented in Table 6
                            by the following symbols:
                              
                                 •
                                 Symbol “<” if the row’s stemmer is worse than the column’s stemmer. The hypothesis 
                                       
                                          
                                             
                                                H
                                             
                                             
                                                0
                                             
                                          
                                       
                                     is rejected.

Symbol “>” if the row’s stemmer is better than the column’s stemmer. The hypothesis 
                                       
                                          
                                             
                                                H
                                             
                                             
                                                0
                                             
                                          
                                       
                                     is rejected.

Symbol “=” if the row’s stemmer is equal to the column’s stemmer. The hypothesis 
                                       
                                          
                                             
                                                H
                                             
                                             
                                                0
                                             
                                          
                                       
                                     is not rejected.

The p-value is calculated for two measures of performance: for average precision MAP (the first symbols in the table) and for R-precision (the second symbols in the table).

From Table 6, we can deduce the behavior of all stemmers in the information retrieval context. In the case of seen data, it was discovered that all tested stemmers perform equally well (there is no significant difference between them).

In the case of unseen data, HPS (both phases), GRAS+HPS, YASS+HPS and rule-based stemmers perform the best. For less inflected languages (ES and EN) the performance of GRAS and YASS is on the same level, but for highly inflected languages (CZ and HU) their performance is significantly worse. This is expected behavior, because many word forms are previously unseen, leading to a significant OOV (out-of-vocabulary) rate. These facts prove that our second stage of HPS is able to work very well with unknown word forms.

In both the cases of seen and unseen data, all evaluated stemmers significantly improve the results of the IR system when compared with no stemming. If we summarize all these results, HPS (both phases), GRAS+HPS, YASS+HPS, and rule-based stemmers consistently give the best results even though HPS was not designed purely for the IR task, but rather to be a multi-purpose stemmer. GRAS and YASS perform slightly worse and Linguistica is the least efficient stemmer for the IR task.

This section presents experiments with the application of stemmers to language modeling. The purpose is to reveal the performance of stemmers in yet another scenario.

Language modeling is a crucial task in many areas of NLP. Speech recognition, optical character recognition, machine translation, information retrieval, and many other areas depend heavily on the quality of the language model that is being used. Each improvement in language modeling can also improve the particular job where the language model is used.

Morphological information has already been proved to be useful in language modeling. For example, in Brychcín and Konopík (2011), we use lemmatization and part-of-speech (POS) tags to significantly improve the perplexity of Czech and Slovak language models. In this section, we present a similar approach, but instead of lemmas we used stems, and instead of POS tags we used suffixes.

We choose class-based language models as the architecture for incorporating the morphological information. We have derived two kinds of class-based models:
                              
                                 •
                                 
                                    Stem: word classes represent the words with the same stem.


                                    Inflection: words with the same inflection (suffix following the stem) are grouped into one class.

We use the modified Kneser–Ney interpolation 
                           Chen and Goodman (1998) for smoothing the baseline n-gram language model as well as the stand-alone class-based models. The order (n) of all models is 3.

These two class-based models are combined with the baseline model by bucketed linear interpolation Bahl, Jelinek, and Mercer (1983):
                              
                                 (23)
                                 
                                    
                                       
                                          P
                                       
                                       
                                          BLI
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                
                                             
                                             |
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                   -
                                                   n
                                                   +
                                                   1
                                                
                                                
                                                   i
                                                   -
                                                   1
                                                
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             K
                                          
                                       
                                    
                                    
                                       
                                          λ
                                       
                                       
                                          k
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                   -
                                                   n
                                                   +
                                                   1
                                                
                                                
                                                   i
                                                   -
                                                   1
                                                
                                             
                                          
                                       
                                    
                                    ·
                                    
                                       
                                          P
                                       
                                       
                                          k
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                
                                             
                                             |
                                             
                                                
                                                   w
                                                
                                                
                                                   i
                                                   -
                                                   n
                                                   +
                                                   1
                                                
                                                
                                                   i
                                                   -
                                                   1
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 )
                              
                            is the weight of the kth language model, 
                              
                                 
                                    
                                       P
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 )
                              
                           . We use the EM (Expectation Maximization) algorithm described in Dempster, Laird, and Rubin (1977) to calculate the optimal weights 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       k
                                    
                                 
                              
                            by maximizing the probability of the held-out data. In bucketed interpolation, the weights are functions of the frequency of word history. The main idea behind the interpolation is that the weights 
                              
                                 
                                    
                                       λ
                                    
                                    
                                       k
                                    
                                 
                              
                            should be different for words with histories of different frequencies. In our experiments, 20 buckets were used.

@&#RESULTS@&#

The performance of a language model is typically measured in terms of the perplexity of the model on the unseen test corpus. The perplexity can be seen as the confusion of the model. Lower perplexity means a better prediction ability of language model. It was shown by many authors that the reduction of perplexity often leads to improving whole system where the language model is used (for example machine translation in Brychcín & Konopík (2014) or speech recognition in Watanabe, Iwata, Hori, Sako, & Ariki (2011)).

The stemmers were trained on the same data as during the inflection removal experiments (see Section 5.3), this means on 50k, 100k, 500k, 1M, 2M, 5M, 10M, and 15M tokens. Each language model was trained on 15M tokens. An additional 2M tokens were used as held-out data. Then, an additional 5M tokens were used to calculate the perplexity of the language model.


                           Fig. 3
                            shows the improvement in perplexity when compared to the baseline language model.

The experiments with language modeling confirm the conclusions of the preceding experiments. Again, HPS obtains on average the highest perplexity drops. GRAS and YASS swapped the order of their results. In this test, YASS tends to perform better than GRAS. For smaller training data sizes, HPS has no competitor. The second stage of HPS again produces large improvements especially when little training data is used.

In this section, we investigate some possible tweaks which decrease the computational costs of the stemming. These tweaks have virtually no impact on the stemming results.

Firstly, it must be remembered that our modification of maximum mutual information clustering (described in Section 4.1) gives accurate results only for words which occur frequently enough in a corpus. The infrequent words have a negligible impact on the average mutual information of the data, and their clustering may be very inaccurate, and can even decrease the quality of the whole stemmer. We recommend clustering only words with a frequency higher than some threshold (for Example 10) by the MMI algorithm. The remaining words should be clustered using only lexical information (presented in Section 4.1.1). Moreover, the complexity of the clustering increases with the square of the number of words being clustered. Thus, limiting the words being clustered has a positive impact on the processing time.

An additional acceleration of our algorithm is possible by incorporating just enough occurring word bigrams in the calculation of the average mutual information of the data. We recommend incorporating word bigrams that occur at least 2 or 3 times in the training corpus. This also leads to a significant speeding up of the clustering, while the efficiency of the final stemmer stays almost the same.

We would like to point out that setting these parameters has no impact on the quality of the stemming results. Setting these parameters is not mandatory.

For the sake of a better grasp of these efficiencies, we also present the running times needed to train HPS, as well as that needed for the stemming itself. We implemented our HPS method on the Java™ platform, and tested it on a computer with a Core i7 3.4GHz processor. Training the model on 5M tokens of Czech data (the same model used for the experiments with the above recommended settings, i.e., word frequency at least 10 and word pair frequency at least 2) takes about 36min. The stemming of 10M tokens of text with this model takes about 33s. These results clearly show that once we already have a trained model, the stemming itself is very fast.

@&#DISCUSSION@&#

In Section 5, we thoroughly tested our HPS from three different perspectives. To put the performance of HPS into the context of the state of the art, we also provided a comparison with other competitive stemmers (namely GRAS, YASS, Linguistica, and rule-based stemmers). In addition, we extended these competitive stemmers with our second stage of HPS (maximum entropy classifier). During our experiments, we also measured the amount of training data needed to give satisfactory results.

The first experiment (Section 5.4) was focused on evaluating how well the stemmers remove the inflection of words by comparing classes with the same stem with classes with the same lemma obtained from manually annotated data. HPS was at the top for all languages, only for Spanish and Hungarian did GRAS performed equally well.

The second experiment (Section 5.5) investigated the use in an information retrieval task. Retrieval effectiveness was tested on Czech, Hungarian, Spanish, and English, for both previously seen and unseen data. Significance testing was done to make the results more appropriate. It was discovered that our stemmer provides significantly better retrieval scores than competitive stemmers in the case of indexing new (unseen) data. HPS tends to be more suitable for languages with rich morphology (Czech, Hungarian) than other stemmers, because of the significant OOV (out-of-vocabulary) rate. For Spanish and English the significance testing proved that there is no significant difference between stemmers for both seen and unseen data. However, such results could have been easily predicted since the stemming of less inflected languages does not play as important a role in the IR task as the stemming of highly inflected languages.

The last experiment (Section 5.6) examined the effect of using stemming in language modeling. Class-based models were derived from stemming results and they were coupled with a baseline n-gram language model. Again, for highly inflected languages, HPS performed best of all. For less inflected languages, HPS is on the same level as YASS, but again, stemming does not play a key role here. It is interesting that GRAS, which in previous experiments performed very well, gives one of the worst results in language modeling. We suppose this is caused by the fact that GRAS is too focused on the recall rate and so it often overstems the words.

We explain the superior performance of HPS by its building a stemmer in two stages. The first stage uses the idea of involving latent semantic information in the stemming task. Other approaches deal with the problem on a purely lexical level. By involving semantic information, our stemmer can better decide about the appropriateness of removing different suffixes. The suffixes that can alter the semantics of the words should be left intact in our approach. For example, compare the words spar and spar-ing. From the lexical point of view, there is no reason to leave the suffix ing intact. However, from the semantic point of view, spar and spar-ing are completely different words. Naturally, semantic information retrieved from the statistical comparison of word contexts is not flawless. Nevertheless, the increased performance of HPS indicates that latent semantics is beneficial in the stemming task. Although, the idea to build a stemmer in two stages is not new (in Xu & Croft (1998), the co-occurrence statistics were used to refine equivalence classes given by some aggressive stemmer, decreasing number of overstemming errors), our second stage goes deeper. It is not limited to work with aggressive stemmers only. The second stage of HPS extracts rules from the word clusters given by the first stage, and combines these rules using a maximum entropy classifier. These rules were proved by our experiments to be very general and efficient, because HPS is able to stem previously unseen word forms.

The preceding paragraph relates to the question of whether a stemmer should or should not remove derivational suffixes. Firstly, it again depends on the task. As we already illustrated, reducing the word friendly to the word friend may be useful for IR but not for machine translation. Secondly, the question also relates to the semantics. We can generally say that we should remove a suffix only in cases where they do not change the meaning of the stemmed word. We believe that employing some semantic information is appropriate, given this perspective.

By taking all the results into account, HPS seems to be the most effective and most universal approach for stemming aimed at languages with rich morphology. GRAS is very efficient in IR tasks and even performs well in inflection removal experiments. YASS provides consistently good results and so it is also very universal. Linguistica was not as efficient as the other stemmers in our tests.

A very positive fact proved by our experiments is that HPS requires only a small amount of training data to give very satisfactory results. This property is caused by our second stage of HPS, the maximum entropy classifier. It was shown that this second stage also improves other stemmers: they are denoted by GRAS+HPS, YASS+HPS, and Linguistica+HPS, when supplemented by this second stage. In this regard, HPS has no competitor. Other stemmers perform poorly if they have little data for training. Our experiments show that a corpus of only 50,000 tokens is sufficient for efficiently training HPS. The corpora of 1,000,000 tokens seem to be more than enough for training, because when increasing the amount of the training data, the results do not improve significantly. The explanation is as follows. In our experiments, we classify only to 4 classes (4 possible lengths of suffix, i.e., from 0 to 3 characters) and we use only a few feature functions, which is something that leads to needing only a small number of parameters to be estimated from the training data. The rules formulating the various endings of word forms are supposed to be common and often repeated in a corpus, and this is the reason why HPS performs well even with as small a training dataset as 50,000 tokens. These facts also explain why the performance of HPS does not improve as much as in the case of other stemmers with increasing amounts of training data.

It may seem that in an era of vast linguistic resources, such a property would be insignificant. However, we would like to point to the idea presented in Hammarström and Borin (2011). There are a huge number of languages that have a very limited number of speakers. For such languages, rich linguistic resources are not obtainable. Our stemmer should be successful particularly in this area. We plan to target these languages in the future.

As stated at the beginning of our paper, precision is preferred over recall in our approach. Thus, it is possible to call our stemmer light. It was proven (for example in Dolamic & Savoy (2009), Savoy (2008)) that aggressive stemmers usually perform better in the retrieval context than light stemmers. By more aggressive stemming, the recall rate is increased and the size of the storing index is decreased at the same time. However, it was not our aim to create an unsupervised stemmer focused solely on information retrieval, but to design a multi-purpose stemmer performing well in multiple scenarios without any modification.

@&#SUMMARY@&#

The contributions of the paper are the following:
                           
                              •
                              We present a new approach to stemming that has a very universal scope. It outperforms the state of the art in unsupervised ways of stemming in the inflection removal test, the information retrieval test with unseen data, and the language modeling task. In the information retrieval test with seen data, it provides comparable results with the state of the art.

Our proposed method is by far the most effective one when little training data are available.

We provide tests on four language families (six different languages).

In the article, we deal with several aspects of the stemming problem, such as the difference between removing inflectional and derivational suffixes, and the relation of stemming to lemmatization.

We also introduce a novel evaluation method for comparing stemmers, which improves on the method presented in Paice (1994) in several ways.

@&#FUTURE WORK@&#

In future work we would like to focus on the analysis of words where the inflected forms are formed by changing significant parts of the words, not just suffixes. This is essentially the weakness of all stemmers. A representative example are the irregular verbs in English. In most languages, the verbs in different tenses often differ in large parts of the words.

We suppose that the rules causing these inflections often repeat in natural language texts and so there has to be a way to discover them. We plan to design more elaborate rules for finding candidates for words with the same stem. In this way, many of the current stemming mistakes can be resolved, thus enhancing the performance of our stemmer.

Another possibility for future work is to investigate different ways of modeling semantic relations. Semantic spaces, which are quite a new branch of corpus statistics, could be used during clustering (Section 4.1) instead of the mutual information loss algorithm. Note that we already experimented with semantic spaces for improving language modeling (see Brychcín & Konopík (2014)).

Also, we plan to target languages with a limited number of speakers and limited linguistic resources in order to prove that our stemmer is suitable for them.

Finally, we would also want to test our stemmer in other scenarios. We have already provided the stemmer to our colleagues. In Habernal, Ptáček, and Steinberger (2013), Habernal and Brychcín (2013), Habernal, Ptáček, and Steinberger (2014), Steinberger, Brychcín, and Konkol (2014), they discovered that our stemmer significantly improves sentiment analysis. Our preliminary results indicate that HPS is also very useful (and significantly better than competing stemmers) in named entity recognition and machine translation tasks.

@&#CONCLUSION@&#

In this article we presented a very effective stemming method that further shifts the boundaries of the current state of the art in unsupervised stemming. We successfully accomplished the goal of creating a multi-purpose stemming tool. Its design opens up possibilities for solving non-traditional tasks, such as approximating lemmas, improving language modeling or sentiment analysis. However, it still provides very good results in the traditional task, information retrieval.

Our approach learns morphological rules from an unannotated corpus without any knowledge about the language or any additional information. A clustering method that discovers semantically related words creates the basis for learning the stemming rules. These rules successfully approximate the morphology of a language and can be used even for unknown (previously unseen) word forms. Our experiments show that the stemming of unknown words is as effective as the stemming of known words, which is essentially one of the greatest advantages of our stemmer compared with other competitive stemmers.

The second very positive property of our approach is that it does not require a huge amount of training data. Our experiments confirm that for successful training, a corpus of only one million tokens is sufficient. Very satisfactory results can be, however, achieved with only 50,000 tokens for training, where other stemmers fail. This property could be very important, mainly for poor-resource languages.

Even in the cases where other stemmers have enough data for training, HPS does not lose. The comparison with other stemmers (namely GRAS, YASS, Linguistica as well as with rule-based stemmers) was done on several languages, including highly inflected languages as well as less inflected languages. In the stemming of less inflected languages, there is no significant difference between the stemmers that have been tested. The stemming however play a key role for highly inflected languages, where our stemmer is significantly better than competing unsupervised stemmers and approximately on the same level as rule-based stemmers. Our HPS implementation is available at https://liks.fav.zcu.cz/HPS.

@&#ACKNOWLEDGEMENTS@&#

This work was supported by Grant No. SGS-2013-029 Advanced computing and information systems, by the European Regional Development Fund (ERDF) and by project “NTIS – New Technologies for Information Society”, European Centre of Excellence, CZ.1.05/1.1.00/02.0090. Access to the MetaCentrum computing facilities provided under the program “Projects of Large Infrastructure for Research, Development, and Innovations” LM2010005, funded by the Ministry of Education, Youth, and Sports of the Czech Republic, is highly appreciated. The access to the CERIT-SC computing and storage facilities provided under the programme Center CERIT Scientific Cloud, part of the Operational Program Research and Development for Innovations, reg. No. CZ. 1.05/3.2.00/08.0144 is acknowledged. We also thank the Czech News Agency for providing a huge number of texts in Czech.

@&#REFERENCES@&#

