@&#MAIN-TITLE@&#Eye blink detection based on motion vectors analysis

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Introduces eye blink detection algorithm while outperforming most of the related work.


                        
                        
                           
                           Introduces the largest real-world dataset on eye blink detection with more than 1800 annotated eye blinks.


                        
                        
                           
                           Proposes the way how to evaluate eye blink detection algorithms.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Eye blink detection

Motion vectors analysis

Statistical standard deviation

@&#ABSTRACT@&#


               
               
                  A new eye blink detection algorithm is proposed. Motion vectors obtained by Gunnar–Farneback tracker in the eye region are analyzed using a state machine for each eye. Normalized average motion vector with standard deviation and time constraint are the input to the state machine. Motion vectors are normalized by the intraocular distance to achieve invariance to the eye region size. The proposed method outperforms related work on the majority of available datasets. We extend the way how to evaluate eye blink detection algorithms without the impact of algorithms used for face and eye detection. We also introduce a new challenging dataset Researcher’s night, which contains more than 100 unique individuals with 1849 annotated eye blinks. It is currently the largest dataset available.
               
            

@&#INTRODUCTION@&#

Recently, there has been an increased attention to eye blink detection mostly because of face liveness detection (Pan et al., 2007; Szwoch and Pieniazek, 2012) (photo can not blink). Eye blinks are often used as a way of interaction between disabled people (Grauman et al., 2003) and computers. Eye blink frequency and duration are reliable signs of sleepiness (Dinges, 1998) that can be used to detect driver’s fatigue (Bergasa et al., 2006; Danisman et al., 2010) and eventually to prevent microsleep.

Application of this work can be for example a measurement of user’s blink rate with common cameras in order to monitor dry eye syndrome (Divjak and Bischof, 2009). One of the main causes of dry eye syndrome is low blink rate. A healthy human blinks 10 to 15 times per minute. 70% of computer users have decreased blink rate up to 60% (Blehm et al., 2005). Dry eye syndrome is often accompanied by symptoms like eyestrain, soreness, irritation, red eyes or blurred vision. Each blink renews the tear film which protects and moisturizes the eye.

Eye blink is defined as a rapid closing and reopening of eyelids. Endogenous eye blink typically lasts from 100 to 400 milliseconds (Stern et al., 1984). Mostly due to dry eye syndrome, only partially closed eye blink often occurs and this is called an incomplete blink (Portello et al., 2013). Extended blinks (Rodriguez et al., 2013) are the opposite when the eye closure (fully closed eye) lasts from 70 ms to around 1 s. Some people blink multiple times in a sequence; for example double blinks or even quadruple blinks can occur.

We focus on detection of endogenous eye blinks, thus a standard camera with 25–30 frames per second (fps) is sufficient. Eye blink detection can be based on motion tracking within the eye region (Divjak and Bischof, 2009). Some methods try to estimate the state of an eye (open, closed (Lee et al., 2010) or the eye closure (Garcia et al., 2012)) for individual frames which is consequently used in a sequence for blink detection. Other methods compute a difference between frames (pixels values (Kurylyak et al., 2012), descriptors (Malik and Smolka, 2014), etc.). We built our algorithm upon the successful method of Drutarovsky and Fogelton (2015) which is a motion tracking state-of-the-art method with very low false positive rate.

One of the major problems within eye blink detection algorithms is the data insufficiency and subsequent over-fitting on existing datasets. We collected new, more challenging datasets during an event called Researcher’s night where more than 100 unique people were recorded and 1849 blinks annotated. Another problem is the evaluation procedure that is often not specified within published algorithms. We propose an evaluation based on intersection over union metric to define the detected blink. We extend an annotation of individual videos with the face and eye corner positions. Consequently, performance of an eye blink detection algorithm is measured without an influence of used face and eye detection method.

@&#RELATED WORK@&#

Most of the methods are initialized by Viola and Jones (2004) type algorithm to detect face and eyes (Danisman et al., 2010; Garcia et al., 2012). Combined with region tracking (Brandt et al., 2004; Lee et al., 2010) is often preferred to achieve higher detection rate for non-frontal faces. Different approaches take place in eye blink detection.


                     Pan et al. (2007) use Conditional Random Field (CRF) to define eye blink as a sequence of following states: open, ambiguous, close, ambiguous, open. They design a low dimensionality feature, constructed by a linear ensemble of series of weak binary classifiers, which measure the eye’s openness. The ZJU eye blink database is introduced (80 short videos). The authors handle the insufficiency of data using the leave-one-out rule. CRF is trained on 79 videos and tested on the remaining one. The results are averaged over 80 test cases achieving detection rate of 95.7% and false alarm rate under 0.1%. The authors do not mention how the false alarm is calculated and in particular how the negative count has been defined. As Drutarovsky and Fogelton (2015) mention, the results can differ significantly, if the number of frames with opened eyes is used instead of the number of non-blinks.

Eye blinks can be also detected by measuring ocular parameters. For example by fitting ellipses to eye pupils (Bergasa et al., 2006) using the modification of the algebraic distance algorithm for conic approximation. The degree of eye openness is characterized by the pupil shape. To eliminate the inaccuracy of the algorithm for fitting ellipses, a state machine is defined to detect eye blinks. This method achieves precision of 80% on a dataset (not available) consisting of 10 videos of people while driving a car. The percentage of eye closure for drowsiness detection (Garcia et al., 2012) is calculated from the ratio between the iris height in the frame and the nominal value assigned during a 10-second calibration.

Another approach based on eye openness measuring is introduced by Lee et al. (2010), who observed that the number of black pixels in a closed eye image is higher compared to an open eye image and introduce two features derived from the binarized image. The first feature F1 represents the cumulative difference of black pixels in a detected eye region estimated from the binary image from consecutive frames. However, the number of black pixels is also influenced by the subject’s distance from the camera. To avoid this, the method uses an adaptive threshold based on a cumulative difference. The second feature F2 represents a ratio of eye height to eye width. To calculate F2, a binarized eye region is processed through erosion and dilation filters. The eye state (open, closed) is estimated by a maximum vertical projection of black pixels. An open eye has higher ratio, because of a higher maximum projection value. To estimate eye openness precisely, the authors use the features F1 and F2 as an input values to SVM. Three SVM classifiers are used for three different face rotations to determine the eye state of the subject. Method achieves recall around 92% on ZJU, Talking face and their own dataset (not available).


                     Suzuki et al. (2006) also measure the eye openness. First, eyelids need to be detected. Eye region is divided into several vertical sections. In each section, candidates for upper and lower eyelids are defined as the maximum and minimum differential values of the gray level distribution. These candidates are grouped into five sections. Two candidates are chosen to represent the upper and lower eyelid. All five sections are then used to calculate an eye gap – an average distance between the eyelid candidates. Eye gap is defined as a degree of eye openness. A blink waveform is formed over time. Eye gap decreases rapidly when the eye blinks. After eye gap reaches the minimum value (eye is considered closed), it increases gradually. This method is used for drowsiness detection in car environment while infrared pulsed projector is used. The method achieves blink detection rate of 95% on their own extensive dataset of 20 people (not available).

Symmetry can be used as a feature for the eye openness. A neural network-based detector is used for precise eye pupil localization (Danisman et al., 2010). The head rotation angle is calculated using vertical positions of both pupils. The region of interest is analyzed using horizontal symmetry to determine whether the eye is open or closed. The region is divided into two halves using the axial symmetry around the line crossing centers of both pupils. The halves represent the upper and lower eyelid regions. These halves are horizontally flipped. If the eye is open, then the horizontally flipped fragment preserves symmetry, unlike the closed eye, because of eyelashes. Therefore the difference between the upper and lower half is used as a discriminative feature to detect the closed eye. The algorithm is tested on ZJU dataset achieving precision of 90.7% and 71.4% of recall.

Open eye template is another approach to estimate eye closure and detect eye blinks (Grauman et al., 2003; Królak and Strumiłło, 2012). Eyes are detected using the correlation coefficient over time. Re-initialization is triggered by the correlation coefficient falling under the defined threshold. Based on the changes in correlation between the eye and its open eye template, an eye blink waveform is established. The correlation score is binarized; open and closed eye. Authors focus on people with disabilities. Unfortunately, datasets used in evaluation are not available.


                     Radlak and Smolka (2013) introduce enhancements to the Weighted Gradient Descriptor (WGD) (Radlak and Smolka, 2012), which is based on computing of partial derivatives per each pixel in the eye region over time. Feature vectors are averaged in two orientations (“up” and “down”) based on location. The vertical distance between their points of origin is used as the input waveform. Closing and opening of the eye is represented by negative as well as positive waveform peak. After noise filtering, zero-crossing point between the local maximum and minimum represents the detected eye blink. In this modification Gauss weighting is used to suppress eyebrow movements often falsely detected as eye blinks. The maximum and minimum of the entire waveform (for given video) are found and used to estimate proper thresholds, which reduces the usability for cameras. The authors report the best obtained results for given datasets while using different parameters. New dataset of 5 people recorded using Basler 100 fps camera is introduced, we will refer to it as Basler5. The reported detection rate on Basler5 is around 90% and 98.8% on ZJU dataset. Only the right eye of the subject is used for the evaluation.

Template matching using histogram of Local Binary Patterns (LBP) can be used to detect eye blinks (Malik and Smolka, 2014). First, the initialization takes place. An open eye template is calculated from several initial images where eye is open and not moving. For each image in a sequence, LBP histogram is computed from the eye region and compared with the template using the Kullback–Leibler divergence measure. The output is a waveform where noise is filtered out using Savitzky–Golay filter and the top hat operator. Afterward, peaks are detected using Grubb test and considered as eye blinks. Detection rate of 99% is reported on ZJU and Basler5 dataset (different parameters are used for each dataset). Because of the Grubb test, this method is not suitable for a real-time stream from camera.

Motion based eye blink detection algorithms do not depend on appearance like feature based methods. Divjak and Bischof use eyelid motion to detect blinks. Features are detected using FAST (Rosten and Drummond, 2006) and tracked with Lucas–Kanade tracker (Tomasi and Kanade, 1991). Features are classified using their location; face, left and right eye. Eye and face regions are tracked based on the features. The authors calculate a normal flow of the regions in the direction of intensity gradients. Eyelid motion also includes head movements, thus compensation based on the already extracted head movement takes place. Dominant orientations of the local motion vectors for the individual classes are extracted from a histogram of orientations, due to which partial invariance to eye orientation is achieved. To filter the eyelid motion, only the flow in the direction perpendicular to the line segment between the eyes is considered. The angle between this line and the horizon is calculated and flow vectors are transformed correspondingly. Corrected and normalized flow is used to calculate an average flow magnitude of the eye regions. The dominant flow direction is recognized based on the individual orientation of local motion vectors (optical flow) in a histogram with 36 bins, each bin representing 10 degrees. Normal flow orientation and magnitude is used as the input parameter for a state machine.

Lucas–Kanade tracker is also used by Drutarovsky and Fogelton (2015) to track the eye region. Six motion vectors are estimated from the eye region and an average motion and variance are calculated to be the input waveforms for a state machine. There is one state machine for each eye. The authors do not have to compensate the head movement as Divjak and Bischof (2009) because variance is invariant to it. Due to this, significantly lower false positive rate is achieved. Introduced dataset Eyeblink8 is characterized by vivid facial mimics of recorded people. Reported recall is 73% on ZJU and 85% on Eyeblink8.

We focus on endogenous blink detection of people in cluttered environment, we omit initialization or training or adjusting parameters for given dataset. Our goal is to detect endogenous eye blinks using camera real-time stream. Based on the review of the mentioned methods, we focus on the motion based method to achieve high invariance to appearance clutter such as glasses, head pose or facial expression. We build our method on motion analysis (Drutarovsky and Fogelton, 2015) to avoid head movement compensation.

We assume the eye corners are available for each frame if face is present. Eye region is represented with a circle defined by eye corners (Fig. 1
                     ). Eye corners are obtained from manually obtained annotation of datasets. We also use automatically localized eye corners by CLandmark (Uřičář et al., 2015) together with Viola and Jones (2004) face detection to measure time demands (Section 5.2).

Lucas–Kanade tracker often converges to strong corners and produces numerous outliers. While analyzing motion, it is desired to have even distribution of motion vectors. Therefore we use Farnebck (2003) algorithm to estimate motion vector for each pixel in the eye region. Visualizations of motion vectors during different kinds of movements are in Figs. 2
                        , 3
                         and 6.

Size of the eye region depends on camera resolution and subject distance from the camera. The eyelid trajectory (measured in pixels) increases with the region size, therefore the magnitude of motion vectors also increases. Eye region size correlates with the intraocular distance. Based on the Table 1
                        , we believe this dependency could be linear. We normalize each motion vector by the intraocular distance. The eye corners are used to calculate eye centers and subsequently the intraocular distance.

During head movements (Fig. 2), all motion vectors are similar in magnitude and orientation. On the other side, motion vectors differ in magnitude while eye blinks (Fig. 3). Further, we calculate the vertical component of the average motion vector μ and the statistical standard deviation σ using Eq. (1). We use the vertical component only because we assume that subject in front of the camera does not rotate their head significantly.

                           
                              (1)
                              
                                 
                                    μ
                                    =
                                    
                                       
                                          
                                             ∑
                                             i
                                          
                                          
                                             y
                                             i
                                          
                                       
                                       n
                                    
                                    ,
                                    
                                    σ
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                                i
                                             
                                             
                                                
                                                   (
                                                   μ
                                                   −
                                                   
                                                      y
                                                      i
                                                   
                                                   )
                                                
                                                2
                                             
                                          
                                          n
                                       
                                    
                                    ,
                                 
                              
                           
                        where yi
                         is the vertical component of ith motion vector. The waveform visualization of the vertical component of the average motion vector (individual motion vectors are already normalized) is in Fig. 4
                         and its standard deviation is in Fig. 5
                        . In these figures, eye blinks could be observed. Peaks in σ waveform could be used to detect eye blinks, but to verify or support the occurred eye blink, zero crossing in the μ waveform takes place.

Rapid head movements also cause higher standard deviation (Fig. 5). We believe it is because of low frame-rate when images get blurry during rapid movements. Consequently Gunnar–Farneback algorithm does not have precise translations to compute motion vectors correctly. Fig. 6
                         presents a visualization of motion vectors during rapid head movement.

The μ and σ waveforms together with the Δt (the time difference between captured consecutive frames) for a given video stream are the input to the state machine. The state machine is designed manually observing the μ and σ waveforms of the train and validation set of the Researcher’s night dataset. σ increases mostly during eye blink, rapid head movement or pupil movement (Fig. 6). Eye blink is a sequence of eye lid moving down and up (zero crossing in μ waveform), so we use this characteristics while designing the state machine. The state machine (Fig. 7
                        ) consists of the following states: the initial state (0), the eye lid moves down state (1), the eye lid moves up state (2) and the eye blink detected state (3). The major movement comes from the upper eye lid, that is why the states are named after it.

Different people blink differently, for example the speed of eye lid movement differs. Some people do not close their eyes completely or the camera captures smaller amount of frames per second. We observe the eye lid movement as the sequence of σ with ∑
                           σ
                         > 3T, where T is the threshold of the minimum σ. At the same time the magnitude of the vertical component of the average motion vector (μ) is also higher than the threshold T, while summing σ in the given direction. The μ orientation is important, it tells us about the movement direction. Different thresholds for σ and μ could be used, however based on our experiments and observation on Researcher’s night training and validation sets, we find one threshold sufficient.

Our state machine works sequentially, while entering the State 1, the starting frame ID is remembered. If the state machine returns to the initial state without previous eye blink detection (State 3), the next frame considered is not the one where the state machine ends up, but the increment of the remembered starting frame ID. The starting frame ID is also remembered to precisely detect the eye blink interval. The end of the eye blink interval is the frame processed while entering the State 3.

The simplified state machine is visualized in Fig. 7. Each transition has an order number. This way the number of conditions is minimized. The first transition in each state is not shown, it is the time constraint condition. In each transition a time variable is incremented with Δt. If the ∑
                           Δt
                         is above the threshold Tt
                        , the state machine returns to the initial State 0. In our experiments we focus on endogenous blinks which last 400 ms in maximum, we set 
                           
                              
                                 T
                                 t
                              
                              =
                              500
                           
                         ms, because of possibility of inaccurate time measurement.

If a partial eye lid movement down ((μ > T) and (σ > T)) occurs while in State 0, the state machine will change to State 1 and ∑
                           σ
                         is initialized with σ. While in State 1 with Σσ
                         > 3T (sufficient eye lid movement down) and partial eye lid movement up (
                           
                              (
                              μ
                              <
                              −
                              T
                              )
                           
                         and (σ > T)) is detected, the state machine changes to State 2 and ∑
                           σ
                         is initialized with σ (eye lid movements are evaluated independently). While in State 1 with Σσ
                         < 3T (non sufficient eye lid movement down – probably some fast head movement) and partial eye lid movement up is detected, the state machine changes to State 0. While partial eye lid movement is detected in State 1 and 2, given Σσ
                         is incremented by σ, otherwise anything else condition increments only the time spent in the state machine by Δt.

Because of noise and small head movements, partial eye lid movement is considered differently during State 1 and 2. The condition on μ is lowered (see Fig. 7). μ or σ under threshold T occurs not only while the eye is fully closed, but sometimes also during the eye lid movement. That is the reason why self transitions occur multiple times in the scheme.

There are two separate state machines, one for each eye. Eye blink is considered detected, if at least one of the state machines triggers the eye blink detected state. The problem is how to merge detected eye blink intervals from both eyes to acquire better precision in eye blink duration. The left and right blink is considered as one, if the intersection over union (
                           
                              I
                              O
                              U
                              =
                              (
                              A
                              ∧
                              B
                              )
                              /
                              (
                              A
                              ∨
                              B
                              )
                              ,
                           
                         where A, B are eye blink intervals) is higher than the given threshold. In our experiments if IOU > 0.2, the left and right blinks are merged together (averaged) to get more precise intervals (Fig. 8
                        ). If 
                           
                              I
                              O
                              U
                              <
                              =
                              0.2
                              ,
                           
                         we assume the left and right blinks represent two independent blinks (Fig. 9
                        ). This is important mostly while detecting incomplete or multiple blinks. Some people blink very fast twice in a row (double blink) or even four times in a row. If multiple blinks are supposed to be detected as one, different settings should take place.

The situation gets more complicated when there is a blink that has intersection over union with multiple blinks of the other eye as in Fig. 10
                        . In this case the largest IOU is found to calculate the average blink interval between them. No frame could be part of two blinks at a time. We enforce consistency of blink intervals (this adjustment occurs rarely).

Most of the presented methods use their own datasets which are mostly not available (Section 2). Even if the datasets are available, the original annotation is not (ZJU, Talking face) and authors often create their own ground truth annotation (Drutarovsky and Fogelton, 2015; Radlak and Smolka, 2012).

We introduce a new dataset which was collected during an event called Researcher’s night 2014. People were asked to read some articles on computer screen or blink while being recorded. We collected 107 videos of different people with cluttered background. People are often acting naturally, wearing glasses (around 20%), touching their face, moving head or even talking to somebody. Some of the blinks are unnaturally long, which can be considered as voluntary (people knew they are being recorded) or extended blinks. Because of image acquisition instability, which is caused mostly by insufficient light conditions, we recorded the frame acquisition time. The difference between two frames of eyelid position increases with Δt.

There are two datasets: Researcher’s night 15 and Researcher’s night 30, that are captured with 15 and 30 frames per second (fps) with resolution 640 × 480. Small deviations can occur. Severe CPU usage can cause some frames to be postponed before delivered to the recording software or not delivered at all. For example video 10 in test set of Researcher’s night 30 has only 20 fps for the first 3 seconds. This is the reason why time-stamp information could be crucial for successful detection. On the other side, we observed that sometimes the same frame could be delivered twice to fulfill the device driver requirement to capture video stream at given frame-rate or it is an error of the encoder (we used x264 vfw) or codec (FFmpeg in OpenCV 2.4.6). These cases happens rarely, mostly during bad light conditions.

While recording, H264 codec was set to the baseline profile level 3, which is primarily used for lower-cost applications with limited computing resources like video conferencing or mobile applications. The quantizer was set to 23 (range 0–51). Therefore, the video quality corresponds to common video quality, which could be acquired using a mobile phone. Dataset is divided into train, validation and test set with ratio 1/4, 1/4 and 1/2. More precise information about the number of annotated frames for individual sets are presented in Table 2
                        .

Often, there are more people in front of the camera. Therefore, annotations are made for individual subjects. Often failure of eye blink detection algorithms is caused by an incorrect input of face or eye detector. To avoid this, we also include the face and eye corner positions to each annotated frame. Frames are annotated if the subject’s eyes are visible. Several annotators worked on annotations. Light inconsistencies occur, for example a face looking sideways is not always annotated. Sometimes it was hard to say whether the subject blinked or just squinted. All annotations are checked by one person to achieve as high consistency as possible. We believe this real-world dataset can help researchers to develop more precise algorithms.

Annotations of face bounding box and eye corners are obtained manually by annotators. Annotators were instructed to maintain precision 5 px maximum out of the correct eye corner locations, sample can be seen in Fig. 11
                           . Thanks to these annotations, evaluation of eye blink detection algorithm will be no more influenced by the face or eye detector. We provide the information about the eye blink interval with the information of whether eye is fully closed or not, separately for individual eyes. The annotation consists of following information: frame ID : blink ID : NF : LE_FC : LE_NV : RE_FC : RE_NV : F_X : F_Y: F_W : F_H : LE_LX : LE_LY : LE_RX : LE_RY: RE_LX : RE_LY : RE_RX : RE_RY.

                              
                                 1.
                                 
                                    frame ID – Frame counter based on which the time-stamp can be obtained (separate file).


                                    blink ID – Unique blink ID, eye blink interval is defined as a sequence of the same blink ID frames.


                                    non frontal face (NF) – While subject is looking sideways and eye blink occurred, given variable changes from X to N.


                                    left eye (LE), right eye (RE)


                                    eye fully closed (FC) – If subject’s eyes are closed from 90% to 100%, given flag will change from X to C.


                                    eye not visible (NV) – While subject’s eye is not visible because of hand, bad light conditions, hair or even too fast head movement, this variable changes from X to N.


                                    face bounding box (F_X,F_Y,F_W,F_H) – x and y coordinates, width, height
                                 


                                    left right corner positions – RX (right corner x coordinate), LY(left corner y coordinate)

During annotation process, we found out that some people have a tendency to blink differently with individual eyes. For example, one eye could be already fully closed but the other is not. In our dataset double blinks occur but more interestingly one subject has triple and quadruple blinks. The individual eye blinks are distinguished from multiple blinks in our annotations. Challenging eye blinks are annotated either with non-frontal face tag or eye not visible for the given eye. This information is not so consistent because of different annotators. Mostly some eye blinks are not augmented by this information even if they should be, usually the non-frontal face tag. Fully closed tag for the individual eyes is also not so consistent. For now we do not use frames or blinks in which non-frontal face or one of the eye is not visible in our evaluation. Our aim was to develop extensive dataset for other types of eye blink detection algorithm which for example use closed eye detection to detect eye blinks.

Dataset Talking face
                        
                           1
                        
                        
                           1
                           
                              http://www-prima.inrialpes.fr/FGnet/data/01-TalkingFace/talking_face.html [accessed: 7.3.2015].
                         consists of images of one subject sitting and talking in front of the camera. There are 5000 frames captured with 25 fps with resolution 720 × 576, in total 200 s of video stream. Dataset is annotated with 68 facial landmarks. Original ground truth annotation of eye blinks does not exist. We annotated 61 eye blinks as related work (Divjak and Bischof, 2009; Drutarovsky and Fogelton, 2015; Radlak and Smolka, 2012). The intervals of individual eye blinks can differ because we decided to do completely new annotation. We use the eye corner locations from the original facial landmarks annotation.


                        ZJU dataset (Pan et al., 2007) consists of 80 videos, each lasting few seconds. 20 individuals are recorded using four clips (frontal view with and without glasses (2 types) and upward view) stated to be captured at 30 fps with resolution 320 × 240. Different numbers of the ground truth eye blinks are reported for this dataset by related work and summarized in Table 3
                        . This is because different annotators could consider as eye blink also an eye opening or eye closing. These often occurs in this dataset at the beginning and at the end of a video. We report double blinks as two individual eye blinks, that is why we report 261 eye blinks instead of 255 as reported in most of the related work. There are also very short eye blinks (2 frames long for example). Based on this finding, we believe that videos are not captured using 30 fps the entire time. On the other side there are also very long (20 frame long for example) eye blinks that can be considered as intentional and not endogenous eye blink. Subjects in the dataset are still, almost without any head movement during the recording.

There are 408 eye blinks on 70,992 annotated frames with resolution 640 × 480. People are sitting in front of the camera and mostly act naturally with vivid facial mimics, similarly to Talking face dataset. Drutarovsky and Fogelton (2015) talk about 353 eye blinks, which is probably a mistake. Based on the original annotation, there are 390 eye blinks. Our annotation type differs, which was the reason to create a new annotation augmented with face bounding boxes, eye corner positions and some additional data as explained in Section 4.1.1. We compared the original annotation with ours and we notice that mostly some small (fast) eye blinks are not annotated in the original annotation and a few situations are considered differently.

This dataset is captured with a high speed Basler camera using 100 fps with resolution 640 × 480. There are five subjects captured in close distance and controlled environment. We annotated the dataset with face bounding box and eye corner positions. Because of converting the dataset images into compressed video, we lost few last frames from the dataset because of the used codec (around 30 last frames of each video). We annotated 58, 884 frames from the original count of 59, 031. We converted the blink interval annotations from the original one. The precision differs from ours. The blink intervals are not annotated so precisely as ours. The annotated blink interval usually starts several frames before the eye blink starts and ends a few frames after it ends. Another difference is what to consider as double blink. The original annotation uses this term also for micro stops of an eyelid while it moves down. Sometimes one frame could be a part of two consecutive blinks in the original annotation. Also, these blinks are not considered as double blink even if they are very close to each other.

@&#EVALUATION@&#

What is the true positive while talking about eye blink detection? Drutarovsky and Fogelton (2015) create an interval of three frames around the detected eye blink. Each detected blink has seven frames (an average eye blink length at 30 fps) in this case and if there is any intersection with the ground truth, eye blink is evaluated as detected. Radlak and Smolka (2013) consider eye blink as detected if the detected eye blink peak is between the start and end frame of the ground truth annotation. We use the intersection over union metric. If the detected eye blink has IOU greater than 0.2 with the ground truth eye blink, it will be considered as true positive. We use precision-recall curves to evaluate our method on all available datasets (Fig. 12
                     ) using the following equations:

                        
                           (2)
                           
                              
                                 Precision
                                 =
                                 
                                    
                                       T
                                       P
                                    
                                    
                                       T
                                       P
                                       +
                                       F
                                       P
                                    
                                 
                                 ,
                                 
                                 Recall
                                 
                                    (
                                    T
                                    P
                                    rate
                                    )
                                 
                                 =
                                 
                                    
                                       T
                                       P
                                    
                                    
                                       T
                                       P
                                       +
                                       F
                                       N
                                    
                                 
                                 ,
                              
                           
                        
                     where TP is True positive count, FP is False positive count and FN is False negative count.


                     Fig. 12 shows the difference in detection performance over datasets. The time constraint used as a threshold parameter in state machines for the maximum eye blink duration is 0.5  s. This threshold is higher than the maximum length of the endogenous eye blink (400 ms) because of the time stamp inaccuracy (discussed earlier in Section 4.1). All available datasets are recorded in controlled environment and are not so challenging compared to Researcher’s night. At the end, the curve has a tendency to go left, because in our evaluation we assume that one frame belongs to one detected eye blink only. Because of this, individual TP and FP eye blinks influence each other. In Table 4
                      we show the best parameter choices of T for given datasets.

@&#DISCUSSION@&#

We assume that the right choice of T also depends on camera acquisition speed and Gunnar–Farneback tracking precision (which depends on the camera speed too) because they influence the magnitude of motion vectors. Based on the results in Table 4, where different thresholds T achieve the best results on different datasets, we assume it is also the result of our wrong assumption that the magnitude of motion vectors is linearly dependent on the intraocular distance. Probably this dependency is not linear or it is also influenced by the Gunnar–Farneback tracker. The intraocular distance depends on the image resolution and the subject’s distance from the camera. For example ZJU is recorded at 320 × 240 and Talking face at 720 × 576, which almost represent the marginal values for the parameter T. Other datasets are in 640 × 480 resolution, but in Basler5 subject’s face is very close to the camera and 100 fps significantly change the situation. In Eyeblink8 and Researcher’s night the distance from the camera varies.

Most of the false negatives are caused due to too long eye blinks (often caused on purpose) and small (below threshold 3T) eye lid movement up or down (incomplete eye blinks), which can be caused by the improper waveform normalization as described before. For example if we extend the time constraint to 0.7 s on ZJU dataset we get just two false negatives and zero false positives (one is part of a double blink and the other is even longer). If we extend the threshold even further to 0.8 s we do not detect only one incomplete blink, which is part of a double blink, but we get one false positive caused by head motion.

Our method has difficulties to detect eye blink that occurs during the opposite head movement. There are multiple aspects causing lower results on Researcher’s night, mostly non-frontal faces, variable distance of subjects from the camera, very challenging multiple blinks or differently aged subjects with thick glasses frame and strong reflection on them. Most of the false positives are caused by fast head or eye pupil movements up and down.

We bring comparison on automatically localized eye corners using Viola–Jones (OpenCV implementation) face detection for each frame followed by CLandmark (Uřičář et al., 2015). CLandmark detects facial landmarks including eye corners that are the input for our method. We localize eye corners automatically for ZJU, Basler5, Eyeblink8 and Talking face dataset because there is only one user at a time. The results (Table 5
                        ) are a bit worse compared with manual annotation (Table 4) and still significantly outperforming the related work on most datasets (Figs. 13,14,15
                        
                        
                        ).


                        Table 6
                         compares the processing time while localizing the eye corners annotation with our blink detection method on different datasets. Presented times are measured on Intel core i5 3.3 GHz with CPU usage from 25 to 50%. Overall processing time (face, facial landmark and blink detection) is under 20 ms per image, which suits this method for real-time use.

The number of applications for mobile platforms increases dramatically. Eye blink detection method could be interesting for emotional or healthcare functionalities. An android version of the algorithm was created with Android Native Development Kit to measure the processing time requirements on ARM processor. The input is the built-in camera using a resolution of 640 × 480 pixels. We tested on ARM Cortex-A9 (Exynos 4412, 4 × 1.4 GHz) with NEON instructions enabled. OpenCV implementation of Viola–Jones allows us to detect only the biggest face, which speeds up the face detection dramatically. Therefore, we set three different face distances from the camera to measure the time demands of the individual algorithms. Table 7
                         shows that it takes around 120 ms to process one image which results in performance about 8 fps. Time required for blink detection increases, because the number of motion vectors increases with the eye region size.

We have significantly outperformed most of the related work using manual or either automatically obtained annotations. Fig. 13 compares the results on ZJU dataset, while Fig. 14 shows precision-recall curve on Talking face dataset and Fig. 15 compares results on Eyeblink8 dataset. Individual comparison on Basler5 dataset is in Table 8
                         using only manual annotations. The related methods report their results on individual videos separately while using the same parameters.

Unfortunately, precise comparison of our method to related work is not possible. First, most papers do not provide information on evaluation process. Almost all methods use their own annotations. No method provides precision-recall curves, there is usually one point from it and sometimes the parameters for individual datasets are adjusted. Even if other metrics are provided like Mean accuracy or False positive rate, negatives; they are not specified (Drutarovsky and Fogelton, 2015). Only two combinations method–dataset achieve similar results; Radlak and Smolka (2013) on ZJU and Malik and Smolka (2014) on Basler5. Radlak and Smolka do not merge detected eye blinks from both eyes. They use only the information from the right eye on both ZJU and Basler5 dataset. Malik and Smolka achieves a bit higher performance on Basler5 dataset, but compared to our method they initialize for each video (person) individually.

@&#CONCLUSION@&#

We have brought an overview of all important methods focusing on eye blink detection. We have outperformed almost all methods on available datasets. For example, on Eyeblink8 we achieve higher precision by 10% over Drutarovsky and Fogelton (2015) while similar recall. We have used Gunnar–Farneback tracker to omit outliers and to obtain evenly distributed motion vectors. Uniform distribution is important during motion analysis. We have normalized the motion vectors to achieve partial invariance to the person’s distance from the eye region size. We have designed very robust state machine using as input a normalized average motion vector and standard deviation with just one parameter, which detects complete eye blink intervals. We have found standard deviation more appropriate to analyze eye blink motion vectors compared to variance.

Available datasets within this area were quite limited. We created the largest annotated real-world dataset Researcher’s night, containing more than 100 people and 1849 eye blinks. We have proposed an annotation that also includes the face bounding box and eye corner positions. Thanks to this annotation, the evaluation of eye blink detection algorithms is no more influenced by face or eye detector. All annotations and datasets are available for academic use on demand. More over we have discussed different evaluation procedures and proposed one that should unify the evaluation of eye blink detection algorithms. We have evaluated our method on all available datasets outperforming most of the related work.

@&#ACKNOWLEDGMENTS@&#

This work was partially supported by Slovak Scientific Grant Agency VEGA 1/0625/14 and Tatra Bank Foundation e-talent 2012et009. My thanks belong to people who helped with recordings and mostly annotating the Researcher’s night dataset: Magdaléna Novotná, Lenka Kutlíková, Mária Mikulová, Ján Handzuš, Dominika Červeňová, Peter Šarišský, Tomáš Drutarovský. My thanks for wonderful comments belong to Matej Makula.

@&#REFERENCES@&#

