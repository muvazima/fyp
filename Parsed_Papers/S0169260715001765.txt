@&#MAIN-TITLE@&#Breast cancer detection and classification in digital mammography based on Non-Subsampled Contourlet Transform (NSCT) and Super Resolution

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a new algorithm for breast cancer detection and classification.


                        
                        
                           
                           Our system can accurately detect the probability of benign or malign breast lesion.


                        
                        
                           
                           We utilize NSCT transform and super resolution to improve the quality of the images.


                        
                        
                           
                           The results on MIAS database show significant performance of the proposed method.


                        
                        
                           
                           Our system achieves 91.43% and 6.42% as a mean accuracy and FPR, respectively.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Computer-Aided Diagnosis (CAD) system

Breast cancer

Mammography

Non-Subsampled Contourlet Transform (NSCT)

Super Resolution (SR)

BI-RADS

@&#ABSTRACT@&#


               
               
                  Breast cancer is one of the most perilous diseases among women. Breast screening is a method of detecting breast cancer at a very early stage which can reduce the mortality rate. Mammography is a standard method for the early diagnosis of breast cancer. In this paper, a new algorithm is proposed for breast cancer detection and classification in digital mammography based on Non-Subsampled Contourlet Transform (NSCT) and Super Resolution (SR). The presented algorithm includes three main parts including pre-processing, feature extraction and classification. In the pre-processing stage, after determining the region of interest (ROI) by an automatic technique, the quality of image is improved using NSCT and SR algorithm. In the feature extraction part, several features of the image components are extracted and skewness of each feature is calculated. Finally, AdaBoost algorithm is used to classify and determine the probability of benign and malign disease. The obtained results on Mammographic Image Analysis Society (MIAS) database indicate the significant performance and superiority of the proposed method in comparison with the state of the art approaches. According to the obtained results, the proposed technique achieves 91.43% and 6.42% as a mean accuracy and FPR, respectively.
               
            

@&#INTRODUCTION@&#

Breast cancer is known as one of the important diseases in medical science in which its early detection can reduce mortality rate and improve the survival rate of the patients. Mammography is the only broadly accepted and used screening test for early breast cancer detection. Two cases of the most prevalent symptoms of breast cancer are masses and calcifications. Since some calcifications are very small and the density difference between the healthy tissue and masses may be very low, diagnosis process will be difficult. Therefore, considering the importance of the accurate diagnosis, computer-aided diagnosis (CAD) techniques were presented in recent years [1–11] to help physicians and also reduce False Positive Rate (FPR) to perform diagnosis action faster, more easily and more accurately.

CAD systems generally include three main steps containing pre-processing, feature extraction and classification. In the pre-processing stage, regions of interest (ROIs) which contain suspicious regions are usually detected and localized. Then, the quality of mammographic images is improved. Several techniques have been proposed for pre-processing such as thresholding [12–14], region-based techniques [15–17] and edge detection techniques [18,19]. In the feature extraction stage, the features are extracted from mammographic images so that the system can correctly classify benign and malign lesions. Among the proposed feature extraction methods, Gabor Filter [20], Zernike Moments [21] and Wavelet Transform [22–29] are more popular than the others. In the classification as a final stage, suspicious regions are classified to two groups of benign or malign lesions. There are several methods for classification such as Artificial Neural Networks [2,14,20,27], Nearest Neighbor [30], Fuzzy [25,31] and Support Vector Machine (SVM) [3,7,13,15,30,32].

Timp and Karssemeijer [1], developed CAD techniques to study interval changes between two consecutive mammographic screening rounds. They proposed methods for the detection of malignant masses based on the features extracted from single mammographic views. They focus on improving the detection by including temporal information in the CAD techniques. Zhang et al. [2], presented a method for developing a fully automated CAD system to help radiologist in detecting and diagnosing micro-calcifications. The purpose of this research is to increase the effectiveness and efficiency of the screening procedures and also extract and analyze the characteristics of vary lesions in an objective manner and then improving the diagnostic accuracy. They used automatic segmentation, feature extraction, suspicious area detection and neural network for classification in their CAD system. Tzikopoulos et al. [3], also presented a CAD system for a fully automated segmentation and classification based on breast density estimation and detection of asymmetry. They firstly utilized image pre-processing and segmentation techniques. Then, the features for breast density categorization are extracted, including a new fractal dimension features and finally SVM is employed for classification. In addition, Miranda and Felipe [4] proposed a method based on fuzzy logic to improve the representation of the features corresponding to the image description in order to make it semantically more consistent. Moreover, they developed a CAD system for automatic categorization of breast lesions. The state of the art CAD techniques for breast cancer detection and classification are summarized in Table 1
                     .

Among the mentioned approaches, Wavelet Transform is one of the well-known methods in this research field which in addition to extract the features, is able to determine breast lesions [22–29] due to correspondence of the lesions on high frequency components [33]. Despite various applications of Wavelet Transform in image processing task, this transform has limitation in capturing directional information in images such as smooth contours and the directional edges. Contourlet Transform (CT) which has been presented by Do and Vetterli [34] is a developed version of Discrete Wavelet Transform for solving these problems. Contourlet Transform has further properties such as directionality and anisotropy in addition to properties of Wavelet Transform. Although Contourlet Transform is more powerful technique than Wavelet Transform in image representation, is not shift-invariant due to down-sampling and up-sampling. Non-Subsampled Contourlet Transform (NSCT) was presented by Cunha et al. [35] for compensating this limitation. Due to the beneficial properties of this transform, in [36] we used NSCT for determining the breast lesions according to its properties to improve the quality of the breast images.

In this paper, by developing our previous study [36], a new algorithm is proposed for breast cancer detection and classification. In the presented method, NSCT is utilized for improving the quality of the images after determining the regions of interest (ROIs), then, Super Resolution (SR) algorithm is used to increase the resolution of the images and a high-pass filter is also utilized to highlight the desired regions. Afterwards, several features are extracted from the image components and skewness of each feature is calculated. Finally, AdaBoost algorithm is employed to classify the extracted features.

The main contributions of our proposed algorithm are summarized as follows:
                        
                           1.
                           Improvement of mammographic images: in the proposed method, unlike the previous techniques which have used Wavelet Transform for determining edges of images, NSCT is utilized due to its powerful capability in image representation. Moreover, fuzzy learning-based SR algorithm is used to properly predict the high-frequency components and remove distortions in the proposed method.

Categorization of breast lesion characteristics: The characteristics of calcifications and masses are categorized based on size, shape, scattering and density and also the severity of abnormality is determined based on appearing symptoms in different categories. This kind of categorization leads to better description for mammographic images and extracting the suitable features from them.

Feature extraction: in the proposed method, in order to distinguish normal and abnormal tissues, seven features based on regional, boundary and density descriptors are extracted from the objects in mammographic images and each feature is analyzed by calculating skewness of each feature to decide if they are malign or benign.

Effectiveness investigation for the proposed algorithm is conducted using the standard MIAS database [37]. The system performance is compared with the performances of some popular CAD systems. Experimental results indicate that the proposed method outperformed other state of the art algorithms from the aspect of all the different evaluation criteria.

In the following, Section 2 presents the proposed algorithm and its subsections in details. In Section 3 the proposed method is evaluated and compared with some popular CAD systems. Finally, the paper concludes in Section 4.

The proposed algorithm includes 3 main stages including pre-processing, feature extraction and classification. Fig. 1
                      shows structure of the proposed method. The following subsections will present the proposed algorithm in details.

To obtain desirable results and distinguish between benign and malign lesions, pre-processing is performed in the proposed technique in two stages containing regions of interest (ROIs) detection and the quality improvement of mammographic images.

The first stage of our pre-processing method is to remove additional margins, which makes images smaller and finally reduces computational burden. On the other hand, as shown in Fig. 2
                           , some features of masses and calcifications are very similar to regions of tissue such as pectoral muscle or similar to some artifacts such as label that include information of the patient in corner of the images. These regions should be removed for reducing false positive rate to obtain ROIs in mammographic images. For this purpose, all images are aligned to the left side to place pectoral muscle for all images on the left side and label on the right side. To remove pectoral muscle and label, thresholding method and erosion morphological operator are utilized so that firstly, the aligned image is binarized using a thresholding technique and the label is removed. After negating the binary image, the result is subtracted from the aligned image. Also, if there is salt and pepper noise, median filtering is utilized for removing the noise. Finally, pectoral muscle is removed by applying thresholding method and erosion morphological operators. All the steps of pre-processing are shown in Fig. 2.

Improvement of mammographic images is necessary for highlighting masses and calcifications in tissue of breast, because it causes the edges and regions of the image to be effectively extracted and studied. Since edges of the image are superposed with high frequency components, a method should be used in frequency domain to achieve them. For this purpose, NSCT is utilized on the images and then SR algorithm is used for increasing resolution of the images. Finally, a high-pass filter is employed for sharpening and highlighting the desired regions.

In Contourlet Transform, the Laplacian Pyramid (LP) is first used to capture point discontinuities, and then followed by a Directional Filter Bank (DFB) to link point discontinuities into linear structures [34]. The overall result is an image expansion using basic elements like contour segments, and thus called Contourlet Transform, which is implemented by a Pyramidal Directional Filter Bank (PDFB) [34]. The LP decomposition at each level generates a down-sampled low pass version of the original image, and the difference between the original image and the prediction results in a band pass image. Due to down-sampling and up-sampling presented in both LP and DFB, Contourlet Transform is not shift-invariant. So, to achieve the shift-invariance property, NSCT was proposed by Cunha et al. [35]. The NSCT is built upon Non-Subsampled Laplacian Pyramids (NSLP) (Fig. 3
                              ) and Non-Subsampled Directional Filter Bank (NSDFB) (Fig. 4
                              ) which is fully shift-invariant, multi-scale and multi-direction image decomposition.

The goal of SR is to achieve a high resolution (HR) image based on one or a set of low resolution (LR) images. Since SR technique is a way of increasing resolution without altering the existing imaging hardware, it can be suitable for medical images [38]. The goal of SR algorithm is to improve resolution of the images which firstly include high-frequency components and secondly aliasing and degradation have occurred in the images with high-frequency components. Therefore, SR can increase sampling rate by utilizing information of the high-frequency components and also reducing aliasing effects caused by application of NSCT [38]. In general, the SR image techniques can be classified into four classes [39] containing frequency domain-based approaches, interpolation-based approaches, regularization-based approaches and learning-based approaches. In this research, SR technique based on learning algorithms has been used among the SR techniques because it is suitable for single image problems and is able to predict high-frequency components.

In the proposed method, after determining ROIs, dimensions of the image is decreased by half of the main dimensions due to largeness of the mammographic images and its high computational rate (Fig. 5(b)). Then, three-level NSCT decomposition is used and then, two sub-bands in the first level, four sub-bands in the second level, eight sub-bands in the third level and one low-pass sub-band which has low-frequency components are obtained. Among the obtained sub-bands, only sub-bands with high-frequency components are used. To achieve edges of the images, “prewitt” edge detector is used on each sub-bands and to obtain stronger edges of the images, the standard deviation of each sub-bands are calculated and utilized as a threshold in “prewitt” edge detector. After finding the edges in each sub-bands, a weight is given to some regions of sub-bands which include edges to highlight the mentioned edges. Then, the image is reconstructed after making changes in its high-frequency sub-bands. Results of this section can be found in Fig. 5(c). In the proposed method, after reconstruction of the image, to improve its quality and also increase its resolution, SR algorithm is used based on fuzzy learning algorithm [40] (Fig. 5(d)). All the steps of fuzzy learning algorithm are summarized in Algorithm 1. Finally, a high-pass filter is used for sharpening and highlighting the desired regions. The final improved image is shown in Fig. 5(e). It should be mentioned that the presented values for two parameters involved in Algorithm 1 (i.e., [3,3] for the window size and 0.8 for the standard deviation) are the tuned values which are stated in [40]. Since these values make our proposed method obtained its maximum performance, so, in our experiments, we also choose [3,3] as the window size and 0.8 as the standard deviation.
                                 Algorithm 1
                                 Super resolution algorithm based on fuzzy learning.
                                       
                                          
                                             
                                             
                                                
                                                   
                                                      Fuzzy Learning SR (
                                                      ImprovedImageNSCT)
                                                
                                                
                                                   
                                                      
                                                      1. Offline phase: generate a rule base system and tune the parameters of the system to have the better performing learned system.
                                                
                                                
                                                   
                                                      
                                                      
                                                      1.1 Collect the HR sharp images from “flickr” database.
                                                
                                                
                                                   
                                                      
                                                      
                                                      1.2 Generate LR versions corresponding to each of these HR images:
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      a. BluredImage=GaussianFilter (HR, [3,3], 0.8).
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      
                                                      % [3,3] is window size for Gaussian filter and 0.8 is standard deviation.
                                                   
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      b. ResizedImage=DownSampling (BluredImage).
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      c. LR=UpSampling (ResizedImage, bicubic).
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      
                                                      % bicubic is text string specifying interpolation method
                                                   
                                                
                                                
                                                   
                                                      
                                                      [End of step 1.2.]
                                                
                                                
                                                   
                                                      
                                                      
                                                      1.3 Divide the HR and LR images into overlapping patches of size (n×n).
                                                
                                                
                                                   
                                                      
                                                      
                                                      1.4 Normalized each patch.
                                                
                                                
                                                   
                                                      
                                                      
                                                      1.5 Concatenated LR and HR patch pairs: x*=(x;y).
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      
                                                      % x: feature patches in LR image and y: feature patches in HR image.
                                                   
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      
                                                      % x*: comprising LR and HR feature patch pair.
                                                   
                                                
                                                
                                                   
                                                      
                                                      [End of the step 1.]
                                                
                                                
                                                   
                                                      
                                                      2. Online phase: apply trained rule based system on the patches of LR image to have a HR version of LR ones.
                                                
                                                
                                                   
                                                      
                                                      
                                                      2.1. clusteredPatch=K-means (x*, c). % c: number of cluster.
                                                   
                                                
                                                
                                                   
                                                      
                                                      
                                                      2.2. Analyzed clusteredPatch to extract fuzzy rules.
                                                
                                                
                                                   
                                                      
                                                      
                                                      2.3. Find the optimal membership functions using Expectation Maximization.
                                                
                                                
                                                   
                                                      
                                                      
                                                      2.4. LRTestPatches=Entry the LR test sample and extract feature from its overlapping patches.
                                                
                                                
                                                   
                                                      
                                                      
                                                      2.5. Repeat stage 2.2 and 2.3 for LRTestPatches.
                                                
                                                
                                                   
                                                      
                                                      
                                                      2.6. [selective patch processing]
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      a. RoughnessPatches=Variance (LRTestPatches).
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      b. 
                                                         If
                                                       RoughnessPatches is high
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      
                                                      The LRTestPatches belongs to texture region.
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      
                                                      
                                                         else
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      
                                                      The LRTestPatches belongs to smooth region.
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      [End of 
                                                         If
                                                       structure.]
                                                
                                                
                                                   
                                                      
                                                      
                                                      c. Set a judiciously threshold to choose the texture or smooth regions for the LRTestPatches.
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      i. 30% of all LRTestPatches belong to texture region so using SR based on fuzzy rule.
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      ii. 70% of all LRTestPatches belong to smooth region so using SR based on bicubic interpolation.
                                                
                                                
                                                   
                                                      
                                                      
                                                      
                                                      [End of step 2.6.]
                                                
                                                
                                                   
                                                      
                                                      
                                                      2.7. Generate HR version.
                                                
                                                
                                                   
                                                      
                                                      [End of the step 2.]
                                                
                                                
                                                   
                                                      [End of Fuzzy Learning SR]
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

In the feature extraction stage, it is necessary to extract the information from mammographic images so that the system can correctly distinguish normal and abnormal tissues. Indeed, using variety information causes obtaining high dimensionality feature matrix that reduce accuracy and increase computation burden. So, taking apart some unhelpful information can lead to the better description from mammographic images and increase the accuracy. Hence, the characteristics are categorized based on size, shape, scattering and density. In this research, the characteristics of calcification and masses which are categorized in Tables 2 and 3
                        
                         have been provided based on two main Refs. [41,42] and also the knowledge of radiologist expert. It should be noted that there are not certain size and shape of masses.

Since masses and calcifications have higher density than other tissues of the breast and are brighter than them, the histogram of the improved images is obtained and the highest value between the values that are bigger than 200 is selected. The selected value is a desired threshold value in the proposed method to produce binary images including masses and calcifications (Fig. 6
                        ). It should be mentioned that, to find the proper interval which the desired threshold value can be found in it, different intervals have been examined and finally the values that are bigger than 200 is empirically selected. After producing the binary images, the regions available in the images are labeled and objects of the images are obtained. At the end, features are extracted from objects of the images. Due to high importance of shape of the lesions in determination of benign or malign probability, the features which extract and study shape of the lesions are mostly used in this paper as a shape analysis. These features include Area [43,44], Compactness [43,44], Fractal [44], Central Moment [43], Eccentricity [43] and Spread [43]. In addition, another feature as an average gray level is extracted which can be described the density of objects.

Shape analysis groups features by major categories; regional and boundary descriptors. The regional descriptors describe the object as a region and hence describe the morphology and include size measurements. Boundary descriptors describe the shape and the contour of an object [43]. Area, Compactness and Fractal are regional descriptors while Eccentricity, Central Moment and Spread that are moment-based features are boundary descriptors. Average gray level as final extracted feature describes the density of objects.

Area of an object is the most trivial shape parameter that can be computed from a detected object on an image. It can be defined as the number of pixels contained within (and including) the boundary of a segmented object of interest. It is the most basic shape parameter but offers a suitable descriptor for describing the size of the objects available in the image.

Compactness is a dimensionless quantity which provides a simple measure for complex counters. It is independent of translation, rotation and scale. It is one of the most common features used in pattern recognition and classification techniques. Compactness can be defined in a variety of ways and its definition may affect classification which has been elaborated elsewhere. In this research, the definition of Compactness that produced the lowest classification error for this application is chosen which is defined as Eq. (1):
                                 
                                    (1)
                                    
                                       
                                          C
                                          =
                                          
                                             
                                                
                                                   P
                                                   2
                                                
                                             
                                             
                                                4
                                                π
                                                A
                                             
                                          
                                       
                                    
                                 
                              where, P is perimeter and A is the area of the objects. According to this definition, a circle is theoretically the most compact object with the smallest C
                              
                              
                              1. Elongated objects have a value of C
                              >1. So, a larger value of this feature describes an irregular and elongated object while a smaller value is representative of a more symmetric and regular object.

The perimeter of the object is measured using increasingly larger ‘rulers’. As the ruler size increases, decreasing the precision of the measurement, the observed perimeter decreases. As Fig. 7
                               indicates, plotting these to values on a log scale and measuring the downward slope gives an approximation to the fractal dimension. As with all the shape features, a higher value corresponds to a less regular contour and so to a higher probability of malignancy [44].

Micro calcifications are also small light local anomaly points which represent sharp local changes in contrast of the image from the fractal standpoint and rare events in global sense [45]. It should be noted that, the Hausdorff-Besicovitch and Box-Counting methods are used for fractal dimension [46]. The Hausdorff-Besicovitch and Box-Counting techniques can be expressed by the Eqs. (2) and (3) respectively:
                                 
                                    (2)
                                    
                                       
                                          F
                                          D
                                          =
                                          
                                             
                                                log
                                                 
                                                N
                                             
                                             
                                                log
                                                (
                                                L
                                                /
                                                U
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                              
                                 
                                    (3)
                                    
                                       
                                          F
                                          D
                                          =
                                          
                                             
                                                lim
                                             
                                             
                                                n
                                                →
                                                ∞
                                             
                                          
                                          
                                             
                                                (
                                                log
                                                (
                                                
                                                   N
                                                   
                                                      n
                                                      +
                                                      1
                                                   
                                                
                                                (
                                                U
                                                )
                                                )
                                                −
                                                log
                                                (
                                                
                                                   N
                                                   n
                                                
                                                (
                                                U
                                                )
                                                )
                                                )
                                             
                                             
                                                log
                                                (
                                                1
                                                /
                                                (
                                                
                                                   U
                                                   
                                                      n
                                                      +
                                                      1
                                                   
                                                
                                                )
                                                )
                                                −
                                                log
                                                (
                                                1
                                                /
                                                (
                                                
                                                   U
                                                   n
                                                
                                                )
                                                )
                                             
                                          
                                       
                                    
                                 
                              where, FD is the fractal dimension, N is the number of parts in each iteration and L is the initial length of the object U and the length of each segment. It should be noted that, Box-Counting technique is used in the proposed method.

The theory of moments gives a number of useful and practical shape descriptors which can obtain some information about roughness of the shapes and can be used to distinguish between the different shape categories of calcifications. The central moment of order k of a real-valued random variable X is defined as Eq. (4):
                                 
                                    (4)
                                    
                                       
                                          
                                             m
                                             k
                                          
                                          =
                                          E
                                          [
                                          
                                             
                                                (
                                                X
                                                −
                                                E
                                                (
                                                X
                                                )
                                                )
                                             
                                             k
                                          
                                          ]
                                       
                                    
                                 
                              where, E denotes the expectation operator. Since higher order moments are very sensitive to noise [43], k is set to 4. The range of values for this feature is also between 0 and 1. If this value is larger and close to 1, irregularity in the shapes will occur more.

It measures the degree in which an object mass is concentrated along a particular axis. This feature is defined as Eq. (5):
                                 
                                    (5)
                                    
                                       
                                          ε
                                          =
                                          
                                             
                                                
                                                   
                                                      (
                                                      
                                                         m
                                                         
                                                            2,0
                                                         
                                                      
                                                      −
                                                      
                                                         m
                                                         
                                                            0,2
                                                         
                                                      
                                                      )
                                                   
                                                   2
                                                
                                                +
                                                4
                                                
                                                   m
                                                   
                                                      1,1
                                                   
                                                   2
                                                
                                             
                                             
                                                
                                                   
                                                      (
                                                      
                                                         m
                                                         
                                                            2,0
                                                         
                                                      
                                                      +
                                                      
                                                         m
                                                         
                                                            0,2
                                                         
                                                      
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                              where, m
                              
                                 pq
                               is the moment of order p
                              +
                              q for an image f(x,y) and is defined as Eq. (6):
                                 
                                    (6)
                                    
                                       
                                          
                                             m
                                             
                                                p
                                                q
                                             
                                          
                                          =
                                          
                                             ∑
                                             x
                                          
                                          
                                             
                                                ∑
                                                y
                                             
                                             
                                                
                                                   x
                                                   p
                                                
                                                
                                                   y
                                                   q
                                                
                                                f
                                                (
                                                x
                                                ,
                                                y
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           

It should be noted that, the range of values for this feature is between 0 and 1. The closer value to 0 will give circular objects and the closer value to 1 will give linear objects.

It measures how unevenly objects are distributed about their centroid and it is based on the central moments of the boundary pixels. This feature is also defined as Eq. (7):
                                 
                                    (7)
                                    
                                       
                                          S
                                          =
                                          
                                             μ
                                             
                                                o
                                                ,
                                                2
                                             
                                          
                                          +
                                          
                                             μ
                                             
                                                2,0
                                             
                                          
                                       
                                    
                                 
                              where, μ
                              
                                 pq
                               is the central moment of order p
                              +
                              q for an image f(x,y) and is defined as Eq. (8):
                                 
                                    (8)
                                    
                                       
                                          
                                             μ
                                             
                                                p
                                                q
                                             
                                          
                                          =
                                          
                                             ∑
                                             x
                                          
                                          
                                             
                                                ∑
                                                y
                                             
                                             
                                                
                                                   
                                                      
                                                         (
                                                         x
                                                         −
                                                         
                                                            x
                                                            ¯
                                                         
                                                         )
                                                      
                                                   
                                                   p
                                                
                                                
                                                   
                                                      
                                                         (
                                                         y
                                                         −
                                                         
                                                            y
                                                            ¯
                                                         
                                                         )
                                                      
                                                   
                                                   q
                                                
                                             
                                          
                                          f
                                          (
                                          x
                                          ,
                                          y
                                          )
                                       
                                    
                                 
                              where, 
                                 
                                    
                                       x
                                       ¯
                                    
                                    =
                                    
                                       m
                                       
                                          10
                                       
                                    
                                    /
                                    
                                       m
                                       
                                          00
                                       
                                    
                                 
                               and 
                                 
                                    
                                       y
                                       ¯
                                    
                                    =
                                    
                                       m
                                       
                                          01
                                       
                                    
                                    /
                                    
                                       m
                                       
                                          00
                                       
                                    
                                 
                               are also the centroid coordinates of the image.

The range of values for this feature is also obtained between 0 and 1. Also, a lower value represents a circular object while a large value defines a linear and non-uniform object.

Since masses and calcifications are brighter than the background, therefore, average values of pixels of the image objects can be applied as a suitable feature.

Considering the introduced features and their definition, skewness (SK) of each feature is calculated as Eq. (9):
                              
                                 (9)
                                 
                                    
                                       S
                                       K
                                       =
                                       
                                          
                                             E
                                             
                                                
                                                   (
                                                   x
                                                   −
                                                   μ
                                                   )
                                                
                                                3
                                             
                                          
                                          
                                             
                                                σ
                                                3
                                             
                                          
                                       
                                    
                                 
                              
                           where, x is a feature and μ and σ are the mean and the standard deviation of x, respectively. Also, E denotes the expectation operator. Skewness is a measure of the asymmetry of the data on which basis can find roughness or smoothness, regularity or irregularity, symmetry or asymmetry, circularity or linearity and brightness of the objects of the image and decide if they are malign or benign. Therefore, feature vector will be obtained for each image as Eq. (10):
                              
                                 (10)
                                 
                                    
                                       F
                                       =
                                       [
                                       S
                                       K
                                       (
                                       A
                                       )
                                        
                                       S
                                       K
                                       (
                                       C
                                       )
                                        
                                       S
                                       K
                                       (
                                       F
                                       )
                                        
                                       S
                                       K
                                       (
                                       M
                                       )
                                        
                                       S
                                       K
                                       (
                                       E
                                       )
                                        
                                       S
                                       K
                                       (
                                       S
                                       )
                                        
                                       S
                                       K
                                       (
                                       S
                                       )
                                        
                                       S
                                       K
                                       (
                                       A
                                       G
                                       L
                                       )
                                       ]
                                    
                                 
                              
                           
                        

In this paper, AdaBoost [47] is used for classification of the extracted features. American college of radiology (ACR) [48] classifies mammographic images using BI-RADS in 6 general categories considering breast lesions:
                           
                              •
                              BI-RADS 0: Evaluation is not complete

BI-RADS I: Normal

BI-RADS II: Benign finding

BI-RADS III: Probably Benign

BI-RADS IV: Suspicious finding

BI-RADS V: Highly Suspicious

However, in the proposed method, based on the available information of the existing database, we classify the mammographic images in 3 general categories including: normal (BI-RADS I), benign (BI-RADS II, III) and malign (BI-RADS IV, V) which findings in BI-RADS III have a very high chance of being benign while findings in BI-RADS V look like cancer and have a high chance of being cancer. BI-RADS for number of the images in MIAS database [37] are shown in Table 4
                        . The mentioned original images in Table 4 are shown in Appendix.

@&#EXPERIMENTAL RESULTS@&#

To assess the performance of the proposed algorithm, an extensive experimental investigation was conducted using Mammographic Image Analysis Society (MIAS) database [37]. This database includes 322 images in 7 groups with dimensions of 1024×
                     1024. Table 5
                      shows the available information of this database for each mammogram that includes type of tissue, kind of abnormality and severity and also class of abnormality that includes 7 groups.

In this paper, images from groups 3 up to 7 in which 208 images are normal and 80 images include masses and calcifications are used for the experiments. 47 images from these 80 abnormal images have benign lesion and 33 images have malign lesion. It should be mentioned that a set of 288 normal and abnormal images was used to test the proposed method.

In the proposed algorithm, after determining ROIs, three-level NSCT decomposition was utilized for improving the quality of mammographic images and in each level, 2, 4 and 8 sub-bands were obtained respectively. In image decomposition with NSCT, “maxflat” and “dmaxflat7” were used as NSLP and NSDFB filters. After improving the images and extracting features of image objects, the feature matrix was given to AdaBoost for classification. MCBoost algorithm [49] was used in classification stage. To evaluate the proposed algorithm, 18-fold cross validation strategy was employed. In addition, to show the suitability of the proposed technique, some evaluation criteria such as Accuracy, Sensitivity, Specificity, Area Under Curve (AUC), Equal Error Rate (EER), False Positive Rate (FPR) and F-measure were also utilized. In particular, F-measure or balanced F-score is a weighted average of Precision and Recall where Precision is the fraction of retrieved instances that are relevant and Recall is the fraction of relevant instances that are retrieved. How to evaluate the proposed algorithm and the equation of employed evaluation criteria are given in Tables 6 and 7
                     
                     , respectively.

In this section, to investigate the capability of the proposed method, NSCT is compared with other time-frequency transform including Contourlet and Wavelet Transforms from the viewpoint of objective and subjective evaluations. The average value of various assessment criteria containing Accuracy, Sensitivity, Specificity, AUC, F-measure, EER and FPR for NSCT, Contourlet and Wavelet Transforms are tabulated in Table 8
                        . From Table 8, it can be seen that the performance of all transforms is high. This is not unexpected because all the transforms are powerful techniques. Also, as we expected, NSCT and Contourlet Transform performed better than the Wavelet Transform. This could be due to the capability of these transforms in capturing the directional information of the images. Table 8 also shows that NSCT outperformed Contourlet and Wavelet transforms from the aspect of all the different evaluation criteria.

The Detection Error Trade-off (DET) curves for the different transforms are also displayed in Fig. 8
                        . It can be observed from Fig. 8 that NSCT performed better than the Contourlet and Wavelet Transforms.

Moreover, a part of mammographic images that contains calcification or mass were selected and their quality after using different transforms were visually compared in Fig. 9
                        . As shown in Fig. 9, NSCT obtained better quality than the Contourlet and Wavelet Transforms.

In the second stage of improving the quality of mammographic images, SR algorithm was used based on fuzzy algorithm. As explained in Section 2.1.2 in Algorithm 1, fuzzy learning algorithm was used on 30% of all patches that have sharp edges and to increase the resolution of the rest of the patches that have smooth regions, a bicubic interpolation method was used and finally, the obtained results are composed together. As shown in Fig. 10
                        , to indicate the superiority of the proposed algorithm, the proposed method has been compared with bicubic interpolation method in such a way that bicubic interpolation method is only used on all patches. In bicubic interpolation method, the mean accuracy is 90.97% and FPR is 0.0677 which are lower Accuracy and higher FPR than the proposed SR technique. The performances obtained by the other SR methods are also compared with fuzzy learning-based SR method which is shown in Fig. 10.

As described in Section 2.2, after acquiring binary images including masses and calcifications and labeled regions available to obtain objects of the images, 7 features were extracted from the objects in mammographic images and each feature was analyzed by calculating skewness of each feature. Among 7 features, 3 features containing Area, Compactness and Fractal are based on regional descriptor and 3 moment-based features including Central Moment, Eccentricity and Spread are based on boundary descriptor and the rest 1 feature namely Average Gray Level is based on density descriptor.

To evaluate the extracted features, different combinations of these features were studied and different results were obtained. When all features were used, mean accuracy 91.43% and maximum accuracy 100% were achieved while in the case of utilizing two of the moment-based features (Central Moment and Spread), Compactness and Average Gray Level, the maximum accuracy of 100% was obtained, but the mean accuracy was decreased to 90.97%. These results indicate that these 4 features are the most effective features for increasing the system performance, but due to decreasing mean accuracy, each 7 features were used in the proposed algorithm.

Number of boosting iterations is an important parameter in MCBoost algorithm which should be properly selected. In each iteration of the AdaBoost algorithm, a week learner was added and the weight was assigned to samples based on their importance. Moreover, in each iteration, the weight of the samples is increased if that is classified incorrectly and the weight of the samples is decreased if that is classified correctly. So, the new learner will be focused on learning the harder samples. The curve in Fig. 11
                         shows the performance of the proposed algorithm for different number of iterations in AdaBoost algorithm. This diagram exhibits that the lowest EER is obtained by using 400 boosting iterations. As shown in Fig. 11, the mean error rate on 400 iterations of boosting is 9.63% and after 400 iterations it remains stable. It should be noted that, the scattering of errors are also shown in each iteration.

To demonstrate the capability of the employed classifier, the result of AdaBoost was compared with different classifiers in Fig. 12
                        . In this experiment, Support Vector Machine (SVM), Artificial Neural Network (ANN), K-Nearest Neighbor (KNN) and Naïve Bayes (NB) classifiers were compared with AdaBoost. The results are expressed by using box-whisker plots. Horizontal axis shows different measures for evaluating the performance of the classifier and vertical axis shows the values are obtained for the measures. Box-plot can be shown as average and median. On each box, the horizontal line denotes median, the circle denotes mean and the horizontal lines outside each box identify the upper and lower whiskers, and dot points denote the outliers. The dotted line in Fig. 12 shows the highest mean accuracy for AdaBoost among the other classifiers.

As mentioned before, in order to assess the proposed method, 18-fold cross validation strategy was used. It should be mentioned that, different folds were studied and according to the diagram of Fig. 13
                        , the best result was obtained with 18-fold cross validation. The mean and maximum accuracy obtained for different folds are indicated in Fig. 13.

In this section, training and testing errors are evaluated to study the accuracy and correctness of the proposed algorithm. As shown in DET curve in Fig. 14
                        , the training error of the proposed technique is less than the testing error. The horizontal and vertical axis in DET curve denotes FPR and FNR respectively. It should be noted that the definitions of FPR and FNR are shown in Table 6.

In breast cancer detection systems, it is necessary to notice to both errors of FPR and FNR and reduce these errors. It should be mentioned that for occurrence of FPR error, we will suffer additional cost. So, in the breast cancer detection systems, the FPR should be reduced to lower than the 15% (FPR of the radiologists is equal to 15% in mammographic images). In the proposed method, the mean and the maximum of FPR (when the accuracy is the lowest) were reduced to 6.42% and 12.5%, respectively. The minimum FPR (when the accuracy is the highest) was also reduced to 0. The values obtained for FPR, indicate the effectiveness of the proposed algorithm.

It should be mentioned that the computational time for 18-fold cross validation on 288 breast images in our experiments was obtained 181.54s. Therefore, the time required for each iteration is approximately 10.08s. All the experiments were conducted on a PC with 2.40GHz CPU and 4GB RAM using MATLAB software (version 7.11.07.2) running Windows platform.

In this section, the performance of the proposed algorithm is compared with the following popular methods which have been tested on MIAS database.
                           
                              •
                              Technique for preprocessing of digital mammogram [17]: In this paper, the authors proposed three distinct steps. The initial step involves contrast enhancement by using the Contrast Limited Adaptive Histogram Equalization (CLAHE) technique. Then define the rectangle to isolate the pectoral muscle from the region of interest (ROI) and finally suppress the pectoral muscle using their proposed modified Seeded Region Growing (SRG) algorithm.

An automated method to segment and classify masses in mammograms [50]: in this paper, an automated CAD system for detection and classification of massive lesions in mammographic images has been presented. The authors used an edge-based method for mass detection, Gray Level Co-occurrence Matrix (GLCM) for textural feature extraction and artificial neural network for classification.

Microcalcification detection using Tsallis entropy & a type II fuzzy index [51]: in this algorithm the thresholding is performed using the Tsallis entropy characterized by a parameter which its optimal value is found using a type II fuzzy index.

Pectoral muscle segmentation based on homogenous texture and intensity deviation [52]: in this paper, first two anatomical features of the pectoral muscle including homogeneous texture and high intensity deviation are employed to identify the initial pectoral muscle edge. Then Kalman filter is utilized to refine the ragged initial edge.

Modeling mammographic microcalcification clusters using persistent mereotopology [53]: in this paper, a method to classify microcalcification clusters has been proposed by representing discrete mereotopological relations between the individual microcalcifications over a range of scales in the form of a mereotopological barcode.

The experimental results together with the results obtained by the above mentioned algorithms are tabulated in Table 9
                        . As Table 9 indicates, our proposed method outperformed other state of the art algorithms from the aspect of all the different evaluation criteria containing Accuracy, Sensitivity, Specificity and AUC. The superiority of our proposed method over other algorithms could be due to the fact that in the preprocessing step of our proposed algorithm, we used NSCT which is more powerful technique than the methods which have been used in the compared algorithms in capturing the directional information of the images. Also, Super Resolution (SR) is utilized in our method for predicting the high-frequency components.

@&#CONCLUSION@&#

In this paper, a new breast cancer detection and classification method is proposed based on Non-Subsampled Contourlet Transform (NSCT) and Super Resolution (SR). The presented algorithm includes 3 main parts including pre-processing, feature extraction and classification. In the proposed technique, unlike the previous algorithms which have used Wavelet Transform for determining edges of images, NSCT which has powerful capability in capturing the directional information has been used for improving the quality of the images. Also, to properly predict the high-frequency components and removing the distortions, SR algorithm has been employed. Moreover, to describe mammographic images, the characteristics of breast lesions have been categorized based on size, shape, scattering and density which they can extract the suitable features from the images. In order to distinguish normal and abnormal tissues, several features based on regional, boundary and density descriptor were extracted from the objects in mammographic images. Each feature was analyzed by calculating its skewness to decide if it is malign or benign. In addition, we inferred from the extracted features in the proposed algorithm that two of the moment-based features (Central Moment and Spread), Compactness and Average Gray Level are the best and the most discriminative features for increasing the system performance. The proposed method has been evaluated using Mammographic Image Analysis Society (MIAS) database [37] and compared with some other popular and state of the art algorithms. Experimental results indicate that the proposed technique can obtain mean accuracy of 91.43% based on cross validation strategy which outperformed other popular compared algorithms. It is a very encouraging finding that in the proposed method FPR of the radiologists which is equal to 15% in mammographic images was reduced averagely to 6.42%. Moreover, the obtained maximum FPR which is 12.5% was less than FPR of the radiologists.

Some sample images from MIAS database [37] which has been mentioned in Table 4 for BI-RADS detection:


                     
                        
                     
                  


                     
                        
                     
                  


                     
                        
                     
                  

@&#REFERENCES@&#

