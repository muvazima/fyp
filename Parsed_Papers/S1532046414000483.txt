@&#MAIN-TITLE@&#Tree testing of hierarchical menu structures for health applications

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We conducted online tree tests to evaluate navigation structure in a health interface.


                        
                        
                           
                           In tree testing, participants navigate through a menu abstraction of the interface.


                        
                        
                           
                           Participants are asked to navigate while completing representative tasks.


                        
                        
                           
                           Performance metrics include completion time, task accuracy, and path length.


                        
                        
                           
                           Comparison with in person usability testing identified similar performance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

User–computer interface

Usability methods

Information system evaluation

@&#ABSTRACT@&#


               
               
                  To address the need for greater evidence-based evaluation of Health Information Technology (HIT) systems we introduce a method of usability testing termed tree testing. In a tree test, participants are presented with an abstract hierarchical tree of the system taxonomy and asked to navigate through the tree in completing representative tasks. We apply tree testing to a commercially available health application, demonstrating a use case and providing a comparison with more traditional in-person usability testing methods. Online tree tests (N
                     =54) and in-person usability tests (N
                     =15) were conducted from August to September 2013. Tree testing provided a method to quantitatively evaluate the information structure of a system using various navigational metrics including completion time, task accuracy, and path length. The results of the analyses compared favorably to the results seen from the traditional usability test. Tree testing provides a flexible, evidence-based approach for researchers to evaluate the information structure of HITs. In addition, remote tree testing provides a quick, flexible, and high volume method of acquiring feedback in a structured format that allows for quantitative comparisons. With the diverse nature and often large quantities of health information available, addressing issues of terminology and concept classifications during the early development process of a health information system will improve navigation through the system and save future resources. Tree testing is a usability method that can be used to quickly and easily assess information hierarchy of health information systems.
               
            

@&#INTRODUCTION@&#

Health Information Technologies (HIT) are highly complex systems, with interfaces that incorporate multiple menu labels navigating across both broad and deep structures. The user interface (UI) also contributes towards challenges in navigation with dense displays, cluttered layout, icons, and popups. We demonstrate the use of tree testing as a method to evaluate an abstraction of navigation structure, in particular, focusing on only the menu labels and their organization within the navigational hierarchy. In a tree test, users navigate the disembodied representation of the UI, consisting of only menu labels and their sublevels while trying to complete representative tasks. This approach isolates the conceptual component of the navigation structure from the UI, though it is recognized that elements of the UI also contributes towards navigation within the HIT system.

@&#BACKGROUND@&#

Health Information Technologies (HIT) range in breadth including various applications such as electronic health records (EHR), electronic prescription systems (eRX), and computerized provider order entry systems (CPOE). These systems are often highly complex and have significant potential of improving patient care and organizational efficiency. However serious challenges behind the design and construction of HIT systems have created interfaces that are often frustrating to use for health care providers. This has led to a call for a stronger evidence-base behind development of HIT systems [1,2]. The need to address such concerns becomes even more significant as examples of failed systems come to light, highlighting high costs of implementation and operation, and negative impact on patients [3–5]. Koppel identifies a need to apply greater scientific rigor to the development of HIT with a focus on navigation, usability, information graphics, and ethnographic evaluation [1]. The lack of a strong evidence-based approach towards HIT design can often be attributed to the complexity of health systems, the limited time and resources within clinical practice, and the difficulty of applying randomized control trials to pervasive health systems [1].

The breadth of information available within HIT systems presents a challenge of defining appropriate terminology. Chute refers to terminology as a naming system that applies controlled terms to reference formal concepts organized by a classification schema [6]. Terminologies in health sciences is a well-established area of work and include examples such as the International Classification of Diseases (ICD), Systematic Nomenclature of Medicine (SNOMED), and Logical Observations Identifiers, Names, and Codes (LOINC) [7–9]. Defining appropriate terminology and concept classification is essential towards creating effective information architectures of HIT system. In this research, we are interested in evaluating the existing terminology defined within a commercially available multi-media health interface, focusing on how it supports navigation for the user.

To address the need for greater evidence-based evaluation of HIT systems applicable throughout the development cycle, we introduce a method of usability testing grounded in human factors engineering. Termed tree testing, this technique focuses on evaluating information architecture, often within the context of website design [10]. In this study, we extend tree testing to health applications, demonstrating a use case and providing a comparison with more traditional usability testing methods.

In a tree test, users are presented with an abstraction of the information architecture through a set of menu and submenu navigations. By focusing only on the abstraction of the information architecture, contextual and aesthetic related content is isolated from the organization of information. Users are then given a set of tasks and asked to navigate through the menu structure to find an appropriate location to answer each of the tasks. The menu structure often consists of a collapsible tree. Quantitative metrics derived from tree testing include completion time, navigation path length, and task completion accuracy. With well-chosen tasks representative of the breadth of the information architecture, tree testing can be a valuable tool to evaluate a system’s taxonomy against the user’s expectations. Matching a user’s mental model with the internal system architecture is important towards reducing inefficiencies and errors [11,12].

Spencer introduced tree testing as a user research technique out of the need to systematically evaluate hierarchy structures, separate from interactions due to the interface [13,14]. Initial variants of tree testing involved paper and pencil index cards with menu label headings. A moderator would manipulate submenu interactions to simulate navigation within the hierarchy. Since then, remote tree testing tools have surfaced (Treejack, UserZoom, Plainframe) to allow for larger scale testing [15–17]. Tree testing is used often within the context of website architecture with a primary focus on findability and label organization [10]. This has yet to be extended to HIT systems.

We address these limitations by describing the development and implementation of a web based tree test to evaluate the taxonomy of a commercially available multimedia health and wellness tool. The multimedia tool combines both health specific aspects and entertainment and games that ultimately aim to reduce anxiety and improve cognitive and overall well-being for older adults. The interface for the multimedia tool consists of pictorial icons with menu labels describing content to engage, entertain, and interact with participants. The menus lead to further sub-categorizations or to applications that users can launch. These applications include games, movies, news media, music, exercise videos, and arts and crafts. However, given the breadth of applications available, questions arose as to the navigability of the system. This was evaluated through tree testing of the information hierarchy. We present a quantitative analysis of results from the tree test and also develop a network path visualization of aggregate navigational patterns for each task. In addition, we provide a comparison of tree testing as a user research method with more traditional scenario based methods where users are asked to think-aloud while navigating through the live version of the multimedia tool. The objectives of this research are to present tree testing as a usability testing method, focusing on implementation and analysis as opposed to providing usability recommendations on a specific multimedia tool.

We created a web-based implementation of tree test using a combination of HTML, JavaScript, and PHP. We used collapsible accordion menu structures to simulate navigation across the tree such that once a node is clicked, its siblings are collapsed and hidden from view. Alternatively, if the node is already hidden, clicking it will expand its siblings. This creates a linear path of navigation, preventing users from jumping across branches within a tree unless they navigate up to a parent node and expand its children. The tree test tool takes as input a formatted data file to build up the tree structure along with a file representing the tasks presented for tree testing. As output, the tree test generates a file storing each node clicked and the time taken to click the node from either the start of the task or the most recently clicked node, for each task. We hosted the web based tree test on a university provided server for public access. The customizable tree test tool along with full documentation is provided for access at (http://staff.washington.edu/tle23/TreeTest.zip).

We transposed the underlying menu taxonomy of the multimedia health tool to the tree test. This resulted in a tree with 70 nodes overall, of which 53 were leafs and 17 were branches. The depth of the tree varied from two to four levels. Given the voluntary nature of the survey, we limited the number of tasks to seven scenarios. These were chosen to span across the tree, while also highlighting paths across a range of difficulty as identified through preliminary discussion with the research team (Table 1
                        ). An example of the tree structure is provided for Task 2 (Fig. 1
                        ).

As part of the implementation of the tree test, participants were exposed to an initial demonstration tree and a set of three trial tasks. We used a different taxonomy for the demonstration run compared to that of the true trial. The demonstration consisted of scenarios asking participants to find different food items (such as lasagna or tomato soup) within a categorization of food groups. Since a moderator was not present for these web-based sessions, we wanted to use the trials as an opportunity to familiarize participants with the collapsible menu structure and task goals. The instructions were:
                           “Please navigate through the tree to find the appropriate location for the item. There are no right or wrong answers and you can navigate back up levels of the tree. Please choose the best fit under “I’d find it here”. If there are no appropriate locations, select “No Category Found”. The task is: (scenario).”
                        
                     

Upon completion of the demonstration trials, participants were then presented with a randomized order of the seven tasks from the true experimental session. For each task, participants navigated through the taxonomy until they reached the end of the tree. If participants believed the menu label corresponded to an appropriate location to address the scenario, they would select “I’d find it here” before moving onto the next task. At any time during the navigation process an option of “No Category Found” was available if participants believed no appropriate category within the taxonomy fit the scenario.

We provide a comparison of the web-based tree test with more traditional in-person usability tests. During the in-person sessions, we asked participants to think aloud as they navigated to complete tasks using the actual multimedia interface. We used the same tasks as the online tree test, randomized at the start of each session. A moderator helped explain study procedures while a note taker recorded paths taken during navigation. We also recorded the sessions for transcription and analysis. Participants were distinct from the online tree test. Participation was voluntary and lasted between 15 and 30min. The university institutional review board approved all human subject procedures of the study.

We applied a mixed methods approach towards recruitment that involved fliers, email list subscriptions, and snowball sampling. All participation was voluntary, restricted to English speakers. We sampled based on convenience across a primarily, though not restricted to, university population. Fliers and emails contained a link to the online tree test along with contact information for in-person usability testing. We restricted participation to only one of either online or usability testing by verifying an email identifier.

For the online survey, we calculated metrics of completion time, task accuracy, and path length. We used descriptive statistics including mean, standard deviations (SD), and confidence intervals (CI) to summarize the data. However, these metrics are primarily aggregate measures of task performance indicating which tasks are easier or more efficient to complete.

For greater insight into path taken, we developed a network path visualization grounded from social network analysis [18]. Each node within the network represents the number of times a menu label was selected across all trials of a given task. A directional edge between two nodes exists if participants selected a given label and then selected a following label as part of the navigation path. This edge is weighted by the number of occurrences across all trials for the task and can be selectively filtered for visualization. This resulted in a network path visualization that provides a gross overview of the paths taken towards completing a task.

For the in person usability tests we recorded navigation path and used this to calculate task accuracy along with relative path length. We chose not to record time primarily because the think aloud process and moderator prompted discussions would unfairly bias time to completion. We assessed joint dependence of task type and task accuracy with testing method (remote tree test or in-person) using Fisher’s Exact Test. We applied a one-way ANOVA to test if mean relative path lengths differed by task type and testing method. In addition, we provide a gross overview of the qualitative think-aloud to complement the results from the tree testing. All statistical analyses and visualizations were completed with R V.2.15.2 statistical software [19].

@&#RESULTS@&#

We conducted the survey and in-person usability tests from August–September 2013. Over this period, 54 participants responded to the remote survey and 15 participants were involved with in-person usability testing.

We found a broad distribution of accuracy rates across the seven tasks (Fig. 2
                        ). This was consistent with the design of the scenarios, intended to sample across the spectrum of difficulty. The most accurate tasks involved Video Chat (94.4% success, CI: 83.7–98.6%) and Funny Clip (83.3% success, CI: 70.2–91.6%). It should be noted that these two highly accurate tasks involved the shortest paths, requiring participants to navigate across two levels of the tree before finding the appropriate location. However, when comparing across tasks within the same path lengths, there were still statistically significant differences. For example, both Karaoke (59.3%, success CI: 45.1–72.1%) and Classic Comedy (53.7% success, CI: 39.7–67.2%) were statistically different from the least accurate tasks of Cooking Video (3.70% success, CI: 0.644–13.8%) and Rome Trip (25.9% success, CI: 15.4%–39.9%). These scenarios all involved clicking four total nodes. Ranked from least to most accurate are the tasks: Cooking Video→Rome Trip→Solitaire→Classic Comedy→Karaoke→Funny Clip→Video Chat.

For task completion times and path length analysis, we filtered out times that were unusually long (above 300s). We also filtered out paths that were greater than 15 clicks (this also applied for in-person results). Those who completed the tasks correctly took on average 64.3s (CI: 53.7–74.9s) while those who incorrectly completed the tasks took 72.4s (CI: 61.7–83.2s). This difference was not statistically significant (p
                        =0.29). Within task, there was also no statistically significant difference in correct compared to incorrect completion times (Fig. 2 and Fig. 3
                        ). The quickest tasks to complete correctly were Video Chat (24.6s, CI: 19.3–30.0s) and Funny Clip (32.1s, CI: 20.4–43.8s). Overall, from longest correct completion time to shortest, we found a relationship closely matching that of accuracy rate: Cooking Video, Rome Trip, Solitaire, Karaoke, Classic Comedy, Funny Clip, and Video Chat.

We defined relative path length as the number of additional steps taken by the user in order to arrive at a correct solution compared to the optimal shortest path. This provides a measure of path directness. The most direct paths involved Video Chat, Funny Clip, and Classic Comedy, taking on average 0.2–0.6 additional clicks (Fig. 4
                        ). There was a strong separation of this group of tasks compared with Karaoke, Rome Trip, and Solitaire, which required an additional 1.9–3.2 clicks. The Cooking Video task had only one correct response and so conclusions related to that task are limited. These findings align with completion time and accuracy rate.

To better understand differences in performance across tree tasks, we developed network path visualizations. The visualizations provide a technique to represent the breadth of path decisions made by participants for each task. In the visualization, the size of the nodes represents the number of instances across all trials that the node was selected during a task. A directed edge between nodes (A→B) represents a navigation path from node A to node B during the navigation task, weighted by the number of occurrences. We filtered the network to remove nodes and edges less than three.

The visualizations are demonstrated with two tasks: Karaoke and Solitaire (Fig. 5
                        ). For the Karaoke task, participants were asked to find an application to sing Karaoke (Relax→Music→Karaoke→I’d find it here). From the visualization, there are three primary clusters. One represents the correct pathway selected by approximately 65% of participants. Once on the path, there was limited deviation, indicating that subsequent levels provided clear choices for the participant. However, there were two other clusters leading to incorrect choices, represented by {Get Silly, Entertain, No Category Found} and {Engage, Games, Sound Games}. These indicate potential points of confusion for participants.

For the Solitaire task (Engage→Games→Simple Strategy→I’d find it here), there was confusion at the top level with participants selecting between Relax, Entertain, and Engage. In addition, within the Engage pathway, participants were unclear where solitaire might be found, looking amongst Matching Games, Touch Games, and Puzzles along with the correct location of Simple Strategy. In contrast to the prior example where the correct path is straight, this task shows divergence at the sublevels (see Fig. 5).

We did not find evidence to reject the joint independence of usability method (remote tree and in-person sessions) with task type and accuracy through Fisher’s Exact Test (N
                        =54, p
                        =0.5). This indicates that the relationship between task type and task accuracy is not impacted by the usability method employed. However an ANOVA examining independence of mean relative path length with task type and usability method did have statistical significance (p<.001). Within task type, the mean relative path lengths differed (p<.001). This would be expected since the tasks were selected to range in difficulty and path length reflects this difference in performance. The ANOVA also indicated that within usability test type the mean path lengths also differed (p
                        =0.03). Those who correctly answered the in-person usability tasks often took longer paths compared to those who participated in the remote survey.

@&#LIMITATIONS@&#

Some limitations exist with this current study. We selected a commercially available multimedia tool as a use-case demonstration of tree testing methodology. The tool is broad in scope, focusing on general activities that promote health and wellness. The terminology used does not reflect the controlled vocabulary of medical terms often found in HIT systems; this is an area of future work, applying tree testing to medical systems. The taxonomy of the system has already been defined; tree testing provides insights into navigation within the system however it does not address defining the initial terminology. We applied tree testing towards a small taxonomy (70 nodes overall) and issues of scale have yet to be addressed for testing HIT systems. Because we conducted remote web-based surveys, we were unable to control for factors such as screen size or mobile platform that may impact completion time. The selection of scenarios is also a biasing component of tree tests. They may not represent a natural case interaction with the system and terminology may unfairly influence search pathways. Though tree testing can identify areas within an information hierarchy that users struggle with during navigation, this technique does not provide the depth of insights compared with in-person usability testing. However, remote tree testing is easier to scale and requires limited resources. This allows researchers and developers to quickly gather data that can be statistically analyzed to evaluate the information architecture of HIT systems throughout the development process.

@&#DISCUSSION@&#

We found that a few issues helped facilitate successful implementation of remote tree testing. Having a demonstration run with a pseudo tree and tasks was useful to familiarize participants to the structure of the experiment. However, in addition to this, it helps to explicitly state that backtracking is allowed during the tree test. During preliminary testing, participants were unclear if this was allowed. As a result, they were fixated on the first path available and would stop navigation if an appropriate location was not found. We chose to provide an option of “No Category Found” constantly available as an egress to the next task. This was to alleviate potential frustration from being unable to find an appropriate location for a task. Tree testing provides insights into path navigation, however this becomes limited if participants are so focused on a task that they explore the full tree during the search process.

Tree testing provides a method to quantitatively evaluate the information structure of a system. Performance on tasks indicated distinct levels of navigability. The tree test metrics of accuracy rate, completion time, and completion task length provide consistent ranking of tasks. For example, the tasks Video Chat, Funny Clip, and Classic Comedy were much easier to complete based on all three metrics, while Cooking Video, Rome Trip, and Solitaire were more difficult to complete. These differences in task performance were validated with in-person usability testing.

In addition to quantitative metrics of overall performance, it is possible to identify parts of the taxonomy where users struggle during navigation. This feedback is valuable towards reorganizing content within the taxonomy such that it better aligns with the user’s mental model. For example, the top-level taxonomy in the system often led to confusion and navigation down an erroneous path. During the in-person sessions this issue was further explicated as participants noted the lack of clarity in the top-level headers: “The title, like engage, really doesn’t tell you much unless you’ve been there before. Clicking in it makes more sense. Engage makes more sense right now because I’ve seen it, but otherwise when I look back at the main page… these are really broad type of titles.” Even within a pathway, tree testing identified divergence of choices. For example, in completing the Solitaire task, participants recognized it belonged under Games, but the subcategories were unclear, leading to choices amongst Matching Games, Touch Games, and Simple Strategy. Participants also pointed this out during the in-person sessions: “It’s not really… well it’s sort of a matching game. I clicked on matching games. I don’t know, that’s a tough one. It’s kind of just how you would categorize solitaire. I guess it’s a matching game. It could also be a strategy game.” Participants perceived multiple paths as valid towards answering the scenario and selected a best fit based on their mental model of solitaire. This incongruence with the system architecture was reflected through performance within the tree test.

There has been limited work comparing tree testing with navigation tasks on a full system. Sauro reports a preliminary 78% completion rate for live site testing compared to a 66% completion rate in tree testing in web navigation studies [20]. Differences in completion rate could be attributed to the additional context of a live site compared with the hierarchical abstraction in the tree test. Within our study, there were no statistical differences in task accuracy across usability methods. However, there were qualitative differences between tree testing and in person usability tests.

On a functional level, the skeleton structure of the tree offered no clues as to the contents of each category; during the in person testing with the actual system, participants often used the pictures accompanying the category as a guide to help them make decisions. As an example the picture for the “Engage” category appears to be two older adults playing backgammon which led participants to assume it included games. While there were no statistical differences between the remote test and the in person test in this study, other studies might experience differences due to aesthetic elements in a system not being represented in the tree structure.

Maybe the most important and obvious difference is that the researcher is able to hear the subject’s thoughts as they attempt each scenario allowing the researcher to not only see where navigational discrepancies are but also why the discrepancy is a problem and possibly how it can be fixed. For example, one subject when trying to find a game said, “Hmm, maybe relax. I don’t see anything else there that would indicate that there might be a game in it…”. In this scenario the researcher would realize that the top level categories are not aptly defined to fit their items. In these sessions the researcher can also get the user’s opinions on how the navigation could be structured. As an example in this quote the user was trying to explain what they expected to find under the “Engage” category, “engage, I was thinking was more kind of a synonym with stay connected, like you would actually, when you clicked on it you would interact with somebody else.” Depending on what the researcher is trying to fix they should consider these differences when choosing a usability method.

Applications of tree testing can be exploratory in nature, with the goal of identifying areas of confusion or uncertainty within taxonomies. This provides a complementary tool to more traditional card sorting techniques. In card sorting, participants are asked to group a set of cards into open ended or pre-labeled categories [13]. This allows researchers to understand groupings within a level of a hierarchy, which can then be used to successively build up a tree. Tree testing can then be used to evaluate the hierarchy and the parent–child relationships within the tree. Tree testing can also be used as a comparison method to evaluate different variants of taxonomies. Quantitative metrics of performance make it possible to identify differences and validate updates made to a taxonomy from prior testing.

Within the context of Borycki’s classification of user research techniques (usability inspection methods, usability testing, and clinical simulations), tree testing falls within usability testing [21]. However, there are key differences with more traditional usability testing techniques. Tree testing involves feedback on a narrowly defined component of the system. It allows researchers to isolate the underlying information architecture from aesthetic related components such as interface or layout [10]. In addition, remote tree testing provides a quick, flexible, and high volume method of acquiring feedback in a structured format that allows for quantitative comparisons. It does not require a high fidelity prototype of the system. Addressing issues of taxonomy early during the development process of a system saves future resources, especially since the information architecture forms the foundation for navigation. Changing structure post implementation can be challenging and may have a negative impact on users who have already adopted an existing taxonomy.

This paper provides a use case demonstration of tree testing within a multimedia health interface. The results identified nodes within the menu structure that were inconsistently classified. These findings are valuable as part of the iterative process of refinement within the navigation health interface. Within the context of health information systems, clinicians need to efficiently and effectively look up medications, navigate through patient history, and review active problems [1]. This is especially true as the electronic health record has made it possible to amass a large amount of clinical data. However, having the data readily available does not mean it is easy to access, as clinicians have to sift through medications, laboratory test results, allergies, and notes. This wealth of data, though valuable, can be overwhelming, inefficient and prone to errors. Challenges associated with usability of HIT systems can impact adoption rates [22] leading to a system’s eventual success or failure [2,23]. Better aligning the clinician’s mental model of organization with the HIT system’s information architecture can help reduce these challenges [24]. We introduce tree testing as a usability technique within health informatics to quickly evaluate the taxonomy of content within HIT. Applications include improving organization of medications within eRX systems, diagnosis and treatment selection within electronic medical records, and categorization of laboratory or test results. This can have an essential role in facilitating provider access to data and knowledge.

@&#CONCLUSIONS@&#

Systems with poor usability have been found to negatively impact patient mortality [25,26]. In contrast, well-designed user tested systems can have a positive impact on patient safety [27]. The need for testing of HIT systems is well-recognized within the literature, while its financial benefits have also been established in terms of identifying and fixing usability problems prior to widespread release [28]. We introduce tree testing as a usability method to assess information hierarchy of health information systems. This can be applied early during the development process to evaluate an existing taxonomy or to compare amongst potential variants of taxonomies. Tree testing is not the sole approach towards evaluation of HIT systems and instead a continuum of testing methods should be applied from laboratory to in situ settings [29]. However tree testing does provide a flexible, evidence-based approach for researchers to evaluate the information structure of HITs.

@&#ACKNOWLEDGMENT@&#

This work was supported in part by National Library of Medicine (NLM) Training Grant T15LM007442.

@&#REFERENCES@&#

