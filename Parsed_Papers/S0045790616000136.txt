@&#MAIN-TITLE@&#Image and medical annotations using non-homogeneous 2D ruler learning models

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A metric learning model directly calibrating measured numerics of feature data.


                        
                        
                           
                           A 2D semi-metric model by relaxing one degree of freedom.


                        
                        
                           
                           Simple and intuitively natural models solvable by convex quadratic programming.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Semidefinite programming

Image annotation

Medical annotation

Prediction model

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           Image, graphical abstract
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

In recent decades, there is a misleading trend in the artificial intelligence community that researchers devise increasingly complicated methods, which is not based on an in-depth analysis of the intrinsic properties of the training and testing datasets. Instead, they focus on other aspects such as the convexity of the optimization problem. As a result, they lose the potential opportunity to discover certain ingenious places where adaptive parameters exist. If these adaptive parameters can be learned in a formalized learning model, a learning process or intelligence happens.

A traditional mode for machine learning researchers to solve problems is that, they firstly devise a new method from mathematics or biological inspirations, and then try to apply the new method to a variety of problems. In this paper, we attempt to show an inverse methodology: it originates from the problem itself by analyzing the dataset [1], aiming to change some underlying assumptions for achieving higher recognition accuracy in several simple experiments. If such underlying assumptions and the corresponding successful ways to change them are identified, these underlying assumptions can then be broken, generalized and modeled as adaptive parameters to be learned from the training set, and applied to the testing set [2].

We use the theory of evolution as a philosophical analogue to compare the aforementioned two methodologies for their contribution to the evolution of artificial intelligence. The former traditional one is more like the process of natural selection: a newly devised method, which is either partly or entirely different from previous methods, is just like the mutation of the genes, and its theoretical and experimental validation act like the natural selection, in which, only the ones with the highest performance
                        1
                     
                     
                        1
                        Here the performance can be recognition accuracy, robustness, computational cost, extensibility, flexibility etc.
                      survive. It is well-known that, the natural selection process is of low efficiency. What makes the later inverse methodology more efficiently contribute to the development of artificial intelligence is that, it incorporates the intelligence of artificial intelligence researchers (from human brains that can think symbolically) when they analyze the dataset, to some extend.

In this paper, we present such a case study using the later inverse methodology. At first glance, our final proposed model seems to like building a non-linear embedding to project the sample data points with the same label closer to each other in the embedding space. But after a closer scrutiny and comparison with the LLE (Locally Linear Embedding) or other non-linear embedding methods, there is fundamental difference: The LLE is to find low-dimensional global coordinates when the sample points approximately lie on a manifold embedded in a high- dimensional space [3]. Its essential idea is to perform a different linear dimensionality reduction at each sample point (locally a manifold looks linear) and then combine these with minimal discrepancy [13]. But in our proposed models, there is no dimensionality reduction or increase.

The organization of the paper is as below: In Section 2, we first discuss some simple preliminary experimental results, based on which, our non-homogeneous 2D ruler model is proposed in Section 3; experiments are reported and discussed in Sections 4, and 5 gives some conclusions and also directions for future work.

We commence solving the image classification problem with the simplest kNN classifier. It is acknowledged that, even combined with the simplest kNN classifiers, learning a distance metric from labeled examples yields quite competitive results, such as reported in [4]. Without the loss of generality, the LabelMe dataset [6] is used as an example. All experimental settings are the same as those in Section 4.

Denote S
                     =(
                        X
                     
                     
                        i
                     ,
                        L
                     
                     
                        i
                     ) (1≤i≤N) as a training set of N labeled samples with feature vectors 
                        X
                     
                     
                        i
                      ∈ Rq
                      and their associated labels 
                        L
                     
                     
                        i
                     . We use 
                        X
                     
                     
                        i
                     [k] to denote the kth component (or dimension) of 
                        X
                     
                     
                        i
                     . Given two q-dimensional sample points 
                        X
                     
                     
                        i
                      and 
                        X
                     
                     
                        j
                     , the Lp norm distance is defined as:

                        
                           (1)
                           
                              
                                 d
                                 
                                    (
                                    
                                       X
                                       i
                                    
                                    ,
                                    
                                       X
                                       j
                                    
                                    )
                                 
                                 =
                                 
                                    
                                       (
                                       
                                          
                                             ∑
                                             
                                                k
                                                =
                                                1
                                             
                                             q
                                          
                                          
                                             
                                                |
                                             
                                             
                                                X
                                                i
                                             
                                             
                                                [
                                                k
                                                ]
                                             
                                             −
                                             
                                                X
                                                j
                                             
                                             
                                                [
                                                k
                                                ]
                                             
                                             
                                                
                                                   |
                                                
                                                p
                                             
                                          
                                       
                                       )
                                    
                                    
                                       1
                                       /
                                       p
                                    
                                 
                              
                           
                        
                     
                  

Note that when p
                     =2 and 1, it is the commonly used L2 norm Euclidean distance and L1 norm absolute difference value sum respectively. We used various p values, and the recognition rate by the simplest kNN classifier (k
                     =7) is listed in Table 1
                     . As a comparison, we cite the reported results in [7]: the SVM achieved 71%, and the Wang's proposed method achieved 77%.

It is seen from Table 1 that, the recognition rate peaks when p is around 0.6. To give the readers a natural intuitive explanation, we have to first revisit the meaning of the measured feature data 
                        X
                     
                     
                        i
                      (see [6–8] and Section 4): 
                        X
                     
                     
                        i
                      is the 256-dimensional histogram of the ith training image, in which, 
                        X
                     
                     
                        i
                     [k] counts the number of image patches belong to the kth codeword generated by the k-means clustering (i.e. 
                        X
                     
                     
                        i
                     [k] is the number of the kth-class image patches). As a human being, if we see 
                        X
                     
                     
                        i
                     [k] and 
                        X
                     
                     
                        j
                     [k] differ greatly, it means the ith and jth images have thoroughly different number of kth-class image patches. Thus when |
                        X
                     
                     
                        i
                     [k]-
                        X
                     
                     
                        j
                     [k]| is very large, |
                        X
                     
                     
                        i
                     [k]-
                        X
                     
                     
                        j
                     [k]|=100, 200 or 500 should make almost no difference in the distance penalty of d(
                        X
                     
                     
                        i
                     ,
                        L
                     
                     
                        i
                     ). This observation explains why the L2 norm performs so badly, and why L0.6 norm beats L1 norm here.

Instead of using subtraction to measure the difference between the two quantities 
                        X
                     
                     
                        i
                     [k] and 
                        X
                     
                     
                        j
                     [k], we use the division arithmetic:

                        
                           (2)
                           
                              
                                 d
                                 
                                    (
                                    
                                       X
                                       i
                                    
                                    ,
                                    
                                       X
                                       j
                                    
                                    )
                                 
                                 =
                                 
                                    
                                       (
                                       
                                          
                                             ∑
                                             
                                                k
                                                =
                                                1
                                             
                                             q
                                          
                                          
                                             
                                                |
                                                log
                                             
                                             
                                                
                                                   
                                                      X
                                                      i
                                                   
                                                   
                                                      [
                                                      k
                                                      ]
                                                   
                                                   +
                                                   1
                                                
                                                
                                                   
                                                      X
                                                      j
                                                   
                                                   
                                                      [
                                                      k
                                                      ]
                                                   
                                                   +
                                                   1
                                                
                                             
                                             
                                                
                                                   |
                                                
                                                p
                                             
                                          
                                       
                                       )
                                    
                                    
                                       1
                                       /
                                       p
                                    
                                 
                              
                           
                        
                     
                  

Note that, the logarithms must be taken here, so that a/b and b/a lead to the same distance penalty for any a,b ∈ R, as ln|a/b|=ln|b/a|. Otherwise, d(
                        X
                     
                     
                        i
                     ,
                        X
                     
                     
                        j
                     ) does not equal to d(
                        X
                     
                     
                        j
                     ,
                        X
                     
                     
                        i
                     ). All feature data are added by 1 to avoid division by zero. Similar as above, Table 2
                      tabulates the results of the various p values used.

It is seen from Table 2 that, the recognition rate increases by approximately 1% when using the log Lp norm Eq.(2). An intuitive explanation is below: for the same |
                        X
                     
                     
                        i
                     [k]-
                        X
                     
                     
                        j
                     [k]| value, the actual meaning might be very different, e.g. if 
                        X
                     
                     
                        i
                     [k]=10 and 
                        X
                     
                     
                        j
                     [k]=5, it means the ith image has twice kth-class image patches than the jth image; if 
                        X
                     
                     
                        i
                     [k]=110 and 
                        X
                     
                     
                        j
                     [k]=105, it means the ith and jth image have almost the same kth-class image patches. This observation explains why division is better than subtraction here.

Note that the kNN classifier used above is the simplest and most primitive hard kNN (i.e. the k neighbors have equal voting weight), if we simply change the traditional kNN to the well-known soft kNN with the voting weight defined as 1/(d+100000), where d is the distance between the testing sample point and a k-nearest neighbor, the recognition rate increases to 76.89%, which is the same as the state-of-the-art method by Wang et al. [7]. Because tuning parameters are not the focus of this study, we use this kNN classifier (k=7) through the rest of the paper.

Here the L0.6 norm and the logarithmic space have no generally applicable meanings: it depends on the datasets, and is not optimal. The optimal norm might be L0.62 or somewhere else, the optimal space to measure the difference of two scalar quantities might be a linear combination or a more complicated transitional space between the original space and its logarithmic space. And there are different ways to map to various logarithmic spaces.

But anyway, the recognition rate is sensitive to any of the changes we made in these simple experiments, and thus inspired us that, if these changes can be generalized and parameterized, they are exactly the pivotal adaptive parameters that should be learned in a formalized learning model.

In this paper, we propose one feasible method of such generalization and parameterization: if we calibrate the measured numerics of the observed feature data in a deliberate way, even simple kNN can achieve competitive recognition accuracy.

In the proposed non-homogeneous ruler model, the pivotal adaptive parameters to learn is the distance between two uncalibrated numerics x and y. It can be considered as the 1D model with one degree of freedom added: the distance between the numerics x and y is parameterized as R[x,y]. The variables (or the adaptive parameters to learn) are R[0,1],R[0,2],R[0,3],….,R[0,V] with 2D indices, so it is called the non-homogeneous ruler 2D model. Where V is the maximal value of 
                        X
                     
                     
                        i
                     [k]. As 
                        X
                     
                     
                        i
                      is a histogram, V cannot exceed the number of image patches per image (which is 2500 for the joint image classification/annotation problem). Actually, there is far less than V variables here, because many numerics x ∈ [1,V] never appear in 
                        X
                     
                     
                        i
                     [k], especially when x is large.

Because for any numeric x, y, z, it is possible that R[x,y]+R[y,z]<R[x,z], so the 2D model violates the triangle inequality, one of the metric axioms. But still, we force the 2D model to comply with the constraints R[x,x]=0, R[x,y]=R[x,y] so that there are reduced number of variables, i.e. R[x,y] is a variable only if x<y. We force

                        
                           (3)
                           
                              
                                 
                                    
                                       
                                          R
                                          
                                             [
                                             
                                                x
                                                ,
                                                y
                                             
                                             ]
                                          
                                          ≤
                                          R
                                          
                                             [
                                             
                                                x
                                                ,
                                                y
                                                ,
                                             
                                             ]
                                          
                                          
                                          if
                                          
                                          y
                                          <
                                          
                                             y
                                             ′
                                          
                                          
                                          and
                                          
                                          R
                                          
                                             [
                                             
                                                x
                                                ,
                                                y
                                             
                                             ]
                                          
                                          ≥
                                          R
                                          
                                             [
                                             
                                                
                                                   x
                                                   ′
                                                
                                                ,
                                                y
                                             
                                             ]
                                          
                                          
                                          if
                                          
                                          x
                                          <
                                          
                                             x
                                             ′
                                          
                                          ,
                                          
                                             for
                                             
                                             any
                                             
                                             numeric
                                          
                                          
                                          x
                                          ,
                                          y
                                          ,
                                          
                                             x
                                             ′
                                          
                                          ,
                                          
                                             y
                                             ′
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                  

So that the resulted distance function is a semi-metric
                        2
                     
                     
                        2
                        So far we did not see a successful distance function learning methods (such as [4,9]) violate either the non-negativity, or the symmetry axioms. Strictly speaking, all the so-called metrics are pseudo-metrics, as the identity of indiscernibles axiom is not satisfied: there exist many ddij
                           =0 for i≠j, as long as 
                              L
                           
                           
                              i
                           
                           
                              =L
                           
                           
                              j
                            i.e. 
                              X
                           
                           
                              i
                            and 
                              X
                           
                           
                              j
                            have exact the same labels.
                     . To develop our proposed objective function, we still follow the basic idea of the Lp norm metric in Eq. (1). We set p
                     =1 to make our model easy to solve

                        
                           (4)
                           
                              
                                 d
                                 
                                    (
                                    
                                       X
                                       i
                                    
                                    ,
                                    
                                       X
                                       j
                                    
                                    )
                                 
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    q
                                 
                                 
                                    R
                                    [
                                    min
                                    
                                       {
                                       
                                          X
                                          i
                                       
                                       
                                          [
                                          k
                                          ]
                                       
                                       ,
                                       
                                          X
                                          j
                                       
                                       
                                          [
                                          k
                                          ]
                                       
                                       }
                                    
                                    ,
                                    max
                                    
                                       {
                                       
                                          X
                                          i
                                       
                                       
                                          [
                                          k
                                          ]
                                       
                                       ,
                                       
                                          X
                                          j
                                       
                                       
                                          [
                                          k
                                          ]
                                       
                                       }
                                    
                                    ]
                                 
                              
                           
                        
                     
                  

When solving machine learning problems, attention should be paid to equality constraints. Many equality rarely holds either theoretically (because the problem is over-constraint) or practically (because error is inevitable). In fact, most equality only implies approximate equality. Often, the difference between the left and right side of the equality is measured by a defined norm, and then accumulated (or integrated for continuous case) as an energy objective function for optimization. Using the least square criterion to measure the difference between both sides of Eq. (4) yields an energy objective function

                        
                           (5)
                           
                              
                                 E
                                 
                                    (
                                    R
                                    )
                                 
                                 =
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    N
                                 
                                 
                                    
                                       ∑
                                       
                                          j
                                          =
                                          i
                                          +
                                          1
                                       
                                       N
                                    
                                    
                                       
                                          
                                             (
                                             
                                                
                                                   ∑
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   q
                                                
                                                
                                                   R
                                                   [
                                                   min
                                                   
                                                      {
                                                      
                                                         X
                                                         i
                                                      
                                                      
                                                         [
                                                         k
                                                         ]
                                                      
                                                      ,
                                                      
                                                         X
                                                         j
                                                      
                                                      
                                                         [
                                                         k
                                                         ]
                                                      
                                                      }
                                                   
                                                   ,
                                                   max
                                                   
                                                      {
                                                      
                                                         X
                                                         i
                                                      
                                                      
                                                         [
                                                         k
                                                         ]
                                                      
                                                      ,
                                                      
                                                         X
                                                         j
                                                      
                                                      
                                                         [
                                                         k
                                                         ]
                                                      
                                                      }
                                                   
                                                   ]
                                                
                                                −
                                                d
                                                
                                                   d
                                                   
                                                      i
                                                      j
                                                   
                                                
                                             
                                             )
                                          
                                       
                                       2
                                    
                                 
                              
                           
                        
                     
                  

Because d(
                        X
                     
                     
                        i
                     ,
                        X
                     
                     
                        j
                     ) equals to d(
                        X
                     
                     
                        j
                     ,
                        X
                     
                     
                        i
                     ) and d(
                        X
                     
                     
                        i
                     ,
                        X
                     
                     
                        i
                     )=0, we only count d(
                        X
                     
                     
                        i
                     ,
                        X
                     
                     
                        j
                     ) when i<j. Where ddij
                      is the desired distance between 
                        X
                     
                     
                        i
                      and 
                        X
                     
                     
                        j
                     . Since our basic logic is to make samples with similar labels closer in the calibrated space, any two training samples 
                        X
                     
                     
                        i
                      and 
                        X
                     
                     
                        j
                      should have a desired distance ddij
                      completely determined by their labels. [4] deals with 7 single-label classification datasets, in which, ddij
                      is 0 if 
                        L
                     
                     
                        i
                     
                     
                        =L
                     
                     
                        j
                     , and 1 otherwise
                        3
                     
                     
                        3
                        Actually, it is equivalent to the intuition of the LDA (Linear Discriminant Analysis), i.e. minimize the distance of data points within the same class, and maximize the distance of data points between classes. The importance of the minimization and the maximization may be adjusted via a weight. But in [4] and our methods, this weight is implicitly set to 1. There might be some slight performance increase by tuning this weight.
                     . Although the feature vectors can be incorporated for computing the desired distance ddij
                      in an iterative framework to avoid overfitting and outliers [9], it is beyond the scope of this study. Here only inter-label dissimilarity is used to determine ddij
                     .

If aligning all variables R[x,y] (x<y) into a vector R, Eqs. (3) and (5) also constitute a convex quadratic programming problem [10]. It is obvious that E(R)≥0, so the resulted quadratic form is positive semidefinite, and the quadratic programming is convex. There are some publicly available generic packages for solving this kind of problems effectively [11]. More specifically, the optimization of the proposed 2D model costs 13160 seconds averagely on an Itanium 7–5500U (Quad Core 2.4GHz, 8G Ram memory) computer using Matlab R2013b.

It is not difficult to see that, the proposed 2D model is a generalized form of all the distance functions we used in Section 2, for whatever p values. The diagram of the complete main workflow of our proposed model is shown in Fig. 1.
                  

@&#EXPERIMENTAL RESULTS@&#

We evaluate our proposed 2D model in three application scenarios.


                        Dataset description and feature extraction: We used two benchmark colorful image datasets: a subset from LabelMe [6] and the UIUC-sports dataset [8], where each image contains one class label and several annotation terms. The LabelMe dataset contains eight classes: coast, forest, highway, inside-city, mountain, open-country, street and tall-building. In order to keep the balance of the number of images for each class, followed [7] we unified the size of each image data as 256×256×3 and then randomly selected 200 images for each class. Thus, the total number of images is 1600. The UIUC-sports dataset is an event dataset composed of eight classes as well: badminton, bocce, croquet, polo, rockclimbing, sailing and snowboarding. The number of images in each class varies from 136 (bocce) to 250 (rowing). The total number of images is 1578. In both datasets, we remove the annotation terms that occurred less than 3 times in each class. We refined the annotation terms based on the following two reasons. On one hand, if the number of data point with a certain annotation term is too small, from a statistic point of view, it is difficult to let the machine learn this annotation efficiently. On the other hand, since we will do two-fold cross validation in our experiment, if the annotation term has too small number of data points, we cannot evenly split the data to obtain the training sets and testing sets. At last, we get a refined LabelMe dataset with 58 distinct annotation terms and a refined UIUC-sports dataset with 90 distinct annotation terms respectively. On average, there are 4 terms per annotation in the processed LabelMe data and 6 terms per annotation in the processed UIUC-sports data.

Following the setting in [7], we used the 128-dimensional SIFT region descriptors selected by a sliding grid (5×5) and choose 256 as the number of codewords generated by K-means algorithm. Therefore, after the visual quantization, we get a histogram with 256 dimensions as the descriptor for the corresponding image data.


                        Image classification results: The recognition rates are compared with a recently proposed method [7]. As a baseline, we also present the results using the popular Support Vector Machines (SVMs) with radial basis function kernel [14], in which the optimal values for the parameters C and γ are adopted. Figs. 2
                         and 3
                         illustrate the results in the form of confusion matrices. It is seen that, the average recognition rate of our 2D model surpasses all others on both datasets.


                        Image annotation results: Two standard multi-label classification performance metrics precision and F1 score are used for evaluation purpose. Compared with four state-of-the-art methods, a deep-learning method i.e. the CPNNs (Compositional Pattern-producing Neural Networks) [5], the SVMs [14], the Local Shared (LS) subspace [15], and the multi-label informed Latent Semantic Index (LSI) [16] method, the results on the LabelMe and UIUC-sports datasets are tabulated in Table 3
                        . It is seen that, deep neural network is not quite suitable for the image annotation tasks, which is mainly due to the fact that, tuning the empirical parameters for the neural networks are difficult. It is also seen that, our 2D model has the highest performance over both performance indexes and datasets.

Three commonly used publicly available image annotation datasets, TREC Video Retrieval Evaluation (TRECVID) 2005 development set [17], Microsoft Research Cambridge (MSRC) dataset [18], and Barcelona dataset [19] are adopted for testing purpose. The multi-label image annotation problem is a very challenging task, for example, in the world-renowned TRECVID competition, there are 39 annotation labels, and the total number of possible annotation results to classify is 239 (approximately 512 billion). We compare our methods with eight state-of-the-art methods [[5],[20–24],[26],[27]].


                        Dataset description and experimental setup: The TRECVID 2005 dataset [17] contains 137 broadcast videos from 13 different programs in English, Arabic and Chinese, which are segmented into 74523 sub-shot. According to LSCOM-lite annotations [25], 39 concepts are labeled on each sub-shot, which consist of a wide range of genres including program category, setting/scene/site, people, object, activity, event, and graphs. Many of these concepts have significant semantic dependence between each other. Many sub-shots (75.67%) have more than one label, and some sub-shots are even labeled with as many as 17 concepts. Fig. 4
                         illustrates some sample images in this dataset. In our experiments, the images in this dataset are resized to half on both horizontal and vertical side.

The MSRC dataset [18] contains 591 images with 23 classes. Around 80% of the images are annotated with at least one classes and around three classes per images on average. The MSRC dataset provides image annotation at pixel level, where each pixel is labeled as one of 23 classes or “void”. As suggested by MSRC, “horse” and “mountain” are treated as “void” because they have few labeled instances. Therefore, there are 21 classes in total. In our experiments, we use only image level annotation built upon the pixel level.

The Barcelona dataset [19] contains 139 images with 4 categories, i.e., “building”, “flora”, “people” and “sky”. Each image has at least two labels.

For each dataset, we conduct 10-fold cross validation. Specifically, the images are randomly split into ten parts with equal size, and we selected each of the ten parts as testing set and the rest as training set. The average performance over the ten iterations is reported for evaluation.


                        Evaluation metrics and results: For performance evaluation, we adopt the widely-used performance metric, Average Precision (AP), as suggested by TRECVID [17]. Precision and recall are single-value metrics based on the whole list of object queries returned by the system. By computing a precision and recall at every position in the ranked sequence of objects, one can plot a precision-recall curve, plotting precision p(r) as a function of recall r. Average precision computes the average value of p(r) over the interval ranging from r=0 to r=1. We compute the AP for each concept and average the APs over all the concepts to obtain the Mean average AP (MAP) [24] as the overall performance evaluation, which are listed in Table 4
                        . It is seen that, our 2D model outperforms all other five methods by more than 5% and 10% on all three datasets, respectively. As analyzed in [24], the high performance may attribute to the usage of data-label correlations. As seen in Table 4, the results of our proposed method and the method in [24] do not outperform other methods over the Barcelona dataset as much as those in the other two datasets. This is partly due to the fact that, there are only four classes in the Barcelona data set which is much less comparing with the other two datasets, so there is actually no much label correlation information to be employed.

Spontaneous abortion is a kind of common pregnancy complications. Among young and healthy women, the ratio of abortion in the first three month of pregnancy is approximately a little more than 10%, according to clinical observation. Recurrent Spontaneous Abortion (RSA) refers to the successive occurrence of three or more spontaneous abortion, and its probability is about 1%. Nowadays, some researchers deem that, the successive occurrence of two or more spontaneous abortion can also be considered as the RSA. The RSA is owing to a variety of causes, such as chromosome abnormalities, endocrine abnormalities, immune factors, infection factors, and anatomical factors. Now it has been proved that, prostaglandin E2 (PGE2) plays an important role in the adhesion reaction and decidual process in embryos [28]. We detect the level of PGE2 test in venous blood of RSA patient subjects to predict the relations and occurrences of the RSA.

We select 32 RSA patient subjects visiting the Jinhua Polytechnic Obstetrics and Gynecology Clinics from Nov 2013 to Feb 2015, as the Spontaneous Abortion (SA) group. These subjects are with a small amount of bleeding in vaginal, and ultrasonic B scan implies the death of the embryo, or the SA has already occurred and the curettage of uterine is needed. 32 subjects with normal early pregnancy are randomly selected as the normal control group. All selected subjects satisfy the following condition: (1) within 12-week pregnancy period; (2) without performing any means of hormone replacement therapy in the recent three months; (3) no exposure to radiation and hazardous chemical substances; (4) no abnormality in the test of chromosome karyotype, gynecological examination, Coombs, TORCH and anticardiolipin antibodies; (5) with normal spouse semen examination results. The ages of both groups are between 23 and 33, and the pregnancy is from 7 to 12 weeks. After analyzing both the ages and the pregnancy weeks using statistics, no statistical significance is found.

After obtaining the consent from the subjects, 3ml fasting peripheral venous blood samples are collected, and their corresponding blood serum after centrifugation is immediately put into a −70° refrigerator for further experimentation. The level of hormone are determined as follows: after on-time thawing, the blood serum estrogen and the blood serum progesterone levels are obtained in the blood serum samples by the radio-immunity method, the prostaglandin E2 level is obtained quantitatively by the ELISA method. Our reagent boxes are bought from the Shanghai Lanji biotechnology Co. Ltd. The proposed method uses serum estrogen, serum progesterone, PGE2, pituitary prolactin, luteinizing hormone, estradiol, testosterone, follicle stimulating hormone, TORCH, infertility antibody, free three iodine thyroid original acid, free thyroid hormone, thyroid stimulating hormone, thyroid globulin antibody, thyroid peroxidase antibody and thyroid globulin to predict the probability of a subject having the RSA. An equal amount of subjects are selected from both groups for testing, and random cross validation is used. The following approximation is adopted for real feature elements: for any real feature elements 
                           X
                        
                        
                           i
                        [k], whose integer and decimal parts are denoted as integer(
                           X
                        
                        
                           i
                        [k]) and decimal(
                           X
                        
                        
                           i
                        [k]), respectively, its approximate feature value in the calibrated space is computed as (1−decimal(
                           X
                        
                        
                           i
                        [k]))R[integer(
                           X
                        
                        
                           i
                        [k])]+ decimal(
                           X
                        
                        
                           i
                        [k])R[integer(
                           X
                        
                        
                           i
                        [k])+1]. It can be verified that, the proposed energy minimization function is still a convex positive semidefinite quadratic programming problem after replacing 
                           X
                        
                        
                           i
                        [k] with this approximate feature value.

The prediction rate and false alarm rate of the two groups is listed in Table 5
                        , from which it is seen that, the deep learning method CPNNs is better than the SVMs, and our proposed method has the highest predication rate and the lowest false alarm rate.

Spontaneous abortion refers to stoppage of pregnancy within 28 weeks and with the weight of the fetus less than 1kg. With the recurrence of abortion, the spontaneous abortion rate of RSA patients at the third and the fourth pregnancy are as high as 40–60%, which seriously affects the reproductive health of women. The cause of the RSA is complicated, of which, 60-70% are still unknown.

The occurrence of spontaneous abortion probably connects to the abnormal contraction of uterine smooth muscle. Various kinds of ovarian hormone such as prostaglandin, estrogen, and progesterone adjust and control the contractivity of the uterine smooth muscle. Prostaglandin is an important intermediary for the physiological and pathological metabolic adaptations of humans, among which, PGE1, PGE2, and PGF2 have relatively strong biological effects to the reproductive system. All these three substances can be detected and measured in the blood, urine, and amniotic fluid of pregnant women, and their concentration increases as the pregnancy week increases. The activity of prostaglandin synthase (PGHS) and the PGHS-mRNA expression level go up in the amnion of normal or early-born infants [29]. The increase of prostaglandin makes the level of Calcium ion in uterine muscle cells going up, and activates myosin light-chain kinase, which causes uterine smooth muscle to contract, and increases the connections of uterine muscle intercellular gap for keeping the coordination of uterine contraction. Our research shows that, the level of PGE2 test among RSA patient subjects is obviously lower than the subjects in the normal control group, which implies that the lack of PGE2 is a probable cause of the RSA.

Estrogen can stimulate uterine and vagina smooth muscle to contract. During the period of ovulation, the level of estradiol (E2) increases, and meanwhile the frequency of uterine contraction is also increased. The effect of progesterone is to promote uterine rest, via cell nucleus receptor PRs (PRA and PRB). The PRB is more active in early pregnancy to promote uterine rest, while the PRA is more active in late pregnancy, and the PRA refrains the activity of the PRB [30]. In this research, we found that, the concentration of blood serum estrogen and progesterone among RSA patient subjects is obviously lower than the subjects in the normal control group. We also found that, there is positive correlation between the level of peripheral blood serum PGE2 and the level of progesterone in the RSA group, which lays some experimental foundations for the further investigation of the pathogenesis of the RSA, and also provides some reference value for the clinical prevention of the RSA.

In this paper we propose a novel non-homogeneous 2D ruler model for the sake of solving various types of annotation problems, and they are applied to several interesting real-world problems. The derivation of our proposed method shows an inverse methodology, compared to the traditional mode for machine learning researchers to solve problems. We use Darwin's theory of evolution as a philosophical analogue to compare the traditional and our new mode of thinking. Specifically, our non-homogeneous 2D ruler model is motivated by the in-depth analysis of some preliminary experiments, in which, sensitive changes of pivotal adaptive parameters in these simple experiments are generalized and parameterized to constitute a formalized learning model. Mathematically speaking, our model can be turned into a standard convex quadratic programming problem and resolved effectively by some existing public libraries. Generally, our proposed model is suitable for the feature data with high non-linearity, in which a direct calibration of the measured numerics of the observed feature data is necessary before performing further classification or regression. An interesting result of this paper is about the application of our model to the recurrent spontaneous abortion prediction medical annotation problem.

It has been realized by researchers [12] that, 8-month-old infants have access to a powerful mechanism of statistical learning for English word segmentation that its counterpart artificial intelligence tools cannot achieve. Therefore, a change of our prevailing mode of research methodology might be necessary, and this paper hopes to shed some light.

A possible future work is that, because our models only calibrate the feature data, it may be used as an enhancement, based on which, other more powerful methods can be developed, not just the kNN classifier.

@&#ACKNOWLEDGMENT@&#

This research work was supported by the Zhejiang Provincial Natural Science Foundation of China under Grant No. Y12H040007 (with the approval number LY12H04003) and the project of Shanghai Universities Young Teacher Training Scheme under Grant No. ZZJG15003.

@&#REFERENCES@&#

