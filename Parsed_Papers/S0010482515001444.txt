@&#MAIN-TITLE@&#Discrimination of retinal images containing bright lesions using sparse coded features and SVM

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           Automatic feature extraction from retinal images.


                        
                        
                           
                           Discrimination between retinal images containing bright lesions.


                        
                        
                           
                           Retinal images classification using sparse coding.


                        
                        
                           
                           Better performance than bag-of-word approach.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Diabetic retinopathy

Exudates

Drusen

Sparse coding

Classification

@&#ABSTRACT@&#


               
               
                  Diabetic Retinopathy (DR) is a chronic progressive disease of the retinal microvasculature which is among the major causes of vision loss in the world. The diagnosis of DR is based on the detection of retinal lesions such as microaneurysms, exudates and drusen in retinal images acquired by a fundus camera. However, bright lesions such as exudates and drusen share similar appearances while being signs of different diseases. Therefore, discriminating between different types of lesions is of interest for improving screening performances. In this paper, we propose to use sparse coding techniques for retinal images classification. In particular, we are interested in discriminating between retinal images containing either exudates or drusen, and normal images free of lesions. Extensive experiments show that dictionary learning techniques can capture strong structures of retinal images and produce discriminant descriptors for classification. In particular, using a linear SVM with the obtained sparse coded features, the proposed method achieves superior performance as compared with the popular Bag-of-Visual-Word approach for image classification. Experiments with a dataset of 828 retinal images collected from various sources show that the proposed approach provides excellent discrimination results for normal, drusen and exudates images. It achieves a sensitivity and a specificity of 96.50% and 97.70% for the normal class; 99.10% and 100% for the drusen class; and 97.40% and 98.20% for the exudates class with a medium size dictionary of 100 atoms.
               
            

@&#INTRODUCTION@&#

Diabetic Retinopathy (DR) is a chronic progressive disease of the retinal microvasculature that has became a major cause of vision loss in the world. Due to the burden of diabetes over the past decades, the prevalence of DR is expected to grow exponentially and affect over 300 millions people worldwide by 2025 [1,2]. Despite the high risk factor, it has been established that early detection and timely treatment can reduce the development of severe vision loss in 60% of cases [3]. However, DR does not exhibit any distinctive symptoms which the patient can easily perceive until a severe stage is reached. It is therefore important to develop Computer Aided Diagnosis (CAD) systems that can help clinicians in tasks such as large scale population screening and early diagnosis.The benefit of such a CAD system is to quickly pre-screen patients and orient those at risk to an ophthalmologist, reducing time and cost of a visit.

The diagnosis of DR is based on the detection of lesions or other vascular abnormalities in retinal images acquired by fundus cameras. Lesions of interest include microaneurysms, cotton wool spots, exudates, macular edema, and hemorrhages [4]. Many algorithms have been developed for DR detection over the past two decades, following a general pipeline: image quality verification, vessel segmentation, lesion detection, and automatic classification with a machine learning technique [5,6].

However, most of the existing algorithms are designed to detect a specific type of lesions in retinal images, though different lesions might be present in a single image indicating the stage of the disease. Moreover, the lesions can appear very similar while being signs for different diseases. For instance, DR is characterized, at early stages, by red lesions (microaneurysms) and bright lesions (exudates) which appear as small white or yellowish deposits with sharp edges in retinal images. On the other hand, Age Related Macular Degeneration (ARMD), which causes a gradual loss of central vision, is mainly characterized by variable sized yellowish deposits in the retina called drusens. Therefore, discriminating between different types of lesions is of great interest for decision making in an automatic CAD system.

In this paper, we propose an automatic classification of retinal images that discriminates between images containing different bright lesions. The method is based on sparse signal representation techniques that can capture strong structures of retinal images and produce discriminant descriptors for classification. In particular, the proposed methodology does not require any pre-processing of the images such as blood vessels segmentation or optic disc removal, and achieves higher performance than other feature representation techniques such as bag-of-visual-words approaches. This method can be used as an initial step for guessing the type of lesions a retinal image contains and then specific detection algorithms can be used for each type of lesion, increasing the performance of each specific detector by reducing the false positives.

The rest of the paper is organized as follows. We briefly review existing approaches for retinal images classification and discrimination in Section 2. Section 3 gives a detailed presentation of the proposed approach. In Section 4, different experiments are performed and the results are analyzed showing the good performances of our approach. Finally, concluding remarks are given in Section 5.

@&#RELATED WORKS@&#

Several approaches have been proposed in the literature for the detection of retinal features and lesions. The methods usually involve three main steps [5â€“7]. First, a pre-processing step is applied in order to compensate for the great variability between and within retinal images. The green channel is usually considered the most preferable choice, because it provides a maximum contrast between different retinal lesions and structures. The second step detects candidate regions that may correspond to retinal lesions and extracts features, while the last step classifies the candidate regions as retinal lesions or not.

Many of the methods proposed in the literature deal with the detection of a specific type of lesion, exudates or drusen for instance. A method to detect hard exudates from color fundus images was introduced by Garcia et al. [8]. The proposed method comprises a pre-processing step for luminosity and contrast normalization, candidate regions extraction based on local adaptive histogram thresholding, and classification using a neural network classifier. Note that prior to classification, the optic disk is removed since its characteristics are similar to that of exudate lesions. The method was evaluated on a dataset of 117 images, and achieved a sensitivity of 100%, a specificity of 77.78%, and a mean accuracy of 91.04%. An automatic method for identification of hard exudates in retinal color fundus images was proposed by Osareh et al. [9]. In this method, after image normalization via histogram specification and local contrast enhancement, the images are segmented using Fuzzy C-means (FCM) clustering algorithm. Finally, a three layers perceptron neural network is used for classification. Using 75 color images for training and 67 for testing, the method achieves a sensitivity of 95%, and a specificity of 88.9% for the image based classification task. Giancardo et al. [10] proposed a method for the diagnosis of diabetic macular edema (DME) based on exudates detection. The method uses a SVM classifier trained with a single feature vector per image obtained through color and wavelet analysis. The authors also provided a new publicly available dataset (HEI-MED) consisting of 169 patients from various ethnic groups and different levels of DME, and the method achieves an AUC (area under curve) of 88%. A detection and classification approach of DME severity is introduced by Deepak and Sivaswamy [11]. The detection is based on a supervised learning of characteristics of normal images. Thus, any deviation from these normal characteristics is considered as an indication of abnormality. The characteristics are computed from color and Radon features from different orientations. The method achieves a sensitivity of 100% and a specificity between 74% and 90% on a dataset of 644 images from publicly available datasets. Recently, Ali et al. [12] proposed an exudate detection method using a statistical atlas. The method first build a statistical atlas using a set of normal color retinal images. The atlas captures the chromatic distribution of the retinal images for a given ethnic group. Then, any test image from the same group is warped to the atlas coordinates frame and a simple distance map between the atlas and the test image suppresses anatomical structures such as optic disk, vessels and macula, and gives a good segmentation of the lesions. This method has the advantage of avoiding complex pre-processing of the images.

Age Related Macular Degeneration (ARMD), which causes a gradual loss of central vision, is mainly characterized by drusens. Hijazi et al. [13] proposed a method for drusen detection in retinal images based on angular and circular decomposition. The output of the decomposing step is a set of trees representing the images, and a weighted frequent sub-tree mining approach is used to determine the most frequent sub-trees. The weighted frequent sub-trees are then employed as features to train a classifier. Two classifiers are used for final classification, SVM and Naive Bayes. A total of 258 images from two different publicly dataset were used and the system achieved an accuracy of 100% and 95% with SVM and Naive Bayes respectively. Akram et al. [14] proposed an algorithm to automatically segment drusens in fundus images for ARMD diagnosis. The images are first pre-processed using morphological operations and contrast enhanced by applying an adaptive histogram equalization. Then, Gabor filters are used to select candidate regions from which color and texture features are extracted. Finally, a SVM is used for classification. The method achieves a sensitivity, a specificity and an accuracy of 95%, 98.4% and 97% respectively, using the STARE dataset of 400 images. Zheng et al. [15] introduced a system that uses a two-step procedure for drusen detection. First, a pixel-wise classification is performed using color features and SVM to predict whether a pixel corresponds to drusen or not. Then, a group-wise classification is exploited to remove false positive components from the pixel-wise classification. The system is validated by comparing its output with manually segmented drusen on a pixel by pixel basis. The system achieves an accuracy between 80% and 86% depending on the dataset used. In [16], Garnier et al. proposed an ARMD detection method from fundus images using a multiresolution texture analysis. The texture is analyzed at several scales using a wavelet decomposition in order to identify all the relevant texture patterns, and an image is finally described with the textural pattern distributions of the wavelet coefficient images obtained at each level of decomposition. The method achieves an accuracy of 93.3% on a small dataset of 45 images.

As mentioned in [17], a common limitation of current algorithms for the detection and classification of bright lesions in retinal images is the need for complex and empirical pre- and post-processing steps depending on the lesion of interest. These include image size and illumination normalization [18], the segmentation and removal of blood vessels [19,20], and the detection and removal of optic disc [21].

To avoid these pre- and post-processing steps, Rocha et al. [22] proposed a framework based on bags-of-visual-words (BoVW) representation which can be used for the detection of different lesions. This BoVW approach first extracts low-level features from the retinal images and build a visual vocabulary, or codebook, by quantization of the low-level features. The visual vocabulary is then employed to code each image and the obtained representation in terms of visual words in the dictionary is used to train a classifier that can distinguish different retinal lesions. The method has shown promising results for detection of both hard exudates and microaneurysms [22], and has been the basis of future work addressing the task of retinal images discrimination [17]. In particular, the system achieved an AUC of 95.3 for white lesion detection and 93.3 for red lesion detection using a dataset of 1232 images.

Few methods were proposed for the task of discriminating between retinal images containing different types of bright lesions. Niemeijer et al. [23] proposed a method that can automatically detect bright lesions in retinal images and can differentiate among exudates, cotton wool spots, and drusen. The method first detects potential lesions pixels in the images using a set of 14 digital filters and a k-nearest neighbor (k-NN) classifier. These pixels are grouped into probable lesion regions from which features such as contrast, shape and size characteristics, and proximity to vessels are extracted. Finally, a linear discriminant analysis classifier is trained to classify bright lesions into exudates, cotton-wool or drusen. The system achieves a sensitivity of 95% and a specificity of 88% using a validation set of 300 images. Grinsven et al. [24] proposed an algorithm to automatically retrieve and classify images with bright lesions, namely drusen and exudates, using the BoVW approach. The algorithm initially partitions the image into a fixed number of square patches and extracts a set of features from each patch. The features set includes color histograms, histogram of oriented gradients (HoG) and local binary patterns (LBP). A visual dictionary is created using these features and classification is performed with a weighted nearest neighbor classifier. The method achieves an AUC of 0.9 for the classification task with a dataset of 415 images. Pires et al. [25] have also used the BoVW approach in order to identify images with bright lesions such as hard exudates, cotton wool spots, and drusen, in addition to images with red lesions like hemorrhages and microaneurysms. They created a visual vocabulary and trained a SVM detector for each type of lesion. Then, the results of the individual detectors are combined into a meta-classification step that indicates whether or not a patient should be referred to an ophthalmologist for further review. The system achieves an AUC score of 93.4 using a dataset of 1077 images. A similar approach is adopted by Sadek et al. [26] where authors create a dictionary for different features (LBP, HoG and SURF) and finally concatenate the obtained histograms representation into a unique feature vector. This representation is then used with a SVM classifier to distinguish between retinal images containing drusen or exudates. Experiments with a dataset of 430 images show a mean classification accuracy of about 97.2%. A different approach for retinal images discrimination is proposed by Ujjwal et al. [27] based on visual saliency framework. They used a visual saliency detection method [28] to extract potential locations of abnormalities in retinal images, and regions with lesions are detected based on their saliency values and LBP features using a k-NN classifier. The method is reported to achieve a correct discrimination rate of 96.41% between respectively 171 drusen and 217 hard-exudates images.

Although the previously described methods reported high correct bright lesions classification and discrimination rates, they still require some pre-processing step or some manual annotation of the images. Indeed, the BoVW approach [24] needs a prior knowledge about the location of the optic disk and the macula, while the method described in [25] requires manual annotation of unhealthy regions. The correct classification rate of the method of Ujjwal et al. [27] depends on the output of the visual saliency detector, and the method in [26] requires the removal of blood vessels. In this paper, we propose an automatic classification and discrimination method for retinal images that require no prior knowledge such as the locations of some retinal structures (optic disc or macula), and that can learn discriminant features from the images directly based on the sparse coding principle.

@&#METHODOLOGY@&#

This section describes the proposed method for the discrimination of retinal images containing bright lesions based on sparse coding techniques. Sparse signal representations are becoming increasingly popular and lead to state-of-the-art results in various applications such as face recognition [29], image denoising and impainting [30], and image classification [31]. The main reason being the intrinsic sparse nature of image representations when using fixed bases such as DCT or wavelets [32,33]. In addition, the basis vectors can be learned from the data itself and be constrained to produce a sparse representation.

The goal of sparse modeling is to find efficient representations of signals (or images viewed as two-dimensional signals) as a linear combination of a few typical patterns, called atoms, selected from a dictionary. Given a dictionary matrix 
                           D
                           âˆˆ
                           
                              
                                 R
                              
                              
                                 n
                                 Ã—
                                 K
                              
                           
                         that contains K atoms as column vectors 
                           
                              
                                 d
                              
                              
                                 j
                              
                           
                           âˆˆ
                           
                              
                                 R
                              
                              
                                 n
                              
                           
                           ,
                           j
                           =
                           1
                           ,
                           â€¦
                           K
                        , the sparse coding problem of a signal 
                           y
                           âˆˆ
                           
                              
                                 R
                              
                              
                                 n
                              
                           
                         can be stated as finding the sparsest vector 
                           x
                           âˆˆ
                           
                              
                                 R
                              
                              
                                 K
                              
                           
                         such that 
                           y
                           â‰ƒ
                           Dx
                        . The problem is therefore to solve the following optimization problem:
                           
                              (1)
                              
                                 
                                    
                                       min
                                    
                                    
                                       x
                                    
                                 
                                 â€–
                                 x
                                 
                                    
                                       â€–
                                    
                                    
                                       0
                                    
                                 
                                 
                                 subjectto
                                 
                                 â€–
                                 y
                                 âˆ’
                                 Dx
                                 
                                    
                                       â€–
                                    
                                    
                                       2
                                    
                                 
                                 â‰¤
                                 Ïµ
                                 ,
                              
                           
                        where Ïµ is the reconstruction error of the signal 
                           y
                         using the dictionary 
                           D
                         and the sparse code 
                           x
                        .

Alternatively, one can also solve the following optimization problem:
                           
                              (2)
                              
                                 
                                    
                                       min
                                    
                                    
                                       x
                                    
                                 
                                 â€–
                                 y
                                 âˆ’
                                 Dx
                                 
                                    
                                       â€–
                                    
                                    
                                       2
                                    
                                 
                                 
                                 subjectto
                                 
                                 â€–
                                 x
                                 
                                    
                                       â€–
                                    
                                    
                                       0
                                    
                                 
                                 â‰¤
                                 Î»
                                 ,
                              
                           
                        where Î» is a specified sparsity level.

The vector 
                           x
                           âˆˆ
                           
                              
                                 R
                              
                              
                                 K
                              
                           
                         contains the representation coefficients of the signal 
                           y
                         w.r.t. the dictionary 
                           D
                        . As opposed to other representations such as PCA, in sparse coding one is looking for the sparsest vector 
                           x
                        , i.e. the vector with the smallest number of nonzero coefficients. In the above formulation, the l
                        0-norm (which is actually a pseudo-norm) counts the nonzero entries of a vector.

Since exactly solving the above optimization problem is a NP hard problem [32], approximate solutions are obtained using greedy algorithms such as matching pursuit (MP) [34] or orthogonal matching pursuit (OMP) [35]. A second class of methods relies on relaxation and replace the l
                        0-norm with an l
                        1-norm making the optimization problem a convex one that can be solved efficiently. Such methods are called basis pursuit (BP) [36].

A key issue for practical applications is the choice of the dictionary 
                           D
                        . One can use a pre-determined and fixed dictionary as is the case with wavelets, curvelets, or steerable filters transforms. But the pre-specified dictionary does not necessary well describe a given signal at hand. Hence, it is more appealing to learn the dictionary from a given set of training data. The K-SVD algorithm [37] is a standard unsupervised dictionary learning algorithm that generalizes the well-known K-means clustering algorithm.

Given a set of training signals 
                           Y
                           =
                           [
                           
                              
                                 y
                              
                              
                                 1
                              
                           
                           ,
                           â€¦
                           ,
                           
                              
                                 y
                              
                              
                                 m
                              
                           
                           ]
                        , where each 
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           âˆˆ
                           
                              
                                 R
                              
                              
                                 n
                              
                           
                        , K-SVD jointly finds a dictionary 
                           D
                           =
                           [
                           
                              
                                 d
                              
                              
                                 1
                              
                           
                           ,
                           â€¦
                           ,
                           
                              
                                 d
                              
                              
                                 K
                              
                           
                           ]
                         and an associated sparse codes matrix 
                           X
                           =
                           [
                           
                              
                                 x
                              
                              
                                 1
                              
                           
                           ,
                           â€¦
                           ,
                           
                              
                                 x
                              
                              
                                 m
                              
                           
                           ]
                           ,
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           âˆˆ
                           
                              
                                 R
                              
                              
                                 K
                              
                           
                        , by solving the following problem:
                           
                              (3)
                              
                                 
                                    
                                       min
                                    
                                    
                                       D
                                       ,
                                       X
                                    
                                 
                                 â€–
                                 Y
                                 âˆ’
                                 DX
                                 
                                    
                                       â€–
                                    
                                    
                                       2
                                    
                                 
                                 
                                 subjectto
                                 
                                 âˆ€
                                 i
                                 ,
                                 â€–
                                 
                                    
                                       x
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       â€–
                                    
                                    
                                       1
                                    
                                 
                                 â‰¤
                                 Î»
                                 .
                              
                           
                        
                     

The K-SVD algorithm iteratively solves this optimization problem by alternating between computing the sparse code matrix 
                           X
                         and the dictionary 
                           D
                        . Given 
                           D
                        , the sparse coding problem is solved using any pursuit algorithm (MP, OMP or BP). Given the codes 
                           X
                        , the dictionary is sequentially updated, i.e. one atom at a time, using singular value decomposition (SVD). This provides a very effective unsupervised dictionary learning technique. After learning the dictionary, each given signal 
                           
                              
                                 y
                              
                              
                                 i
                              
                           
                           âˆˆ
                           
                              
                                 R
                              
                              
                                 n
                              
                           
                         is represented by the corresponding feature vector (or sparse code) 
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           âˆˆ
                           
                              
                                 R
                              
                              
                                 K
                              
                           
                        . This feature can then be used for recognition or classification tasks.

The first step in the process of using sparse representation for image classification is to learn a dictionary that captures strong and discriminative structures of the images. We use the K-SVD algorithm for dictionary learning [37].

The dictionary is learnt from low-level features extracted from the training data. Several works have shown that applying sparse coding to local parts or descriptors of the images can capture higher-level features compared to raw image patches [38,39]. In our work, we divide the image into square patches of fixed size and extract low-level features from each of the patches. Since the images used in the experiments are from different sources, as explained in Section 4, we resize them to a fixed resolution of 512 Ã— 512 using bilinear interpolation, then we extract the following types of low-level features:
                              
                                 â€¢
                                 
                                    Color features: Color features are extracted in image patches of size 8 Ã— 8. For an image size of 512 Ã— 512, one thus have 4096 feature vectors extracted. Each of the feature vectors is the concatenation of color histograms from normalized 
                                       r
                                       ,
                                       
                                       g
                                     and b components from RGB color space, and from 
                                       h
                                       ,
                                       
                                       s
                                       ,
                                       
                                       v
                                       ,
                                       
                                       Cb
                                     and Cr from HSV and YCbCr color spaces respectively. We use histograms of size 8 in our experiments, so that, in total, every feature vector is of dimension 64.


                                    SIFT: Dense SIFT features are extracted over a regular grid on the image. The grid points correspond to the corners of the square patches, so we use a grid step size of 8 pixels. For every grid point, a SIFT descriptor of dimension 128 is extracted.


                                    HOG: Histogram of oriented gradients (HOG) is extracted over a regular grid on the image, with a grid step size of 8 pixels. For every grid point, a descriptor of dimension 31 is extracted.


                                    LBP: Local binary patterns (LBP) are extracted over a regular grid on the image, with a grid step size of 8 pixels. For every grid point, we extract a feature vector of size 58.

Color features capture the local color appearance of image regions and are good indicator of the presence of bright lesions which appear as small white or yellowish deposits in the retina. LBP features capture the local texture of image regions, while SIFT and HOG features are based on the local distribution of gradient information. Thus, they capture the local shape of the regions. Note that except for color features, all other features are extracted from the green channel of the fundus image. For each feature type, the features extracted from all training images are put as columns of a matrix 
                              Y
                              =
                              [
                              
                                 
                                    y
                                 
                                 
                                    1
                                 
                              
                              ,
                              â€¦
                              ,
                              
                                 
                                    y
                                 
                                 
                                    m
                                 
                              
                              ]
                           , where each 
                              
                                 
                                    y
                                 
                                 
                                    i
                                 
                              
                              âˆˆ
                              
                                 
                                    R
                                 
                                 
                                    n
                                 
                              
                            is a feature vector and m is the total number of local patches from the training images. We use the K-SVD algorithm, explained in Section 3.1, to learn a dictionary 
                              D
                              =
                              [
                              
                                 
                                    d
                                 
                                 
                                    1
                                 
                              
                              ,
                              â€¦
                              ,
                              
                                 
                                    d
                                 
                                 
                                    K
                                 
                              
                              ]
                           , where K is the size of the dictionary.

Every image I can then be coded using the learnt dictionary 
                              D
                           . First, a set of low-level features is extracted from the image to form a feature matrix 
                              
                                 
                                    F
                                 
                                 
                                    I
                                 
                              
                              âˆˆ
                              
                                 
                                    R
                                 
                                 
                                    n
                                    Ã—
                                    p
                                 
                              
                           , where p is the number of local patches. Given the dictionary 
                              D
                           , the sparse codes matrix 
                              
                                 
                                    X
                                 
                                 
                                    I
                                 
                              
                              âˆˆ
                              
                                 
                                    R
                                 
                                 
                                    K
                                    Ã—
                                    p
                                 
                              
                            is found that solves the optimization problem in Eq. (1), so that 
                              
                                 
                                    F
                                 
                                 
                                    I
                                 
                              
                              â‰ƒ
                              
                                 
                                    DX
                                 
                                 
                                    I
                                 
                              
                           . Note that each column of 
                              
                                 
                                    X
                                 
                                 
                                    I
                                 
                              
                            is a sparse vector that represents the corresponding feature vector from the image I, i.e. the corresponding column in 
                              
                                 
                                    F
                                 
                                 
                                    I
                                 
                              
                           . Finally, the set of sparse codes is combined into a single and global descriptor as follows [31]:
                              
                                 (4)
                                 
                                    
                                       
                                          f
                                       
                                       
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          max
                                       
                                       
                                          j
                                       
                                    
                                    (
                                    |
                                    
                                       
                                          X
                                       
                                       
                                          I
                                       
                                    
                                    (
                                    i
                                    ,
                                    j
                                    )
                                    |
                                    )
                                    ,
                                    
                                    âˆ€
                                    i
                                    =
                                    1
                                    ,
                                    â€¦
                                    ,
                                    K
                                    .
                                 
                              
                           
                        


                           Fig. 1
                            shows the overall flowchart of the proposed feature extraction method based on sparse coding.

After dictionary learning, every image is sparse coded using the obtained dictionary and represented as a single feature vector as explained in Section 3.2.1. These feature vectors are then used as an input for training a SVM classifier. We used linear SVM as they can be implemented with linear complexity in training and testing phases [40].

The sparse coding technique described above is closely related to the common bag-of-visual-worlds (BoVW) approach. In the BoVW model [41], a visual dictionary, or codebook, is created by quantization of the low-level features extracted from the training images. A traditional unsupervised method for computing the dictionary is to apply K-means clustering technique to the set of low-level features, and use the k centroids as codewords, i.e. elements of the visual dictionary. A new image is then represented by a vector indexed by the codewords: for each element d
                           
                              i
                            of the dictionary, ones count the number of low-level features in the image whose closest codeword is d
                           
                              i
                           .

The visual dictionary in the BoVW model corresponds to the one obtained via sparse coding (Section 3.1). However, a key difference lies in the fact that the BoVW approach creates the dictionary from quantization of the low-level features directly and assigns each feature to the nearest codeword. Sparse coding techniques, on the contrary, encode each low-level feature as a weighted sum of dictionary elements, optimizing a tradeoff between reconstruction error and the l
                           1 norm of the reconstruction weights. This non-linear encoding scheme provides better representation of the images, as we will see in the experiments (Section 4).

@&#EXPERIMENTS AND RESULTS@&#

In this section, we describe the different experiments performed to evaluate our method, and present the results obtained in discriminating between retinal images containing bright lesions, namely exudates and drusen, and normal retinal images which contain no lesions. In particular, we compare different low-level features and compare our proposed method using sparse coding with BoVW approaches used in [26,24].

We use in this study a dataset of 828 images with 452 normal images, 85 images with drusen and 291 images containing exudates. These images are sampled from six public datasets and one private database, listed in Table 1
                        . The idea of using images from different datasets is to evaluate the robustness of the proposed method to images acquisition variability. Indeed, the images from the several datasets were collected in different environments with different cameras. The partition of the images into the three classes is given in Table 1, and some images are shown in Fig. 2
                        . Note the variability in the image appearances, different illumination and contrast.

In all our experiments, we have used the KSVD-box
                           1
                        
                        
                           1
                           
                              http://www.cs.technion.ac.il/~ronrubin/software.html.
                           
                         for sparse modeling, and the Vlfeat library
                           2
                        
                        
                           2
                           
                              http://www.vlfeat.org.
                           
                         for features extraction.

In this section, we evaluate the performances of the proposed retinal images discrimination method. We perform different experiments, and use 10-fold cross validation. The dataset is split into 10 subsets, and the dictionaries are learnt with 9 of the subsets while classification is performed with the remaining subset. This process is repeated 10 times ensuring that each image is used for both training and testing. Thus, the method is tested with 10 different training and testing sets and the average classification performance is reported. The performance is evaluated in terms on classification accuracy, sensitivity and specificity. Since, we have three classes (normal, exudates and drusen) we compute the classification performance scores for each individual class in a one-vs.-rest manner. For example, for the drusen class we consider all drusen images as positive samples and all normal and exudates images as negative samples.

In the experiments, we vary the size of the dictionary from 10 to 1000, and we set the sparsity level (the parameter Î» in Eq. (3)) to 3 as this value gives the best classification results.

In this experiment, we test the performance of the proposed method in correctly identifying a normal image, i.e. a retinal image free of lesions. All 452 normal images in the dataset are used as positive examples and all drusen and exudates images are used as negative ones. The results obtained with the proposed sparse coding approach using SIFT features are given in Table 2
                           . As can be seen, the classification performance increases with the size of the dictionary. For a medium size of 100, the method achieves an accuracy of 97.50%, a sensitivity of 96.50% and a specificity of 97.70%. For a larger dictionary of size 300, the method achieves near perfect classification results despite the high variability in the images which are selected from various sources. The method achieves an accuracy of 99.80%, a sensitivity of 100% and a specificity of 99.70%. This shows that the proposed method is able to correctly discriminate a retinal image containing no lesions from images with bright lesions.

This second experiment aims at evaluating the performance of the proposed method in correctly identifying an image containing drusen. All 85 drusen images of the dataset are used as positive examples and all normal and exudates images are used as negative ones. The results obtained with the proposed sparse coding approach using SIFT features are given in Table 3
                           . As in the previous experiment, the classification performances increase as the dictionary size increases. However, for the identification of drusen images, near perfect results are obtained with a dictionary of medium size 100 with an accuracy of 99.80%, a sensitivity of 99.10% and a specificity of 100%. For a dictionary of size 300 or above, all drusen images are correctly identified and no other image is misclassified. The proposed method achieves an accuracy, a sensitivity and a specificity of 100%.

In this last experiment, we evaluate the performance of the proposed method in correctly identifying an image containing exudates. All 291 exudates images of the dataset are used as positive examples and all normal and drusen images are used as negative ones. The results obtained with the proposed sparse coding approach using SIFT features are given in Table 4
                           . As can be seen, the proposed method performs well in this experiment and is able to correctly identify retinal images containing exudates from other images. In particular, the method achieves an accuracy, a sensitivity and a specificity of 99.80%, 100% and 100% respectively, with a dictionary of size 300.

A comparison of the classification results obtained for each one of the three classes is shown in Figs. 3â€“5
                           
                           
                           . We can observe from these figures that the performances obtained for the normal class (normal-vs.-rest experiment) are slightly lower than those obtained for the drusen class (drusen-vs.-rest experiment) and the exudates class (exudates-vs.-rest experiment). For example with a dictionary of size 100, the accuracy values are 97.5%, 99.8% and 97% for the normal, drusen and exudates classes respectively. The sensitivity values are 96.5%, 99.10% and 97.4% for the three classes, and the specificity values are 97.7%, 100% and 98.2%. For larger dictionaries, a size of above 500 atoms, the method achieves perfect or close to perfect results for all three classes.

In this section, we evaluate the performances of the different feature types introduced in Section 3.2.1, namely SIFT, LBP, HOG and COLOR features. Again, we evaluate the performances of the classification method using different types of features for each individual class in a one-vs.-rest manner. We use for this comparison a dictionary of size 100 (the main observations are similar for varying dictionary sizes).

The results for the experiments are given in Table 5
                         and in Figs. 6â€“8
                        
                        
                        . We can see from these results that sparse coding applied to SIFT features outperforms other feature types. The classification accuracy obtained with SIFT features is higher than the ones obtained with other features whose performances are comparable. We obtain an accuracy of 97.5%, 92.6%, 88.3% and 92.3% with SIFT, COLOR, LBP and HOG features respectively, for the normal class. For the drusen class, COLOR features achieve very good classification results but slightly below the results obtained with SIFT features. However, for the exudates class, the difference is more significant in favor of SIFT features. For this class, using SIFT features we obtain an accuracy of 97.7%, a sensitivity of 97.4% and a specificity of 98.2%, while using COLOR features these figures are respectively 93.8%, 95.4% and 91.7%.

From Figs. 6â€“8, we can conclude that SIFT features perform best among all features for all three classes. In particular for the difficult classes of normal images, SIFT achieves a sensitivity of 96.5%, while the second best feature COLOR achieves a sensitivity of only 80.6%. We can also see that all features provide slightly better results for the drusen and exudates classes in comparison with the normal class.

The results in Table 5 also show the standard deviation of the different performance measures for each feature type. As can be seen, the results obtained with SIFT features are more stable than results with other features as shown by lower standard deviation values.

In this section, we propose to combine different feature types to improve the classification performances. Based on the results of the previous section, where we observed that SIFT features give the best classification performances, we tested the combination of SIFT features with other features. More precisely, we tested the following combinations: COLOR+SIFT, LBP+SIFT, HOG+SIFT and LBP+HOG+COLOR.

The results in Figs. 9â€“11
                           
                           
                            show that combining SIFT features with other feature types slightly improve the classification results. In particular, we observe a slight improvement when using SIFT combined with COLOR features. The classification accuracy increases from 97.50% with SIFT to 98.90% COLOR+SIFT for the normal class, and from 97.7% to 99.8% for the exudates class. For the drusen class, the performances are the same when using a dictionary of size 100. The combinations LBP+SIFT and HOG+SIFT give similar results compared with SIFT features alone. It is also interesting to observe that the combination LBP+HOG+COLOR performs the worse achieving lower performance than SIFT alone. This is particularly true for the sensitivity measure (Fig. 10) as LBP+HOG+COLOR gives a sensitivity of 80.4% for the normal class while SIFT achieves a sensitivity of 96.5% and COLOR+SIFT a sensitivity of 98.9%.

The good results obtained when combining COLOR and SIFT features can be explained by the fact that SIFT features are extracted from the green channel of the color fundus image, thus ignoring the chromatic information. Thus adding color information to SIFT features, the descriptor captures more information. However, as seen in the results, combining LBP, HOG and COLOR features does not provide better performances compared with SIFT.

In this section we compare the proposed method based on sparse coding with the bag-of-visual-words (BoVW) approach. The BoVW approach is used by Grinsven et al. [24] and by Sadek et al. [26] to create a dictionary and represent an image by a histogram of visual words occurrences.


                        Table 6
                         shows the results of the comparison for the normal class. We can see that both methods perform well for this class with the same accuracy of 93.7% for a small dictionary of size 50, and a slightly better accuracy for sparse coding for larger dictionaries. With a dictionary of size 100, the sparse coding approach and the BoVW method give an accuracy of 97.50% and 95.30% respectively. However, there is a significant difference when considering the sensitivity and specificity measures. For example, with a dictionary of size 100, the BoVW approach achieves a sensitivity of 87.30% while the sparse coding approach achieves a sensitivity of 96.50%. With a larger dictionary, 500 atoms, these two figures are 92.5% and 98.5%. Thus, we can see that the sensitivity of BoW, for the normal class, reaches a plateau of around 92% when the sensitivity value of sparse coding keeps increasing to reach 100% with 1000 atoms. The same observations apply to the other classes and the results are shown in Tables 7 and 8
                        
                        . It is important to note that the classification results obtained with the proposed sparse coding approach are more stable than those obtained with the BoVW approach with respect to the training set used. This can be seen in the standard deviation values in Tables 6â€“8.

@&#CONCLUSION@&#

In this paper, we have proposed an automatic classification of retinal images that discriminates between images containing different bright lesions, namely drusen and exudates. The method is based on sparse signal representation techniques that capture strong structures of retinal images and produce discriminant descriptors for classification. In particular, the proposed methodology does not require any pre-processing of the images such as blood vessels segmentation or optic disc removal, and achieves higher performance than other feature extraction schemes such as bag-of-visual-words approaches. We obtained excellent classification results with a dataset of 828 retinal images from various sources, showing the robustness of the approach to images acquisition variability such as varying illumination, contrast and resolution. In particular, using a dictionary of size 100 and a linear SVM with sparse coded features, the method achieves a sensitivity and a specificity of 96.50% and 97.70% for the normal class; 99.10% and 100% for the drusen class; and 97.40% and 98.20% for the exudates class.

The proposed framework can be easily extended to include other type of lesions such microaneurysms which are subtle and difficult to distinguish. Another future work direction would be the identification of the precise location of the lesions as well as their size and quantity in order to estimate the degree of severity of the disease.

None declared.

@&#REFERENCES@&#

