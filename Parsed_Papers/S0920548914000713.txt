@&#MAIN-TITLE@&#A process capability based assessment model for software workforce in emergent software organizations

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A software workforce assessment model based on software processes is developed.


                        
                        
                           
                           The model focuses on processes and roles of emergent software organizations.


                        
                        
                           
                           It helps to identify the level and priority of knowledge units for software roles and actors.


                        
                        
                           
                           The results facilitate the improvement of the software roles, practitioners and process.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Software personnel assessment

Software training assessment

Software process improvement

SW-CMM

SWEBOK

@&#ABSTRACT@&#


               
               
                  Software process improvement frameworks for software organizations enable to identify opportunities for improving the processes as well as establishing road maps for improvement. However, software process improvement practice showed that to achieve a sustained, leveraged state, software organizations need to focus on the workforce as much as the process. Software process improvement frameworks address the people dimension indirectly through processes. To complement process assessment models/methods, there is a need of mechanisms that address the problem of “how to assess, identify and prioritize detailed skill and knowledge improvement needs in relation to roles and processes of software organizations”. In this study, we developed a Software Workforce Assessment Model (SWAM) for emergent software organizations to perform role based workforce skill assessment aligned with software processes by coupling SW-CMM and SWEBOK models. SWAM is developed in accordance with the widely accepted assessment and evaluation theory principles. It is composed of an assessment baseline for software roles, criteria and scales for assessment. A SWAM based assessment process uses specific techniques such as Euclidian distance and dendogram diagrams to obtain useful results from data obtained from assessments. Through a case study, SWAM is shown to be applicable and the results are valuable for an emergent software organization. Specifically, the assessment enables the organization to identify priority knowledge units, to decide the extent of trainings for groups of individuals, to effectively assign project roles, to identify improvement priorities for the practitioners related to their roles and finally facilitates enactment and improvement of the software processes.
               
            

@&#INTRODUCTION@&#

The software process is a set of activities, methods, practices and transformations that software engineers and users use to develop and maintain software products [1]. During the last two decades, a number of software process improvement frameworks including software process assessment methodologies and underlying process reference models have been developed. Among these, Software Capability Evaluation (SCE) [2], Standard Capability Maturity Model Integrated (CMMI), Appraisal Method for Process Improvement (SCAMPI) [3,4] and Software Process Improvement and Capability Determination (SPICE) [5,6] have been widely used.

Software process improvement frameworks enable software organizations to identify opportunities for improving the processes as well as establishing road maps for improvement [7]. However, software process improvement practice showed us again and again that to achieve a sustained, leveraged state, software organizations need to focus on people more than anything else [8–11]. Software process improvement frameworks address the people dimension indirectly through processes. The underlying process model describes roles and associated practices defined in each process area and accordingly software practitioners should be trained in accordance with the roles they are assigned.

To improve software practitioner skills with the methods based on execute–evaluate–change cycle P-CMM [12,13], Personal Software Process (PSP) [14], to guide practitioners to build a self-directed teams Team Software Process (TSP) [15,16] and to guide organizations in managing and developing their workforce People Capability Maturity Model (P-CMM) [12,13] have been developed. However, these models/methods, do not specifically address the problem of “how to the organization will identify detailed improvement opportunities of software practitioners according to organization's software processes”.

In this study, we have developed a Software Workforce Assessment Model (SWAM) for emergent software organizations. The aim of SWAM is to perform role based personnel skill assessment aligned with software processes. SWAM is developed in accordance with the assessment and evaluation principles. It is composed of an assessment baseline for software roles, criteria and scales for assessment. A SWAM based assessment process uses specific techniques such as Euclidian distance and dendogram diagrams to obtain useful results from data obtained from assessments. SWAM couples the process and workforce aspect and is an enabler to identify and prioritize specific and concrete improvement and training needs in relation to software processes and roles. Its outputs can be effectively used for training programs at individual and organizational levels. Specifically, the knowledge profiles and dendogram diagrams for roles generated help to identify top priority knowledge units for the organization, to decide the type of the trainings for determined groups of individuals, to effectively assign project roles, and to identify improvement priorities for the practitioners related to their roles in the projects. In this respect, SWAM assessment is complementary to mostly used software process assessment methods (SCE and SCAMPI) as well as people capability maturity models (PSP, P-CMM and TSP).

In the following sections, first existing assessment models, and their positions related to workforce assessment are summarized. Then, the development of software workforce model in accordance with the evaluation principles is given. Thirdly, the SWAM based assessment process coupled with a case study to validate the model and findings of the conducted case study is presented. Finally, possible improvement directions and limitations of the study are given.

The most frequently used process assessment methods by the software community are SCE [2] and SCAMPI [3]. These methods are based on the Capability Maturity Model for Software (SW-CMM) and CMMI, developed by the Software Engineering Institute (SEI). The SEI's models describe five levels of process maturity. An organization willing to improve its software process must evolve through all these levels. Moreover, (in SW-CMM and staged version of CMMI) each maturity level is composed of a set of key process areas (KPA), and each KPA consists of key practices that accomplish the goal of the process areas. In addition to evaluation to KPAs, SCAMPI includes evaluation of technology and experience level of the software development personnel to a certain extent. The underlying process model describes roles and associated practices defined in each process area and accordingly software practitioners should be trained in accordance with the roles they are assigned. However, training needs assessment of software workforce and role assignment mechanism based on capability profile of practitioners is not explicitly defined.

In addition to organizational process oriented approach of CMMI, Humphrey [12,13] suggested PSP as a self-improvement framework defined in terms of a set of large-system software methods and practices. In essence, the PSP provides a software engineer with the tools necessary to improve his skills using an execute–evaluate–change cycle. As a complementary to PSP, the TSP extends and refines the CMMI and PSP methods to guide engineers in their work to build a self-directed team and to perform as an effective team member. Humphrey [16] argues that, the CMMI, PSP, and TSP provide an integrated three-dimensional framework for process improvement. However, neither PSP nor TSP provides a concrete and explicit model to assess the workforce of software development from the abstraction level of an organization.

It has been agreed in many studies that human resource management practices are among the indispensable critical success factors for quality software processes [17,11]. In order to complement the above approaches, SEI developed the P-CMM [12,13] to guide organizations in managing and developing their workforce. P-CMM uses the same principles and structure as the CMMI. P-CMM has four KPAs that address training issues: one at level 2 and three at level 3. However, the P-CMM does not bring an organizational focus to training at level 2. In fact, the “Training” KPA resides at level 2 and describes the training program for the unit or project as in the case of ISO 15504. Only, at level 3 the “Knowledge and Skills Analysis” KPA focuses on the identification of the core competencies of the organization and the knowledge/skills required to perform the processes. Even if the P-CMM was to be tailored to be implemented according to the needs and capabilities of small organizations, it does not suggest an assessment baseline or best target profiles for roles in accordance with enacted software development processes. It only provides the roadmap for a generic human resource management process and in fact could be applied to any organizations of other domains.

Differently from SW-CMM and CMMI (Staged Model of CMM), ISO/IEC TR 15504's model is based on two dimensions: a process and a process capability dimension. A software organization is assessed in the process dimension against the process attributes in the capability dimension. ‘Human resource management process’ defined in ISO/IEC TR 15504 is aiming to provide the organization and projects with individuals who possess skills and knowledge to perform their roles effectively and to work together as a cohesive group. If this process is successfully implemented, firstly the roles and skills required for the operations of the organization and the project will be identified through timely review of the organizational and project requirements; secondly training will be identified and conducted to ensure that all individuals have the skills required to perform their roles and finally individuals with the required skills and competencies will be identified and recruited by using objective mechanisms, or they will be trained as appropriate to perform the organizational and project roles. Although, it is clear that in order to reach these objectives an assessment for workforce is needed, ISO 15504 does not explicitly define or suggest a methodology for this purpose. Furthermore, ISO 15504 suggests that the trainings must be performed mostly on project bases when needed. However, the ISO/IEC TR 15504 base practices themselves require knowledge and skills in certain areas irrespective to any project domain. The issue of how necessary training will be identified to perform these is not addressed.

Finally, ISO 9001 2000 [18] model provides organizations with guidance to achieve compliance with this standard. The quality system compliance with the standard requirements confirms that the company has achieved such a maturity level that it is capable of defining the processes and performing them according to the definitions [19]. However, the model says nothing about which methods organizations should use to meet these requirements or what kind of knowledge and skill are required for the practitioners of these activities. The method is left completely to the particular organization. For instance, if an item in the ISO model demands that the organization define and set up the procedures for staff training, in fact it is assumed that the organization has a defined training scheme and documented procedures for identifying training needs with respective contents.

Current improvement paradigms define why/how an identified process is to be performed in the context of the goals, objectives and constraints of a project or organization. For improving the quality of software production, process dimension has been the main aspect used and in addition to organizational wide assessment models, even process standards for specific software life-cycle phases are developed [20]. The basic assumption is that process assessment scores are positively related with organizational effectiveness. They do not, however, provide the necessary details to assess the capabilities of workforce to enact these processes. Especially, in small organizations, the quality of workmanship involved in the software process is as important as the process. Thus an explicit and detailed method is also needed for assessing the workforce component especially for emergent software organizations.

SWAM is developed based on general evaluation and assessment principles [21–23] which are widely used in education field. In essence five main steps must be considered for an evaluation:
                        
                           1.
                           Determination of the object under assessment.

Elicitation of the characteristics/criteria.

Determination of the ideal characteristics of the object attributes to be compared to: the assessment baseline.

Selection or development of the assessment technique to be used for collecting information about the actual object.

Development of evaluation process in order to organize and synthesize the information.

The object of the software practitioner assessment model is the software practitioner. We define software practitioner as somebody who is performing some of the activities needed to produce a software system, organized according to a particular organizational structure and equipped with knowledge and skills to perform those activities. This definition although delimits the software practitioner from a high abstraction level a more detailed and explicit delimitation is required. In order to do this we use two reference frameworks, SWEBOK [24] and SW-CMM [25]. Although other reference frameworks could be used for processes and knowledge description, in this study we have considered SWEBOK [24] which explicitly defines the sub knowledge components of the software engineering domain. SWEBOK helps to define ‘the software practitioner in terms of knowledge profile’ whereas SW-CMM [25] explicitly defines the extent of the activities that a software practitioner performs. By taking into account the process areas and activities of associated roles, we decomposed the role descriptions to manageable components to be able to determine the required body of knowledge units (KU) to conduct associated activities. In this way the object in the proposed assessment model is composed of the software practitioners delimited with roles assigned to them and their software engineering knowledge and skills. In this way, it becomes possible not only to assess the workforce capabilities but also to check that the practitioners are positioned to appropriate roles. However, there are some scope constraints using these two reference models.

Firstly, The SW-CMM was written to provide good software engineering and management practices for any project in any environment. The model is described in a hierarchy with 5 maturity levels including 18 key process areas with 316 key practices. In addition, SW-CMM proposes more than 20 roles or groups with varying responsibilities and tasks on level 2. In a small organization, there might not be enough people to take all those roles for implementing SW-CMM (as regards money, time and resources) [26] and the model must be scaled down according to the resources of small organizations. For this reason, we propose high level generic roles associated with level 2 and the SPE process area of level 3 because they include the fundamental activities regardless of the maturity of the software organization. These roles are depicted in Fig. 1
                        . In addition, Table 1
                         shows the occurrences of these software roles in these process areas.

On the other hand, the SWEBOK is a structured model based on the Software Life Cycle Process defined in ISO/IEC 12207 [27] standard that describes the knowledge and skills that institute the software engineering discipline. SWEBOK is composed of ‘Software Product Engineering’, ‘Software Management’, ‘Software Domains’ and ‘Computing Fundamentals’ knowledge categories which are in turn composed of knowledge areas. Although SWEBOK does not provide the relative effect of the knowledge areas on the body of knowledge, a balance between appropriate depth and detail of knowledge descriptions exists. The well-defined structure and its emphasis on balanced knowledge descriptions enabled us to use SWEBOK as the second reference model.

For two reasons, only Software Management (SM) and Software Product Engineering Knowledge Categories (SPE) are taken in the scope of the SWAM. Firstly, the SW-CMM is applicable to software organizations regardless of the application domain and does not include any domain specific process or activity. Instead, SW-CMM refers to application domain experts or people who are experienced in developing software for real time systems or artificial intelligence. Secondly, ‘Computing Fundamentals’ knowledge category is a prerequisite for software practitioners and most of the formal education programs provide adequate coverage.

There are three dimensions considered for establishing the criteria to be checked:
                           
                              Roles:
                              Each role has a target knowledge profile and each practitioner considered should be assessed according to the roles he is assigned to. These roles have been presented in Table 1 and Fig. 1.

The role requirements must be analyzed with associated activities performed. This criterion is composed of practices of L2 KPAs appended with SPE process area of the SW-CMM. This enables the determination of knowledge areas that the software practitioners need.

Criteria required in assessing the knowledge and skill of the software practitioner. This criterion is composed of Knowledge units described in SWEBOK.

Furthermore, to respond to the question; “how the required knowledge can be acquired”, the Knowledge and Skill criterion is further decomposed into 4 specific criteria presented in Table 2
                        . The first two criteria represent the same construct which is educational contribution to knowledge levels. These can be grouped while determining the knowledge level of the practitioner; either of the scores obtained from these criteria can be taken into account. The third criterion represents the personal interest of the software practitioner to a subject matter. The fourth criterion is related to practical experience which is more related to learning by doing.

Having defined the specific criteria, we focused on the scales to be referred while assessing them. For the four specific criteria we used scales presented in Bloom's knowledge taxonomy as these are widely used. Bloom [28] has defined the taxonomy of educational objectives that describes several levels of knowledge, intellectual abilities, and skills that a student might derive from education. The scale in Table 3
                         is adapted from the Bloom's knowledge taxonomy and is applicable to each specific criterion defined in Table 2. In addition to knowledge level scale, we proposed a priority assessment scale to determine improvement priorities of knowledge units for given software role. This scale is defined in Table 4
                        .

The assessment baseline used in the model is a collection of software role profiles developed from the reference models, criteria and scales defined in the previous sections. The baseline is composed of a series of tables detailing the roles in the form [role→[activities performed or responsible for→[knowledge units required, level/information]]]. This structure is illustrated in Fig. 2
                         and corresponds with the three issues: roles, activities, and knowledge units. Each role has an associated table, which delimits the activities and knowledge requirements for that role.

As an example, an extract of the determination of the software engineering role profile is provided in Table 5
                        . Knowledge unit descriptions in SWEBOK and keywords of Blooms Taxonomy are used to determine knowledge levels for involved software activities. And this process is validated with expert opinion. Table 5 describes the knowledge level assignment for only SPE2 activity (Software Product Engineering — Activity 2). The same elicitation process is done for all activities of software engineering role, which can be found in [29]. The result of the similar elicitation process for other roles is given in the Table in Appendix B. This table can be considered as the assessment baseline for emergent software organizations widely based on SW-CMM process model of level 2. The first column shows the level for the KU's needed by software engineering role. This is considered as target profile for software engineering role.

The SWAM based assessment is performed with a questionnaire followed by interviews with the practitioners. The criteria scales and the assessment baseline are used to develop the questionnaire (see Appendix A). The level descriptions for the criteria in the questionnaire are inferred from Bloom's knowledge scales. In this way knowledge scores obtained for each criterion reflect the same level. The questionnaire includes 47 knowledge units to be scored according to 4 criteria based on a 0–5 scale and 120 indicators to be checked of existence.

Knowledge unit indicators derived from SWEBOK's descriptions have been included for all KUs. These indicators are included to refine and clarify the knowledge units in the questionnaire. These can be considered detailed requirements for each of the knowledge units in question. In this way, firstly the assessment is more complete, secondly the practitioners or assessors can more clearly understand the meaning and requirements of each knowledge unit. Thirdly, they can be used to verify the levels obtained from the questionnaire whenever necessary.

As examples, the following knowledge unit indicators for Requirements Elicitation are considered:
                           
                              ▪
                              Methods to acquire information about the application domain and the operational environment.

Technique to identify, represent and manage the viewpoints of many different types of stakeholders.

How to identify business process conditioned by the structure, culture and internal politics of the organization.

Knowledge of advantages and limitations of different type interviews and how they should be conducted.

Knowledge of scenario based framework and conceptual modeling notations.

Knowledge of prototyping techniques, which range from paper mock-ups of screen designs to beta-test versions of software products.

Experience to organize and conduct facilitated meetings.

As a part of the assessment technique development, it is important to make explicit the calculation procedure we use to reach the assessment scores. The first two formal education and professional education criteria converge to a same domain: the contribution of the educational background to accumulated knowledge. In order to combine these, we take into account the highest score obtained from one of these two sub-criteria. We reach the initial knowledge level by adding this score to the scores obtained from personal interest practical knowledge criteria scores divided by number of criteria because these two criteria represent different domains. Thus the following formula is used:
                           
                              
                                 
                                    Knowledge
                                    
                                    level
                                    =
                                    
                                       
                                          max
                                          
                                          
                                             
                                                s
                                                1
                                                ,
                                                
                                                s
                                                2
                                             
                                          
                                          +
                                          s
                                          3
                                          +
                                          s
                                          4
                                       
                                    
                                    /
                                    3
                                    .
                                 
                              
                           
                        
                     

This formula assumes that either of the first two criteria and third and fourth criteria has equivalent contribution ratio to knowledge accumulation of a given knowledge unit. It is a sound approximation because there is a one to one match between the levels presented for each question (criteria) in the questionnaire as Bloom's scale had been used to reach criteria levels. Fuzzy numbers rather than [0 to 5] intervals and fuzzy operators are not used to keep the model simpler.

Having obtained the initial scores, we verify if the knowledge unit indicators support each of the knowledge units' scores. If there is a mismatch between the indicators and the scores these are considered to be issues. There are different ways to resolve issues such as adjusting the scores by expert opinion, further interviews with practitioners and/or management, etc. This process ends with the consensus of the parties participating in the assessment and provides us with the valid knowledge level scores for each of the knowledge units.

After collecting the data, to determine the general state of the organization against the requirements of the roles, we determine the overall insufficient knowledge areas for each role defined by the organization. In order to identify the organizational improvement areas, we use the similarity between the target profiles based on SW-CMM level 2 practices and actual profiles of the practitioners of the organization. For this purpose, we use “Euclidian distance” as a measure of dissimilarity as this measure is commonly used and prevents over interpretation.


                        
                           
                              
                                 
                                    Dissimilarity
                                    
                                    
                                       
                                          Euclidian
                                          
                                          distance
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                
                                                   
                                                   i
                                                
                                                =
                                                1
                                             
                                             
                                                #
                                                of
                                                _
                                                KU
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         t
                                                         i
                                                      
                                                      −
                                                      
                                                         a
                                                         i
                                                      
                                                   
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where i is the knowledge unit, t is the target level for the knowledge unit and a is the actual level of the knowledge unit. The higher the dissimilarity score, the lesser the practitioner fits to the knowledge requirement of the role. One may argue that the scores higher than the target profile will reveal also less similarity, however in this case job satisfaction will decrease because of over qualification problem.

In addition, to analyze the best fit between the actual profiles and target profiles we use dendogram diagrams. A dendogram diagram helps to see the degree of similarity between the cases in a data set. In our model, the dendogram is a tree in which practitioners or role profiles with high similarity are adjacent. Lines will indicate the degree of similarity or dissimilarity between practitioner profiles with the desired profiles. This diagram is exemplified in the case study.

A set of dissimilarity vectors can be developed for each of the roles determined in the assessment baseline. The dissimilarity vector may also help to effectively assign roles to practitioners, or choosing team leaders. This information is more meaningful when there are numerous practitioners that could be assigned to a new project.

Furthermore, all this information can be used to prepare training plans in the light of organizational needs. With this, the organization can predict the training needs according to the process improvement initiative. The higher priority KUs can be determined by comparing the median of knowledge levels of the practitioners performing a role with the target levels. If the difference is less than 1, 2, and 3 levels then this knowledge unit can be identified as Significant, Highly Important and Top priority respectively. Furthermore, priority KUs may be also determined for each of the practitioners rather than for the whole organization.

The developments of the assessment baseline and assessment technique are presented in the previous section. In the following section we present the assessment process and specific techniques to evaluate the data obtained coupled with a case study.

By means of the case study, the applicability of the assessment model and process is illustrated. However, on behalf of the model described above, a more detailed and explicit description of how to conduct the evaluation is required. For this purpose, we use the general evaluation process and principles defined in ISO 15504 [5,6] for two reasons. First it has become an industrially accepted standard. Second it is explicit and complete. Specifically, the 15504-3, 15504-4 and 15504-7 assessment process guidelines are used.

In SWAM, roles and their knowledge requirements are defined based on SW-CMM processes of level 2. This reposes that the organization under assessment uses similar processes as the SW-CMM process model. After having performed initial meetings with 7 software organizations, we decided that the organizations X is the most appropriate for the case study as a small immature software company. This organization had 18 personnel and 11 engineers were mainly working for developing software within the scope of automation solutions for contracts with public sector organizations. The type of development projects were mostly database driven applications. The organization has an ISO 9001:2000 certificate. Recently, software process improvement works started to be carried. This organization's management agreed that it used base practices similar to SW-CMM practices on level 2 and the management is willing to improve organizational maturity. Furthermore, the characteristics of this organization matched with the definition of immature software organization given by Paulk [25].

The activities we performed during the preparation phase of the assessment were making initial contact with the management and obtaining general information about the organization. We explained the positive outcomes of this study; we showed them sample questionnaire sheets, role transition tables and knowledge indicator list. Accordingly, we discussed with them about the data collection procedures and importance of the information they will be provided with by participating in this study. This was important in order to gain management commitment.

After fixing the schedule of the assessment, we prepared an action plan and sent it to the managers for approval. In the mean time, we prepared specific assessment tools including questionnaire sheets, role transition tables and a knowledge indicator list, for this organization.

After the approval, we determined the practitioners of the organization who will be participating in the assessment with the management. We selected a team of 11 practitioners that took roles in software development. We initially distributed the questionnaires to all practitioners and asked to return them in 7days. We have verified each knowledge unit in the assessment scope after collecting the assessment forms. We have checked each data item for completeness and checked that the knowledge unit indicators supported the ratings provided by the individual. Then, we conducted meetings with the individuals for inaccurate and unsupported ratings for these issues. These meetings took around 60–90min depending on the number of issues. In summary, the assessment action plan and output of the assessment included the following items:
                           
                              1)
                              The schedule of the assessment;

Selection of the assessment input: the software practitioners participating in the assessment;

The identification of the evidence gathered: knowledge unit indicator compared with ratings;

The assessment approach used: questionnaire and interviews gathered;

The set of software practitioner profiles resulting from the assessment and role transition tables;

Any additional information collected during the assessment that was not identified in the assessment method to support improvement plans.

During verification, we identified 27 to 49 non-existing KU indicators conflicting with initial scores given by the practitioners. As examples, missing KU indicators of Software Testing for 8 practitioners were:
                           
                              ▪
                              Black-box techniques such as: Equivalence partitioning, Boundary-value analysis, Decision table, Finite-state machine-based testing, Random testing

White-box techniques such as: Reference models for code-based testing, Control flow-based criteria, and Data flow-based criteria

Standards such as the IEEE standard for Software Test Documentation

Systematic integration strategies for integrating the software components.

As a first step in analyzing the information collected, we determined the general state of the organization against the requirements of the roles. This step show overall insufficient knowledge areas for each role defined by the organization. Since our purpose here is to describe this process, we take software engineering role as an example.

In order to identify the organizational improvement areas, we use the similarity between the target profiles based on SW CMM level 2 practices and actual profiles of the practitioners of the organization. The SPSS statistical package is used to determine the dissimilarity matrix. By transposing the cases (practitioner profiles) to variables (knowledge units) we obtain the correlation of each of the actual profiles with the proposed profiles instead of correlation between variables. Dissimilarity scores are rescaled to show percentage of dissimilarity. The higher the dissimilarity score, the practitioner fits less to the knowledge requirement of the software engineering role.


                        Table 6
                         shows the dissimilarity vector of the practitioner profiles of the organization compared to software engineering role defined in the assessment baseline. All the practitioners performing software engineering role show more than 40% dissimilarity to the requirements of software engineering role. This situation already shows that organization under consideration have severe problems in terms of workforce capability. Exceptionally, Practitioner 1 has only 25% of dissimilarity. Controversially, he is responsible for software quality assurance.


                        Fig. 3
                         shows the dendogram diagram of software practitioners plotted based on the Euclidian distances grouping of practitioner based on software engineering role. Dendogram graph shows that the profiles of practitioner numbers 4 and 1 are closer to the profile required by the software engineering role which supports the dissimilarity scores. However, the dendogram diagram groups the cases with high similarity in contrast to dissimilarity vector where practitioners 10 and 11 are grouped together although their dissimilarity scores against SE role considerably differ. This is due to the fact that practitioners 10 and 11 are closer matches to each other than they are to the target software engineering profile. The Dendogram diagram is more useful for organizations with numerous practitioners to group practitioners and provide improvement areas related to groups rather than individuals. When the knowledge profiles vary and the organization is willing to assign tasks or roles for a new project or a project phase, the organization can use this information.

Improvement areas for practitioners performing software engineering role can be seen in Fig. 4
                        . An analysis of scores for KUs reveals that the lack of “managerial domains” knowledge is the main reason for high dissimilarity scores. Especially, requirement engineering, software configuration management, software testing domains and secondly software quality assurance domain knowledge levels are not suitable for accurately performing the software engineering role. However, knowledge level on software design, software coding and software testing domains mostly go with the requirements.


                        Fig. 4 also shows the knowledge profile of practitioner 7 in comparison to the target profile of software engineering role and organizational median for each of the knowledge units. It can be seen that the practitioner's profile is inappropriate in managerial domains similar to organizational median. Specifically, software configuration identification, and secondly software quality assurance domain knowledge levels are not suitable for software engineering role.

Lastly, Table 7
                         provides priorities for improving the software engineering role in terms of knowledge units which are determined based on comparing the medians provided in Fig. 4. If the difference between the median of practitioner's knowledge levels and the ideal value is less than 1, 2, and 3 levels then this knowledge unit is identified as Significant, Highly Important and Top priority respectively. Furthermore, the same kind of priority table may be produced for each of the practitioner based on their knowledge profiles if a practitioner based training is envisaged.

Analysis of the assessment results showed the current strengths and weaknesses of the organization's workforce and indicated opportunities for improvement. In general, the knowledge profiles and dendogram diagrams for roles generated with the assessment can be used as the following:
                           
                              1)
                              Knowledge units with very low ratings can indicate serious weakness: an example of this is Test Documentation knowledge unit in which almost all of the practitioners of organization X were below comprehension scale.

Knowledge units with insufficient rating that are needed to achieve a specific need of the organization: for example organization X was willing to improve Configuration Management Activities however 7 of the practitioners have only comprehension knowledge on Configuration Identification Knowledge Unit.

Unbalanced specific criteria ratings within a knowledge unit such as low formal education and high practical experience scores: this information could be used to decide the strategy that is needed to be used in planning the training areas. For instance, the training programs for organization X would give more emphasis on theory and descriptive knowledge than tool and standards use or vice versa.

Dendogram diagram is useful for larger organizations with many practitioners. It helps the organization to group practitioners and provide improvement areas related to groups rather than individuals.

The organization can use dendogram diagrams for effective assignment of project roles and knowledge profiles to identify improvement priorities for the practitioners, related to their roles in the projects.

In general, targets for improvement are quantified for each priority area. These are either target values for knowledge units for all workforce, or target software role profiles, or combinations of the two. For instance, organization X decided that test documentation knowledge unit rating is identified as inappropriate. A set of actions to improve software workforce in test documentation knowledge unit were developed. These were:
                           
                              ▪
                              Embedding a knowledge management system in support of the software testing process including testing documentation templates, guidelines and tools.

Designing and embedding training programs for test documentation standards.

Encouraging peer learning; for example assigning practitioners with low and high scores to the same tasks.

Promoting usage of industrially accepted standards for software testing.

Encouraging attending graduate courses related to software testing.

Such actions, taken together, should meet the qualitative goals and quantified targets. Likewise, a set of goals were developed based on organization X resources and priorities. To do so, improvement actions which would yield clear short term benefits, in order to encourage acceptance of improvement initiative were preferred.

With increased process improvement efforts, one year after the SWAM assessment, the organization fulfilled CMMI Level 2 certificate. The management said that the identified priority improvement areas by SWAM were highly specific and saved them time and money. They also agreed that without SWAM, it would be very difficult for them to enact the level 2 processes and devise such a specific training program for obtaining CMMI Level 2 certificate. They also claimed that the improvement focus to high priority and significant priority areas leveraged their improvement initiative. They also used the assessment outcomes to effectively assign project roles.

SWAM is shown to be applicable in an emergent software organization and to produce results which can be effectively used for improvement efforts at individual and organizational levels. Significant information could be obtained as an input for improvement and training plans of the organization. Specifically, the knowledge profiles and dendogram diagrams for roles generated with the assessment helped to identify top priority knowledge units for the organization, to decide the type of the trainings for determined groups of individuals, to effectively assign project roles, and to identify improvement priorities for the practitioners related to their roles in the projects.

Our aim was to complement the process improvement frameworks that do not address explicitly the question of how to identify and improve the workforce aspect of especially emergent software organizations. Therefore, a software workforce assessment model in accordance with the evaluation theory principles is developed to couple the process and workforce aspect of emergent software organizations. Process areas and activities of associated roles of SW-CMM (processes in level 2 and Software product engineering process in level 3) are used to determine the required body of knowledge units (KU) from SWEBOK to conduct associated activities. The contributions of the study can be stated as follows: SWAM is an enabler to identify the current knowledge profile of the software organization, and prioritize detailed organizational and individual skill, knowledge and improvement needs in relation to processes and roles. In this respect, SWAM is complementary to SW-CMM based assessment method for emergent software organizations. In addition, it explicitly describes the object under assessment, constructs an assessment baseline and defines the criteria that assessment questionnaire is based on and uses knowledge indicators to validate scores; all these lead to a robust assessment model. Finally, SWAM defines a dissimilarity measure based on Euclidian distance and utilizes dendogram diagrams for effective role assignment and group formation. Hence, we believe that this explicit construction permits it to be adapted for other process assessment frameworks.

The study has some limitations. The proposed assessment framework is based on the SW-CMM process model considering Level 2 process areas and the software product engineering process area because the focus of SWAM is emergent software organizations. For organizations using radically different industrially accepted process models, assessment baselines should be revised. This will enable the use of the framework for organizations using other process models. Secondly, computing fundamentals knowledge area is not included, as these KUs are considered elementary to any software practitioner. However assessment questionnaire and knowledge unit indicators can be extended to include these.

In addition to scope limitation, there are some limitations concerning the determination of knowledge scores. Firstly the assessment assumes that all the practitioners are objective and honest while giving the knowledge scores. Although by verifying the existence of knowledge unit indicators we planned to overcome this problem this may take time and there could be cases that the data does not reflect the actual situation. Secondly, in order to determine the knowledge levels of practitioners we used a calculation process which does ratio operations on the ordinal data however we defined this process considering the construct and criteria definitions.


                     
                        
                           
                        
                     
                  


                     
                        
                           
                              
                              
                              
                              
                              
                              
                              
                              
                              
                              
                                 
                                    
                                    Software engineering role
                                    System engineering role
                                    System and acceptance testing role
                                    Software configuration role
                                    Software project management role
                                    Software quality assurance role
                                    Software management role
                                    Senior management role
                                 
                              
                              
                                 
                                    Requirements elicitation
                                    3
                                    4
                                    4
                                    –
                                    4
                                    3
                                    4
                                    4
                                 
                                 
                                    Requirement analysis
                                    4
                                    4
                                    4
                                    3
                                    3
                                    3
                                    4
                                    3
                                 
                                 
                                    Requirement specification
                                    4
                                    4
                                    3
                                    3
                                    3
                                    3
                                    4
                                    2
                                 
                                 
                                    Architectural design
                                    4
                                    4
                                    3
                                    2
                                    2
                                    3
                                    3
                                    2
                                 
                                 
                                    Abstract specification
                                    4
                                    3
                                    2
                                    2
                                    2
                                    3
                                    3
                                    2
                                 
                                 
                                    Interface design
                                    4
                                    2
                                    2
                                    2
                                    2
                                    3
                                    3
                                    2
                                 
                                 
                                    Data structure design
                                    4
                                    2
                                    –
                                    –
                                    2
                                    3
                                    2
                                    –
                                 
                                 
                                    Algorithm design
                                    4
                                    2
                                    –
                                    –
                                    2
                                    3
                                    2
                                    –
                                 
                                 
                                    Coding implementation
                                    4
                                    1
                                    –
                                    
                                    2
                                    –
                                    –
                                    –
                                 
                                 
                                    Code reuse
                                    3
                                    –
                                    –
                                    3
                                    3
                                    4
                                    4
                                    –
                                 
                                 
                                    Coding standards and documentation
                                    4
                                    –
                                    –
                                    3
                                    3
                                    4
                                    4
                                    3
                                 
                                 
                                    Unit testing
                                    4
                                    –
                                    2
                                    3
                                    3
                                    2
                                    2
                                    –
                                 
                                 
                                    Integration testing
                                    4
                                    3
                                    3
                                    3
                                    3
                                    2
                                    2
                                    –
                                 
                                 
                                    System testing
                                    1
                                    4
                                    4
                                    3
                                    3
                                    2
                                    2
                                    3
                                 
                                 
                                    Performance testing
                                    1
                                    4
                                    4
                                    2
                                    3
                                    2
                                    2
                                    2
                                 
                                 
                                    Acceptance testing
                                    1
                                    4
                                    4
                                    4
                                    3
                                    2
                                    3
                                    4
                                 
                                 
                                    Installation testing
                                    1
                                    3
                                    4
                                    1
                                    2
                                    2
                                    3
                                    –
                                 
                                 
                                    Test documentation
                                    4
                                    3
                                    4
                                    3
                                    3
                                    4
                                    4
                                    4
                                 
                                 
                                    Software installation and operation
                                    1
                                    2
                                    3
                                    –
                                    2
                                    –
                                    2
                                    2
                                 
                                 
                                    Software maintenance operations
                                    1
                                    –
                                    2
                                    3
                                    2
                                    –
                                    2
                                    2
                                 
                                 
                                    Software maintenance process
                                    1
                                    2
                                    2
                                    3
                                    3
                                    –
                                    3
                                    3
                                 
                                 
                                    Software maintenance management
                                    1
                                    2
                                    2
                                    3
                                    3
                                    –
                                    3
                                    3
                                 
                                 
                                    Software reengineering
                                    1
                                    –
                                    –
                                    4
                                    2
                                    –
                                    3
                                    3
                                 
                                 
                                    Project planning
                                    3
                                    2
                                    2
                                    4
                                    4
                                    3
                                    4
                                    4
                                 
                                 
                                    Project organization
                                    3
                                    2
                                    –
                                    3
                                    4
                                    3
                                    4
                                    4
                                 
                                 
                                    Project forecasting
                                    3
                                    2
                                    –
                                    1
                                    4
                                    –
                                    4
                                    4
                                 
                                 
                                    Project scheduling
                                    3
                                    2
                                    –
                                    1
                                    4
                                    –
                                    4
                                    4
                                 
                                 
                                    Project control
                                    3
                                    –
                                    –
                                    4
                                    4
                                    3
                                    4
                                    4
                                 
                                 
                                    Risk analysis
                                    3
                                    3
                                    –
                                    –
                                    4
                                    3
                                    4
                                    4
                                 
                                 
                                    Risk management planning
                                    2
                                    3
                                    –
                                    2
                                    4
                                    3
                                    4
                                    4
                                 
                                 
                                    Risk monitoring
                                    1
                                    –
                                    –
                                    3
                                    4
                                    –
                                    4
                                    3
                                 
                                 
                                    Software quality assurance
                                    3
                                    3
                                    4
                                    3
                                    2
                                    4
                                    3
                                    4
                                 
                                 
                                    Verification and validation
                                    3
                                    3
                                    4
                                    4
                                    2
                                    4
                                    3
                                    3
                                 
                                 
                                    Software metrics
                                    2
                                    2
                                    3
                                    2
                                    3
                                    4
                                    4
                                    4
                                 
                                 
                                    Dependable systems
                                    1
                                    4
                                    4
                                    4
                                    2
                                    2
                                    2
                                    –
                                 
                                 
                                    Software configuration identification
                                    3
                                    –
                                    –
                                    4
                                    3
                                    3
                                    2
                                    –
                                 
                                 
                                    Software configuration control
                                    2
                                    –
                                    –
                                    4
                                    3
                                    3
                                    3
                                    2
                                 
                                 
                                    Software configuration audit
                                    1
                                    –
                                    –
                                    4
                                    3
                                    3
                                    4
                                    4
                                 
                                 
                                    Software configuration status account
                                    2
                                    –
                                    –
                                    4
                                    3
                                    2
                                    –
                                    –
                                 
                                 
                                    Quantitative software process management
                                    1
                                    –
                                    –
                                    2
                                    3
                                    4
                                    4
                                    3
                                 
                                 
                                    Software process improvement
                                    1
                                    –
                                    –
                                    2
                                    3
                                    4
                                    4
                                    4
                                 
                                 
                                    Software process assessment
                                    1
                                    –
                                    –
                                    2
                                    2
                                    4
                                    4
                                    4
                                 
                                 
                                    Software process automation
                                    1
                                    –
                                    –
                                    2
                                    2
                                    3
                                    4
                                    4
                                 
                                 
                                    Software process engineering
                                    2
                                    –
                                    –
                                    2
                                    2
                                    4
                                    4
                                    4
                                 
                                 
                                    Procurement management
                                    –
                                    3
                                    3
                                    –
                                    3
                                    3
                                    4
                                    4
                                 
                                 
                                    Acquisition planning
                                    –
                                    3
                                    3
                                    –
                                    3
                                    2
                                    4
                                    4
                                 
                                 
                                    Performance management
                                    –
                                    4
                                    4
                                    –
                                    3
                                    2
                                    4
                                    4
                                 
                              
                           
                        
                     
                  

@&#REFERENCES@&#

