@&#MAIN-TITLE@&#Robust obstacle detection based on a novel disparity calculation method and G-disparity

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a disparity calculation algorithm based on multi-pass aggregation and local optimisation.


                        
                        
                           
                           Disparity calculation is fast and accurate in real-world scenarios.


                        
                        
                           
                           We propose the G-disparity image which can be used with U–V-disparity for obstacle detection.


                        
                        
                           
                           Obstacle detection is more efficient and accurate.


                        
                        
                           
                           Free-space calculation is simplified after obstacle detection.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Obstacle detection

U–V disparity

Free space calculation

Stereo vision

@&#ABSTRACT@&#


               
               
                  This paper presents a disparity calculation algorithm based on stereo-vision for obstacle detection and free space calculation. This algorithm incorporates line segmentation, multi-pass aggregation and efficient local optimisation in order to produce accurate disparity values. It is specifically designed for traffic scenes where most of the objects can be represented by planes in the disparity domain. The accurate horizontal disparity gradient for the side planes are also extracted during the disparity optimisation stage. Then, an obstacle detection algorithm based on the U–V-disparity is introduced. Instead of using the Hough transform for line detection which is extremely sensitive to the parameter settings, the G-disparity image is proposed for the detection of side planes. Then, the vertical planes are detected separately after removing all the side planes. Faster detection speed, lower parameter sensitivity and improved performance are achieved comparing with the Hough transform based detection. After the obstacles are located and removed from the disparity map, most of the remaining pixels are projections from the road surface. Using a spline as the road model, the vertical profile of the road surface is estimated. Finally, the free-space is calculated based on the vertical road profile which is not restricted by the planar road surface assumption.
               
            

@&#INTRODUCTION@&#

Obstacle detection has been an active research area for two decades. It is widely used in many Advanced Driver Assistance Systems (ADAS) and Intelligent Transportation Systems (ITS). The most important goal is to accurately locate obstacles in front and measure the distances to the obstacles. Existing solutions are based on either active sensors or passive sensors. Active sensor based systems provide simple and accurate distance measurements. However, they cannot produce results with adequate spatial resolution in real-time. Active sensors also face the problem of mutual interference. Although this problem has been solved by the pseudo-random laser modulation scheme [1], the LiDAR system requires an unreasonably high power to detect objects in the far-field if adequate spatial resolution is required. Therefore, our research is focused on passive sensor based obstacle detection.

The passive sensor based systems can be further divided into three main types: segmentation-based [2–7], motion-based [8–10] and depth-based [11–16] obstacle detection. The segmentation-based solutions separate the obstacles from the background by identifying and analysing potential obstacle features in the scene. Various features have been used, such as colour [4], edge [6], edge symmetry [3], texture [7] and so on. The distances to the detected obstacles are normally estimated using fixed camera parameters and a flat-road assumption. Most of the motion-based methods are based on perspective transformation [9,17] and global-motion estimation algorithms [8,10]. Differentiating the motion of local image areas from the global motion introduced by the vehicle ego-motion allows the detection of objects. If the ego-motion of the vehicle is directly available, perspective transformation can be applied to warp the current frame, 
                        
                           f
                           (
                           i
                           )
                        
                     , to produce an ‘expected’ next frame, 
                        
                           
                              
                                 f
                              
                              
                                 ^
                              
                           
                           (
                           i
                           +
                           1
                           )
                        
                     , assuming a flat road. Then, for obstacle detection, the actual frame, 
                        
                           f
                           (
                           i
                           +
                           1
                           )
                        
                     , can be compared with frame, 
                        
                           
                              
                                 f
                              
                              
                                 ^
                              
                           
                           (
                           i
                           +
                           1
                           )
                        
                     . When the global motion is unavailable, algorithms such as optical flow can be applied. The global motion needs to be first identified by analysing the flow field. The positions where the motion flows do not agree with the global motion field indicate possible dynamic objects which are very likely to be obstacles such as vehicles or pedestrians [18–20]. Generally, both shape-based and motion-based algorithms are able to provide accurate detection. However, the distance measurements can only be achieved with a flat-road assumption (or accurate prior road vertical profile detection), due to the lack of direct distance measurement. In reality, only near-field roads can be treated as flat planes. Therefore, the accuracy of the far-field distance measurements become unreliable with vertical road gradient variations.

Stereo vision-based solutions hold an advantage over shape-based and motion-based approaches. Direct distance calculations can be achieved by evaluating the disparities between the two images without any assumption of the road structure. By doing so, obstacles are detected based on the disparity maps instead of intensity images and so the process can be significantly simplified. However, calculating a dense disparity map requires a significant amount of computation which is burdensome for the overall system. Block-based methods evaluate the similarities of local image areas. The position where the highest similarity is achieved can be selected as the correspondence. Global optimisation-based algorithms include more constraints, such as a smoothness term. Combining the constraints with the cost function, a global energy function representing the goodness of the current match can be formed. Many optimisation techniques [21–23] have been applied to find the maximum/minimum of the global energy function with varying degrees of success, depending on the test images. However, the global optimisation process is normally computationally demanding and difficult to implement in real-time. Dynamic programming (DP)-based approaches [24,25] optimise disparity values based on a Disparity Space Image (DSI), generated using a scanline. It is difficult for this method to incorporate inter-scanline information and to relax the monotonicity or ordering constraint (described in [26,27]).

When designing a disparity calculation algorithm, one of the most important steps is to decide the cost function. Traditional cost functions are generally based on intensity differences such as the SSD, MD and the SAD. Intensity difference based cost functions are the simplest as they do not include divisions and are suitable for implementation on fixed-point embedded systems. However, these functions are sensitive to the differences of the intensity gains between the two cameras. The NCC is also a popular cost function. Although the NCC demands higher computational complexity, it is definitely preferred when intensity differences are non-negligible. Other methods are also proposed, focusing on reducing the sensitivity towards intensity differences, including image gradient and non-parametric transforms (rank and census transforms) [28–30] based cost functions.

One of the general problems during stereo matching is that errors are likely to be introduced in the homogeneous areas (pixels in a large image area that share very similar intensities). In automotive applications, large homogeneous regions exist on the road surface. This leads to the matching cost being unpredictable in various locations. The position corresponding to the minimum cost might not be the correct correspondence. Grouping the pixels together within the homogeneous region during the cost aggregation solves the problem but it is difficult to allow smooth and gradual disparity changes within the group. Another factor that induces error is the occlusion, which is introduced due to the position difference between the left and right cameras. Existing solutions are mainly based on the left and right consistency check or global optimisation [22,21], which are computationally complicated.

In this paper, we propose a disparity calculation algorithm which readily solves the homogeneous area problem. It also has partial occlusion handling capability. The algorithm consists of four main steps: cost calculation, image line segmentation, cost aggregation and optimisation. Line segmentation is applied to the reference image horizontally and vertically. The calculated costs are aggregated in both the horizontal and vertical directions. The horizontal aggregation enables a correct correspondence to be found in the homogeneous and occluded areas. Vertical aggregation allows gradual disparity variation within each horizontal segment. Segment based optimisation is then performed to identify the final disparity for each pixel. The proposed technique is especially suitable for this application since a gradual change of disparity in the road area and side planes must be encouraged (many global optimisation-based algorithms involve a smoothness term which adds a small penalty even to gradual disparity changes). During the optimisation, the proposed technique also generates an accurate horizontal disparity gradient which simplifies the side plane recognition during obstacle detection.

Once the disparity map is available, the potential obstacles in the scene can be extracted. Many existing systems assume the road surface is planar [11]. This assumption allows the reference image to be perspective projected and matched with the other image. The final intensity difference map reveals the potential obstacles. The biggest advantage of such systems is that the disparity matching step can be omitted. However, if the flat road assumption is violated, the system performance will be heavily affected. In order to relax the flat road assumption while saving computation, disparities are only calculated at certain feature points in [31–33]. However, sophisticated feature extractors are computationally complicated. Most importantly, a large amount of valuable distance information is not included and cannot be shared with the other components in the autonomous vehicle system. With fast growing computing systems, dense disparity matching can be achieved in real-time. One of the most significant contributions, based on a dense disparity map, is called the U–V-disparity map [13]. By calculating the line disparity histograms horizontally and vertically, the V-disparity and U-disparity images can be generated respectively. Each plane in the world coordinate system will be shown as a line on the U–V-disparity map. Categorising road surfaces and obstacles into different types of planes simplifies the obstacle detection problem into a line detection one. The Hough transform has been the most popular algorithm to extract lines (mostly corresponding to horizontal or oblique planes) in the U–V-disparity space [13,34]. However, the Hough transform requires pre-set parameters whose settings may cause serious over-detection or the neglect of small obstacles. It is also extremely difficult to find one set of parameters that works in the fast varying traffic environment. In [13], the detection of the road plane is carried out prior to the obstacle detection. The obstacles are found by evaluating the disparity difference in the road disparity profile. This type of algorithm assumes that a very large percentage of the scene is occupied by the road surface, so that the longest line in the V-disparity map corresponds to the road surface. However, in the cases when obstacles are in the near-field, the road surface can be blocked and this results in large errors.

In our system, the detection is separated into side and vertical plane detection. Side planes are located first on the U-disparity and the proposed G-disparity, which is calculated based on the disparity gradient. The detected obstacles are removed so that minimum outliers are involved during vertical plane detection. This approach does not include sensitive parameter settings and it can be implemented very efficiently. Furthermore, the proposed obstacle detection does not rely on the prior road surface exclusion and allows the presence of obstacles in the foreground. Another advantage is that, once all the obstacles are removed from the scene, the road surface can be easily detected and modelled. As proposed in [35], the vertical profile of the road surface is modelled using a spline. In our system, the model parameters are then optimised using the least squares method. An extra refinement step ensures the boundary between the obstacles and the road surface is accurately defined.

The rest of this paper is organised as follows: Section 2 introduces the proposed stereo matching algorithm. Section 3 focuses on the obstacle detection. Section 4 describes the road modelling and free space computation. Section 5 presents experimental results and corresponding discussions. Section 6 concludes the paper.

First of all, we define the world coordinate system as shown in Fig. 1
                        . The relationship between disparity and depth is a result of perspective projection. As shown in Fig. 2
                        , defining a point in the 3D space as P and projecting this point onto the stereo vision system with a horizontal baseline B, the resultant points on the left and right image plane can be represented as 
                           
                              
                                 
                                    p
                                 
                                 
                                    L
                                 
                              
                           
                         and 
                           
                              
                                 
                                    p
                                 
                                 
                                    R
                                 
                              
                           
                         respectively. The relationship between these two points and the depth Z can be found as shown in Eq. (1).
                           
                              (1)
                              
                                 d
                                 =
                                 
                                    
                                       Bf
                                    
                                    
                                       Z
                                    
                                 
                              
                           
                        where d is the disparity which can be calculated by the horizontal position difference between 
                           
                              
                                 
                                    p
                                 
                                 
                                    L
                                 
                              
                           
                         and 
                           
                              
                                 
                                    p
                                 
                                 
                                    R
                                 
                              
                           
                         on the image space and f is the focal length.

In order to locate the correct correspondence for each pixel in the reference image, we need to utilise the intensity information surrounding the point of interest. Even calibrated cameras could include a small amount of intensity difference which could lead to incorrect correspondence. In the case where the pixel disparities within a homogeneous region are identical, this problem can be solved by image segmentation and restricting the disparities of pixels within the same segment. However, in obstacle detection applications, the disparity of the road surface changes gradually so that assigning a single disparity is not an option. Furthermore, changes in distance also introduce changes in object appearance from one image to the other which then causes a correspondent segment not being found. Another factor that induces errors is occlusion, which occurs due to the position difference between the left and right cameras. This causes background pixels to be visible in the reference image, but not in the other one (as shown in Fig. 3
                        ). In theory, the disparity of the occluded areas cannot be measured and should be excluded from the final result. Existing ways of excluding these pixels are mainly based on the left and right consistency check and global optimisation [21–23] which are computationally complex. However, in most situations, the disparities of occluded areas can be estimated with the disparity of the neighbouring background object.

In this paper, a multi-pass cost aggregation approach based on line segmentation is presented in order to solve the problems introduced by homogeneous and occluded regions. The system block diagram of the proposed disparity calculation algorithm is shown in Fig. 4
                        . As discussed in [34], planes within a normal road scene can be separated into horizontal, oblique, vertical and side planes. Although shapes of the road surfaces on the two images are different, the widths of the road areas on both images are identical. Therefore, line segmentation instead of region segmentation is implemented in our system to calculate the disparities of the oblique and vertical planes. Aggregating the costs horizontally allows vertical disparity changes while restricting the segment disparities using object widths. In order to cope with the occluded areas, a weighted aggregation is conducted to suppress the contributions of occluded pixels. When the object is a side plane, the disparity values change linearly in the horizontal direction. In this case, vertical line segmentation is also employed. By aggregating the cost within each vertical segment, the disparity of side surfaces can be calculated accurately. The final segment disparities are then optimised based on the multi-pass aggregation results. The horizontal aggregation results are first optimised using the inter-scanline information. After that, an energy function is evaluated based on the horizontal segment DSI s [36] to determine the final disparity for each pixel.

Cost calculation provides the most basic information needed by all high-level algorithms such as the popular BP and the SGM in order to produce the final disparity map. While choosing the cost function, the most important factors are accuracy and speed. Some of the simple functions such as the SAD and the SSD can produce accurate results if the input images are well calibrated (including brightness) and contain rich texture details (many colour/intensity changes). As many of our test image sets from EISATS
                           1
                           Enpeda Image Sequence Analysis Test Site.
                        
                        
                           1
                         contain significant brightness differences between the left and right views, NCC is implemented due to its robustness against intensity changes. The NCC coefficient or cost 
                           
                              
                                 
                                    δ
                                 
                                 
                                    n
                                    ,
                                    m
                                 
                              
                              (
                              d
                              )
                           
                         of pixel 
                           
                              I
                              (
                              n
                              ,
                              m
                              )
                           
                         can be calculated as given in Eq. (2).
                           
                              (2)
                              
                                 
                                    
                                       δ
                                    
                                    
                                       n
                                       ,
                                       m
                                    
                                 
                                 (
                                 d
                                 )
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       
                                          
                                             (
                                             2
                                             h
                                             +
                                             1
                                             )
                                          
                                          
                                             2
                                          
                                       
                                       -
                                       1
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          x
                                          =
                                          n
                                          -
                                          h
                                       
                                       
                                          n
                                          +
                                          h
                                       
                                    
                                 
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          y
                                          =
                                          m
                                          -
                                          h
                                       
                                       
                                          m
                                          +
                                          h
                                       
                                    
                                 
                                 
                                    
                                       (
                                       
                                          
                                             I
                                          
                                          
                                             l
                                          
                                       
                                       (
                                       x
                                       ,
                                       y
                                       )
                                       -
                                       
                                          
                                             
                                                
                                                   I
                                                
                                                
                                                   l
                                                
                                             
                                          
                                          
                                             ‾
                                          
                                       
                                       )
                                       (
                                       
                                          
                                             I
                                          
                                          
                                             r
                                          
                                       
                                       (
                                       x
                                       +
                                       d
                                       ,
                                       y
                                       )
                                       -
                                       
                                          
                                             
                                                
                                                   I
                                                
                                                
                                                   r
                                                
                                             
                                          
                                          
                                             ‾
                                          
                                       
                                       )
                                    
                                    
                                       
                                          
                                             σ
                                          
                                          
                                             l
                                          
                                       
                                       
                                          
                                             σ
                                          
                                          
                                             r
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    δ
                                 
                                 
                                    m
                                    ,
                                    n
                                 
                              
                              (
                              d
                              )
                           
                         represents the normalised cross-correlation coefficient at disparity d. 
                           
                              
                                 
                                    
                                       
                                          I
                                       
                                       
                                          l
                                       
                                    
                                 
                                 
                                    ‾
                                 
                              
                           
                         and 
                           
                              
                                 
                                    
                                       
                                          I
                                       
                                       
                                          r
                                       
                                    
                                 
                                 
                                    ‾
                                 
                              
                           
                         denote the mean of the intensities within the left and right blocks respectively and 
                           
                              σ
                           
                         represents the standard deviation. The normalising term ensures 
                           
                              
                                 
                                    δ
                                 
                                 
                                    m
                                    ,
                                    n
                                 
                              
                              (
                              d
                              )
                           
                         is insensitive to any intensity variations.

Unlike the SAD and SSD, larger NCC coefficients correspond to the likely matches. Therefore, in this paper, disparities corresponding to large costs are regarded as desirable matches.

If the Winner-Take-All (WTA) optimisation method is used to locate the maximum cost location, the disparity map can be produced as shown in Fig. 5
                        .

This is a typical road scene with vehicles and boundary fences. A large amount of error is produced in the homogeneous regions and occluded areas. By inspecting the cost of a pixel inside the homogeneous region (shown in Fig. 6
                        ), we can see that the local cost calculation does not reflect the correct match, and the maximum in this case corresponds to an incorrect disparity value. In the occluded regions, correct correspondences do not exist. Therefore, errors cannot be avoided at this stage.

In order to illustrate the influence of a homogeneous region during cost calculation, the NCC coefficients of one pixel inside a homogeneous regions are shown in Fig. 6. As all intensities inside the region are very similar, the maximum corresponds to an incorrect position. Therefore, the costs of pixels inside an homogeneous area need to be aggregated to find the correct disparity values.

The proposed cost aggregation method is based on the horizontal and vertical line segmentations. First, the gradient of the reference (left) image is calculated horizontally by using a Sobel mask. Then, a small threshold is applied to identify the intensity changes. This threshold is easy to determine since only the areas with very little texture need to be grouped. Once the vertical edges are found, any pixels between the two edge points are grouped into a segment. The cost 
                           
                              
                                 
                                    δ
                                 
                                 
                                    n
                                    ,
                                    m
                                 
                              
                              (
                              d
                              )
                           
                         can therefore be represented as 
                           
                              
                                 
                                    δ
                                 
                                 
                                    s
                                 
                              
                              (
                              j
                              ,
                              d
                              )
                           
                        , where s is the segment index and j is the pixel index within segment s.

The distribution of disparity values within a homogeneous line segment can be separated into three cases:
                           
                              •
                              The segment of interest corresponds to either the horizontal (parallel to the X–Z plane), oblique (the X-axis is parallel to this type of planes) or the vertical planes (parallel to the X–Y plane) in the 3D space, with no occlusions. These planes are all parallel to the X-axis. While projecting points on these planes to the image plane, only the ones lying on the same line parallel to the image plane will be projected onto the same image row. Therefore, resultant disparities within the line segment are identical.

The horizontal segment also corresponds to either the horizontal, oblique or vertical planes (parallel to the X–Y plane) in the 3D space with occlusions.

Finally, if the line segment corresponds to a side plane (planes that are perpendicular to the road plane), the true disparities gradually change horizontally. Assigning a single disparity value to all pixels in the segment is thus inappropriate. A multi-pass aggregation method is developed in our system to accommodate each case differently.

For the first case, the DSI of a image line segment within a homogeneous region is shown in Fig. 7
                        . Each pixel in Fig. 7 corresponds to a cost value (normalised correlation coefficient). The x-axis represents the number of pixels inside this line segment. The disparity range increases along the y-axis. Defining the disparity d and the pixel index j within a segment s as two random variables, the normalised cost 
                           
                              
                                 
                                    
                                       
                                          δ
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    s
                                 
                                 
                                    A
                                 
                              
                              (
                              j
                              ,
                              d
                              )
                           
                         can be treated as a probability density of correct correspondence and is shown below as:
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             δ
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       s
                                    
                                    
                                       A
                                    
                                 
                                 (
                                 j
                                 ,
                                 d
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             δ
                                          
                                          
                                             s
                                          
                                       
                                       (
                                       j
                                       ,
                                       d
                                       )
                                    
                                    
                                       ∬
                                       
                                          
                                             δ
                                          
                                          
                                             s
                                          
                                       
                                       (
                                       j
                                       ,
                                       d
                                       )
                                       dj
                                       
                                       dd
                                    
                                 
                              
                           
                        The denominator is a normalising term to ensure the probability density integrates to one. The objective is to find the cost of the whole segment depending on d and to eliminate the term j. This is achieved by calculating the marginal density function as follows:
                           
                              (4)
                              
                                 
                                    
                                       C
                                    
                                    
                                       s
                                    
                                    
                                       A
                                    
                                 
                                 (
                                 d
                                 )
                                 =
                                 ∫
                                 
                                    
                                       
                                          
                                             δ
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       s
                                    
                                    
                                       A
                                    
                                 
                                 (
                                 j
                                 ,
                                 d
                                 )
                                 dj
                              
                           
                        where 
                           
                              
                                 
                                    C
                                 
                                 
                                    s
                                 
                                 
                                    A
                                 
                              
                              (
                              d
                              )
                           
                         represents the probability of d being the correct disparity for the whole segment. If d results in a large number of high values in 
                           
                              
                                 
                                    
                                       
                                          δ
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    s
                                 
                                 
                                    A
                                 
                              
                              (
                              j
                              ,
                              d
                              )
                           
                        , the integrated result will provide a large value at d and vice versa. The aggregated cost should contain a unique peak which corresponds to the correct disparity. Fig. 8
                         illustrates the cost aggregation result of the segment containing the pixel whose cost is shown in Fig. 6. As the figure shows, the maximum position is now correct at 
                           
                              d
                              =
                              22
                           
                        . This indicates that cost aggregation within a homogeneous region can result in a distinct peak which is more suitable when used to estimate the disparity map.

For the second case, an example of the DSI of a segment including an occluded region is shown in Fig. 9
                        . The costs in the occluded area are completely different from the rest. If the cost is aggregated using Eq. (4), there is a possibility that the final result will be influenced by the occluded region and cause errors. Therefore, it is important to ignore the occluded areas during cost aggregation. However, the determination of the occluded area from the DSI is not an easy task. In [36], the authors detect the occluded areas using Dynamic Programming (DP). Since occlusion detection needs to be performed on every image row, the overall computational complexity will be dramatically increased. As the left image is selected as the reference, occlusions can only occur on the right side of an object. In order to restrict the confidence of the costs of right pixels during the aggregation, a weight function is applied to give higher support to the pixels close to the left end of a segment while suppressing the cost contribution of the pixels close to the right end. In the proposed approach, a Gaussian weighting function 
                           
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                              
                           
                         is employed. It provides more flexibility while tuning since the weight distribution is controlled by its mean and standard deviation values. Simpler weights function such as the ramp funtion can also be taken into consideration. In our case, linearly distributed weighting coefficients do not suppress the occlusion problem in cases of large (wide) segments. Therefore, a Gaussian function is employed for good searching range coverage on the left end of the segment and sufficient restriction of occlusion on the right side. When the Gaussian weighting function 
                           
                              
                                 
                                    w
                                 
                                 
                                    1
                                 
                              
                           
                         is applied to 
                           
                              
                                 
                                    δ
                                 
                                 
                                    s
                                 
                              
                              (
                              j
                              ,
                              d
                              )
                           
                        , the normalisation process becomes:
                           
                              (5)
                              
                                 
                                    
                                       
                                          
                                             δ
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       s
                                    
                                    
                                       L
                                    
                                 
                                 (
                                 j
                                 ,
                                 d
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             w
                                          
                                          
                                             1
                                          
                                       
                                       (
                                       j
                                       )
                                       
                                          
                                             δ
                                          
                                          
                                             s
                                          
                                       
                                       (
                                       j
                                       ,
                                       d
                                       )
                                    
                                    
                                       ∬
                                       
                                          
                                             w
                                          
                                          
                                             1
                                          
                                       
                                       (
                                       j
                                       )
                                       
                                          
                                             δ
                                          
                                          
                                             s
                                          
                                       
                                       (
                                       j
                                       ,
                                       d
                                       )
                                       dj
                                       
                                       dd
                                    
                                 
                              
                           
                        Hence, another aggregation function is created as shown below:
                           
                              (6)
                              
                                 
                                    
                                       C
                                    
                                    
                                       s
                                    
                                    
                                       L
                                    
                                 
                                 (
                                 d
                                 )
                                 =
                                 ∫
                                 
                                    
                                       
                                          
                                             δ
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       s
                                    
                                    
                                       L
                                    
                                 
                                 (
                                 j
                                 ,
                                 d
                                 )
                                 dj
                              
                           
                        If j is normalised within the range 
                           
                              [
                              -
                              1
                              
                              1
                              ]
                           
                        , the mean of the Gaussian function is set at 
                           
                              -
                              1
                           
                         and the standard deviation is chosen to be 
                           
                              0.4
                           
                        . This parameter is determined by manually tuning the value and evaluating the resultant disparity map using selected image sets from the Enpeda Image Sequence Analysis Test Site. The small standard deviation indicates the weights at the right side of the segment are very low. This choice is based on the fact that some of the occlusion effect is severe, which can occupy a big proportion in a segment or even cover the whole segment. If the complete segment is within an occluded area, the aggregated result of Eq. (6) cannot indicate the correct disparity. However, the possibility of this occurring is generally low in a road scene.

With the above two aggregation functions, the costs of pixels within a segment are aggregated twice. The aggregation results of Eq. (4) could be affected by the occluded areas but it produces excellent results in occlusion-free areas. The results of Eq. (6) are less sensitive to the negative influence of the occluded areas but may produce errors due to the exclusion of a large portion of pixels on the right side of the segment. In order to make sure that at least one of the aggregation results corresponds to the correct disparity, another aggregation is performed using Eq. (6) but with a different weighting function. This time, the mean of the Gaussian function is still at 
                           
                              -
                              0.7
                           
                         but the standard deviation is chosen to be 
                           
                              0.8
                           
                         (decided by tuning the value manually and evaluating the experimental results). This function assigns higher weights to more pixels while preventing errors introduced by small occluded areas. The resultant cost normalisation function is shown as follows:
                           
                              (7)
                              
                                 
                                    
                                       
                                          
                                             δ
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       s
                                    
                                    
                                       M
                                    
                                 
                                 (
                                 j
                                 ,
                                 d
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             w
                                          
                                          
                                             2
                                          
                                       
                                       (
                                       j
                                       )
                                       
                                          
                                             δ
                                          
                                          
                                             s
                                          
                                       
                                       (
                                       j
                                       ,
                                       d
                                       )
                                    
                                    
                                       ∬
                                       
                                          
                                             w
                                          
                                          
                                             2
                                          
                                       
                                       (
                                       j
                                       )
                                       
                                          
                                             δ
                                          
                                          
                                             s
                                          
                                       
                                       (
                                       j
                                       ,
                                       d
                                       )
                                       dj
                                       
                                       dd
                                    
                                 
                              
                           
                        The marginal density is therefore:
                           
                              (8)
                              
                                 
                                    
                                       C
                                    
                                    
                                       s
                                    
                                    
                                       M
                                    
                                 
                                 (
                                 d
                                 )
                                 =
                                 ∫
                                 
                                    
                                       
                                          
                                             δ
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       s
                                    
                                    
                                       M
                                    
                                 
                                 (
                                 j
                                 ,
                                 d
                                 )
                                 dj
                              
                           
                        The final decision about which aggregation result corresponds to the correct disparity will be discussed in detail in Section 2.4.

For the third case, in order to allow gradual changes within the group while generating reliable aggregated results, segmentation is applied again vertically with the horizontal edges. The heights of a projected line on a side surface should be identical on the left and right images. Therefore, if d corresponds to the true disparity, a vertical segment in the reference image should find the one on the right image with the same height to be the correspondence. If v and l are used to represent the vertical segment and pixel indexes respectively, the normalisation process can be expressed by:
                           
                              (9)
                              
                                 
                                    
                                       
                                          
                                             δ
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       v
                                    
                                    
                                       A
                                    
                                 
                                 (
                                 l
                                 ,
                                 d
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             δ
                                          
                                          
                                             v
                                          
                                       
                                       (
                                       l
                                       ,
                                       d
                                       )
                                    
                                    
                                       ∬
                                       
                                          
                                             δ
                                          
                                          
                                             v
                                          
                                       
                                       (
                                       l
                                       ,
                                       d
                                       )
                                       dl
                                       
                                       dd
                                    
                                 
                              
                           
                        The aggregation of pixels inside a vertical segment is therefore:
                           
                              (10)
                              
                                 
                                    
                                       C
                                    
                                    
                                       v
                                    
                                    
                                       A
                                    
                                 
                                 (
                                 d
                                 )
                                 =
                                 ∫
                                 
                                    
                                       
                                          
                                             δ
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       v
                                    
                                    
                                       A
                                    
                                 
                                 (
                                 l
                                 ,
                                 d
                                 )
                                 dl
                              
                           
                        The occluded areas are not considered during vertical aggregation since they only exist on the right side of an object and it is difficult to identify them in the vertical direction.

The proposed multi-pass approach aggregates the cost four times in total. Each aggregation utilises different information, aiming to solve a specific problem. These aggregation results need to be further processed in order to generate an optimised disparity map. The four available aggregation results are 
                           
                              
                                 
                                    C
                                 
                                 
                                    s
                                 
                                 
                                    A
                                 
                              
                              (
                              d
                              )
                              ,
                              
                                 
                                    C
                                 
                                 
                                    s
                                 
                                 
                                    L
                                 
                              
                              (
                              d
                              )
                              ,
                              
                                 
                                    C
                                 
                                 
                                    s
                                 
                                 
                                    M
                                 
                              
                              (
                              d
                              )
                           
                         and 
                           
                              
                                 
                                    C
                                 
                                 
                                    v
                                 
                                 
                                    A
                                 
                              
                              (
                              d
                              )
                           
                         (Eqs. (4), (6), (8) and (10)).

With the above four sets of cost functions, the WTA optimisation is applied to each set and produces four disparity maps 
                           
                              
                                 
                                    D
                                 
                                 
                                    W
                                 
                                 
                                    i
                                 
                              
                           
                        , where 
                           
                              i
                              =
                              1
                              ,
                              2
                              ,
                              3
                              ,
                              4
                           
                         as shown in Fig. 10
                        . Each pixel should select its disparity among the four results. As Fig. 9 shows, each of the four disparity maps contains errors. However, most of the errors are not shared by all. For example, 
                           
                              
                                 
                                    D
                                 
                                 
                                    W
                                 
                                 
                                    4
                                 
                              
                           
                         produces accurate disparities for the side planes but a large number of errors is introduced on the oblique planes. The objective of the optimisation is to find the optimum value among the four for each pixel or segment. Many global optimisation methods (such as SA [37], SGM [21] and BP) can be applied to solve the problem. As the disparity levels are restricted to only four values, the required computation to achieve the final result is significantly reduced. In our system, a local optimisation method is conducted based on segments instead of pixels in order to further decrease the required computational power.

Although the computational complexity of a 2D optimisation is greatly reduced after the cost aggregation, it is still difficult to achieve real-time performance. In our system, the horizontal segmented disparity maps (
                           
                              
                                 
                                    D
                                 
                                 
                                    W
                                 
                                 
                                    i
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    3
                                 
                              
                           
                        ) are optimised first. The vertical smoothness of local image areas are evaluated. These local areas are determined by the horizontal and vertical segmentation. In practice, it is reasonable to assume that the disparity within a vertical segment should be identical (such as in the obstacle areas) or slowly changing (such as in the road surface area). Any sudden changes are irregular and should be removed. On each of the three maps based on horizontal segmentation, vertical disparity differences are calculated. The locations where the differences are greater than 1 are labelled with 1s on a positive difference map 
                           
                              
                                 
                                    σ
                                 
                                 
                                    +
                                 
                                 
                                    i
                                 
                              
                           
                         where 
                           
                              i
                              =
                              1
                              ,
                              2
                              ,
                              3
                           
                        . The locations where the differences are smaller than 
                           
                              -
                              1
                           
                         are labelled with 1s on a negative difference map 
                           
                              
                                 
                                    σ
                                 
                                 
                                    -
                                 
                                 
                                    i
                                 
                              
                           
                        . The final uncertainty map 
                           
                              
                                 
                                    σ
                                 
                                 
                                    i
                                 
                              
                           
                         can be calculated using information contained in both difference maps as follows:
                           
                              (11)
                              
                                 
                                    
                                       σ
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 v
                                 ,
                                 l
                                 )
                                 =
                                 min
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   l
                                                
                                             
                                          
                                          
                                             
                                                σ
                                             
                                             
                                                +
                                             
                                             
                                                i
                                             
                                          
                                          (
                                          v
                                          ,
                                          l
                                          )
                                          ,
                                          
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   l
                                                
                                             
                                          
                                          
                                             
                                                σ
                                             
                                             
                                                -
                                             
                                             
                                                i
                                             
                                          
                                          (
                                          v
                                          ,
                                          l
                                          )
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 
                                    σ
                                 
                                 
                                    i
                                 
                              
                           
                         provides an accurate measure of the number of distinctive disparity changes within a vertical segment. At pixel 
                           
                              (
                              v
                              ,
                              l
                              )
                           
                        , the disparity 
                           
                              
                                 
                                    D
                                 
                                 
                                    H
                                 
                              
                           
                         can be represented as:
                           
                              (12)
                              
                                 
                                    
                                       D
                                    
                                    
                                       H
                                    
                                 
                                 (
                                 s
                                 ,
                                 j
                                 )
                                 =
                                 
                                    
                                       D
                                    
                                    
                                       W
                                    
                                    
                                       
                                          
                                             i
                                          
                                          
                                             p
                                          
                                       
                                    
                                 
                                 (
                                 s
                                 ,
                                 j
                                 )
                                 
                                 where
                                 
                                 
                                    
                                       σ
                                    
                                    
                                       
                                          
                                             i
                                          
                                          
                                             p
                                          
                                       
                                    
                                 
                                 (
                                 s
                                 )
                                 =
                                 
                                    
                                       min
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   j
                                                
                                             
                                          
                                          
                                             
                                                σ
                                             
                                             
                                                i
                                                =
                                                1
                                                ,
                                                2
                                                ,
                                                3
                                             
                                          
                                          (
                                          s
                                          ,
                                          j
                                          )
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    D
                                 
                                 
                                    H
                                 
                              
                           
                         is the optimised result based on horizontally segmented disparity maps. This optimisation process enforces the inter-scan relationship on each horizontal segment so that errors can be identified while a smooth vertical disparity change is not interrupted.

Although 
                           
                              
                                 
                                    D
                                 
                                 
                                    H
                                 
                              
                           
                         is now optimised, it is only based on the horizontal segment results. The disparity values representing the side planes are still inaccurate. In order to get the final disparity map, these disparity values in 
                           
                              
                                 
                                    D
                                 
                                 
                                    H
                                 
                              
                           
                         must be replaced by the corresponding information stored in 
                           
                              
                                 
                                    D
                                 
                                 
                                    W
                                 
                                 
                                    4
                                 
                              
                           
                         (a disparity map calculated based on the vertical segments). Unlike the inter-scanline optimisation discussed above, the disparities for the side planes are selected based on the DSIs of horizontal segments.

It is important to note that unlike 
                           
                              
                                 
                                    D
                                 
                                 
                                    H
                                 
                              
                              ,
                              
                                 
                                    D
                                 
                                 
                                    W
                                 
                                 
                                    4
                                 
                              
                           
                         has not yet been analysed and optimised. It could contain errors introduced by insufficient pixels in a segment or occlusions. If 
                           
                              
                                 
                                    D
                                 
                                 
                                    H
                                 
                              
                           
                         is directly compared with 
                           
                              
                                 
                                    D
                                 
                                 
                                    W
                                 
                                 
                                    4
                                 
                              
                           
                         without preprocessing, it is very likely to cause inaccurate optimisation. The objects most likely to appear on the sides of the vehicle could be other vehicles, buildings and fences. The side surfaces of these objects are approximately planar. Even in some extreme cases where the objects have curved side surfaces, it is very unlikely for the complete curvature surface to be a homogeneous area which cannot be separated during line segmentation. If the curved surface is segmented into a few segments, disparity changes within each segment can be treated as approximately linear. For the above reasons, the disparity contained in a horizontal segment of 
                           
                              
                                 
                                    D
                                 
                                 
                                    W
                                 
                                 
                                    4
                                 
                              
                           
                         is restricted to be on a straight line on the DSI. The parameters of the line corresponding to horizontal segment s are calculated with least squares fitting based on smoothed and sub-sampled 
                           
                              
                                 
                                    D
                                 
                                 
                                    W
                                 
                                 
                                    4
                                 
                              
                              (
                              s
                              )
                           
                         where 
                           
                              
                                 
                                    D
                                 
                                 
                                    W
                                 
                                 
                                    4
                                 
                              
                              (
                              s
                              )
                           
                         contains 
                           
                              
                                 
                                    D
                                 
                                 
                                    W
                                 
                                 
                                    4
                                 
                              
                              (
                              s
                              ,
                              j
                              )
                           
                         for all j. Since sub-pixel resolution is not required, the lines are then sampled, digitised and stored in 
                           
                              
                                 
                                    D
                                 
                                 
                                    V
                                 
                              
                              (
                              s
                              ,
                              j
                              )
                           
                        . It is important to note that the value of 
                           
                              
                                 
                                    D
                                 
                                 
                                    V
                                 
                              
                              (
                              s
                              ,
                              j
                              )
                           
                         changes according to the fitted line, whereas 
                           
                              
                                 
                                    D
                                 
                                 
                                    H
                                 
                              
                              (
                              s
                              ,
                              j
                              )
                           
                         contains the same value for different j. For 
                           
                              
                                 
                                    D
                                 
                                 
                                    H
                                 
                              
                           
                        , each segment disparity is unique and so it shows a horizontal line on the DSI according to the disparity value. The confidence for the segment disparities being correct is evaluated by averaging the cost values crossed by the line as follows:
                           
                              (13)
                              
                                 F
                                 (
                                 s
                                 ,
                                 
                                    
                                       D
                                    
                                    
                                       x
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             j
                                          
                                       
                                       (
                                       
                                          
                                             δ
                                          
                                          
                                             s
                                          
                                       
                                       (
                                       j
                                       ,
                                       
                                          
                                             D
                                          
                                          
                                             x
                                          
                                       
                                       (
                                       s
                                       ,
                                       j
                                       )
                                       )
                                       )
                                    
                                    
                                       J
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    D
                                 
                                 
                                    x
                                 
                              
                           
                         is a disparity map which can be either 
                           
                              
                                 
                                    D
                                 
                                 
                                    V
                                 
                              
                           
                         or 
                           
                              
                                 
                                    D
                                 
                                 
                                    H
                                 
                              
                           
                        . Since the negative values of normalised correlation values are set to 0, the cost values vary from 0 to 1. J is the number of pixels within segment s. 
                           
                              F
                              (
                              s
                              ,
                              
                                 
                                    D
                                 
                                 
                                    x
                                 
                              
                              )
                           
                         varies from 0 to 1 for each group. If 
                           
                              F
                              (
                              s
                              ,
                              
                                 
                                    D
                                 
                                 
                                    V
                                 
                              
                              )
                              >
                              F
                              (
                              s
                              ,
                              
                                 
                                    D
                                 
                                 
                                    H
                                 
                              
                              )
                           
                        , the final disparities of the segment 
                           
                              D
                              (
                              s
                              ,
                              j
                              )
                           
                         are set to 
                           
                              
                                 
                                    D
                                 
                                 
                                    V
                                 
                              
                              (
                              s
                              ,
                              j
                              )
                           
                        . Similarly, If 
                           
                              F
                              (
                              s
                              ,
                              
                                 
                                    D
                                 
                                 
                                    V
                                 
                              
                              )
                              <
                              =
                              F
                              (
                              s
                              ,
                              
                                 
                                    D
                                 
                                 
                                    H
                                 
                              
                              )
                           
                        , the final disparities of the segment 
                           
                              D
                              (
                              s
                              ,
                              j
                              )
                           
                         are set to 
                           
                              
                                 
                                    D
                                 
                                 
                                    H
                                 
                              
                              (
                              s
                              ,
                              j
                              )
                           
                        .

This line fitting process not only correctly optimises the disparity map, but also calculates the disparity gradients of segments. The detection of the side surfaces can be significantly simplified by utilising this information during the obstacle detection stage (discussed in detail in Section 3).

Finally, the disparity values of the edge positions are filled with the WTA optimisation based on the NCC cost. High frequency components on the disparity map D are removed and interpolated. An example of the final disparity map is shown in Fig. 11
                         and more experimental results and comparisons can be found in Section 5.

With the disparity maps extracted earlier, we now focus on obstacle detection. Our work is based on the U–V-disparity representation [13] and it is extended for faster and more accurate detection. The projection of a disparity map in U-disparity and V-disparity domains can be treated as two histograms of disparity values. From this point, ‘the projection of the disparity map in V/U-disparity domain/space’ is generally stated as ‘U/V-disparity’. For the V-disparity, the histogram of each horizontal line is calculated. If the number of pixels sharing the same disparity on each line is represented by brightness, an image in which vertical and road planes are represented by lines is formed. An example of V-disparity is shown in Fig. 12
                        . The longest diagonal line in the V-disparity represents the road plane. The vertical lines represent the obstacles with vertical planes. The length of each vertical line indicates the height of the corresponding obstacle. The U-disparity is similar to the V-disparity but it is the histogram of columns of the disparity map. Likewise, the U-disparity (as shown in Fig. 12) encodes the horizontal information of obstacles. The road plane is no longer visible but the side surfaces are included and are represented with non-horizontal lines. As carefully analysed in [34], the U–V-disparity includes essential information for most planes that would appear in a road scene. It reduces the time consuming plane detection problem to a much simpler line detection one. However, all objects are included in the histogram and it is difficult to extract an obstacle without the interference of other outliers. Many algorithms detect the road surface first with a flat road assumption. The relationship between the road plane and vertical planes can then be defined to restrict the pixels of interest. The Hough transform has been one of the most commonly used algorithms for U–V-disparity based line detection. However, it has been found in our previous work [38] that the parameters of the Hough transform are very difficult to determine in changing environments. It requires a threshold to binarise the U–V-disparity, another threshold for the Hough space accumulator, a parameter to determine the acceptable point spacing, a line orientation restriction and more parameters involved in the 2D peak detection. Even though the parameters are carefully selected based on a single set of test data, errors are unavoidable if shorter lines (far range obstacles) need to be detected. Including a post processing step is almost essential if the Hough transform is used. However, implementing this step indicates the inclusion of even more parameters. In summary, the Hough transform takes a large amount of computational power and memory space but cannot consistently produce promising results.

In our system, obstacle detection is based on the U-disparity and the proposed G-disparity space (encloses disparity gradient information). The V-disparity is not used to extract the vertical profiles of obstacles. The most important reason to exclude the V-disparity is that the heights of the side planes are difficult to extract. Furthermore, errors will be introduced in a special case. If two obstacles with different heights have an identical disparity, the V-disparity will result in single line with changing intensities. Although the two obstacles can be separated horizontally by analysing the U-disparity, the difference in their height is very difficult to distinguish. A similar situation will happen to the U-disparity when two obstacles with similar disparity exist vertically. Fortunately, this happens very rarely in traffic scenes. In the proposed approach, the disparity gradient information extracted earlier is used, and the obstacle detection problem has been separated into two stages: side plane recognition and vertical plane detection. By doing so, the detection of obstacles can be faster and more accurate.

The detection of side planes is a more complicated problem than the vertical plane detection since the orientations of the lines are not priorly available. When the Hough transform is used for line detection, it is almost essential to include the local edge orientation to achieve accurate and fast performance. However, the orientations of the edge points in the U-disparity are not accurate due to the discrete nature of the disparity map (low resolution in the disparity axis). In most cases, disparity changes slowly in the horizontal direction on a side plane. The disparity level only changes every few pixels. In this case, the orientations of the edges corresponding to the side planes are inaccurate and mostly vertical as shown in Fig. 13
                        . Thus, a range of orientations needs to be allowed which increases the system complexity. Another problem is that once detection of small objects is allowed (represented by very short lines on the U-disparity), the horizontal segments of the side lines can be easily confused as small vertical planes.

The proposed obstacle detection system extracts lines representing the side planes before the detection of vertical planes. Once the detected side planes are removed from the disparity map, vertical planes can be very easily extracted. During the disparity calculation stage (discussed in Section 2), the disparities of some of the horizontal segments are selected from 
                           
                              
                                 
                                    D
                                 
                                 
                                    V
                                 
                              
                           
                        . These segment disparity values are fitted with a straight line with a non-zero gradient. They are very likely to be included in the side planes and the gradient of the fitted lines can be useful. However, it is possible that not all pixels from the side planes are correctly chosen from 
                           
                              
                                 
                                    D
                                 
                                 
                                    V
                                 
                              
                           
                        . Therefore, using these pixels to represent the side planes will result in incorrect sizes and positions. In this paper, we propose a similar approach to the generation of the U-disparity. The gradients of the segments corresponding to the same side plane can be aggregated which results in the proposed G-disparity and allows convenient side line detection.

Defining the disparity gradient map as 
                           
                              
                                 
                                    D
                                 
                                 
                                    G
                                 
                              
                           
                        , the corresponding gradient for each pixel can be represented as 
                           
                              
                                 
                                    D
                                 
                                 
                                    G
                                 
                              
                              (
                              n
                              ,
                              m
                              )
                           
                         or 
                           
                              
                                 
                                    D
                                 
                                 
                                    G
                                 
                              
                              (
                              s
                              ,
                              j
                              )
                           
                         where n and m represent the column and row indices. Like before, s and j represents the jth pixel within a horizontal segment s. A histogram of the disparity gradient can then be calculated for each column of 
                           
                              
                                 
                                    D
                                 
                                 
                                    G
                                 
                              
                           
                         like the calculation of the U-disparity. The result is called the G-disparity. In most situations, many pixels on 
                           
                              
                                 
                                    D
                                 
                                 
                                    G
                                 
                              
                           
                         are zero, which corresponds to oblique or vertical planes. These pixels are excluded during the G-disparity calculation since the objective is to locate the side planes. An example G-disparity is illustrated in Fig. 14
                        . As the figure shows, similar gradients on the same column are accumulated and form horizontal lines on the G-disparity. The width and the horizontal positions of these lines are identical to the side lines in the U-disparity (as shown in Fig. 14 and 15
                        ). This indicates that the gradient and horizontal profiles of side lines in the U-disparity can be extracted by locating the horizontal lines in the G-disparity. In our system, the detection of these horizontal lines is achieved by analysing the pixel connectivities but other methods can also be applied. Once the horizontal lines are found, the corresponding horizontal positions, widths and gradients of the side lines on the U-disparity space are available. The only unknown parameter of each line on the U-disparity is the line offset. For every horizontal line, a one dimensional Hough transform with fixed line orientation is applied to extract the offsets. The number of Hough transform executions is equivalent to the number of horizontal lines on the G-disparity space. Each time, only a vertical band on the U-disparity is used. This vertical band is determined by the position and width of the corresponding horizontal lines on the G-disparity. Unlike applying the Hough transform directly on the U-disparity, the proposed algorithm finds the horizontal profile and line orientation prior to the use of the Hough transform. Another important fact is that only the offset corresponding to the maximum in the Hough space needs to be selected. During the whole process, no manually selected parameter is needed except a small threshold 
                           
                              
                                 
                                    T
                                 
                                 
                                    U
                                 
                              
                           
                         on the U-disparity for binarisation. The determination of this threshold is based on the performance of the disparity calculation algorithm and the size of the smallest detectable obstacle. In our system, it has been chosen to be 10 so that most of the pixels are preserved (this indicates a small object with height of only 10 pixels will be included). A side line detection result is shown in Fig. 15. The resultant line detection is fast, accurate and requires only one parameter which can be easily determined.

Vertical profiles of the side planes are detected based on the original disparity map. For each side line, the indicated disparities are used for side plane detection. The height of a plane is detected by locating vertically the locally connected pixels with approximately identical disparity values. The result of a side plane detection is shown in Fig. 17. The side planes are highlighted in yellow. As the figure illustrates, the side planes are detected accurately.

Once the side planes are detected, all the pixels belonging to the sides are removed from the disparity map. The U-disparity is recalculated and contains almost only the horizontal lines. This step is very important since significantly less outliers are included during vertical plane detection. The same threshold 
                           
                              
                                 
                                    T
                                 
                                 
                                    U
                                 
                              
                           
                         is applied to the U-disparity for binarisation.

For each row d, the adjacent pixels are grouped and form a line. If the distance between two valid pixels is smaller than 
                           
                              
                                 
                                    T
                                 
                                 
                                    L
                                 
                              
                           
                        , they are treated as being on the same line. This parameter is related to 
                           
                              
                                 
                                    T
                                 
                                 
                                    U
                                 
                              
                           
                        , since if more pixels are preserved, a lower 
                           
                              
                                 
                                    T
                                 
                                 
                                    L
                                 
                              
                           
                         is needed to connect the points together. Thus, this relationship is assumed to be linear as shown below:
                           
                              (14)
                              
                                 
                                    
                                       T
                                    
                                    
                                       L
                                    
                                 
                                 =
                                 γ
                                 ∗
                                 
                                    
                                       T
                                    
                                    
                                       U
                                    
                                 
                              
                           
                        It has been found by experiments that setting 
                           
                              γ
                              =
                              0.4
                           
                         is a suitable choice for our test data.

Finally, the lines on row d that are shorter than a object size threshold 
                           
                              
                                 
                                    T
                                 
                                 
                                    S
                                 
                              
                              (
                              d
                              )
                           
                         (shown in Eq. (15)) are removed. This is a linear equation so that 
                           
                              
                                 
                                    T
                                 
                                 
                                    S
                                 
                              
                           
                         changes from small to large according to the disparity. This is because objects appear smaller in the far-field and larger in the near-field. The size threshold should also change with depth.
                           
                              (15)
                              
                                 
                                    
                                       T
                                    
                                    
                                       S
                                    
                                 
                                 (
                                 d
                                 )
                                 =
                                 
                                    
                                       T
                                    
                                    
                                       S
                                    
                                 
                                 (
                                 0
                                 )
                                 +
                                 d
                                 ·
                                 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             S
                                          
                                       
                                       (
                                       
                                          
                                             d
                                          
                                          
                                             max
                                          
                                       
                                       )
                                       -
                                       
                                          
                                             T
                                          
                                          
                                             S
                                          
                                       
                                       (
                                       0
                                       )
                                    
                                    
                                       
                                          
                                             d
                                          
                                          
                                             max
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    T
                                 
                                 
                                    S
                                 
                              
                              (
                              0
                              )
                              =
                              5
                           
                         and 
                           
                              
                                 
                                    T
                                 
                                 
                                    S
                                 
                              
                              (
                              
                                 
                                    d
                                 
                                 
                                    max
                                 
                              
                              )
                              =
                              20
                           
                         defines the size threshold at the furthest and nearest distance respectively. This size limitation is very small which allows the detection of small objects and it is extremely rare that an obstacle in the road scene is smaller than these limits. All the above choice of thresholds are based on 
                           
                              640
                              ×
                              300
                           
                         input image with 30 disparity levels. If the resolution of the input data changes significantly, these thresholds need to be tuned accordingly. This is reasonable since all these parameters are in pixel units. Identical objects will be represented by more pixels in high resolution images. The result of this line selection process is shown in Fig. 16
                        . As we can see, the detection is accurate due to the exclusion of outliers.

The height and vertical position of a vertical plane is extracted based on the vertical image band defined with the horizontal line obtained earlier. Furthermore, the corresponding d of the line indicates that only pixels with the same d belong to the obstacle of interest. Therefore, a one dimensional accumulator can be used to store the number of pixels having the same d within the vertical region. A similar pixel connecting process is carried out as described for the detection of horizontal lines. If more than one line is detected, extra potential obstacles are created to allow the detection of multiple vertically positioned obstacles having an identical disparity value.

The final obstacle detection result is shown in Fig. 17
                        . The proposed algorithm uses the minimum amount of parameters, which are easy to determine, to detect both small and large obstacles and identifies the side and vertical planes quickly and accurately. It does not rely on the prior road plane detection and successfully avoids the disadvantages of applying the Hough transform.

In this section, we include a free-space calculation algorithm to illustrate the benefit of the proposed obstacle detection approach. It is important to note that our aim is to detect all the image areas that fit with the road surface and are free of any obstacles. Therefore, some off-road areas and road surfaces beneath obstacles could be included in the results. With the obstacles removed from the disparity map, the free-space calculation becomes a much easier task. The remaining areas are mostly occupied by road, sky and some other far-field objects. The most significant advantage of the system is that the road surface is not assumed to be a very large area. Also, the amount of outliers involved during model fitting is reduced to the minimum which enables easier and faster computation. The vertical profile of the road surface is modelled using a spline which is proposed in [35]. This flexible model has very few restrictions, and therefore, allows most of the road vertical profiles to be accurately represented.

The first step is to pick the road region from the disparity map. In order to remove the area of sky, the furthest possible distance of the road to be modelled is limited to 
                        
                           100
                           m
                        
                     . This is the case where no obstacle is ahead of the vehicle. If an obstacle blocks the view of the far-distance road surface, the distance of that obstacle is used as the furthest modelling distance.

An example of a disparity map without obstacles and extreme far-field pixels is shown in Fig. 18
                     . Since the nearest road surface pixels always lie on the lowest row of the image, the search for the road surface starts expanding from the centre (can vary depending on the mounting position of the camera) of the lowest line. The width of the area is set to be limited by any discontinuities of valid pixels. From the lowest line upwards, the search stops once no valid pixels can be found in the upper row or the number of the pixels on the row is smaller than a threshold. This threshold is set to confirm that enough pixels are available for the following steps. In our experiments, the threshold is chosen to be 60 while the image width is 640.

Then the V-disparity is constructed based on the selected road area. The resultant image contains a clear line corresponding to the road as shown in Fig. 19
                     . On each row of the V-disparity, the d corresponding to the maximum intensity is selected to represent the road disparity. This process reduces the number of pixels involved during line fitting to the minimum and removes any potential outliers.

A least squares line fit is then performed based on the data points extracted earlier. The B-spline equation is defined as:
                        
                           (16)
                           
                              X
                              (
                              t
                              )
                              =
                              
                                 ∑
                              
                              
                                 
                                    B
                                 
                                 
                                    io
                                 
                              
                              (
                              t
                              )
                              
                                 
                                    Q
                                 
                                 
                                    i
                                 
                              
                           
                        
                     where 
                        
                           X
                           (
                           t
                           )
                        
                      represents the curve defined by the B-spline basis function B and control points Q. The degree of the spline is represented by o. 
                        
                           t
                           ∈
                           [
                           
                              
                                 t
                              
                              
                                 o
                              
                           
                           ,
                           
                              
                                 t
                              
                              
                                 K
                                 -
                                 o
                                 -
                                 1
                              
                           
                           ]
                        
                      denotes sampling points on the curve where K is the number of knots. The control point index is represented by i. In our system, a cubic B-spline is used (
                        
                           o
                           =
                           3
                        
                     ).

The least squares algorithm finds the minimum of the summed squared error between the spline to each data point by setting the differentiation of the error equation to 0. The resultant function is shown in Eq. (17).
                        
                           (17)
                           
                              
                                 
                                    B
                                 
                                 
                                    T
                                 
                              
                              BQ
                              -
                              
                                 
                                    B
                                 
                                 
                                    T
                                 
                              
                              P
                              =
                              0
                           
                        
                     
                  


                     B contains all components of B. Q is a matrix containing all 
                        
                           
                              
                                 Q
                              
                              
                                 i
                              
                           
                        
                      and P represents the data points. The control points (
                        
                           Q
                        
                     ) can then be calculated. The advantage of the least squares method is that a global minimum can be guaranteed. However, the algorithm can be sensitive to outliers since the difference of all points is involved in the calculation. This problem will not cause any issue in our case since all data points are carefully sampled from the road surface. The result of curve fitting based on Fig. 19 is shown in Fig. 20
                     . The chosen disparity d at each image row m is illustrated as small black circles. The blue line is the spline fitted to these points. As this figure shows, although the resolution of d is low, the fitted line is accurate and represents a flat road. More spline fitting results can be found in supplementary data.

The original disparity map is compared to the disparities obtained from the vertical road profile. The initial selection of the road area is expanded if the horizontally adjacent non-road area contains the same disparity level as the road profile. In far distances where the road disparity is not modelled, the spline is linearly extrapolated based on the gradient at the minimum of the modelled d. This step enables the road area to be extended to very far distances. Finally, morphological processing (erosion and dilation) is applied to the road area to remove expansion errors. The final free-space calculation result is shown in Fig. 21
                     . As the figure shows, the free-space is detected correctly up to a very far distance.

@&#EXPERIMENTAL RESULTS@&#

Although the proposed disparity calculation algorithm is specifically designed to work on traffic scenes, its performance was also evaluated using the stereo images provided by Middlebury.
                           2
                           Middlebury Computer Vision website. (http://vision.middlebury.edu/stereo/).
                        
                        
                           2
                         These stereo image sets are very commonly used for result evaluations and the ground truths are also available. It is worth noting that these test images contain rich textures and are very different from road scenes. These textures produce a large amount of edges and this leads to serious over segmentation for edge based approaches. This further reduces the performance of the occlusion handling part of the proposed algorithm (the occluded areas are not grouped with the non-occluded areas so that cannot be corrected). Ideally, a texture-based segmentation algorithm should be used instead of an edge based one. However, since the focus of the proposed algorithm is on road scenarios with the smallest amount of computation, edge detection is still used for segmentation during the evaluation.

Four sets of images that are regarded as standard evaluation data are used in our tests. During the tests, the parameter settings remain constant and the results are compared with the ground truth. Performance evaluation is conducted based on the bad-pixel percentage 
                           
                              μ
                           
                         as shown below [26]:
                           
                              (18)
                              
                                 μ
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       MN
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          m
                                          ,
                                          n
                                       
                                    
                                 
                                 (
                                 |
                                 D
                                 (
                                 n
                                 ,
                                 m
                                 )
                                 -
                                 
                                    
                                       D
                                    
                                    
                                       T
                                    
                                 
                                 (
                                 n
                                 ,
                                 m
                                 )
                                 |
                                 >
                                 
                                    
                                       η
                                    
                                    
                                       d
                                    
                                 
                                 )
                              
                           
                        where D is the disparity map and 
                           
                              
                                 
                                    D
                                 
                                 
                                    T
                                 
                              
                           
                         represents the ground truth. 
                           
                              
                                 
                                    η
                                 
                                 
                                    d
                                 
                              
                           
                         is a threshold controlling the error tolerance. In the experiments, 
                           
                              
                                 
                                    η
                                 
                                 
                                    d
                                 
                              
                           
                         is set to 1 as it is the default choice for the published Middlebury evaluation results. The proposed algorithm does not estimate the disparities on the left most side of the reference image due to the fact that this section might not be included in the right image. Apart from this region, all the available pixels in the ground truth are used instead of only the non-occluded areas. Fig. 22
                         illustrates the test images, the disparity map calculated with the proposed approach and the difference image with the ground truth.

As the figure shows, most of the errors are concentrated on the edge areas and are caused by occlusions and interpolations. The disparities within the texture-less regions are accurate and smooth. The error percentage is shown in Table 1
                         along with the comparison with several top ranked algorithms in Middlebury. The parameters for the listed algorithms are manually tuned so that optimum performance can be achieved. The same parameters are used for all four images in our test. According to the error rate, the proposed algorithm will not be placed on the top of the chart in the Middlebury evaluation rank since many algorithms are specifically tuned for the Middlebury data sets. Nevertheless, the performance of the proposed results is still higher than the traditional Winner-Take-All (WTA) methods (e.g. SSD+MF [39]), and achieved a lower error rate in all four tests. The only algorithm employing WTA that stays at the top for the Middlebury chart is [40]. However, this algorithm utilises the Belief Propagation (BP) based technique for cooperative optimisation [40]. In fact, most of the top ranked methods in Middlebury are BP based algorithms (e.g.[41–44]). When compared to BP based approaches, the proposed algorithm achieved a relative higher error rate in the first three test sets. However, the error rate of ‘Cones’ came very close to the results produced using BP based methods. It is worth noting that Middlebury tests are not truly indicative of the performance of an algorithm based on real-world traffic scenes. As also suggested by [28,45], the performance scenes are completely different. This explains the fact that the proposed algorithm resulted in a competitive performance in Cones, since the large number of horizontal and vertical surfaces within the scene demonstrates high similarity to a real-world traffic scene. The Middlebury images were tested here in order to compare the performance of the proposed algorithm in well controlled situations with existing algorithms.

Following the testing under well controlled indoor environments, the real-world performance of the proposed disparity calculation algorithm was evaluated based on the stereo images sets provided on Kitti.
                           3
                           Kitti Vision Benchmark.
                        
                        
                           3
                         The results obtained with the proposed approach were compared to the ones obtained with the traditional Winner-Take-All (WTA) [26] and one of the simplest BP method [22], both of which are based on the NCC cost function. It is not suitable to compare to the results achieved using the SAD cost function, since the test images contain large intensity variations. Because the WTA technique only uses local information, comparison with the WTA optimisation results is the most appropriate way of illustrating the improvements achieved by the proposed algorithm. BP based approaches are high performance global optimisation techniques. Many algorithms based on BP have achieved leading performance in the Middlebury stereo evaluations as well as the comparisons using real-world traffic images [46]. Our implementation is based on the same algorithm tested in [46] but with the NCC cost function instead of the census transform. The authors in [28] also illustrated the BP performs better based on the edge map instead of the original image. In our experiments, both types of input images achieved a similar performance due to the fact the NCC calculation is insensitive to brightness changes.

All the cost calculations for the compared algorithms (the WTA example, the BP example and the proposed algorithm) are based on the same 
                           
                              5
                              ×
                              5
                           
                         block size. The WTA approach does not require any parameter settings. It is certainly the fastest but a large amount of errors is introduced in the homogeneous and occlusion regions. The BP algorithm contains a few parameters mainly aimed at tuning the overall smoothness of the disparity map. These parameters are not easy to determine and the final result is sensitive to modifications. In our implementation of BP, a set of optimum parameters are selected based on experiments in order to balance between the depth smoothness and accuracy. For the proposed approach, the required parameters are discussed in detail in the previous sections. They are straightforward to determine and not as sensitive as the ones included in the BP approach.

Some of the experimental results are shown in Fig. 23–25
                        
                        
                        . The test images shown in this section are some of the most typical from the sequences. Most of these scenes contain large homogeneous and occluded areas along with large camera gain differences. In the evaluations of the three algorithms, the proposed algorithm achieved a comparable performance to the BP approach. Much better disparity maps are obtained when compared to ones based on the WTA optimisation. The proposed algorithm has a partial occlusion handling capability. However, unlike the BP approach, the occlusion handling strength cannot be controlled by parameters. The performance of the proposed algorithm in homogeneous regions is the most impressive. The errors are minimised by selecting the horizontal segment disparity for each individual pixel. Meanwhile, smooth disparity transitions are allowed by incorporating the vertical segments disparities. As a result, accurate and high resolution disparity maps can be obtained. The proposed algorithm also provides important gradient information to allow the simplification of side plane recognition during obstacle detection. Finally, the proposed algorithm is a line based method which is much faster than the BP approach and most of the global optimisation algorithms. The implementation of the proposed algorithm is in Matlab. Apart from the cost calculation, optimisation based on BP takes 
                           
                              92.6
                           
                         seconds and the proposed algorithm takes 
                           
                              7.05
                           
                         seconds on average with an i7 CPU. It is important to note that Matlab implementations are not accurate measurements of the algorithm speed. The processing speeds are shown here only for indication purposes.

As Fig. 23–25 show, comparing with the other two, the proposed algorithm achieved the best performance in road areas. The disparities on the road surface are accurate and smooth. For the vertical planes, both the proposed algorithm and the BP achieved high performance. The proposed algorithm is also able to distinguish the side planes and produce smooth disparity transitions in these areas. In the occluded areas, the proposed algorithm demonstrated high degrees of handling ability if the occluded areas are within a large segment with more non-occluded pixels. However, if this condition is violated, the occluded areas will introduce errors. Generally, the BP exhibits a stronger ability to correct the disparities in occluded areas. However, the strength of occlusion handling is controlled by adjusting the general smoothness for the whole disparity image. It is difficult for the BP to remove all occlusions while not producing errors in other areas. Both the proposed algorithm and the BP approach are able to produce accurate disparities up to far distances. The performance of the BP approach relies on parameter tuning whereas the proposed algorithm is less constrained. Until now, it is fair to say that the proposed approach has achieved significantly better results than the WTA method in real world traffic scenes. In comparison with the global optimisation algorithm, BP, the proposed algorithm also demonstrated advantages in terms of parameter sensitivity, speed and calculation accuracy on the non-occluded areas.

While the proposed algorithm demonstrates a more impressive performance compared to general disparity calculation approaches, further evaluation were carried out based on the stereo images sets from Kitti vision benchmark. The Kitti evaluation results are produced by stereo vision algorithms specially designed and tuned for real-world traffic scenes so that the proposed algorithm can be compared with other ”road traffic specialists”. The proposed algorithm computed disparity maps of 195 Kitti test image pairs. Those test sequences contain challenging illumination conditions, high image resolution and large pixel displacements. The results were evaluated according to the ground truth disparity maps generated by a Velodyne HDL-64E laser scanner. The Kitti evaluation chart ranks all methods according to the number of non-occluded erroneous pixels at the specified disparity error threshold. All methods providing less than 100 percentage density have been interpolated using simple background interpolation. All test pairs were processed using the same parameter in order to reflect the adaptability of the tested algorithm across various real-world-traffic scenarios. All details of the listed methods can be found in Kitti.
                           4
                           Kitti Vision Benchmark. (http://www.cvlibs.net/datasets/kitti/).
                        
                        
                           4
                         The main evaluation ranking is decided according to the number of non-occluded erroneous pixels at the 3 pixels error threshold. Table 2
                         shows the performance evaluation of the proposed system compared with methods from Kitti stereo benchmark. In Table 2, the evaluation categories are Out-Noc (Percentage of erroneous pixels in non-occluded areas), Out-All (Percentage of erroneous pixels in total), Avg-Noc (Average disparity error in non-occluded areas) and Avg-All (Average disparity error in total). It is worth noting that some methods (e.g. PCBP-SS) receive support from time space information rather than purely stereo information.

During the early stages of the algorithm development, our proposed algorithm was coded and tested under a Matlab environment. However, in the Kitti bench evaluation chart, all the top ranked methods have their results obtained in a C/C++ environment. In order to present a clearer processing speed comparison with other algorithms in Kitti bench evaluation chart, a C/C++ version of the proposed algorithm has been coded and this has undergone the same set of evaluation image sequences. The repetitive nature of the core section of the proposed algorithm results in a much lower processing time in C than its Matlab version. The Matlab version of the proposed algorithm takes 4min on average to compute a single disparity image, on a PC with a single core i7 2.8GHz. The C version of the proposed algorithm runs on the same PC with the same set of parameters but with the processing time dramatically cut down to 5 s. Furthermore, it produces more accurate results of 16.73% of erroneous pixels in non-occluded areas than its result of Matlab version (19.04%). It can be shown that, although the accuracy of the proposed algorithm is not as impressive as the top ranked algorithms in Kitti, it demonstrates a very short processing time which is critical in applications in the automotive industry. By employing a state-of-art Digital Signal Processor such as TI C6678 with the strength of multi-core processing, a real-time processing speed of 18–20 frames per second is predicated.

Some of the testing results are shown in Fig. 26,27
                        
                        . SNCC [47] employs a cost function based on a modified version of the Normalised Cross-Correlation (NNC) calculation, which is ideal for comparison with the proposed algorithm. ELAS [48] is one of the top performance methods dealing with scenes containing a large amount of homogeneous surfaces. It has been used for comparison with the proposed algorithm to evaluate the performance in homogeneous areas. Fig. 26 demonstrates a typical street scene with a wide road and vehicle side surfaces. The proposed algorithm shows complete disparity calculation results with ELAS in road surfaces and vehicle side surfaces. SNCC, employing a more complex cost function calculation, had a similar performance to the proposed algorithm while smoothness suffered in pavements areas. In Fig.27, all three compared algorithms demonstrated good performance. However, the small tree area was given fault disparity values by ELAS and nearly completed missed out by SNCC. However, the proposed algorithm retained good accuracy in local disparity calculations while still performing competitively in large homogenous regions.

The proposed obstacle detection algorithm utilises the disparity gradient and simplifies the detection of side planes by constructing the proposed G-disparity. The obstacle detection results shown in Fig. 28
                         are obtained based on the disparity map calculated using the proposed algorithm shown in Fig. 23–25. As the figures show, the proposed algorithm is able to clearly identify side planes and vertical planes and both far-field and near-field obstacles can be quickly detected. Fig. 29
                         illustrates a comparison between the proposed approach and the Hough transform line detection on the U-disparity space. The Hough transform is based on peak detection instead of thresholding in order to avoid severe over detection. The parameters for each image are tuned carefully (different parameters for different images) so that optimum results can be achieved. The parameters of the proposed approach are fixed for all test data. It can be seen that the proposed line detection approach successfully minimises the outliers included in the operation and the detection result is more accurate. Once the side lines are removed from the U-disparity, the problem is simplified to the detection of horizontal lines. For the Hough transform, parameters are tuned in order to detect the small object in the centre in Fig. 29. However, detection errors still exist in other regions. The experimental results also demonstrate the proposed algorithm is less sensitive to parameter changes compared with the Hough transform-based approach.

By excluding the obstacles from the disparity image, the vertical road profile can be easily determined using least squares line fitting. Fig. 20 shows a line fitting result based on the spline model. The data points are correctly sampled from the road surface and the vertical road gradient changes are accurately estimated. The final free-space is calculated based on the initial road area selection and the vertical profile. Some of the detection results are shown in Fig. 30
                        . As the figure shows, free-spaces can be correctly calculated in various conditions. Even when the foreground road area is small, the detection result is still accurate. Detecting the free-space after obstacle detection is a much easier job where no assumption is needed.

@&#CONCLUSION@&#

In this paper, a novel stereo vision based obstacle detection system is presented. For the disparity calculation, a multi-pass horizontal and vertical aggregation process allows accurate cost evaluation in textureless regions. The algorithm also has a good partial occlusion handling capability. A accurate horizontal disparity gradient is extracted during disparity optimisation, which simplifies the detection of side planes. In our experiments, the proposed stereo vision algorithm achieved comparable performance to the BP based global optimisation technique but at a much lower complexity. Based on the calculated disparity and disparity gradient, the proposed G-disparity can be constructed to be used in conjunction with the U–V-disparity images to achieve efficient and accurate detection. Finally, vertical profiles of road surfaces are modelled with splines and free-space calculation is performed. By removing the detected obstacles from a disparity map, the free driving spaces can be conveniently located even with the presence of large obstacles in the near-field. For future work, the possibility of using temporal information for stabilisation will be explored.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.cviu.2014.02.014.


                     
                        
                           Supplementary material
                           
                              More Experimental Results.
                           
                           
                        
                     
                  

@&#REFERENCES@&#

