@&#MAIN-TITLE@&#Hankelet-based dynamical systems modeling for 3D action recognition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We model an action as sequence of outputs of linear time invariant (LTI) systems.


                        
                        
                           
                           We represent the outputs of LTI systems by means of Hankelets.


                        
                        
                           
                           We adopt an HMM to model the transitions from one LTI system to another.


                        
                        
                           
                           We formulate an inference and supervised learning formulation for our model.


                        
                        
                           
                           We also present a deep analysis of the parameter settings for our action representation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Hidden Markov Model

Hankel Matrix

Linear time invariant system

Discriminative learning

Action

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

In recent years, a large portion of the research in computer vision has focused on the problem of action recognition and modeling. Detection, recognition and analysis of actions are of great interest in several application domains such as surveillance [1–4], human–computer interaction [5], assistive technologies [6], sign language [7–9], and, more recently, computational behavioral science [10,11] and consumer behavior analysis [12].

The wide diffusion of cheap depth cameras, and the seminal work by Shotton, et al. [13] for estimating the locations of the joints of a human body from depth maps, have given new stimulus to the research in 3D action classification both by quickening the development of novel applications, and by providing a setting to test new ideas and frameworks. Therefore, very recently, we have seen a proliferation of works introducing novel body pose representations for action recognition given depth maps and/or skeleton data [14–20].

In this paper, we propose to represent an action as a series of movements to exploit their temporal structure while discriminating among different action classes. As an example, consider the action of handshaking which can be modeled by the following ordered sequence of movements: moving the whole body to approach the other person, raising the arm, and shaking the hand. Furthermore, each of these movements can be represented as a sequence of observations (for example a sequence of body poses) which are characterized by their own dynamics. Therefore, an action can be represented in terms of a “sequence of simpler dynamics”.

This reasoning leads to the idea that an action should be modeled by a hierarchical dynamical model, such as a mixture of Hidden Markov Models (HMMs) [21], coordinated mixture of factor analyzers [22] or switching models [23]. However, the burden of learning the model parameters and the size of the required training set may limit the applicability of these methods.

Here, we propose to approximate the abovementioned complex hierarchical dynamical model by adopting a simpler representation for the movements. In particular, we focus our attention on the switching of the dynamics across time. For this purpose, we represent movements using body motion templates. A body motion template may be either an ordered set of trajectories (i.e. trajectories of body parts such as hands, arms, legs, head, torso) or a sequence of frame descriptors (based on bag-of-words, oriented flow, dense trajectories, etc.) within a temporal window. For simplicity, in the remainder of the paper we will assume that a body motion template is an ordered set of trajectories of 3D body joints within a temporal window. However, our framework may be used with other feature representations as long as they have an ordering relation.


                     Fig. 1
                      illustrates the basic idea of our approach. An action is a temporal series of body motion templates (movements). Each body motion template is a series of raw observations in a temporal window (eventually of varying duration) which is characterized by a specific dynamic. Thus, we aim at decomposing an action into sub-trajectories that are modeled as the outputs of a sequence of atomic linear time invariant (LTI) systems, using an HMM to model the transitions from one atomic LTI system to another. Furthermore, each body motion template is described by means of a truncated Hankel matrix (Hankelet) [24], which embeds the parameters of the LTI system [25]. In summary, an action is modeled by an HMM where the observations are Hankel matrices, computed in a sliding window, and where each hidden state represents an LTI system for which only a Hankelet is known. Finally, for classification purposes, we train a set of HMMs (one for each action class) using a discriminative approach.


                     Fig. 2
                      contrasts traditional HMM representations against our approach. Instead of learning the parameters of a switching HMM (Fig. 2(a)), we consider a probabilistic switching LTI system (Fig. 2(b)) where an HMM is used over the Hankel matrices of the systems, avoiding the need of performing any system identification (Fig. 2(c)).

The results presented here are an extension of our preliminary work [26]. In particular, in this paper:
                        
                           •
                           we account for the learning of the atomic LTI systems via a discriminative method that encourages correct predictions of the HMMs;

we provide a deeper description of our discriminative learning approach in relation to former models;

we present an extensive validation of our Hankelet-based action representation for different parameter settings.

The paper is organized as follows: in Section 2 we review previous work on action recognition and modeling, and on discriminative learning of HMM parameters; in Section 3 we present our Hankelet-based action representation; in Section 4 we explain our action model and describe our classification and LTI inference methods; in Section 5 we describe the discriminative learning of the model parameters and of the atomic LTI systems; in Section 6 we present experimental results on publicly available datasets and analyses of the performance of our technique for varying settings and parameters of the Hankelet-based representation. Finally, in Section 7 we present conclusions and outline future research directions.

@&#RELATED WORK@&#

The literature about action recognition and time series modeling is very extensive. Here, we focus on three main aspects of the methods at the state-of-the-art: action representation, especially for 3D data, modeling of time-varying dynamics, and discriminative learning of parameters. We refer the reader to the following surveys for more general discussions on these topics: [27–31].

Most approaches for human action recognition in still images and RGB video [29] attempt to extract features that may be correlated with the human body pose (human body pose represents the configuration of body parts including head, arms, and legs). Descriptors such as Histogram of Oriented Gradients (HOG) [32], 3D-SIFT [33], Local Binary Pattern (LBP) [34] have been widely used in the literature. Often, a bag-of-words approach is used to compute a histogram of visual words based on a dictionary of local features [35].

Good motion representations can help to discriminate among actions during recognition. Several techniques have tried to combine body pose representation with motion information. Recently, Spatio-Temporal Interest Points (STIPs) [36] and Dense Trajectories (DT) [37,25], jointly with Motion Boundary (MB) [38], have proved to increase accuracy of action recognition in video sequences.

Since the introduction of depth cameras and the work by Shotton, et al. [13] for estimating the body part locations in depth maps, several researchers have focused on the problem of recognizing actions from depth maps and/or 3D skeletons of the body.

A depth map stores the distance of each point in the scene to the camera. This allows reasoning about body surfaces and shapes across time. Li et al. [39] proposed to use an action graph where each node is a bag of 3D points that encodes the body pose. In Wang et al. [19], a 3D action sequence is treated as a 4D shape and a Random Occupancy Patterns (ROP) feature is extracted. Sparse coding is used to encode only the features that contain information useful for classification purposes. In Vieira et al. [40], space and time axes are divided in cells, and space-time occupancy patterns are computed to represent depth sequences. Oreifej et al. [16] describe the depth sequence as histograms of oriented surface normals (HON4D) captured in the 4D volume, based on depth and spatial coordinates.

The main difficulty of working directly with 3D skeleton data arises from inaccuracy or failures of the skeleton estimation method. Moreover, “one of the biggest challenges of using pose-based features is that semantically similar motions may not necessarily be numerically similar” [41]. Most of the research using only 3D skeleton data tries to extract features to represent the correlation among the locations of the joints. In [15], the body pose is represented by concatenating the distances between all the possible pairs of the joints in the current frame, the distances between the joints in the current frame and the ones in the previous frame, and the distances between the joints in the current frame and in a neutral pose (computed by averaging the initial skeletons of all the actions). Principal component analysis (PCA) is applied for dimensionality reduction providing a descriptor called EigenJoints.

In Xia et al. [14], a histogram of the locations of 11 manually selected 3D skeleton joints is computed to get a compact body pose representation that is invariant to the use of left and right limbs (Histogram of 3D Joints (HOJ3D)). Linear Discriminant Analysis (LDA) is used to project the histograms and compute the K discrete states of the HMM classifier. In [42], each action is represented by spatio-temporal motion trajectories of the joints. Trajectories are represented as curves in the Riemannian manifold of open curve shape space, and a dynamic programming-based elastic distance is used to compare them. Classification is performed by KNN on the Riemannian manifold.

Due to the difficulty of achieving high accuracy with just 3D skeleton data [20], other approaches combine skeleton data with other sources of information (depth maps or RGB video). In Wang et al. [20], depth data and the estimated 3D locations of the joints are used to compute the local occupancy pattern (LOP) feature. The Fourier temporal pyramid is used to capture the temporal structure of the action. Data mining techniques are used to discover the most discriminative actionlets and a multiple kernels learning approach is used to weight the actionlets. Sung et al. [43] combine HOG on RGB and depth data, hand positions, body pose and motion features from skeleton data. Then, a two-layer maximum-entropy Markov model is adopted for classification. In [44] the authors fuse skeleton information and STIPS within the random forest framework to perform feature selection and action classification.

In this paper, we only use the 3D locations of the joints in skeleton data. We adopt a Hankelet-based representation [24] to describe body motion in a sliding window, and a set of HMMs to perform action classification.

Our approach is related to both linear parameter varying model identification [45] and switched system identification [46]. In linear parameter varying models, the parameters of each autoregressive model may change over time based on a scheduling variable. Our method may be considered as a discretization of linear parameter varying models; we model the switching of the LTI systems as a Markov process and, instead of estimating the scheduling variable, we infer the atomic LTI system that may have generated the given observations. In this sense, our method is more similar to piecewise models and Markovian jump linear models [46–48] where there is a stochastic process that regulates the switching from one LTI system to another. Unlike previous methods [47,48], our goal is not that of segmenting the sequence as outputs of different LTI systems; instead, we parse the sequence with a sliding window of fixed duration, and model probabilistically the switching among the atomic LTI systems to capture the temporal structure of the whole action.

To associate output measurements with a generating LTI system, we could apply system identification techniques to estimate the parameters of the LTI system, as in [49]. However, trajectories produced by a dynamical system can be also represented through a Hankel matrix. The Hankel matrix embeds the observability matrix of the LTI system, and it is invariant to affine-transformations of the trajectory points [24]. Hankel matrices have been successfully used in previous works on action recognition [24], tracking [50], compressive sensing [51], face emotion recognition [52,53] and dynamic textures [49]. In [24], a bag of dynamical models is used for action recognition in RGB video. The method extracts dense trajectories to represent body motion. The truncated Hankel matrices (Hankelets) associated with the detected trajectories are used to learn a dictionary of Hankelets. A histogram of Hankelets is computed to represent each action instance and train a SVM. In such a method, the temporal structure of the action and the switching among LTI systems are not considered.

In our approach we use the space of Hankel matrices as an intermediate space where it is possible to compare the Hankelet-based representations of both body motion templates and atomic LTI systems. We model the transitions between LTI systems' means of an HMM. In this sense, there is an interesting connection between our work and the work of [54]. In [54], each video sequence is associated with a dynamical model. Then, a metric is learned in order to optimally classify these dynamical models. In contrast, we represent a video as a sequence of dynamical models and learn the parameters of an HMM that may regulate this sequence of atomic models.

Among the various statistical learning approaches, we can distinguish between two categories: generative and discriminative learning methods [55]. The former schema aims at estimating the parameters of the probability distribution of the data based on the maximum likelihood or the maximum a posteriori principles. In contrast, discriminative approaches attempt to optimize a scoring function on the available training samples and the classifier's output. Such a scoring function is defined based on criteria that are directly linked with the classification purpose, such as conditional maximum likelihood or maximum mutual information [56,57], empirical risk minimization [58] and large margin estimation [59].

One of the most used discriminative models for time series is the Conditional Random Fields (CRF) [60] model, which has proven to be successful for labeling tasks but, in general, requires a fully annotated training set. Therefore, Quattoni et al. [61] proposed Hidden Conditional Random Fields (HCRF) where the states of a temporal sequence are hidden but depend on the class. In the HCRF model, all classes share the same state space. The class label is inferred by considering the compatibility of the appearance features with the hidden states, the compatibility of the couple of states in the chain, and the compatibility of the inferred states with the class labels. Learning of the parameters for a chain-based HCRF requires inference of the hidden states, which can be solved via belief propagation.

The main difficulties in discriminative training of dynamical models arise when hidden variables are present. In general, hidden variables make the objective function non-convex and increase the complexity of the optimization problem [61]. By borrowing ideas from Latent variable Support Vector Machine (LSVM) [62] and structural SVM [63], Wang and Mori [64] developed Max-Margin HCRF (MMHCRF) for human action recognition in RGB video sequences. Using this approach, learning of the parameters requires a two step procedure: in the first step, given the currently estimated parameters, inference of the hidden states is performed by linear programming for each sample in the training set; in the second step, given the sequences of hidden states, multi-class SVM is learned in the dual space by solving a quadratic programming problem. The optimization is carried out only on the correct labeling and on the most violated constraints.

HMMs have been widely used for modeling time series. In particular, they have been widely used for action recognition [14,65–69]. The standard generative learning scheme for HMMs is the well-known Baum–Welch algorithm [70], which is based on expectation-maximization to alternate between inference of hidden states and maximum likelihood estimation of the parameters. Due to the success of discriminative learning approaches, several researchers [71–74] have attempted to learn the parameters of Markov networks in a discriminative way, especially in speech recognition.

In this paper, we train a set of HMMs (one for each action class) using a discriminative training approach. We define a loss function that measures the misclassification error as a “distance” between the correct labeling and the most competitive but wrong class label. During optimization, the method tries to enforce a “large margin” among the HMMs' decision boundaries.

We represent an action as a sequence of body motion templates, where a template is defined as a set of feature trajectories in a temporal window of τ frames. In particular, we consider a temporal sequence [y
                     
                        o
                     ,…,
                     y
                     
                        τ
                     ], where y
                     
                        t
                      is a vector of the concatenated 3D locations of body joints in the skeleton at time t. As LTI systems are universal approximators [47], the temporal sequence can be regarded as the output of an LTI system of unknown parameters.

The state and measurement equations of LTI systems are linear, where the matrices A and C are constant over time, and w
                     
                        k
                     
                     ∼
                     N(0,
                     Q) is uncorrelated zero mean Gaussian measurement noise:
                        
                           (1)
                           
                              
                                 
                                    
                                       
                                          x
                                          
                                             k
                                             +
                                             1
                                          
                                       
                                       =
                                       A
                                       ⋅
                                       
                                          x
                                          k
                                       
                                       +
                                       
                                          w
                                          k
                                       
                                       ;
                                    
                                 
                                 
                                    
                                       
                                          y
                                          k
                                       
                                       =
                                       C
                                       ⋅
                                       
                                          x
                                          k
                                       
                                       .
                                    
                                 
                              
                           
                        
                     
                  

In these equations, x
                     
                        k
                     
                     ∈
                     R
                     
                        u
                      is the u-dimensional hidden state of the LTI system, while y
                     
                        k
                     
                     ∈
                     R
                     
                        v
                      is the v-dimensional measurement.

We can describe the trajectory produced by a dynamical system through its Hankel matrix. Given a sequence of output measurements [y
                     
                        o
                     ,…,
                     y
                     
                        τ
                     ] from ((1)), its associated block-Hankel matrix is
                        
                           (2)
                           
                              
                                 H
                                 ˜
                              
                              =
                              
                                 
                                    
                                       
                                          
                                             y
                                             0
                                          
                                          ,
                                       
                                       
                                          
                                             y
                                             1
                                          
                                          ,
                                       
                                       
                                          
                                             y
                                             2
                                          
                                          ,
                                       
                                       
                                          …
                                          ,
                                       
                                       
                                          
                                             y
                                             m
                                          
                                       
                                    
                                    
                                       
                                          
                                             y
                                             1
                                          
                                          ,
                                       
                                       
                                          
                                             y
                                             2
                                          
                                          ,
                                       
                                       
                                          
                                             y
                                             3
                                          
                                          ,
                                       
                                       
                                          …
                                          ,
                                       
                                       
                                          
                                             y
                                             
                                                m
                                                +
                                                1
                                             
                                          
                                       
                                    
                                    
                                       
                                          …
                                       
                                       
                                          …
                                       
                                       
                                          …
                                       
                                       
                                          …
                                       
                                       
                                          …
                                       
                                    
                                    
                                       
                                          
                                             y
                                             n
                                          
                                          ,
                                       
                                       
                                          
                                             y
                                             
                                                n
                                                +
                                                1
                                             
                                          
                                          ,
                                       
                                       
                                          
                                             y
                                             
                                                n
                                                +
                                                2
                                             
                                          
                                          ,
                                       
                                       
                                          …
                                          ,
                                       
                                       
                                          
                                             y
                                             τ
                                          
                                       
                                    
                                 
                              
                              ,
                           
                        
                     where n is the maximal order of the system, τ is the temporal length of the sequence, and it holds that τ
                     =
                     n
                     +
                     m
                     −1.

The Hankel matrix embeds the observability matrix Γ of the system, since 
                        
                           H
                           ˜
                        
                        =
                        Γ
                        ⋅
                        X
                     , where X is the sequence of hidden states of the LTI system, and Γ is defined as follows:
                        
                           
                              Γ
                              =
                              
                                 
                                    
                                       
                                          C
                                       
                                    
                                    
                                       
                                          C
                                          A
                                       
                                    
                                    
                                       
                                          …
                                       
                                    
                                    
                                       
                                          C
                                          
                                             A
                                             n
                                          
                                       
                                    
                                 
                              
                              .
                           
                        
                     
                  

Therefore, 
                        
                           H
                           ˜
                        
                      provides information about the dynamics of the temporal sequence.

To compute the Hankelet, we first center the sequence by subtracting its average, then we build the Hankel matrix and normalize it as in [24]:
                        
                           (3)
                           
                              H
                              =
                              
                                 
                                    H
                                    ˜
                                 
                                 
                                    
                                       
                                          
                                             
                                                H
                                                ˜
                                             
                                             ⋅
                                             
                                                
                                                   H
                                                   ˜
                                                
                                                T
                                             
                                          
                                       
                                       F
                                    
                                 
                              
                              .
                           
                        
                     
                  

Based on the above definition, we represent an action sample as a sequence of Hankelets computed in a sliding window of duration τ.

To compare two Hankelets H
                     
                        p
                      and H
                     
                        q
                     , we adopt the same dissimilarity score introduced in [24] and defined as follows:
                        
                           (4)
                           
                              d
                              
                                 
                                    H
                                    p
                                 
                                 
                                    H
                                    q
                                 
                              
                              =
                              2
                              −
                              
                                 
                                    
                                       
                                          H
                                          p
                                       
                                       ⋅
                                       
                                          H
                                          p
                                          T
                                       
                                       +
                                       
                                          H
                                          q
                                       
                                       ⋅
                                       
                                          H
                                          q
                                          T
                                       
                                    
                                 
                                 F
                              
                              .
                           
                        
                     
                  

We highlight that Eq. (4) is a surrogate used to estimate the subspace angle between the spaces spanned by the columns of the Hankel matrices and it does not define a distance. Nonetheless, this score conveys the degree to which two Hankelets may correspond to the same dynamical system [24], and therefore we assume this score represents the degree to which two trajectories have been produced by the same LTI system.

As shown in Fig. 2(c), our switching dynamical system model reduces to an HMM where the hidden variable LTI represents the atomic linear time invariant system that has generated the current trajectory. Each observed trajectory is represented by the corresponding Hankelet H. For each class, we learn a set of M atomic LTI systems (as will be described in Section 5.1). Each atomic LTI system is modeled by means of the Hankelet S of an exemplar output sequence that the system has produced. Therefore, for each class c, the set of atomic LTI systems is represented by a set of Hankelets {S
                     
                        c,i
                     }
                        i
                        =1
                     
                        M
                     .

The probability that a given sequence of measurements is produced by an LTI system is modeled by the following exponential distribution:
                        
                           (5)
                           
                              p
                              
                                 
                                    
                                       H
                                    
                                    
                                       S
                                       
                                          c
                                          ,
                                          i
                                       
                                    
                                    ,
                                    
                                       λ
                                       
                                          c
                                          ,
                                          i
                                       
                                    
                                 
                              
                              =
                              
                                 λ
                                 
                                    c
                                    ,
                                    i
                                 
                              
                              ⋅
                              
                                 e
                                 
                                    −
                                    
                                       λ
                                       
                                          c
                                          ,
                                          i
                                       
                                    
                                    ⋅
                                    d
                                    
                                       H
                                       
                                          S
                                          
                                             c
                                             ,
                                             i
                                          
                                       
                                    
                                 
                              
                           
                        
                     where H is the Hankelet corresponding to the given sequence of measurements, S
                     
                        c,i
                      is the Hankelet used for representing the i-th atomic LTI system of the c-th class model, d(H, S
                     
                        c,i
                     ) is the dissimilarity score in Eq. (4), λ
                     
                        c,i
                      is the parameter of the exponential distribution that corresponds to the i-th state of the c-th model and has to be learnt from training data.

The switching process that generates an action is assumed to be a Markovian process and is modeled by an HMM. The HMM of the c-th action class is characterized by a stochastic transition matrix T
                     
                        c
                      such that T
                     
                        c
                     (i,
                     j)=
                     P(S
                     
                        t
                     
                     =
                     S
                     
                        c,j
                     |S
                     
                        t
                        −1
                     =
                     S
                     
                        c,i
                     ). We also define a prior probability π
                     
                        c
                      such that π
                     
                        c
                     (i)=
                     P(S
                     0
                     =
                     S
                     
                        c,i
                     ) is the probability that the measurement in the first temporal window (t
                     =0) has been generated by the i-th atomic LTI model.

The joint probability of a sequence of N observed Hankelets H
                     ={H
                     
                        t
                     }
                        t
                        =0
                     
                        t
                        =
                        N
                      and the sequence of generating LTI systems represented by their corresponding 
                        Algorithm 1
                        Inference of action-class


                        
                           
                              
                                 
                              
                           
                        

Hankelets S
                     ={S
                     
                        t
                     }
                        t
                        =0
                     
                        t
                        =
                        N
                      is:
                        
                           (6)
                           
                              
                                 
                                    
                                       p
                                       
                                          
                                             H
                                             ,
                                             
                                                S
                                             
                                             
                                                T
                                                c
                                             
                                             ,
                                             
                                                π
                                                c
                                             
                                             ,
                                             
                                                Λ
                                                c
                                             
                                          
                                       
                                       =
                                       P
                                       
                                          
                                             S
                                             0
                                          
                                       
                                       ⋅
                                       p
                                       
                                          
                                             
                                                
                                                   H
                                                   0
                                                
                                             
                                             
                                                S
                                                0
                                             
                                             ,
                                             
                                                λ
                                                
                                                   c
                                                   ,
                                                   
                                                      S
                                                      0
                                                   
                                                
                                             
                                          
                                       
                                       ⋅
                                       
                                          
                                             ∏
                                             
                                                t
                                                =
                                                1
                                             
                                             
                                                t
                                                =
                                                N
                                             
                                          
                                          
                                             
                                                
                                                   p
                                                   
                                                      
                                                         
                                                            
                                                               H
                                                               t
                                                            
                                                         
                                                         
                                                            S
                                                            t
                                                         
                                                         ,
                                                         
                                                            λ
                                                            
                                                               c
                                                               ,
                                                               
                                                                  S
                                                                  t
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   ⋅
                                                   P
                                                   
                                                      
                                                         
                                                            
                                                               S
                                                               t
                                                            
                                                         
                                                         
                                                            S
                                                            
                                                               t
                                                               −
                                                               1
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                       =
                                       
                                          π
                                          c
                                       
                                       
                                          
                                             S
                                             0
                                          
                                       
                                       ⋅
                                       p
                                       
                                          
                                             
                                                
                                                   H
                                                   0
                                                
                                             
                                             
                                                S
                                                0
                                             
                                             ,
                                             
                                                λ
                                                
                                                   c
                                                   ,
                                                   
                                                      S
                                                      0
                                                   
                                                
                                             
                                          
                                       
                                       ⋅
                                       
                                          
                                             ∏
                                             
                                                t
                                                =
                                                1
                                             
                                             
                                                t
                                                =
                                                N
                                             
                                          
                                          
                                             
                                                
                                                   p
                                                   
                                                      
                                                         
                                                            
                                                               H
                                                               t
                                                            
                                                         
                                                         
                                                            S
                                                            t
                                                         
                                                         ,
                                                         
                                                            λ
                                                            
                                                               c
                                                               ,
                                                               
                                                                  S
                                                                  t
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   ⋅
                                                   
                                                      T
                                                      c
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               S
                                                               
                                                                  t
                                                                  −
                                                                  1
                                                               
                                                            
                                                         
                                                         
                                                            S
                                                            t
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where, with a little abuse of the notation, we are using the states as indices, and Λ
                        c
                     
                     ={λ
                     
                        c,s
                     } is the set of parameters λ
                     
                        c,s
                      associated with each state s of class c.
                        Algorithm 2
                        Apply Viterbi (Decoding of the observed Hankelet sequence)


                        
                           
                              
                                 
                              
                           
                        

Given an action model described by parameters {Λ
                           c
                        ,
                        T
                        
                           c
                        ,
                        π
                        
                           c
                        }, where c is the label of the action to which the model refers, the inference of the sequence of LTI-systems is performed via the Viterbi algorithm [75]. This well-known algorithm is based on Dynamic Programming and attempts to maximize the log-likelihood of the joint probability of the states and the observations sequentially.

The inference of which label should be assigned to a sequence of Hankelets H
                        ={H
                        
                           t
                        } is performed via maximum likelihood. In practice, the predicted label c* is computed by solving:
                           
                              (7)
                              
                                 
                                    c
                                    *
                                 
                                 =
                                 
                                    min
                                    c
                                 
                                 
                                    
                                       −
                                       
                                          max
                                          S
                                       
                                       log
                                       
                                       p
                                       
                                          
                                             H
                                             ,
                                             S
                                             |
                                             
                                                T
                                                c
                                             
                                             ,
                                             
                                                π
                                                c
                                             
                                             ,
                                             
                                                Λ
                                                c
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Then, the label of the model providing the highest likelihood is assigned to the sequence of observations. S
                        ={S
                        
                           t
                        } is the best sequence of hidden states inferred by the Viterbi algorithm.

Algorithm 1 shows how the classification of an input Hankelet sequence is performed. The Viterbi algorithm is applied N times, once for each action class.

For completeness, we report in Algorithm 4 the Viterbi algorithm for inferring the sequence of LTI systems that generated the observed sequence and the negative log-likelihood.

Our discriminative learning procedure learns the parameters of all the HMMs simultaneously by encouraging correct predictions and penalizing the incorrect ones based on the values of the negative log-likelihood provided by the models.

Based on Eq. (6), the negative log-likelihood provided by the HMM of class c for a sequence of Hankelets may be written as:
                        
                           (8)
                           
                              
                                 
                                    
                                       
                                          g
                                          c
                                       
                                       
                                          H
                                       
                                       =
                                       −
                                       log
                                       
                                          
                                             p
                                             
                                                
                                                   H
                                                   ,
                                                   S
                                                   |
                                                   
                                                      T
                                                      c
                                                   
                                                   ,
                                                   
                                                      π
                                                      c
                                                   
                                                   ,
                                                   
                                                      Λ
                                                      c
                                                   
                                                
                                             
                                          
                                       
                                       =
                                    
                                 
                                 
                                    
                                       −
                                       
                                          
                                             ∑
                                             
                                                t
                                                =
                                                0
                                             
                                             
                                                t
                                                =
                                                N
                                             
                                          
                                          
                                             log
                                             
                                                
                                                   p
                                                   
                                                      
                                                         
                                                            H
                                                            t
                                                         
                                                         |
                                                         
                                                            S
                                                            t
                                                         
                                                         ,
                                                         ,
                                                         
                                                            λ
                                                            
                                                               c
                                                               ,
                                                               
                                                                  S
                                                                  t
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       −
                                       
                                          
                                             ∑
                                             
                                                t
                                                =
                                                1
                                             
                                             
                                                t
                                                =
                                                N
                                             
                                          
                                          
                                             log
                                             
                                                
                                                   
                                                      T
                                                      c
                                                   
                                                   
                                                      
                                                         S
                                                         
                                                            t
                                                            −
                                                            1
                                                         
                                                      
                                                      
                                                         S
                                                         t
                                                      
                                                   
                                                
                                             
                                             −
                                             log
                                             
                                                
                                                   
                                                      π
                                                      c
                                                   
                                                   
                                                      
                                                         S
                                                         0
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where S
                     ={S
                     
                        t
                     } is computed by the Viterbi algorithm as
                        
                           (9)
                           
                              S
                              =
                              arg
                              
                                 max
                                 S
                              
                              p
                              
                                 
                                    H
                                    ,
                                    S
                                    |
                                    
                                       T
                                       c
                                    
                                    ,
                                    
                                       π
                                       c
                                    
                                    ,
                                    
                                       Λ
                                       c
                                    
                                 
                              
                              .
                           
                        
                     
                  

The decision about which label to assign to H is taken by maximum likelihood as detailed in Eq. (7). Therefore, if the sample belongs to the k-th class, to obtain a correct prediction we need to have:
                        
                           (10)
                           
                              
                                 g
                                 k
                              
                              
                                 H
                              
                              <
                              
                                 min
                                 
                                    j
                                    ≠
                                    k
                                 
                              
                              
                                 g
                                 j
                              
                              
                                 H
                              
                           
                        
                     where superscripts indicate the class to which the negative log-likelihood refers. The difference 
                        δ
                        g
                        
                           H
                           k
                           j
                        
                        =
                        
                           g
                           k
                        
                        
                           H
                        
                        −
                        
                           min
                           
                              j
                              ≠
                              k
                           
                        
                        
                           g
                           j
                        
                        
                           H
                        
                      represents the distance between the correct model and the most competitive but incorrect one, and can be interpreted as the margin of the classifier. Minimizing δg over the whole training set corresponds to increasing the inter-class distances of the classifier. In case of a correct prediction, then δg(H,
                     k,
                     j)<0. We can enforce a large negative log-likelihood difference by requiring that δg(H,
                     k,
                     j)<−
                     m where m is a positive constant value (we set m to 1).

For this purpose, in our optimization problem, we adopt a hinge loss function defined as:
                        
                           (11)
                           
                              loss
                              
                                 H
                              
                              =
                              max
                              
                                 
                                    0
                                    ,
                                    
                                       g
                                       k
                                    
                                    
                                       H
                                    
                                    −
                                    
                                       min
                                       
                                          j
                                          ≠
                                          k
                                       
                                    
                                    
                                       g
                                       j
                                    
                                    
                                       H
                                    
                                    +
                                    m
                                 
                              
                              .
                           
                        
                     
                  

Whenever this loss is greater than 0, the prediction is incorrect or the achieved margin is not large enough. In both cases, an update of the model parameters is required.

In our formulation, the optimization involves hidden variables, which makes the problem non-convex as in other related frameworks such as [61,64]. In the HCRF [61], classification is based on the conditional likelihood P(y|H) where y is the class-label. This conditional likelihood is computed by summing P(S,
                     y|H) over all the possible state paths. In our discriminative HMMs, the classification is based exclusively on the optimal state path S and on the maximum joint probability P(S,
                     H|y). The main difficulty arises in jointly estimating the optimal state path and learning the parameters. In this sense, we take an approach that is analogous to the approach taken in [62] and [64] and, at each iteration, we adopt a coordinate descent approach. In practice, we optimize our objective function in two steps:
                        
                           1.
                           Holding the parameters of the model fixed, we infer the optimal path S for each sample and for each class, and we store the paths of the correct model and the most competitive but incorrect one;

Holding the sequences of hidden variables (paths), we correct the model parameters by minimizing the loss function.

In our formulation we only select the most competitive but incorrect model by taking the minimum of the negative log-likelihood (NLL) over all the incorrect models. Our optimization is based on the joint probability P(S,
                     H|y). As an alternative, we might have marginalized over all the possible paths S (which means performing inference via the forward algorithm rather than decoding via the Viterbi algorithm) but this choice would have yielded complex expressions for the gradients of the loss function. In contrast, our choices of classifying a sequence based on the NLL of the optimal paths of each HMM, and of considering the joint probability P(S,
                     H|y) result in a straightforward and convenient computation of the gradients of the cumulative loss.

Whilst our model is not a structural SVM, our adopted strategy has an analog in structural learning [63] where the optimization is carried out on the set of most violated constraints. In our method, only the sequences for which the models do not achieve a large margin contribute to the parameter refinement.

The state space, that is the set of atomic LTI systems, is initialized by computing K clusters per class via K-medoids.

In our preliminary work [26], the atomic LTI systems were not updated during the training procedure. When inferring the state-paths given the correct class-label for all the sequences, we obtain a partition of the Hankelets into clusters (state-clusters), each one associated with a state. We empirically found that updating the state space with the medoids of these clusters does not improve the final classification accuracy [26]. Indeed, this strategy does not ensure that the cumulative loss function decreases, and there is no guarantee that the optimization procedure will converge towards a local minima.

In this paper, we retrain the state space and select the atomic LTI systems in such a way to encourage correct predictions of the classifier. We consider the Hankelets of the correctly classified sequences within a state-cluster as candidate representations of the atomic LTI systems. Ideally, we would like to find a joint set of M states for each class that decreases the loss on the training set. However, this problem has an exponential complexity. Therefore, we apply a greedy strategy where we re-estimate one state at a time. For each state S
                        
                           c,i
                         and for each candidate Hankelet C
                        
                           c,j
                        , we compute the cumulative loss on the whole training set on the state space C
                        
                           c,j
                        
                        ∪(S
                        
                           c,k
                        )
                           k
                           ≠
                           i
                        . If this loss is lower than the loss computed on the initial state space, we accept the change; otherwise, we reject the candidate Hankelet. To further reduce the computational complexity of the method, we only consider P random candidates at each step, chosen within the state-cluster.

@&#IMPLEMENTATION DETAILS@&#

In contrast to the HCRF model, in the HMM the parameters of the model are subjected to constraints. In particular, for each class c, the prior parameters π
                        
                           c
                         must be positive and must sum to 1; the transition probability parameters T
                        
                           c
                         must be positive and must form a stochastic matrix, which means that they must sum to 1 per row (each row of T
                        
                           c
                         represents P(S
                        
                           t
                        |S
                        
                           t
                           −1) for a given S
                        
                           t
                           −1). Moreover, considering the emission probability defined by Eq. (5), namely an exponential probability density function, the parameters Λ
                           c
                         must be positive.

We neglect the constraints on Λ
                           c
                         (see [76], page 6), and enforce that these parameters assume a value greater than 0.

As for the priors, we make the following assumption:
                           
                              (12)
                              
                                 
                                    π
                                    c
                                 
                                 
                                    S
                                 
                                 =
                                 
                                    
                                       
                                          
                                             π
                                             ˜
                                          
                                          c
                                       
                                       
                                          S
                                       
                                    
                                    
                                       
                                          ∑
                                          S
                                       
                                       
                                          
                                             
                                                π
                                                ˜
                                             
                                             c
                                          
                                          
                                             S
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

When computing the log-likelihood we consider:
                           
                              (13)
                              
                                 log
                                 
                                    
                                       
                                          π
                                          c
                                       
                                       
                                          S
                                       
                                    
                                 
                                 =
                                 log
                                 
                                    
                                       
                                          
                                             π
                                             ˜
                                          
                                          c
                                       
                                       
                                          S
                                       
                                    
                                 
                                 −
                                 log
                                 
                                    
                                       
                                          ∑
                                          S
                                       
                                       
                                          
                                             
                                                π
                                                ˜
                                             
                                             c
                                          
                                          
                                             S
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

While π
                        
                           c
                        (S) must be positive and must sum to 1, we do not have any constraints on 
                           log
                           
                              
                                 
                                    
                                       π
                                       ˜
                                    
                                    c
                                 
                                 
                                    S
                                 
                              
                           
                        . Therefore, we redefine the variables in our optimization problem as follows:
                           
                              (14)
                              
                                 
                                    β
                                    S
                                    c
                                 
                                 =
                                 log
                                 
                                    
                                       
                                          
                                             π
                                             ˜
                                          
                                          c
                                       
                                       
                                          S
                                       
                                    
                                 
                                 ,
                              
                           
                        so that
                           
                              (15)
                              
                                 log
                                 
                                    
                                       
                                          π
                                          c
                                       
                                       
                                          S
                                       
                                    
                                 
                                 =
                                 
                                    β
                                    S
                                    c
                                 
                                 −
                                 log
                                 
                                    
                                       
                                          ∑
                                          S
                                       
                                       
                                          
                                             e
                                             
                                                β
                                                S
                                                c
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Similar considerations hold also for the parameters T
                        
                           c
                        (S,
                        S
                        '). Therefore, we define the following variable
                           
                              (16)
                              
                                 
                                    α
                                    
                                       S
                                       ,
                                       S
                                       '
                                    
                                    c
                                 
                                 =
                                 log
                                 
                                    
                                       
                                          
                                             T
                                             ˜
                                          
                                          c
                                       
                                       
                                          
                                             S
                                             ,
                                             S
                                             '
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        such that
                           
                              (17)
                              
                                 log
                                 
                                    
                                       
                                          T
                                          c
                                       
                                       
                                          
                                             S
                                             ,
                                             S
                                             '
                                          
                                       
                                    
                                 
                                 =
                                 
                                    α
                                    
                                       S
                                       ,
                                       S
                                       '
                                    
                                    c
                                 
                                 −
                                 log
                                 
                                    
                                       
                                          ∑
                                          S
                                       
                                       
                                          
                                             e
                                             
                                                α
                                                
                                                   S
                                                   ,
                                                   S
                                                   '
                                                
                                                c
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

With these variable re-definitions, the original constrained optimization problem becomes an unconstrained one.

Our cumulative loss over all the training samples is a non-convex function. We use gradient descent to minimize the objective function by adopting a quasi-Newton strategy with limited-memory BFGS updates. We adopt a block-coordinate descent approach such that the optimization is carried out on the parameters {Λ
                           c
                        }
                           c
                        , {T
                        
                           c
                        }
                           c
                         and the prior {π
                        
                           c
                        }
                           c
                         in turn.

Algorithm 3 shows the pseudo-code for our training procedure. After initializing all the models with the same parameters (uniform distributions for T
                        
                           c
                         and π
                        
                           c
                         and 1 for λ
                        
                           c
                        ), the method iteratively estimates the best set of atomic LTI systems by means of the function est_states() (see Section 5.1), and minimizes the objective function f(⋅) on each block of variables. The function check_convergence() checks if some convergence criterion is met (no significant changes in the estimated parameters).
                           Algorithm 3
                           Discriminative learning of the parameters


                           
                              
                                 
                                    
                                 
                              
                           

Algorithm 4 summarizes the main steps to evaluate the cumulative loss function over the training set. For each sample, it computes the negative log-likelihood of the correct model and of the most likely but incorrect model. This is achieved by applying the Viterbi algorithm. If the loss is positive, then the models have produced a wrong prediction or the achieved margin is not large enough; therefore, the gradients are accumulated and returned to the L-BFGS algorithm to update the parameters.
                           Algorithm 4
                           f(): Objective function to minimize


                           
                              
                                 
                                    
                                 
                              
                           

@&#EXPERIMENTAL RESULTS@&#

We evaluated our method on two publicly available datasets: MSRA-3D [20] and UCF [77]. In all our experiments, we initialize the set of states at random and we report both the overall classification accuracy and the average per-class classification accuracy values over 10 runs. The overall accuracy is computed as percentage of correctly classified test samples. The per-class accuracy values are computed as the percentage of correctly classified test samples within each class.

In our experiments we only used the body skeletons without performing any pre-processing of the data; such data are corrupted by various levels of noise/failures of the skeleton estimation method. This affects, in general, the recognition accuracy.

Each dataset has its own recommended evaluation protocol and data splitting, which we briefly describe below.

The MSRA-3D dataset
                           1
                        
                        
                           1
                           
                              http://research.microsoft.com/en-us/um/people/zliu/actionrecorsrc/.
                         provides both skeleton and depth data (taken at about 15fps). The dataset provides the skeleton (20 joints) for 20 actions, performed between 2 to 3 times by each of 10 subjects. During data collection, the subjects were facing the camera. The actions cover various movements of arms, legs, torso and their combinations. If an action is performed by a single arm or leg, the subjects were advised to use their right arm or leg.

We use the 3D coordinates from 557 sequences. The range of the lengths of the sequences is [13,76], with an average duration of 39.6±10. We use the same experimental setting reported on the authors' website; hence, the splitting of the data in training and test set is as follows: subjects 1, 3, 5, 7, and 9 for training, the others for test.

The MSRA-3D dataset includes some sequences where the skeleton data is noisy, corrupted, and/or missing. Fig. 3
                         shows some frames from such sequences, together with the corresponding class labels. In the last image of pick up and throw, no skeleton has been detected likely due to a failure of the skeleton estimation method.

This dataset has been extensively adopted in the literature. We have found two main evaluation protocols used with this dataset: MSRA-3D Protocol 1 and MSRA-3D Protocol 2. We report experimental results for both of these evaluation protocols.
                           
                              •
                              MSRA-3D Protocol 1. The first protocol consists of testing over the whole set of 20 classes: high arm wave (HAW), horizontal arm wave (HoW), hammer (H), hand catch (HCa), forward punch (FP), high throw (HT), draw x (DX), draw tick (DT), draw circle (DC), hand clap (HCl), two hand wave (2HW), side-boxing (SB), bend (B), forward kick (FK), side kick (SK), jogging (J), tennis swing (TSw), tennis serve (TSe), golf swing (GS), pickup and throw (PT).

MSRA-3D Protocol 2. The second evaluation protocol splits the actions into 3 overlapping subsets of 8 classes each. The first action set (AS1) includes the actions horizontal arm wave, hammer, forward punch, high throw, hand clap, bend, tennis serve, pickup and throw. The second action set (AS2) includes high arm wave, hand catch, draw x, draw tick, draw circle, two hand wave, forward kick, side boxing. The third action set (AS3) includes high throw, forward kick, side kick, jogging, tennis swing, tennis serve, golf swing, pickup and throw. The AS1 and AS2 sets group actions that require similar movements, while the AS3 set groups more complex actions.

The UCF dataset
                           2
                        
                        
                           2
                           
                              http://www.cs.ucf.edu/~smasood/research.html.
                         provides only the skeleton data. The framerate for data acquisition is unknown. The dataset provides the skeleton (15 joints) data for 16 actions performed 5 times by 16 individuals. There are 1280 action samples in total with a temporal duration that ranges in [27, 229], with an average length of 66±34 frames. Each action starts and ends from/to a resting pose. We used the 3D coordinates of the locations of the body joints. The actions in this dataset are: balance, climbladder, climbup, duck, hop, kick, leap, punch, run, stepback, stepfront, stepleft, stepright, twistleft, twistright, and vault. We adopted the suggested evaluation protocol, which is a 4-fold cross evaluation [77].

The parameters of our discriminative HMM (DHMM) classifier were initialized as follows: the parameters λ were set to 1, the transition probabilities and the prior probabilities were initialized to uniform distributions. Therefore, all the HMMs are the same at the beginning of the training. In contrast to our earlier work [26], in this paper we train HMMs with a different state space. This largely reduces the number of parameters and speeds-up the training procedure. The atomic LTI systems (state space) were computed by K-medoids, and the initial medoids were selected at random from the training data.

We compare our method with two baselines: a standard HMM classifier (SHMM) where a set of HMMs (one for each action class) has been trained using the Baum–Welch algorithm, and an HCRF. For SHMM and DHMM, the state space was initialized in the same way.

The standard HCRF formulation was modified in such a way that each hidden state refers to an atomic LTI system. In practice, the input for the HCRF is the sequence of dissimilarity scores between the observed Hankelet and the states. The total number of states for HCRF is 8 times the number of classes.

While we can employ the same parameter initialization of our method for the SHMM, we have been forced to initialize the HCRF parameters randomly. Indeed, as the classes share the state parameters, a uniform initialization does not allow the model to differentiate one class from another.

We report accuracy in classification with our Hankelet-based action representation for all the three models. We report the accuracy of our method with (DHMM-SL) and without (DHMM) our atomic LTI systems learning procedure.

Furthermore, to gain more insights about our discriminative learning approach, we present accuracy in classification for SHMM and DHMM when the input of the methods is directly the skeleton (that is, the concatenation of the 3D locations of the body joints on a frame-per-frame basis). In this case, we adopt the Euclidean distance to compare skeletons, and initialization of the states is performed by K-means.

The number of states for each SHMM and DHMM has been set empirically to 8. In Sections 6.2 and 6.4, we performed the experiments with square block-Hankelets, by empirically setting the order n of the Hankel matrix to 4. Hence, the sliding window is of 7 frames. In Section 6.5, we test the SHMM with different values of order and window length.

We conducted a cross-subject evaluation on the MSRA-3D Action dataset, and present the average accuracy values in classification in Table 1
                        .

The first part of the table shows accuracy in classification for methods at the state-of-the-art. The second part of the table shows the accuracy achieved by SHMM and our DHMM using the 3D locations of the joints. The results show that our discriminative learning approach improves the accuracy by approximately 13% with respect to the standard learning approach. The lower part of the table shows the accuracy attained by HCRF, SHMM, DHMM and DHMM-SL using our Hankelet-based action representation. As the table shows, the state learning approach slightly increases the overall accuracy of the DHMM (approximately 0.5%). In comparison to SHMM, the accuracy of our DHMM improves upon it by 3.5% and, when adopting our state learning approach, by 4.25%.

Overall, by employing our Hankelet-based representation, our method can attain state-of-the-art accuracy. In particular, our method attains an accuracy similar to that obtained by [80] and [81] on equal terms of data splitting. Our method does not need any data pre-processing to compute the Hankelet; in contrast, [80] needs to account for biometric differences in the estimated skeletons by performing data normalization and skeleton registration, and applies Dynamic Time Warping (DTW) to a reference sequence to account for varying lengths of the sequences. While we learn a model for each action class, [81] learns one-vs-one SVM (i.e. 190 SVMs on the MSRA-3D dataset) and adopts a voting scheme to classify the actions.

For completeness, we also report in Table 2
                         the results for methods at the state-of-the-art that use hybrid descriptors extracted from skeleton data and RGB video and/or depth maps. As these methods also use RGB/Depth data, they are not directly comparable with the ones in Table 1, which use instead only the 3D joint positions. We may note that still our method achieves competitive accuracy values.


                        Fig. 5
                        
                         shows the confusion matrix for all the classes. This matrix was obtained by averaging the classification results over 10 runs. Therefore, the classes for which we report 100% accuracy were consistently and correctly classified in all the runs.

The figure shows that most of the confusion is between some pairs of action classes. In particular, there is confusion between high arm wave (HAW) and horizontal arm wave (HoW), hand catch (HCa) and high throw (HT), forward punch (FP) and high throw (HT), forward punch (FP) and tennis serve (TSe), high throw (HT) and tennis serve (TSe), side kick (SK) and forward kick (FK). Comparing these pairs of classes, it is possible to note that these actions involve the same joints and may have similar dynamics and, therefore, similar Hankelets. We stress that our Hankelet-based representation can capture the dynamics of the body joints, but not their relative positions, which can instead help in these cases. To solve such ambiguities, it might be possible to combine our Hankelet-based representation with other pose representations. From a modeling point of view, this might be implemented by a Cartesian product of discrete state variables (to jointly represent dynamics and poses) at the cost of a higher computational complexity.

Some confusion is present between hand catch (HCa) and draw x (DX), and between high arm wave (HAW) and draw x (DX). This is probably due to the variability among subjects with which DX has been performed.

As for the pair of actions bend (B) and pickup and throw (PT), we have visually inspected the data and noted that: 1) the action B is a sub-action for PT; 2) due to skeleton tracking failures, in several PT sequences, the skeleton is not reliable or is totally missing. As a result, PT sequences are severely corrupted and several of them reduce to a bend action. This is also shown in Fig. 4: the first and the second lines of skeletons refer to the actions PT and B respectively. As the figure shows, the action B may look like a sub-action of PT since the two action classes are characterized by similar movements of the body. The third and fourth lines show two sequences of the same classes that are strongly corrupted by noise and failures of the skeleton tracker. Out of the 27 samples in the PT class, 6 present missing skeletons, with about 50% of missing on average. Of the 21 samples without missing, about 8 sequences have more than 3 severely corrupted skeletons (see Fig. 3 for examples of corrupted skeletons).

We also conducted a cross-subject analysis on the three subsets AS1, AS2 and AS3 whose results are reported in Table 3
                        .

Looking at the accuracy values of SHMM (Joints) and DHMM (Joints), which are trained directly on the 3D locations of the joints, our discriminative learning approach yields an improvement of the accuracy of about 5.2%, 4.64% and 19.78%, respectively on the sets AS1, AS2 and AS3. On average, the accuracy on the 3 sets increases by 9.89%.

When using the Hankelet-based representation, our discriminative learning approach yields an increase in accuracy of about 2.3%, on average, for the 3 sets. When the state learning approach is also adopted, the average accuracy increases by 3.89%.

Overall, our method attains accuracy similar to that in [42] and [80]. We note that [42] adopts a KNN classifier with a dynamic programming based distance, which means that classification is performed by comparing against all the sequences in the training set. Our classification is based on the Viterbi algorithm, which is applied once for each class. Therefore, our classification procedure has a lower computational complexity with respect to [42].


                        Fig. 6
                         shows the confusion matrices for the three subsets, AS1, AS2, and AS3. These matrices are consistent with the one obtained when testing on all the classes. In AS1, most of the confusion is again between the classes forward punch and tennis serve, forward punch and high throw, high throw and tennis serve, bend and pickup and throw. This time, horizontal arm wave reaches 100% accuracy (on all the 10 runs). Indeed, the AS1 subset does not contain the class high arm wave, which was confusing the classifier in the experiment with all 20 classes.

We performed the experiments on the UCF dataset using 4-fold cross-validation. We split the dataset into 4 sets and used 3/4 of the data to train the models and the remaining 1/4 to test them. The average accuracy of the 4 splits on 10 runs is reported in Table 4
                        .

When comparing SHMM with DHMM on the 3D locations of the body joints, we get an increase of the accuracy of about 5.8%, indicating that on these features our discriminative learning approach outperforms the standard learning approach for HMMs. On the Hankelets, our method attains state-of-the-art performance. On these features, the discriminative learning improves the accuracy of the SHMM by about 0.8% and, jointly with the state learning procedure, the increase of the accuracy is near 1.22%.

In contrast to [18], which performs data normalization to account for cross-subjects biometric differences and noise, along with dimensionality reduction, our method does not require any pre-processing of the data.

Looking at the average confusion matrix in Fig. 7
                        , we observe that most of the confusion is between pairs of classes step back/ step front, twist left/ twist right and step left/ step right. These results are consistent with the ones obtained for the MSRA-3D dataset and highlight once more the limitation of the Hankelets in discriminating between action classes that share similar dynamics but involve movement in different directions.

We have conducted experiments to study the effect of varying the parameters of the Hankelets on the recognition accuracy. Considering the time required for training a model and the fact that we have performed 1100 experiments, we restricted the analysis to the SHMM but we believe similar considerations also hold for our DHMM. All the results are reported in Fig. 8
                        . In the horizontal axes of each plot we report the order and, within brackets, the number of frames used to compute the Hankel matrix. We recall that, as explained in Section 3, the number of frames τ used to compute a Hankel matrix is τ
                        =
                        n
                        +
                        m
                        −1.

The first experiment measures the recognition accuracy while varying the order n of the square block-Hankel matrix in the range [2,10]. As shown in Fig. 8(a), the recognition accuracy on the MSRA-3D dataset and on its subsets AS1 and AS3 increases for n
                        <5 and then starts to decrease. For the subset AS3, the accuracy decreases for n
                        >6. As for the UCF dataset, the accuracy always increases and tends to be consistently higher than 98% for n
                        >6, which means using more than 11 frames. The decrease of performance for higher orders in the MSRA-3D dataset may be a side-effect of the sliding window approach: by using longer sliding window, the switching of LTI systems can affect more windows, and this can make modeling the action more difficult.

Therefore, we have run another experiment where we set the order n to 2 and use longer temporal windows to compute the Hankel matrix. This means that the Hankel matrices are now rectangular, with more columns m than block rows n. The results of this experiment are shown in Fig. 8(b). With this setting, we may note a decrease in the performance for a number of frames higher than 9. On the UCF dataset the average accuracy is higher than 98% when more than 11 frames are used, which is consistent with the former experiment. This experiment suggests that the duration of the sliding window is a crucial parameter for the success of our approach and, considering the different impact of the duration on the subsets AS1, AS2 and AS3, it may be dependent on the action class. Looking at the results, we also note that, when using 19 frames to build a Hankel matrix of order 2, the performance on the subsets AS1 and AS2 decreases more than when using 19 frames to build a Hankel matrix of order 10. Therefore, the order is also an important parameter.

To gain more insights on the role of the order with respect to the duration of the sliding window, we have performed a further experiment, which we show in Fig. 8(c). In this case, we set the duration of the sliding window to 9 and vary the order in the range [2,5]. The figure shows that increasing the order does increase the overall accuracy, even if this increment is of about 1–2% on the MSRA-3D dataset. This might suggest that increased accuracy can be attained with Hankelet of higher order. However, at least 11 frames are required to build a Hankel matrix of order 6 and, as shown in Fig. 8(a) and (b), on the MSRA-3D dataset, using a number of frames higher than 9 results in an overall decrease in the accuracy.

@&#DISCUSSION@&#

Overall, the experiments on the MSRA-3D dataset demonstrate that the discriminative learning and atomic LTI systems learning approaches result in improved classification accuracy for the HMM. It is interesting to note the poor performance of HCRF. In our implementation, we set the number of states of the HCRF equal to #classes N with N
                        =8 in order to have the same number of states used by our classifier. However, since in HCRF all the classes share the same state space, this choice resulted in a higher number of parameters with respect to our model. Therefore this baseline might be slightly unfair. We have made other attempts to use HCRF together with the Hankelets (for example, by decreasing the number of states and training the model on the whole set of dissimilarity scores as input in order to reduce the number of parameters), but in all these experiments HCRF performed poorly. We believe that HCRF is more difficult to train than our model, particularly in the presence of a high number of states and classes.

Moreover, due to the large number of parameters, the time required to train HCRF is much higher than the time needed for training our DHMM. However, training an SHMM is faster than training a DHMM due to the fact that an SHMM is trained on a subset of the training set and requires fewer evaluations of the log-likelihood. Nonetheless, the classification procedure for the SHMM and DHMM is the same. As it is based on the Viterbi algorithm, its computational complexity is polynomial. To be more specific, the classification procedure has computational complexity of O(C
                        ⋅
                        n
                        3) where n
                        =max(#states, #frames) and C is the number of action classes. The Hankel matrix computation requires a re-ordering of the input vector and a normalization by the Frobenius norm, whose computational complexity is of about O(m
                        3) with m
                        =max(#rows of Hankelet, #columns of Hankelet). A similar cost is required for computing the dissimilarity score of two Hankelets.

We believe that in our approach the most relevant parameter is the duration of the temporal window more than the order of the Hankel matrix. The experiments also support the idea of representing an action as a sequence of outputs of atomic LTI systems, where each atomic LTI system represents very simple dynamics (indeed an order equal to 2 is already effective). Our cross-subject experiments on the MSRA-3D dataset suggest that the adopted representation and classification framework are quite robust to different speeds of the actions and, to some extent, temporal warping of the action sequences. This might be ascribable to the adoption of sliding windows of short duration with a HMM. With respect to other techniques for time series, such as dynamic time warping, the adoption of HMMs also helps to account for varying number of repetitions of body movements.

As for the discrepancy between the UCF dataset and the MSRA-3D dataset, we believe that the MSRA-3D dataset is noisier than the UCF dataset and this can have negative effects on the adopted dissimilarity score. Moreover, the UCF dataset has a larger number of training sequences, which allows for better estimates of the learned model parameters.

Finally, we want to highlight some difficulties we have encountered in evaluation using the MSRA-3D dataset. In our experiments, we retain the corrupted sequences in the training and test sets to guarantee a fair comparison with former works and we did not apply any interpolation of the data to reduce the noise. However, we noticed that some authors filter out the corrupted sequences from the dataset or ignore some classes. To make things worse, some works adopt arbitrary data splitting. To have a clear understanding of all these challenges, we refer the reader to [86], which presents an analysis of works that conduct evaluation using the MSRA-3D dataset. We stress that, in our experiments, we have adopted a cross-subjects validation where subjects 1, 3, 5, 7 and 9 are used for training the models while subjects 2, 4, 6, 8 and 10 are used to test the models. Filtering of noisy sequences and other splitting of the data would have probably resulted in higher accuracy.

@&#CONCLUSIONS AND FUTURE WORK@&#

In this paper we have proposed to represent an action in terms of a sequence of outputs of atomic LTI systems. We represent each atomic LTI system by means of a representative Hankel matrix. We have adopted a discriminative HMM to model the transition from one LTI system to the next. We have also presented a novel method for learning the state representations (the atomic LTI systems) where our discriminative learning formulation encourages correct predictions of the models.

In experiments on two challenging action recognition benchmarks, our method achieves state-of-the-art accuracy by considering only the 3D trajectories of body joints. Our experimental results show that our discriminative learning approach seems to be more effective than the standard generative model learning approach for HMMs. However, our experiments have highlighted limitations of our Hankelet-based action representation when dealing with sequences that share the same dynamics but have different semantic labels due to differences in motion directions (such as moving towards left or towards right). Therefore, one possible future extension of our work could consider encoding such information within the action representation. In contrast to other works at the state-of-the-art, our technique does not require any pre-processing of the data to account for cross-subject biometric differences or varying duration of the action sequences.

A deep analysis of the impact on classification accuracy of varying settings for building the Hankelets has shown that the classification accuracy depends on the temporal duration of the sliding window used to compute the Hankelet. On the other hand, the order of the dynamical model had less influence on classification accuracy, and simple dynamical models suffice to get good classification performance for the datasets tested. This work does not deal with segmentation of the trajectory into atomic systems, which remains an interesting topic for future investigation. Under a generative point of view, a semi-Markov model could be employed, which would increase the overall computational complexity of the model.

The proposed framework is general and is not limited to action recognition. In a future work we will study the possibility of applying this framework in other application domains such as event recognition and crowd analysis.

@&#ACKNOWLEDGMENTS@&#

This work was partially supported by Italian MIUR SINTESYS — Security and Intelligence System grant PON0101687, US NSF grant 1029430, US NSF grants IIS-1318145 and ECCS-1404163; AFOSR grant FA9550-12-1-0271, and the Alert DHS Center of Excellence under Award Number 2008-ST-061-ED0001.

@&#REFERENCES@&#

