@&#MAIN-TITLE@&#Face hallucination based on sparse local-pixel structure

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Our framework aims to shape the prior model using sparse representation.


                        
                        
                           
                           Global structure and local-pixel structure are incorporated to produce plausible facial details.


                        
                        
                           
                           A method to learn local-pixel structures based on sparse representation is proposed.


                        
                        
                           
                           The proposed method is competitive with other, state-of-the-art face-hallucination methods.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Face hallucination

Sparse local-pixel structure

Super-resolution

Sparse representation

@&#ABSTRACT@&#


               
               
                  In this paper, we propose a face-hallucination method, namely face hallucination based on sparse local-pixel structure. In our framework, a high resolution (HR) face is estimated from a single frame low resolution (LR) face with the help of the facial dataset. Unlike many existing face-hallucination methods such as the from local-pixel structure to global image super-resolution method (LPS-GIS) and the super-resolution through neighbor embedding, where the prior models are learned by employing the least-square methods, our framework aims to shape the prior model using sparse representation. Then this learned prior model is employed to guide the reconstruction process. Experiments show that our framework is very flexible, and achieves a competitive or even superior performance in terms of both reconstruction error and visual quality. Our method still exhibits an impressive ability to generate plausible HR facial images based on their sparse local structures.
               
            

@&#INTRODUCTION@&#

The idea of super-resolution (SR) was first presented by Tsai and Huang [1], and significant progress has been made with it over the last few decades. Since SR is an ill-posed problem, prior constraints are necessary to attain a good performance. Based on the different approaches to attaining these prior constraints, SR methods can be broadly classified into two categories: one is the conventional approach, which is also widely known as multi-image SR [2–5] or regularization-based SR, and which reconstructs a HR image from a sequence of LR images of the same scene. These algorithms mainly employ regularization models to solve the ill-posed image SR, and use smooth constraints as the prior constraints, which are defined artificially. The other approach is single-frame SR [6–11], which is also called learning-based SR or example-based SR. These methods generate a HR image from a single LR image with the information learned from a set of LR–HR training image pairs. These algorithms attain the prior constraints between the HR images and the corresponding LR images through a learning process. Many example-based or learning-based algorithms [6–16] have been proposed in the field of image processing. Also in SR, Qiu [13] and Baker and Kanade [17] have demonstrated that the smooth prior constraints used in many regularization-based methods will become less effective at solving the SR problem as the zooming factor increases, while example-based approaches have the potential to overcome this problem using advances in machine learning and computer vision. In this paper, we focus on the single-image SR problem.


                     
                     Fig. 1 shows a general framework of example-based SR: the input LR image is first interpolated, using the conventional methods, to the size of the target HR image, and the input interpolated LR image − a blurry image lack of high-frequency information − is then used as the initial estimation of the target HR. The input LR image is also divided into either overlapping or non-overlapping image patch, and the example-based framework will use the image patches to find out the most matched examples by searching a training dataset of LR–HR image pairs. The selected HR examples are then employed to learn the HR information as the prior constraints. Finally, the learned HR information and the input interpolated image are combined to evaluate the target HR image.

The idea of face hallucination was first proposed by Baker and Kanade [18], and it was then used for SR problems in [8,11,16,19]. Example-based face hallucination is a subcategory of the framework shown in Fig. 1, and it is also a specific and important category of image SR. In [20], Liang conducted a good survey of face hallucination, and summarized the existing face-hallucination methods into two approaches: namely global similarity and local similarity. The theoretical backgrounds and practical results of the existing face-hallucination techniques and algorithms are compared. Based on the comparison results, the strengths and weaknesses of each algorithm are summarized, which forms a base for proposing an effective method to hallucinate mis-aligned face images.

In [16], Liu et al. argued that a successful face-hallucination algorithm should meet the following three constraints:
                        
                           1.
                           
                              Sanity constraint: the target HR image should be very close to the input LR image when smoothed and down-sampled.


                              Global constraint: the target HR image should have the common characteristics of human faces, e.g., possessing a mouth and a nose, being symmetrical, etc.


                              Local constraint: the target HR image should have the specific characteristics of the original LR face image, with photorealistic local features.

Furthermore, a two-step approach was developed for face hallucination, in which a Bayesian formulation and a nonparametric Markov network are employed to deal with face hallucination. Of all the various methods for face hallucination, Hu et al. [11] first learned local-pixel structures from the most matched example images explicitly, which are then used directly as reconstruction priors. A three-stage face-hallucination framework was proposed in [11], which is called Local-Pixel Structure to Global Image Super-Resolution (LPS-GIS). In Stage 1, k pairs of example faces which have a similar pixel structure to the input LR face are selected from a training dataset using k-Nearest Neighbors (KNN). They are then subjected to warping using optical flow, so that the corresponding target HR image can be reconstructed more accurately. In Stage 2, the LPS-GIS method learns the face structures, which are represented as coefficients using a standard Gaussian function; the learned coefficients are updated according to the warped errors. In Stage 3, LPS-GIS constrains the revised face structures, namely the revised coefficients to the input LR face, and then reconstructs the target HR image using an iterative method.

Unlike the abovementioned methods, we propose a face-hallucination framework which utilizes the sparse local-pixel structure as the prior model in the reconstruction of HR faces. The use of sparse local-pixel structure allows our method to reconstruct the details in HR faces flexibly. Furthermore, the global structure of faces is also considered, which enables the proposed method to produce plausible facial components.

As for the organization of this paper, Section 2 gives a brief introduction to the theory of sparse representation and its recent applications to super-resolution. Section 3 provides a detailed introduction to a concept called ‘local-pixel structure with sparsity’. The details of our proposed framework are presented in Section 4. Section 5 presents the experiments and an evaluation of the proposed framework. Finally, the concluding remarks are given in Section 6.

Single-image super-resolution produces a reconstructed HR image from an input LR image using the prior knowledge learned from a set of LR–HR training image pairs, and the reconstructed HR image should be consistent with the LR input. An observed model between a HR image and its corresponding LR counterpart is given as follows:
                        
                           (1)
                           
                              
                                 
                                    I
                                 
                                 
                                    l
                                 
                              
                              =
                              
                                 
                                    I
                                 
                                 
                                    h
                                 
                              
                              H
                              S
                              (
                              r
                              )
                              +
                              N
                              ,
                           
                        
                     where I
                     
                        l
                      and I
                     
                        h
                      denote the LR and HR images, respectively; H represents a blurring filter; 
                        S
                        (
                        r
                        )
                      is a down-sampling operator with a scaling factor of r in the horizontal and vertical dimensions; and N is a noise vector, such as the Gaussian white noise. Here, we will focus on the situation whereby the blur kernel is the Dirac delta function as [11,44], i.e. H is the identity matrix. Thus, Eq. (1) can be rewritten as follows:
                        
                           (2)
                           
                              
                                 
                                    I
                                 
                                 
                                    l
                                 
                              
                              =
                              
                                 
                                    I
                                 
                                 
                                    h
                                 
                              
                              H
                              S
                              (
                              r
                              )
                              +
                              N
                              .
                           
                        
                     Therefore, the purpose of SR is to recover as much of the information lost in the down-sampling process as possible. Since the reconstruction process still remains ill-posed, different priors can be used to guide and constrain the reconstruction results. In recent years, the sparse representation model (SRM) has been used as the prior model, and has shown promising results in image super-resolution.

Sparse representation of a signal is based on the assumption that most or all signals can be represented as a linear combination of a small number of elementary signals only, called atoms, from an overcomplete dictionary. Compared with other conventional methods, sparse representation can usually offer a better performance, with its capacity for efficient signal modeling [21]. The sparse representation of signals has already been applied in many fields, such as object recognition [22,23], text categorization [24], signal classification [21], etc.

In the sparse representation, a common formulation of the problem of finding the sparse representation of a signal using an overcomplete dictionary is described as follows:
                        
                           (3)
                           
                              
                                 
                                    
                                       ω
                                       ^
                                    
                                 
                                 
                                    0
                                 
                              
                              =
                              
                              min
                              
                              
                                 
                                    ‖
                                    
                                       ω
                                    
                                    ‖
                                 
                                 
                                    0
                                 
                              
                              ,
                              
                              s
                              .t
                              .
                              
                              ψ
                              =
                              A
                              ω
                              ,
                           
                        
                     where A is an M×N matrix whose columns are the elements of the overcomplete dictionary, with M<N, and 
                        ψ
                        ∈
                        
                           
                              R
                           
                           
                              M
                              ×
                              1
                           
                        
                      is an observational signal. The purpose of sparse representation is to find an N×1 coefficient vector ω, which is considered to be a sparse vector, i.e. most of its entries are zeros, except for those elements in the overcomplete dictionary A which are associated with the observational signal ψ. Solving the sparsest solution for (3) has been found to be NP-hard, and it is even difficult to approximate [25]. However, some recent results [26,27] indicate that if the vector ω in (3) is sparse enough, then the problem can be solved efficiently by minimizing the ℓ
                     1-norm instead, as follows:
                        
                           (4)
                           
                              
                                 
                                    
                                       ω
                                       ^
                                    
                                 
                                 
                                    1
                                 
                              
                              =
                              
                              min
                              
                              
                                 
                                    ‖
                                    
                                       ω
                                    
                                    ‖
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                              s
                              .t
                              .
                              
                              ψ
                              =
                              A
                              ω
                              .
                           
                        
                     
                  

In fact, as long as the number of nonzero components in ω
                     0 is a small fraction of the dimension M, the ℓ
                     1-norm can replace and recover the ℓ
                     0-norm efficiently [22]. In addition, the optimization problem of the ℓ
                     1-norm can be solved in polynomial time [28,29]. However, in real applications, the data in the dictionary A are, in general, noisy. This will lead to the result whereby the sparse representation of an observational signal, in terms of the training data in A, may not be accurate. In order to deal with the problem, (4) can be relaxed to a modified form as follows:
                        
                           (5)
                           
                              
                                 
                                    
                                       ω
                                       ^
                                    
                                 
                                 
                                    1
                                 
                              
                              =
                              
                              min
                              
                              
                                 
                                    ‖
                                    
                                       ω
                                    
                                    ‖
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                              s
                              .t
                              .
                              
                              
                                 
                                    ‖
                                    
                                       ψ
                                       −
                                       A
                                       ω
                                    
                                    ‖
                                 
                                 
                                    2
                                 
                              
                              ≤
                              ε
                              .
                           
                        
                     Lagrange multipliers offer an equivalent formulation, as shown in the following equation:
                        
                           (6)
                           
                              arg
                              
                              min
                              
                              
                                 
                                    1
                                    2
                                 
                              
                              
                                 
                                    ‖
                                    
                                       A
                                       ω
                                       −
                                       ψ
                                    
                                    ‖
                                 
                                 
                                    2
                                 
                                 
                                    2
                                 
                              
                              +
                              λ
                              
                                 
                                    ‖
                                    
                                       ω
                                    
                                    ‖
                                 
                                 
                                    1
                                 
                              
                              ,
                           
                        
                     where 
                        λ
                        ∈
                        
                           
                              R
                           
                           +
                        
                      is a regularization parameter which balances the sparsity of the solution and the fidelity of the approximation to ψ. This is actually a typical convex-optimization problem, and it can be efficiently solved using the method of Large-Scale L
                     1 Regularized Least Squares (L1LS) [30].

In [41–43], Yang et al. used sparse representation for face hallucination. The proposed method, denoted as ScSR, is based on the idea of sparse signal representation whereby the linear relationships among HR training signals can be accurately recovered from their low-dimensional projections. The structures of LR images are used to form a sparse prior model, which is then employed to reconstruct the HR images or HR patches. The differences between ScSR and our method are that ScSR represents image patches as a sparse linear combination of elements from an appropriately chosen overcomplete dictionary, while in our method, a pixel is represented as a sparse linear combination of elements from its neighboring pixels. Our method seeks a sparse representation for each patch of the LR input image from an LR overcomplete dictionary; the coefficients of this representation are then used to generate the HR target image using the HR overcomplete dictionary. One important process in ScSR is the training of two dictionaries for the LR and HR image patches. In our method, central pixels replace patches, and only the HR dictionary is needed; it is constructed directly from the HR example faces.

In [44], Dong et al. also proposed an image interpolation method based sparse representation, which is abbreviated as NARM-SRM-NL. In the method, a nonlocal autoregressive model (NARM) was proposed and taken as the data-fidelity term in the sparse representation model (SRM). The patches in the estimated HR image are reconstructed using the nonlocal neighboring patches. The method assumes that the nonlocal similar patches in an image have similar coding coefficients with the same overcomplete dictionary; the coefficients are then embedded into SRM and NARM to reconstruct the HR images.

Although a lot of the literature, including this paper, has employed sparse representation to deal with the SR problems, differences are still very obvious. For example, sparse models are diverse, which will lead to different ways of constructing the overcomplete dictionary. In ScSR, the method assumes that image patches can be well represented as a sparse linear combination of elements from a specific dictionary, and a pair of HR–LR dictionaries is constructed to force LR–HR patches to have the same sparse coefficients. In NARM-SRM-NL, the sparse model employed assumes that an image patch can have many similar patches among its nonlocal neighboring patches, and the local PCA dictionary is used to span adaptively the sparse domain for signal representation. In our method, the sparse local-pixel structure is proposed and the dictionary is constructed using the neighboring pixels of those missing pixels.

As we know, a HR face and its corresponding LR face have a common global face structure. Therefore, we can assume that they also have similar local-pixel structures, and that the local image information about the input LR alone should be sufficient to predict the missing HR details. In our algorithm, we use the neighboring pixels of a missing pixel to estimate the target HR face. This idea is similar to neighbor embedding in [6,10]. The following formulation describes the model used in our method:
                        
                           (7)
                           
                              I
                              (
                              x
                              ,
                              y
                              )
                              =
                              
                                 
                                    ∑
                                    
                                       μ
                                       ,
                                       ν
                                       ∈
                                       C
                                    
                                 
                                 
                                    
                                       
                                          α
                                       
                                       
                                          μ
                                          ,
                                          ν
                                       
                                    
                                    (
                                    x
                                    ,
                                    y
                                    )
                                 
                              
                              ×
                              I
                              (
                              x
                              +
                              μ
                              ,
                              y
                              +
                              ν
                              )
                              ,
                           
                        
                     where I(x, y) is a pixel at location (x, y), 
                        
                           
                              α
                           
                           
                              μ
                              ,
                              v
                           
                        
                        (
                        x
                        ,
                        y
                        )
                      denotes the weight of the neighboring pixel 
                        I
                        (
                        x
                        +
                        μ
                        ,
                        y
                        +
                        v
                        )
                      contributed to the pixel I(x, y) with a relative displacement of (μ, v), μ and v cannot be zero at the same time, and C denotes a local window centered at (x, y).

Based on the above assumption of similar pixel structures between the HR–LR face pairs, the weights are almost the same at the same position in a HR face and its corresponding LR face image. Our algorithm searches k similar LR example faces to the input LR face from a dataset of LR–HR face pairs. Then, the neighboring weights of the pixel structures in the k-HR example faces are utilized to estimate the information lost in the input LR face. In order to learn the embedded weights for the central pixels or patches, [6,10,11] used the ℓ
                     2-norm methods, such as Gaussian functions and least-square methods. However, a high visual quality image is very sharp because it contains sharp edges, high-frequency information, and discontinuities. A sharp image means that the local-pixel structures have some sparse properties, and this can be interpreted as indicating that the pixel I(x, y) in (7) can be better reconstructed using only a fraction of the neighboring pixels. Thus, our algorithm will use sparse representation as the prior model to learn the embedded weights.

In the previous section, we stated that many face-hallucination methods use the ℓ
                     2-norm model to learn the prior knowledge, and we believe that the local-pixel structures in a high visual quality image exhibit the sparse property. In other words, most of the neighboring pixels of the pixel I(x, y) in (7) can be regarded as outliers. 
                     Fig. 2 shows a central pixel with its neighboring pixels, and it fully represents the sparse property of an image patch. It is obviously observed that only a small number of neighbors are closely related to the central pixel, while others can be regarded as outliers. It has been demonstrated in [22,31–34] that, compared with the ℓ
                     1-norm, the ℓ
                     2-norm is less robust and more sensitive to outliers, which will usually decrease the accuracy of image super-resolution. Furthermore, in [22,34], the use of the ℓ
                     1-norm produces more robust and better results for face image super-resolution and face recognition, respectively, especially when the signal is sparse and discontinuous.

A toy example is provided in 
                     Fig. 3 to illustrate that the ℓ
                     1-norm is more robust to outliers. Suppose 11 points are given, and a line model y = kx+b is used to fit these points. The ℓ
                     1-norm model and ℓ
                     2-norm model are utilized to solve this problem. Fig. 3(a) shows that both the ℓ
                     1-norm and the ℓ
                     2-norm produce a similar estimation when there are no outliers in the input data. However, when there are outliers, the results are quite different. In Fig. 3(b), with the two outliers, the ℓ
                     1-norm still produces similar results to Fig. 3(a), while it can be seen that the ℓ
                     2-norm result is seriously affected by the outliers. This demonstrates that the ℓ
                     1-norm is more robust to outliers, and this toy example makes us believe that, with sparse local-pixel structures, sparse representation can provide a better performance in face hallucination. Furthermore, we believe that using sparse representation to learn and characterize the prior model can improve the performance of our proposed method.

We have one more example to show that sparse representation can better represent sparse local-pixel structures. 
                     Fig. 4(a) shows a randomly selected pixel and its neighbors (p=5) in the ground-truth HR image of a LR face. It is obvious that the 5 pixels at the upper-left corner have the biggest difference to the central pixel. By simply changing the range of the pixel values to [10,70], we can observe that most of the pixels have a similar value, except the five pixels at the top-left corner (Fig. 4(b)). Thus, we can consider these pixels as belonging to another class, and we call them outlier pixels. 
                     Table 1(a) and (b) shows the learned local structures using the Gaussian function method in [11] and sparse representation, respectively (if a value is less than 0.001, it is set at zero). It can be seen that only two pixels in the top-left corner in Table 1(a) have their weights equal to zero, while four pixels in Table 1(b) have their weights equal to zero. In order to compare Table 1(a) and (b) at the same level, we define the following measure to calculate the percentage of outlier pixels’ weights:
                        
                           (8)
                           
                              
                                 
                                    W
                                 
                                 
                                    o
                                    u
                                    t
                                 
                              
                              =
                              ∑
                              
                                 
                                    
                                       α
                                    
                                    
                                       u
                                       ,
                                       v
                                    
                                    
                                       o
                                       u
                                       t
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                              
                              /
                              ∑
                              
                                 
                                    
                                       α
                                    
                                    
                                       u
                                       ,
                                       v
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                              
                              ×
                              100
                              %
                           
                        
                     where 
                        
                           
                              α
                           
                           
                              u
                              ,
                              v
                           
                           
                              o
                              u
                              t
                           
                        
                        (
                        x
                        ,
                        y
                        )
                      is the weight of an outlier pixel. By applying (8) to Table 1, the results are 
                        
                           
                              W
                           
                           
                              o
                              u
                              t
                           
                           
                              a
                           
                        
                        =
                        6.4
                        %
                      and 
                        
                           
                              W
                           
                           
                              o
                              u
                              t
                           
                           
                              b
                           
                        
                        =
                        4.5
                        %
                     . Thus, with the use of sparse representation, the percentage of outlier pixels’ weights decreases. This means that the outlier pixels will contribute less in the iterative reconstruction process, which can help to produce a better SR result.


                     
                     Fig. 5 illustrates the three major steps in our proposed framework. In Step I, the input LR face is used to search a face dataset and identify the k pairs of LR–HR example faces having the most similar local-pixel structures to the input LR face, using Principal Component Analysis (PCA) and KNN. The k pairs of example faces are composed of k LR faces and their corresponding k-HR faces. Then, the k-HR example faces are employed and are warped to the input LR face using optical flow, so as to make the estimation of the target HR face more accurate. This process will produce k-HR warped example faces. Warped errors will be used in the next step. In Step II, the local-pixel structures, which are represented by the weights of the neighboring pixels, are learned from the k-HR warped example faces using sparse representation. Then, the accuracy of the weights is improved by using the warped errors produced in Step I. Finally, in Step III, the weights of the neighbors are employed to estimate the target HR face using an iterative method. Following are the details of these three steps.

In our face-hallucination framework, finding the example faces of the input LR face in a face dataset composed of GT [38] and FERET [39] is the first step. Here, the example faces used for reconstructing the HR face are composed of k LR faces and their corresponding HR counterparts. In the k pairs of example faces, the k LR example faces have the most remarkably similar pixel structures to the input LR face. In our experiments, the dataset contains 1552 LR–HR face pairs; they have all been aligned using the method in [35], and normalized using the illumination–normalization technique employed in [11]. The LR faces are also magnified to the size of the HR faces using bicubic interpolation. Principal Component Analysis (PCA) is used to represent the interpolated LR face images, and the KNN method is adopted to search for the k LR example faces that are the most similar to the LR input face. Then, the selected k corresponding HR faces are used as example faces. A warping operation is employed to make the estimation more accurate. We use optical flow, which has been used in SR in [11,34,36], to warp the example faces. In our paper, firstly, the flow field between the input LR face and each of the k LR example faces is derived, and then the corresponding HR example faces are warped accordingly based on the k flow fields. The purpose of the warping operation is to force the k-HR example faces to have more similar local-pixel structures to the input LR face. By using the warped errors, we can determine the importance of the neighboring pixels in estimating the HR pixels.

In Fig. 2, the sparse local-pixel structure of a face patch has been described. If we can use all the neighboring pixels of a central pixel to reconstruct the central pixel, then it can be represented effectively as a linear combination of its neighboring pixels using sparse representation. 
                        Fig. 6 illustrates the learning of the sparse local-pixel structure of a face patch from the example HR faces in our method. K image patches with the size of p×p from the warped example HR faces provide the data to construct the overcomplete dictionary matrix A and the central pixel vector (marked in red in the dashed box in Fig. 6). Combining the model shown in the dashed box, we propose using the following formulation to model the learned sparse local-pixel structure:
                           
                              (9)
                              
                                 ψ
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       
                                          
                                             p
                                          
                                          2
                                       
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       
                                          ω
                                       
                                       
                                          j
                                       
                                    
                                    (
                                    x
                                    ,
                                    y
                                    )
                                    ×
                                    A
                                    (
                                    :
                                    ,
                                    j
                                    )
                                 
                                 ,
                              
                           
                        where 
                           ψ
                           (
                           x
                           ,
                           y
                           )
                           =
                           
                              
                                 [
                                 
                                    
                                       I
                                    
                                    
                                       e
                                       x
                                    
                                    
                                       1
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 ⋯
                                 
                                    
                                       I
                                    
                                    
                                       e
                                       x
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 ]
                              
                              T
                           
                         and k is the number of HR example faces, and 
                           
                              
                                 ω
                              
                              
                                 j
                              
                           
                           (
                           x
                           ,
                           y
                           )
                         denotes the weight between the pixel I(x, y) and its neighboring pixel 
                           I
                           (
                           x
                           +
                           μ
                           ,
                           y
                           +
                           v
                           )
                         with a relative displacement of 
                           (
                           μ
                           ,
                           v
                           )
                        . The variables μ and v cannot be zero at the same time. The ith row of the matrix A in (6) and (9) are the neighboring pixels of the central pixel ψ(x, y) of the ith example face, and the size of the neighborhood is p×p (p=7 in Fig. 6). The definition and illustration of A are given in (10) and Fig. 6, respectively:
                           
                              (10)
                              
                                 A
                                 =
                                 [
                                 
                                    
                                       I
                                    
                                    
                                       μ
                                       ,
                                       ν
                                    
                                    
                                       1
                                    
                                 
                                 (
                                 x
                                 +
                                 μ
                                 ,
                                 y
                                 +
                                 ν
                                 )
                                 ⋯
                                 
                                    
                                       I
                                    
                                    
                                       μ
                                       ,
                                       ν
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 x
                                 +
                                 μ
                                 ,
                                 y
                                 +
                                 ν
                                 )
                                 ]
                                 ∈
                                 
                                    
                                       R
                                    
                                    
                                       k
                                       ×
                                       (
                                       
                                          
                                             p
                                          
                                          2
                                       
                                       −
                                       1
                                       )
                                    
                                 
                                 .
                              
                           
                        
                     

Here, we assume that ω = 
                           
                              
                                 [
                                 0
                                 ⋯
                                 
                                    
                                       ω
                                    
                                    
                                       μ
                                       ,
                                       v
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 ⋯
                                 0
                                 ]
                              
                              T
                           
                         is the coefficient vector used in (6), whose entries are all zeros, except those associated with the central pixel vector ψ. We use L1LS [30] to solve (6) using the matrix A and the central pixel vector ψ.

After computing the coefficient vector ω using L1LS, a refinement procedure is performed according to the warped errors produced in the first step. ω can be rewritten as 
                           ω
                           ′
                           =
                           
                              
                                 c
                              
                              
                                 x
                                 ,
                                 y
                              
                           
                           ×
                           ω
                        , and c
                        
                           x,y
                         represents the refinement procedure. Thus, (9) can be rewritten as follows:
                           
                              (11)
                              
                                 ψ
                                 ′
                                 =
                                 
                                    
                                       c
                                    
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                                 ×
                                 A
                                 ω
                                 =
                                 
                                    
                                       c
                                    
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                                 ×
                                 ψ
                                 ,
                              
                           
                        where ω has been calculated using L1LS. The definition of c
                        
                           x,y
                         is the same as in [11]:
                           
                              (12)
                              
                                 
                                    
                                       c
                                    
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                                 =
                                 arg
                                 
                                 min
                                 
                                 
                                    
                                       ‖
                                       
                                          
                                             ψ
                                             ′
                                          
                                          −
                                          
                                             
                                                c
                                             
                                             
                                                x
                                                ,
                                                y
                                             
                                          
                                          ψ
                                       
                                       ‖
                                    
                                    
                                       
                                          
                                             B
                                          
                                          
                                             x
                                             ,
                                             y
                                          
                                       
                                    
                                    
                                       2
                                    
                                 
                                 ,
                              
                           
                        where 
                           ∥
                           ⋅
                           
                              
                                 ∥
                              
                              
                                 
                                    
                                       B
                                    
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                              
                              
                                 2
                              
                           
                         is the operator of the weighted ℓ
                        2-norm, and B
                        
                           x,y
                         is a diagonal weight matrix with the form 
                           
                              
                                 B
                              
                              
                                 x
                                 ,
                                 y
                              
                           
                           =
                           d
                           i
                           a
                           g
                           (
                           
                              
                                 b
                              
                              
                                 1
                              
                           
                           (
                           x
                           ,
                           y
                           )
                           ,
                           …
                           ,
                           
                              
                                 b
                              
                              
                                 k
                              
                           
                           (
                           x
                           ,
                           y
                           )
                           )
                        , where b
                        
                           k
                        (x, y) is calculated using the weight assigned to the kth example face at location (x, y). The warping operation from example faces to the target input face may not be sufficiently accurate, so the weights of the warped HR examples should be dependent on the corresponding warping errors. Here the weights are calculated as follows:
                           
                              (13)
                              
                                 
                                    
                                       b
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    
                                       (
                                       
                                          ∑
                                          
                                             E
                                             
                                                
                                                   r
                                                
                                                
                                                   k
                                                
                                             
                                             (
                                             x
                                             +
                                             ρ
                                             ,
                                             y
                                             +
                                             σ
                                             )
                                             +
                                             
                                                
                                                   ε
                                                   )
                                                
                                                
                                                   −
                                                   β
                                                
                                             
                                          
                                       
                                    
                                    /
                                    
                                       
                                          ∑
                                          k
                                       
                                       
                                          (
                                          
                                             ∑
                                             E
                                             
                                                
                                                   r
                                                
                                                
                                                   k
                                                
                                             
                                             (
                                             x
                                             +
                                             ρ
                                             ,
                                             y
                                             +
                                             σ
                                             )
                                             +
                                             
                                                
                                                   ε
                                                   )
                                                
                                                
                                                   −
                                                   β
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where Er
                        
                           k
                         represents the warped errors between the input interpolated LR face and the kth-HR warped face at location (x, y), and 
                           (
                           ρ
                           ,
                           σ
                           )
                           ∈
                           Ω
                        , where Ω is a patch centered at (x, y). In our experiment, the size of Ω is 9×9 , and β is a controlling parameter which can balance the effect of (13). ε is a small positive value used to prevent the denominator from being zero. Thus, the contribution of each example face in computing the weight is dependent on its warping errors. For those examples with large warping errors at location (x, y), the corresponding b
                        
                           k
                        (x, y) will be reduced. Then, c
                        
                           x,y
                         can be calculated as follows:
                           
                              (14)
                              
                                 
                                    
                                       c
                                    
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                                 =
                                 
                                    
                                       ψ
                                       ′
                                    
                                    T
                                 
                                 
                                    
                                       B
                                    
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                                 ψ
                                 /
                                 (
                                 
                                    
                                       ψ
                                    
                                    T
                                 
                                 
                                    
                                       B
                                    
                                    
                                       x
                                       ,
                                       y
                                    
                                 
                                 ψ
                                 +
                                 ε
                                 )
                                 ,
                              
                           
                        where ε has the same effect as it does in (13). Then, the final neighboring weights for the target HR face at location (x, y) can be obtained.

By now, the local structures have been learned. The major task of this step is to apply the sparse local structures from the HR example faces to the input interpolated LR face. An iterative method [37] is employed in this step. The pixels of the input LR image are used as the anchor points in the iterations. The target HR pixel values are confined within the range of [0, 255], i.e. whenever the pixel value is smaller than 0 or higher than 255, it will be set at 0 or 255, respectively. The target HR face pixels are reconstructed as follows:
                           
                              (15)
                              
                                 
                                    
                                       Δ
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    
                                       
                                          I
                                          ^
                                       
                                    
                                    
                                       h
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 −
                                 
                                    
                                       ∑
                                       
                                          μ
                                          ,
                                          ν
                                          ∈
                                          C
                                       
                                    
                                    
                                       
                                          
                                             w
                                          
                                          
                                             μ
                                             ,
                                             ν
                                          
                                       
                                       (
                                       x
                                       ,
                                       y
                                       )
                                       
                                          
                                             
                                                I
                                                ^
                                             
                                          
                                          
                                             h
                                          
                                          
                                             t
                                          
                                       
                                       (
                                       x
                                       +
                                       μ
                                       ,
                                       y
                                       +
                                       ν
                                       )
                                    
                                 
                                 ,
                              
                           
                        
                        
                           
                              (16)
                              
                                 
                                    
                                       
                                          I
                                          ^
                                       
                                    
                                    
                                       h
                                    
                                    
                                       t
                                       +
                                       1
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    
                                       
                                          I
                                          ^
                                       
                                    
                                    
                                       h
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 −
                                 g
                                 
                                    
                                       Δ
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 ,
                              
                           
                        where 
                           
                              
                                 Δ
                              
                              
                                 t
                              
                           
                           (
                           x
                           ,
                           y
                           )
                         is the regularization parameter between the iterations, and g is a scale factor. In each iteration, we have 
                           
                              
                                 I
                              
                              
                                 h
                              
                           
                           (
                           x
                           ,
                           y
                           )
                           =
                           
                              
                                 I
                              
                              
                                 l
                              
                           
                           (
                           x
                           /
                           r
                           ,
                           y
                           /
                           r
                           )
                        , where r is the down-sampling factor in both dimensions in (2). The input interpolated LR face is selected as the initial estimate 
                           
                              
                                 
                                    I
                                    ^
                                 
                              
                              
                                 h
                              
                              
                                 0
                              
                           
                        . Here, we propose a method to observe the relationship between the number of iterations and the variable 
                           
                              
                                 Δ
                              
                              
                                 t
                              
                           
                           (
                           x
                           ,
                           y
                           )
                        . We define 
                           
                              
                                 y
                              
                              
                                 t
                              
                           
                           ′
                           =
                           ∑
                           
                              |
                              
                                 
                                    Δ
                                 
                                 
                                    t
                                 
                              
                              (
                              x
                              ,
                              y
                              )
                              |
                           
                        , where 
                           0
                           ≤
                           x
                           <
                           m
                           
                           and
                           
                           0
                           ≤
                           y
                           <
                           n
                         (i.e. m×n is the size of the target HR face). Then, we set 
                           y
                           =
                           
                           log
                           (
                           
                              
                                 y
                              
                              
                                 t
                              
                           
                           ′
                           /
                           
                              
                                 y
                              
                              
                                 0
                              
                           
                           )
                         to make the curves more intuitive. 
                        Fig. 7 shows the variations of y for 7 randomly selected face samples with the scale factor g=0.05. We can see that, after 150 iterations, the changes become stable. Thus, in the following experiments, we set the number of iterations to be performed at 150.

In our experiments, the experimental subset contains 1552 images which are selected from the GT database [38] and the FERET databases [39]. With a down-sampling factor of 4 for each dimension, the resolutions of the HR faces and the corresponding LR faces are 124×108 and 31×27, respectively. Then, the parameters k in (10), i.e. the number of example faces, and λ in (6) are determined empirically. 
                        Fig. 8 shows the performances of our proposed framework with different values of k and λ. When k=5 and λ=0.04, the average mean squared error (MSE) of 100 samples in our algorithm becomes stable. The average SSIM (Structural Similarity Index) [40] and PSNR with different neighborhood sizes are tabulated in 
                        Table 2. When p=3, both SSIM and PSNR are at their largest. Therefore, in the following experiments, we set k=5, λ=0.04, and p=3.

To measure the performance of our proposed face-hallucination method, we first reconstruct HR face images with a magnification factor (mag) of 4. The resolution of the HR faces and the corresponding LR faces are 124×108 and 31×27, respectively. A testing dataset is formed by choosing 150 HR–LR faces from the 1552 selected images, and the performance is evaluated using the “leave-one-out” method. Our proposed algorithm is compared with Hu’s method [11] and Chang’s method [6], as well as with bicubic interpolation. The reconstructed HR faces of some testing samples with two different magnification factors using the different methods are shown in 
                        Fig. 9. It is obvious that the results using bicubic interpolation are the blurriest, while the other methods can provide results of much better visual quality, especially Hu’s method and our proposed method. To be specific, Hu’s method and our proposed method achieve a better performance on the eyes, mouth and eyebrows. Our method can achieve even better results in edge regions than Chang’s and Hu’s methods can. This is mainly due to the fact that the ℓ
                        1-norm is more robust than the ℓ
                        2-norm. Next, we measure the performances of the different methods in terms of PSNR and SSIM with the testing set, and the average results are tabulated in 
                        Table 3. Our method outperforms all the other methods in terms of PSNR and SSIM. As mentioned above, Hu’s method is superior to Chang’s method, and bicubic interpolation produces the worst results. The results shown in Fig. 9 and Table 3 demonstrate that our method can achieve the best performance in terms of both visual quality and reconstruction error.

We also measure the performance of the different methods when the magnification factor is 6. The sizes of the HR and LR images in the testing set are reshaped to 126×108 and 21×18, respectively. The right-hand side of Fig. 9 shows the reconstructed HR faces using the different methods when the magnification factor is 4. The corresponding average PSNR and SSIM of the testing images are tabulated in 
                        Table 4. We can see that the performance of Chang’s method degrades significantly here, and is even worse than bicubic interpolation. However, our proposed method can still retain a steady performance in terms of both visual quality and reconstruction error. These experiments demonstrate that our proposed framework is robust to larger magnification factors; this is due to the fact that, even with larger magnification factors, the sparse local-pixel structures learned from the k-HR example faces remain unchanged, and can still be captured accurately. The gradual decrease in the performance is mainly due to the reduced amount of initial information on the aligned input LR face when the magnification factor increases.

Because of the use of sparse representation in our method, we also compare our proposed method with the sparse representation-based image-interpolation method, NARM-SRM-NL, proposed in [44]. All the experiments in [44] were conducted with magnification factors of 2 and 3, so we compare NARM-SRM-NL to our method with the magnification factors being 2, 3, and 4. Another testing set was constructed with the same number of images as the previous one. Five of the reconstructed HR faces selected randomly from the testing set and with the different magnification factors are shown in 
                        Fig. 10, and the corresponding average PSNR and SSIM with the different magnification factors are tabulated in 
                        Table 5.

In the provided source code of NARM-SRM-NL, a number of parameters are to be determined; only the parameter values for the magnification factors 2 and 3 are given. In the experiment, we set the parameters for the magnification factor of 4 the same as those for the magnification factor of 3. As shown in Fig. 10 and Table 5, when the magnification factor is 2, NARM-SRM-NR outperforms our method, but when the magnification factor is 3 or 4, our method outperforms it significantly in terms of both image visual quality and reconstruction error. NARM-SRM-NL reconstructs an image patch using nonlocal neighboring patches, and the NARM matrix is used to further improve the incoherence between the sampling matrix and the adaptive local PCA dictionary. The experimental results show that NARM-SRM-NL is more appropriate for reconstructing images which are very smooth or which contain fewer local features. When an input LR image drops plenty of local structures of its HR counterpart, NARM-SRM-NL will produce a weak result. For face hallucination, the facial images are highly structured. When the magnification factor is small, compared with HR faces, the input LR faces still include most of the local structures. Thus, the reconstructed results of NARM-SRM-NL are excellent, as shown in the second row of Fig. 10. However, when the magnification factor increases, more local information will be lost in the LR faces. Therefore, NARM-SRM-NL cannot reconstruct the fine individual facial details or the photorealistic local features, and consequently, the reconstructed faces become much smoother, as shown in the third and the last rows of Fig. 10, and exhibit larger reconstruction errors. Nevertheless, our method still produces plausible faces with smaller reconstruction errors due to the use of local-pixel structures.

The proposed algorithm in this paper was simulated on a computer of 2.7GHz CPU with 8GByte SDRAM, and was implemented using MATLAB. Our code is available online: http://as.nwsuaf.edu.cn/fhsr.html.

@&#CONCLUSIONS@&#

In this paper, we have proposed a method for face hallucination based on learning the sparse local-pixel structures of the target HR facial images. The sparse representation is used to capture the local structures from the HR example faces, and optical flow is applied to make the learning process more accurate. The experimental results have demonstrated that our proposed framework is competitive and can achieve superior performance compared to other state-of-the-art face-hallucination methods. The superior performance of our algorithm is mainly due to the fact that the example faces can provide both the holistic and the pixel-wise information for reconstructing the target HR facial images, and it can estimate the sparse local-pixel structures of the target HR faces more accurately from the example faces using sparse representation. Our proposed method can maintain the impressive capability of inferring fine facial details and generating plausible HR facial images when the input face images are of very low resolution.

None declared.

@&#ACKNOWLEDGMENTS@&#

Project 61202188, 61303125 supported by National Natural Science Foundation of China. Grant EP/J020257/1 supported by the UK EPSRC, research project by the International Doctoral Innovation Centre (IDIC), the University of Nottingham Ningbo China (UNNC), and Project 2012B10055 by Ningbo Science & Technology Bureau Science and Technology.

@&#REFERENCES@&#

