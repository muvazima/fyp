@&#MAIN-TITLE@&#A systematic study of knowledge graph analysis for cross-language plagiarism detection

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Study of the impact of the implicit aspects of knowledge graphs for cross-language plagiarism detection.


                        
                        
                           
                           We present a new weighting scheme for relations between concepts based on distributed representations of concepts.


                        
                        
                           
                           We obtain state-of-the-art performance compared to several state-of-the-art models.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Cross-language

Plagiarism detection

Knowledge graphs

Multilingual semantic network

Distributed representations

Evaluation

@&#ABSTRACT@&#


               
               
                  Cross-language plagiarism detection aims to detect plagiarised fragments of text among documents in different languages. In this paper, we perform a systematic examination of Cross-language Knowledge Graph Analysis; an approach that represents text fragments using knowledge graphs as a language independent content model. We analyse the contributions to cross-language plagiarism detection of the different aspects covered by knowledge graphs: word sense disambiguation, vocabulary expansion, and representation by similarities with a collection of concepts. In addition, we study both the relevance of concepts and their relations when detecting plagiarism. Finally, as a key component of the knowledge graph construction, we present a new weighting scheme of relations between concepts based on distributed representations of concepts. Experimental results in Spanish–English and German–English plagiarism detection show state-of-the-art performance and provide interesting insights on the use of knowledge graphs.
               
            

@&#INTRODUCTION@&#

Given the vastness of the Web, plagiarism, or the deliberate use of someone else’s original material without acknowledging its source, has become a serious problem in areas such as Literature, Education, and Science. The ease of access to copyrighted contents has become matter of concern also for researchers. The problem is exacerbated when the source of plagiarism comes from another language, which is known as cross-language (CL) plagiarism. It is not only the additional difficulty of manually detecting the translation performed, but also the people’s lack of knowledge about the ethical issues derived from plagiarism. A recent survey about scholar practices and attitudes (Barrón-Cedeño, 2012) reveals that only 36.25% of students believe that translating text fragments and including them in their work is plagiarism.

Although the CL plagiarism detection task could be potentially performed manually, the amount of data, languages, and time required make it impossible to perform in practice. Current approaches to CL plagiarism detection exploit syntactic and lexical properties of the writing, statistical dictionaries or similarities with a multilingual collection of documents. However, most of these techniques are designed for verbatim copies and performance is reduced when dealing with light and especially heavy cases of plagiarism (Clough & Stevenson, 2011), which include paraphrasing.

In a previous work, we proposed Cross-Language Knowledge Graph Analysis (CL-KGA) (Franco-Salvador et al., 2013), an approach for CL plagiarism detection aiming at representing context, which employs knowledge graphs both to expand and relate the concepts in a text. Knowledge graphs are generated using BabelNet (Navigli & Ponzetto, 2012a), the most large multilingual semantic network. Thanks to the multilingual representation of concepts available, BabelNet allows for a straightforward comparison of the knowledge graphs obtained in different languages.

In this work, we perform a systematic study of our CL-KGA model. We analyse the impact of the implicit aspects of knowledge graphs on CL plagiarism detection. The research questions we aim to answer are:

                        
                           •
                           
                              What is the contribution of the word sense disambiguation (WSD) performed by the knowledge graphs? These graphs have been explored in the past to perform WSD; our current representation includes disambiguated concepts, which are combined with their intermediate concepts and other disambiguation candidates. We are interested in analysing the performance when the representation is exclusively composed by disambiguated words. This leads us to our next research question.


                              What is the contribution of the vocabulary expansion performed during graph creation? In our previous work we assumed that the new intermediate concepts that relate the original ones could be a key component in order to obtain a common intersection between related texts. In this work we study this aspect in order to determine if the vocabulary expansion is needed as part of the representation or just as a component during the WSD process itself.


                              What is the relationship between CL-KGA and Cross-Language Explicit Semantic Analysis (CL-ESA)? These two models represent text by exploiting a collection of multilingual concepts, for instance employing Wikipedia. We are interested in studying the similarities and the differences between the two models. We aim to clarify the particularities that make the two models perform completely different.

In this paper, we also address key aspects such as the language independence of the knowledge graphs. In addition, we study the relevance of the concepts (nodes) and relations (edges) of the knowledge graphs, and the most suitable threshold to consider that their weighted relations are semantically related. Finally, we compare our model with the state of the art according to different scenarios and criteria: (i) we evaluate CL plagiarism detection using a dataset composed by automatic and manually generated paraphrasing cases of plagiarism; (ii) we study the performance of detection using only paraphrasing cases; and (iii) we compare the computational efficiency of the models and the size of the graphs.

The classical weighting scheme used for the relations between the concepts of the knowledge graphs is based on bag of words generated from short concept definitions as representation of WordNet’s concepts. Because it is exclusively based on the original wording of the definition, this type of representation is very explicit. In addition to the detailed study of our previous model, in this work we follow the recent and popular trend in the use of distributed representations of words (Mikolov et al., 2013a; Pennington et al., 2014), and present a new weighting scheme for relations between concepts which generates distributed representations of concepts. Our distributed concepts are generated using the continuous Skip-gram model to obtain vector representations of definitions of concepts. In contrast to the classical weighting, our proposed representation measures semantic relatedness modelling not only of the original words in a definition, but also their context. This allows our scheme to successfully measure similarity between definitions which do not share the same words but have the same meaning.

Experimental results show that the vocabulary expansion is more useful when it is only employed to perform the WSD, which is the essential component of our model. The differences between CL-KGA and CL-ESA are proved favouring the first model, which offers a higher performance thanks to the high coverage of BabelNet and the concept relatedness. Our new weighting scheme using distributed representations of concepts achieves state-of-the-art performance compared to the classical weighting and several alternative CL plagiarism detectors. The study with CL paraphrasing cases proved also CL-KGA superiority on this type of plagiarism. Finally, a comparison of the computational efficiency of the models demonstrated that our model is more adequate for systems that only require a fast document similarity and perform the indexing in a preprocessing stage.

The rest of the paper is organised as follows. In Section 2 we provide an overview of the state of the art in CL plagiarism detection and distributed representationsof concepts. In Section 3 we describe the knowledge graphs, their weighting schemes, including our new approach, and their main characteristics. In Section 4 we describe the CL-KGA model for CL plagiarism detection. Finally, in Section 5 we evaluate our approach for Spanish–English and German–English plagiarism detection, comparing our results with several state-of-the-art models. We compare also our new weighting scheme based on distributed representations of concepts with the classical weighting. As part of our analysis, we show the results when detecting only paraphrasing cases and evaluate the computational efficiency of the models.

@&#RELATED WORK@&#

In this section we first review the approaches of CL similarity analysis that have been used for CL plagiarism detection. Next, we summarise the last advances in the use of distributed representations for conceptual semantic relatedness.

Similarly to some monolingual models for plagiarism (Clough et al., 2003; Maurer et al., 2006), an effective approach for languages with lexical and syntactic similarities, such as Romance and Germanic languages, is the Cross-Language Character N-Gram (CL-CNG) model (Mcnamee & Mayfield, 2004). This model employs vectors of character n-grams to model texts, and uses a weighting scheme and a measure of similarity between vectors such as the cosine similarity.

Several approaches have been proposed to measure CL similarity between any language pair. Cross-Language Explicit Semantic Analysis (CL-ESA) (Potthast et al., 2008) extends the classical ESA (Gabrilovich & Markovitch, 2007) to work in a cross-language scenario. This model represents each text by its similarities with a document collection D i.e., the topic of a document is qualified using the reference collection D. Despite the fact that the indexing with D is performed at monolingual level, using a multilingual document collection with comparable documents across languages (e.g. Wikipedia), the resulting vectors from different languages can be compared directly. As we discuss in Section 3.4.4, our CL-KGA model is slightly related with CL-ESA, i.e., using Wikipedia and representing text using a collection of multilingual concepts. However, our model exploits also vocabulary expansion and relatedness between concepts, and has a variable concept inventory with regard to the text words.

The use of parallel corpora has been explored too. For example, the Cross-Language Alignment-based Similarity Analysis (CL-ASA) model (Barrón-Cedeño, 2012; Barrón-Cedeño et al., 2008; Pinto et al., 2009) is based on statistical machine translation. This model uses a statistical bilingual dictionary – generated with parallel corpora – to translate words and perform text alignment. The alignment takes into account the translation probabilities and the differences in length of equivalent texts in different languages.

An approach exploiting concepts like this paper is the MLPlag (Ceska et al., 2008) model. It uses the EuroWordNet semantic network
                           1
                        
                        
                           1
                           
                              http://www.illc.uva.nl/EuroWordNet/.
                         (Vossen, 2004) to address synonymy and to obtain language independent identifiers of words which can be directly compared. Similarly, the Cross-Language Conceptual Thesaurus based Similarity (CL-CTS) model (Gupta et al., 2012) aims at measuring the similarity between the texts in terms of shared concepts and named entities, using the Eurovoc conceptual thesaurus.
                           2
                        
                        
                           2
                           
                              http://eurovoc.europa.eu/.
                         It offered an average performance compared to CL-ASA and CL-CNG specially excelling in Spanish–English. In contrast to CL-KGA, these last two models do not employ concept relatedness or vocabulary expansion or WSD, i.e., the assignment of concepts to words is direct and may produce ambiguity. The Cross-Language Knowledge Graph Analysis (CL-KGA) model (Franco-Salvador et al., 2013) uses a multilingual semantic network to create knowledge graphs that model the context of documents. The model achieved interesting results for CL plagiarism detection, also in cases of paraphrasing (Franco-Salvador et al., 2014a). However, it left unanswered questions – relationship with CL-ESA, contributions of WSD, vocabulary expansion, etc. – and room for improvement – weighting scheme and parameter tuning –, that we address in this paper.

Other CL similarity analysis approaches such as the Cross-Language Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) or Similarity Learning via Siamese Neural Network (S2Net) (Yih et al., 2011) linear projection models, could be employed as well for plagiarism detection. In this work we focus on comparing our model with those models that have been evaluated in the past on CL plagiarism detection.

In recent years, plagiarism detection has been actively addressed in the Evaluation lab on uncovering plagiarism, authorship, and social software misuse (PAN)
                           3
                        
                        
                           3
                           
                              http://pan.webis.de/.
                         at the Conference and Labs of the Evaluation Forum (CLEF). The plagiarism detection shared task (Potthast et al., 2014) encourages participants to submit detectors and compete to identify plagiarism cases in the provided corpus. The 2010 and 2011 editions (Potthast et al., 2010a; 2011b) contained also cross-language partitions in German–English and Spanish–English, which we used for our evaluation. In 2015 the task invited for the first time to submit datasets (Franco-Salvador et al., 2015a; Potthast et al., 2015), increasing participation and including new languages such as Urdu, Persian and Chinese. Similarly to Corezola Pereira et al. (2010), the most popular technique to handle CL plagiarism detection at PAN involved machine translation systems, translating all the documents to the language of comparison beforehand. However, this introduces a heavy dependence on the availability of Machine Translation (MT) systems and their quality. In addition, we consider that those methods are not pure CL detectors, but excellent monolingual plagiarism detection systems with a MT preprocessing. Hence, we compare our proposed model to CL plagiarism detection systems that do not depend on fully-fledged MT systems.
                           4
                        
                        
                           4
                           CL-ASA employs a statistical dictionary but includes a complex language alignment model.
                         In Barrón-Cedeño et al. (2013) we can find a comparison of CL-ASA and CL-CNG using the Spanish–English partition of PAN’11 competition, where the models have been also compared with a system (T+MA) employing MT to analyse the similarities at monolingual level. The paper concluded that T+MA is superior in short cases of plagiarism but very close to CL-ASA, which achieved a higher precision in all experiments and better performance for long cases of plagiarism.

A comparison of the CL-CNG, CL-ESA, and CL-ASA models for CL plagiarism detection has been provided in Potthast et al. (2011a). Different performances were observed depending on the task, languages, and dataset employed. For instance, CL-ESA and CL-CNG were more stable across datasets, obtaining a higher performance on the Wikipedia comparable dataset. In contrast, CL-ASA obtained better results on the JRC-Acquis parallel dataset. Finally, CL-CNG achieved lower quality for language pairs without lexical and syntactic similarities. Therefore, in this work we decided to compare CL-KGA with all these models.

We introduce a new weighting scheme, based on the use of distributed representations of concepts, to measure the semantic relatedness between concepts belonging to a knowledge graph. In recent years, the use of log-linear models has been proposed as an efficient way to generate distributed representations of words (Mikolov et al., 2013a), since they reduce the complexity of the neural network hidden layer thereby improving efficiency. These representations have proved to be an excellent alternative for computing semantic relatedness with models such as the continuous Skip-gram model
                           5
                        
                        
                           5
                           The continuous Skip-gram model is available in the word2vec toolkit: https://code.google.com/p/word2vec/.
                         (Mikolov et al., 2013a; 2013b) or GloVe
                           6
                        
                        
                           6
                           
                              http://nlp.stanford.edu/projects/glove/.
                         (Pennington et al., 2014). Recent works have explored also the possibility of modelling words senses (i.e., synsets) for semantic relatedness using distributed representations. Faruqui et al. (2015) refine vector space representations using relational information from semantic resources such as WordNet or FrameNet (Baker et al., 1998). Aletras and Stevenson (2015) provide representations of synonym words derived from WordNet and exploit its hierarchy to generate synset vectors. There has been also interest in representing BabelNet synsets using distributed representations. SensEmbed (Iacobacci et al., 2015) uses Babelfy (Moro et al., 2014) to disambiguate the complete Wikipedia to the BabelNet synset inventory. Then, the continuous Bag of Words model (CBOW) (Mikolov et al., 2013a) is used on top of Wikipedia’s disambiguated text to generate the distributed representation of synsets. Finally, further refinements (including properties of the BabelNet topology) are employed to measure semantic relatedness.

Since we aim at weighting the ∼262 million of relations of BabelNet, we have to employ a fast and efficient model. As disadvantages SensEmbed has the computational complexity required to disambiguate the ∼5 million of pages contained in the English Wikipedia, the possible errors that WSD may introduce (despite the excellent ∼70% of F
                        1 score with Babelfy for English), the unbounded range of weights that SensEmbed provides, and the low performance of CBOW compared to the continuous Skip-gram model when measuring semantic relatedness (Mikolov et al., 2013a). In Section 3.3.2 we opted for an efficient solution which exploits the high-quality definitions provided for the BabelNet’s synsets (i.e., glosses) and the Skip-gram model.

A knowledge graph is a weighted and directed graph that expands and relates the concepts
                        7
                     
                     
                        7
                        Each word has a number of senses. We define “concept” as any of those senses, which may be represented via synsets (see Section 3.1).
                      belonging to a text. We may consider a knowledge graph as a subset of an original knowledge base focused on the concepts pertaining to a text. Knowledge graphs have been used for Natural Language Processing (NLP) tasks such as network text analysis (Popping, 2003), semantic relatedness (Navigli & Ponzetto, 2012b), WSD (Navigli & Ponzetto, 2012a), semantic parsing (Heck et al., 2013), sentiment analysis (Franco-Salvador et al., 2015b) – also from a WSD perspective –, or in cross-language scenarios: CL plagiarism detection (Franco-Salvador et al., 2013), and CL document retrieval and categorisation (Franco-Salvador et al., 2014b). In Fig. 1
                      we show an example of a knowledge graph.

In order to generate knowledge graphs that allow for a direct comparison across languages, we need a knowledge base with a multilingual dimension of the concepts. We could use EuroWordNet or Wikipedia,
                        8
                     
                     
                        8
                        
                           https://en.wikipedia.org/.
                      although in this work we employ the BabelNet multilingual semantic network, since it offers the larger set of concepts and languages to date.

BabelNet
                           9
                        
                        
                           9
                           
                              http://babelnet.org.
                         2.5 (Navigli & Ponzetto, 2012a) is a multilingual semantic network whose concepts and relations are obtained from the automatic mapping onto WordNet of Wikipedia, OmegaWiki,
                           10
                        
                        
                           10
                           
                              http://omegawiki.org.
                         Wiktionary,
                           11
                        
                        
                           11
                           
                              http://wiktionary.org.
                         Wikidata,
                           12
                        
                        
                           12
                           
                              http://wikidata.org.
                        , and Open Multilingual WordNet.
                           13
                        
                        
                           13
                           
                              http://compling.hss.ntu.edu.sg/omw/.
                         Therefore, BabelNet is a multilingual “encyclopedic dictionary” that combines lexicographic information with wide-coverage encyclopedic knowledge. Concepts in BabelNet are represented similarly to WordNet, i.e., by grouping sets of synonyms in the different languages into multilingual synsets. The syntactic categories are exactly the same offered by WordNet: noun, verb, adjective, and adverb. Multilingual synsets contain lexicalizations from WordNet and Open Multilingual WordNet synsets, the corresponding Wikipedia pages, the OmegaWiki, Wiktionary, and Wikidata entries, and additional translations by a statistical machine translation system. The relations between synsets are collected from WordNet, Open Multilingual WordNet, and from Wikipedia’s hyperlinks between pages. The version 2.5 of BabelNet includes 9,348,287 synsets, covers 50 languages,
                           14
                        
                        
                           14
                           Although in this work we employed BabelNet 2.5, the more recent BabelNet 3.0 offers 13,789,332 synsets and 271 languages via a RESTful API. We selected the previous version in order to avoid depending on the API and work offline which allows for a faster creation of knowledge graphs.
                         and has a WordNet-Wikipedia mapping correctness of 91% (Navigli et al., 2013).

Similarly to the aforementioned works, we followed the approach described by Navigli and Lapata (2010) to create our knowledge graphs, which is a four step-approach described as follows:

                           
                              (i)
                              
                                 Part-of-speech tagging and lemmatization. Initially we process a text fragment d with tokenization, multi-word extraction, part-of-speech (POS) tagging, and lemmatization
                                    15
                                 
                                 
                                    15
                                    Due to our multilingual focus we used TreeTagger: http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/. For the multi-word extraction we implemented our own tool based on pattern matching.
                                  to obtain the list of tuples (lemma,tag) T. We discard POS tags not available in BabelNet.


                                 Populating the graph with initial concepts. Next, we create an initially-empty knowledge graph 
                                    
                                       G
                                       =
                                       (
                                       V
                                       ,
                                       E
                                       )
                                       ,
                                    
                                  i.e., such that 
                                    
                                       V
                                       =
                                       E
                                       =
                                       ∅
                                    
                                 . We populate the vertex set V with the set SK
                                  of all the synsets in BabelNet which contain any < lemma,tag > tuple in T in the text fragment language L, that is:

                                    
                                       (1)
                                       
                                          
                                             
                                                
                                                   S
                                                   K
                                                
                                                =
                                                
                                                   ⋃
                                                   
                                                      t
                                                      ∈
                                                      T
                                                   
                                                
                                                
                                                   Synsets
                                                   L
                                                
                                                
                                                   (
                                                   t
                                                   )
                                                
                                                ,
                                             
                                          
                                       
                                    
                                 
                              

where Synsets
                                    L
                                 (t) is the set of synsets which contains a < lemma,tag > tuple t in the language of interest L.


                                 Creating the knowledge graph. We create the knowledge graph by searching in BabelNet the set of paths P connecting pairs of synsets in V. Formally, for each pair {v, v′} ∈ V such that v and v′ do not share any lexicalization
                                    16
                                 
                                 
                                    16
                                    This prevents different senses of the same term from being connected via a path in the resulting knowledge graph.
                                  in T, for each path in BabelNet 
                                    
                                       v
                                       →
                                       
                                          v
                                          1
                                       
                                       →
                                       ⋯
                                       →
                                       
                                          v
                                          n
                                       
                                       →
                                       
                                          v
                                          ′
                                       
                                       ,
                                    
                                  we set: 
                                    
                                       V
                                       :
                                       =
                                       V
                                       ∪
                                       {
                                       
                                          v
                                          1
                                       
                                       ,
                                       ⋯
                                       ,
                                       
                                          v
                                          n
                                       
                                       }
                                    
                                  and 
                                    
                                       E
                                       :
                                       =
                                       E
                                       ∪
                                       {
                                       
                                          (
                                          v
                                          ,
                                          
                                             v
                                             1
                                          
                                          )
                                       
                                       ,
                                       ⋯
                                       ,
                                       
                                          (
                                          
                                             v
                                             n
                                          
                                          ,
                                          
                                             v
                                             ′
                                          
                                          )
                                       
                                       }
                                    
                                 . That is, we add all the path vertices and edges to G. Following the approach of Navigli and Ponzetto (2012a), the path length is limited to maximum length of 3, in order to avoid an excessive semantic drift.
                                    17
                                 
                                 
                                    17
                                    At this point, we removed the edges below a certain threshold that represents a low semantic relationship (see Section 5.3).
                                 
                              

As a result of populating the graph with intermediate edges and vertices, we obtain a knowledge graph which models the semantic context of text fragment d.


                                 Knowledge graph weighting. The next step consists in weighting all the concepts and semantic relations of the knowledge graph G. For weighting concepts, different methods have been tested in the past, including the PageRank (Page et al., 1998) algorithm. In this work, we score each concept using its own outdegree, which has proved to obtain the best results (Navigli & Ponzetto, 2012a). For weighting relations we will describe in detail the two methods that we evaluated in this work. We normalise weights as a function of the total sum of the outgoing relations.

Relations in BabelNet are weighted to quantify the strength of the association between synsets. Knowledge graphs use these weights in order to weight their relations. In this section we describe the original approach which was employed by Navigli and Ponzetto (2012a) in order to measure this degree of association between synsets. Next, in Section 3.3.2 we present our new method based on distributed representations of concepts for weighting their relations.

The weights between relations provided in the original BabelNet 1.0 were computed using methods based on Dice’s coefficient (Jackson et al., 1989). Two different strategies were employed to leverage the high-quality definitions from WordNet, and the large amounts of hyperlinked text from Wikipedia. Similarly to the Extended Gloss Overlap measure (Banerjee & Pedersen, 2003), for computing the semantic relatedness between two WordNet synsets s and s′, they first are independently represented using a bag-of-words (BOW) representation including all the synonyms of the synsets and the lemmatised words of their glosses.
                              18
                           
                           
                              18
                              A gloss is a short definition of the sense represented within that synset.
                            Stopwords are removed. The list of directly linked synsets is also included for s and s′. Next, they employ the Dice’s coefficient over s and s′ to measure the relationship between the two WordNet synsets:

                              
                                 (2)
                                 
                                    
                                       
                                          Semantic Relatedness
                                          
                                             (
                                             s
                                             ,
                                             
                                                s
                                                ′
                                             
                                             )
                                          
                                          =
                                          
                                             
                                                
                                                   2
                                                   |
                                                   s
                                                   ∩
                                                
                                                
                                                   s
                                                   ′
                                                
                                                
                                                   |
                                                
                                             
                                             
                                                
                                                   |
                                                   s
                                                   |
                                                   +
                                                   |
                                                
                                                
                                                   s
                                                   ′
                                                
                                                
                                                   |
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The relationship between two synsets corresponding to Wikipedia pages is computed using a co-occurrence based method (Ito et al., 2008; Ye et al., 2009), which exploits the large amount of hyperlinked text available in Wikipedia. Given two Wikipedia page synsets w and w′, the frequency of occurrence of each individual page (fw
                            and 
                              
                                 f
                                 
                                    w
                                    ′
                                 
                              
                           ) is computed as the number of hyperlinks found in Wikipedia which point to it. The co-occurrence frequency of w and w′ (
                              
                                 f
                                 
                                    w
                                    ,
                                    
                                       w
                                       ′
                                    
                                 
                              
                           ) is computed as the number of times these links occur together within a context.
                              19
                           
                           
                              19
                              
                                 Navigli and Ponzetto (2012a) employed a sliding window of 40 words as context.
                            The relationship between w and w′ applies the Dice’s coefficient to these frequencies:

                              
                                 (3)
                                 
                                    
                                       
                                          Semantic Relatedness
                                          
                                             (
                                             w
                                             ,
                                             
                                                w
                                                ′
                                             
                                             )
                                          
                                          =
                                          
                                             
                                                2
                                                
                                                   f
                                                   
                                                      w
                                                      ,
                                                      
                                                         w
                                                         ′
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   f
                                                   w
                                                
                                                +
                                                
                                                   f
                                                   
                                                      w
                                                      ′
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Using this weighting scheme, we depict in Fig. 2
                            a histogram of the distribution of BabelNet’s relation weights. We observe that only ∼15 million relations are weighted. In our evaluation we refer always to the CL-KGA model with this weighting scheme unless otherwise stated (see Section 3.3.2).

The weighting described in Section 3.3.1 is based on an accurate and explicit representation of concepts, i.e., a concept fingerprint uses the information of its short and clear definition – in the case of WordNet –, or information of samples of text explicitly mentioning that concept – in the case of Wikipedia. However, those definitions and samples of text do not cover all of the possible contexts in which a concept may appear, and the weighting scheme is not able to infer more contexts. In contrast, the use of distributed representations has proved that the context is modelled in a more abstract
                              20
                           
                           
                              20
                              The distributed representations, also known as continuous representations or embeddings, represent information (e.g. words or concepts) using vectors of floating numbers.
                            but precise manner, e.g. citing the words of Mikolov et al. (2013a), “it was shown for example that vector(”King“) - vector(”Man“) + vector(”Woman“) results in a vector that is closest to the vector representation of the word Queen”. This property, allowed their authors to use these representations in scenarios in which the word was never seen before, but its context is the most adequate, e.g. tasks of sentence completion. In this work we aim at measuring the strength of association between concepts modelling their representing context using distributed representations. We introduce a new weighting scheme based on the generation of distributed representations of concepts. In order to generate our distributed representations of concepts, we exploit the high-quality definitions provided by the BabelNet’s synsets (i.e., glosses
                              21
                           
                           
                              21
                              Although the approach described in Section 3.3.1 only uses the glosses provided in BabelNet for WordNet synsets, our weighting scheme is based on the most recent versions of the semantic network, which include also glosses for Wikipedia, OmegaWiki, Wiktionary, and Wikidata-derived synsets.
                           ) and the Skip-gram model.

The continuous Skip-gram model (Mikolov et al., 2013a; 2013b) is an iterative algorithm which attempts to maximise the classification of the context surrounding a word. Formally, given a word wt
                               and its surrounding words 
                                 
                                    
                                       w
                                       
                                          t
                                          −
                                          c
                                       
                                    
                                    ,
                                    
                                       w
                                       
                                          t
                                          −
                                          c
                                          +
                                          1
                                       
                                    
                                    ,
                                    .
                                    .
                                    .
                                    ,
                                    
                                       w
                                       
                                          t
                                          +
                                          c
                                       
                                    
                                 
                               inside a window of size 
                                 
                                    2
                                    c
                                    +
                                    1
                                    ,
                                 
                               the goal is to maximise the average of the log probability:

                                 
                                    (4)
                                    
                                       
                                          
                                             
                                                1
                                                T
                                             
                                             
                                                ∑
                                                
                                                   t
                                                   =
                                                   1
                                                
                                                T
                                             
                                             
                                                ∑
                                                
                                                   −
                                                   c
                                                   ≤
                                                   j
                                                   ≤
                                                   c
                                                   ,
                                                   j
                                                   ≠
                                                   0
                                                
                                             
                                             log
                                             p
                                             
                                                (
                                                
                                                   w
                                                   
                                                      t
                                                      +
                                                      j
                                                   
                                                
                                                |
                                                
                                                   w
                                                   t
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           

Although 
                                 
                                    p
                                    (
                                    
                                       w
                                       
                                          t
                                          +
                                          j
                                       
                                    
                                    |
                                    
                                       w
                                       t
                                    
                                    )
                                 
                               can be estimated using the softmax function (Barto, 1998), its normalisation depends on the vocabulary size W which makes its usage impractical for high values of W. For this reason, more computationally efficient alternatives are used instead. In this work we used the negative sampling (Mikolov et al., 2013b), a simplified version of the Noise Contrastive Estimation (NCE) (Gutmann & Hyvärinen, 2012; Mnih & Teh, 2012), which basically uses logistic regression to distinguish the target word from a noise distribution, having k negative samples for each word. Experimental results in Mikolov et al. (2013b) showed that the negative sampling offers better results at semantic level compared to NCE and Hierarchical softmax (Morin & Bengio, 2005). Sentence vectors (SenVec) (Le & Mikolov, 2014) follow Skip-gram model to train a special vector 
                                 
                                    v
                                    →
                                 
                               representing a complete sentence. Basically, the model uses all words in the sentence as context to train the vector representing its content. In contrast, the original Skip-gram model employs a fixed size window to determine the context (surrounding words) of the iterated words of a sentence. Next we detail the four-step method we used for weighting the BabelNet semantic relations using the continuous Skip-gram and SenVec models:

                                 
                                    (i)
                                    
                                       Getting high-confidence word vectors. The first step consists in obtaining a collection of vectors of words 
                                          
                                             
                                                V
                                                →
                                             
                                             W
                                          
                                        from encyclopedic knowledge using the Skip-gram model.
                                          22
                                       
                                       
                                          22
                                          We used 300-dimensional vectors, context windows of size 8, and 25 negative words for each sample. We preprocessed the text with lowercased word, tokenisation, and removing the words of unit length. We used the same configuration for the SenVec vectors.
                                       
                                       
                                          
                                             
                                                V
                                                →
                                             
                                             W
                                          
                                        will provide a precise and accurate representation of the type of context we are interested in modelling, i.e., sense definitions. For this purpose we used the complete Wikipedia dump
                                          23
                                       
                                       
                                          23
                                          
                                             https://en.wikipedia.org/wiki/Wikipedia:Database_download.
                                        of January 2015 and extracted vectors for ∼15 million of words.


                                       Generating distributed representations of glosses. Next, for all English glosses
                                          24
                                       
                                       
                                          24
                                          The multilingualism of BabelNet synsets allows to obtain multilingual vector representations using only English glosses.
                                        available in BabelNet, we employ SenVec to generate their distributed representations 
                                          
                                             
                                                V
                                                →
                                             
                                             G
                                          
                                       . The 
                                          
                                             
                                                V
                                                →
                                             
                                             W
                                          
                                        collection is used as input word vectors in order to provide the glosses with enough context to generate representative vectors. The 
                                          
                                             
                                                V
                                                →
                                             
                                             G
                                          
                                        collection contains 3,857,795 gloss vectors.


                                       Generating distributed representations of concepts (synsets). BabelNet provides a gloss for each available source (WordNet, Wikipedia, OmegaWiki, etc.) and it is very frequent to have more than one gloss per synset. We take advantage of this observation by generating vectors for all glosses, independently of their source. We get the final representation 
                                          
                                             
                                                v
                                                s
                                             
                                             →
                                          
                                        of a synset s by averaging all its available gloss vectors: 
                                          
                                             
                                                
                                                   v
                                                   →
                                                
                                                s
                                             
                                             =
                                             
                                                n
                                                
                                                   −
                                                   1
                                                
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                n
                                             
                                             
                                                
                                                   v
                                                   →
                                                
                                                g
                                             
                                             
                                                
                                                   (
                                                   s
                                                   )
                                                
                                                i
                                             
                                             ,
                                          
                                        where 
                                          
                                             
                                                (
                                                
                                                   
                                                      v
                                                      →
                                                   
                                                   g
                                                
                                                
                                                   
                                                      (
                                                      s
                                                      )
                                                   
                                                   1
                                                
                                                ,
                                                
                                                   
                                                      v
                                                      →
                                                   
                                                   g
                                                
                                                
                                                   
                                                      (
                                                      s
                                                      )
                                                   
                                                   2
                                                
                                                ,
                                                .
                                                .
                                                .
                                                ,
                                                
                                                   
                                                      v
                                                      →
                                                   
                                                   g
                                                
                                                
                                                   
                                                      (
                                                      s
                                                      )
                                                   
                                                   n
                                                
                                                )
                                             
                                             ∈
                                             
                                                
                                                   V
                                                   →
                                                
                                                G
                                             
                                          
                                        are all gloss vectors available for the synset s. This averaging of distributed vectors has been successfully applied in the past for classification tasks (Franco-Salvador et al., 2015c; 2015d; Le & Mikolov, 2014).


                                       Weighting BabelNet’s semantic relations. Finally, in order to compute the strength of each pair of synsets (s, s′) with a semantic relation in BabelNet, we use the cosine distance between the synset vectors 
                                          
                                             
                                                v
                                                →
                                             
                                             s
                                          
                                        and 
                                          
                                             
                                                v
                                                →
                                             
                                             
                                                s
                                                ′
                                             
                                          
                                       :

                                          
                                             (5)
                                             
                                                
                                                   
                                                      Semantic Relatedness
                                                      
                                                         (
                                                         s
                                                         ,
                                                         
                                                            s
                                                            ′
                                                         
                                                         )
                                                      
                                                      =
                                                      
                                                         
                                                            
                                                               
                                                                  v
                                                                  →
                                                               
                                                               s
                                                            
                                                            ·
                                                            
                                                               
                                                                  v
                                                                  →
                                                               
                                                               
                                                                  s
                                                                  ′
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               ∥
                                                            
                                                            
                                                               
                                                                  v
                                                                  →
                                                               
                                                               s
                                                            
                                                            
                                                               ∥
                                                               ∥
                                                            
                                                            
                                                               
                                                                  v
                                                                  →
                                                               
                                                               
                                                                  s
                                                                  ′
                                                               
                                                            
                                                            
                                                               ∥
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    

In Fig. 3
                               we can see a histogram with the distribution of the weights of the relations of BabelNet using our new weighting scheme. Note that we weighted ∼172 million of semantic relations compared to the ∼15 million of relations originally weighted with the method described in Section 3.3.1. In addition, if we observe Fig. 2, we can appreciate differences in the weight distributions. Ours is more similar to a Gaussian distribution, whereas the former seems to fit a decreasing logarithmic scale. In our evaluation, we refer to the CL-KGA model that employs the proposed weighting scheme using the “Distributed Concept Weighting” (DCW) tag.

Knowledge graphs have several implicit characteristics that make them adequate for NLP tasks related to similarity analysis such as CL plagiarism detection. These characteristics have been used by the CL-KGA model in the past, but they have never been analysed independently for a CL plagiarism detection perspective. In this work we aim at studying the most relevant ones: WSD, vocabulary expansion, language independence, and representation of text using a multilingual collection of concepts.

Knowledge graphs have been successfully used in the past to perform WSD (Navigli & Ponzetto, 2012a). As we stated, the graphs created in Section 3.2 contain a set of SK
                            synsets for each < lemma,tag > tuple extracted from an original text fragment d. However, only one of these synsets corresponds to the disambiguation of the tuple. That means that we are introducing paths between synsets which are not real senses of the meaning of d. The original CL-KGA model kept all candidate synsets of the tuples and the intermediate paths in order to counterbalance possible errors that may be produced if we keep only the disambiguation synsets. We assumed that if there is enough context in d, the knowledge graph G will contain a considerably higher concept mass surrounding the real concepts representing the text d and the error will be reduced. In order to validate our theory, we introduce three additional graph variations:

                              
                                 (i)
                                 
                                    Knowledge graphs restricted to disambiguation source synsets. These graphs use Eq. (6)
to select the disambiguation sWSD
                                     among the SK
                                     synsets of each tuple, where 
                                       
                                          score
                                          (
                                          s
                                          )
                                       
                                     is the outdegree of the synset s in the graph G. Then we filter the path set P which created the graph G, and keep only those paths which contain a disambiguation synset as starting and ending point. As a result we obtain the filtered graph Gf
                                     where we will remove the noise provided for the concepts which are not related to the original text d. We use the “WSD path filter” tag to refer to this model in the evaluation.

                                       
                                          (6)
                                          
                                             
                                                
                                                   
                                                      s
                                                      WSD
                                                   
                                                   =
                                                   arg
                                                   
                                                   
                                                      max
                                                      
                                                         s
                                                         ∈
                                                         
                                                            S
                                                            K
                                                         
                                                      
                                                   
                                                   
                                                   score
                                                   
                                                      (
                                                      s
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 


                                    Knowledge graphs for extracting weighted disambiguations. Using the knowledge graph Gf
                                    , this representation removes the intermediate concepts between source synsets, i.e., we use the knowledge graphs only to disambiguate d and discard the vocabulary expansion. However, we keep the original weights of the concepts of the graph Gf
                                    , which are generated using the vocabulary expansion. We use the “WSD concepts” tag to refer to this model in the evaluation.


                                    Knowledge graphs for extracting bag-of-words of disambiguations. Similarly to the previous model, we extract the disambiguations by keeping only the source synsets of the knowledge graph Gf
                                    . In contrast, in order to analyse if the weighting produced when keeping only disambiguations is noisy, we include these disambiguation concepts in a bag-of-words without weights. We use the “WSD concepts w/o weighting” tag to refer to this model in the evaluation.

The vocabulary expansion of the knowledge graphs is an interesting characteristic to study in CL plagiarism detection. When plagiarising, the text is often obfuscated via paraphrasing. The use of knowledge graphs allows to relate the original concepts of a text, including also intermediate concepts between them. If the text has been modified, it is quite likely having an intersection between the expanded concepts of the original text and the plagiarised one. This vocabulary expansion has proved to be useful in tasks such as sentiment analysis (Franco-Salvador et al., 2015b). In the evaluation we will compare the performance using vocabulary expansion for CL plagiarism detection using the models introduced in Section 3.4.1.

As we mentioned at the beginning of Section 3, using BabelNet to generate knowledge graphs allows to compare them directly despite being generated from texts in different languages. This is possible because the multilingual dimension of the BabelNet’s concepts. To illustrate this, let us describe an example. When we query BabelNet with the English word “plagiarism”, the first two sense ID’s we obtain are plagiarism#n#1 – “A piece of writing that has been copied from someone else and is presented as being your own work” –, and plagiarism#n#2 – “The act of plagiarizing; taking someone’s words or ideas as if they were your own”. If we query now BabelNet with the Spanish word “plagio” (plagiarism), we get exactly the same two sense ID’s on top of the results. If we observe the words contained inside the senses, we can see that BabelNet employed lexicalizations of the senses in different languages to match our query. In Figs. 4
                            and 5
                            we can see the knowledge graphs obtained for the English sentence “text with plagiarism” and its translation into Spanish. As can be seen, both graphs share the same core concepts and can be compared directly with some graph similarity algorithm.

We are interested in analysing the analogies of our knowledge graph-based model with CL-ESA.
                              25
                           
                           
                              25
                              Most of our statements are valid also for ESA.
                            Both represent text using a collection of multilingual concepts. In addition, the concept inventory and the multilingual dimension is extracted (not completely in our case) using Wikipedia.
                              26
                           
                           
                              26
                              We assume a classical CL-ESA model based on Wikipedia.
                            Finally, in the worst case, if our model has not enough context to generate a representative knowledge graph, we will have a non-related (and possibly dense) collection of multilingual concepts. In that case, it is possible that our model would produce a similar “wrong” collection of concepts for both languages and would exploit the similarities between them to counterbalance the conceptual and relational errors, i.e., in a similar way to the nature of CL-ESA. However, the differences do not go beyond. We employ a multilingual semantic network to extract the concepts of a text and, in order to model its context, we use knowledge graphs to expand and relate these concepts. In contrast, CL-ESA employs a collection of Wikipedia pages as concepts, and computes the similarities directly with the original text. This method allows to model the context but it is not computing relatedness between concepts and nor expanding the vocabulary or performing WSD. Finally, the fixed collection of pages that CL-ESA employs (several thousands compared to the millions of BabelNet) is restricting the concept inventory and the possibility of modelling the context exploiting the analogies with concepts. In Section 5 we compare our model with CL-ESA to show the differences in performance at detecting CL plagiarism.

In this section we describe more in detail the CL-KGA model for CL plagiarism detection. We discuss the original description of Franco-Salvador et al. (2013) and the algorithm for the detailed analysis and postprocessing of similarities between text fragments. Given a source document dL
                      in a language L and a suspicious document 
                        
                           d
                           
                              L
                              ′
                           
                           ′
                        
                      in a language L′, we compare documents in a four-step process:

                        
                           (i)
                           
                              Segmentation into text fragments. In order to detect plagiarised sections of text between the documents dL
                               and 
                                 
                                    
                                       d
                                       
                                          L
                                          ′
                                       
                                       ′
                                    
                                    ,
                                 
                               we first segment them to obtain the sets of fragments FL
                               and 
                                 
                                    F
                                    
                                       L
                                       ′
                                    
                                    ′
                                 
                              . We use a 5-sentence sliding window with a 2-sentence step to make the segmentation into fragments.


                              Creation of knowledge graphs. We next use the method described in Section 3.2 to create the graph collections GC and GC′ of the text fragments FL
                               and 
                                 
                                    F
                                    
                                       L
                                       ′
                                    
                                    ′
                                 
                              . At this point the language tag has been removed due to the graph multilingualism.


                              Comparison of knowledge graphs. For each pair of graphs (G, G′), G ∈ GC and G′ ∈ GC′, we adapt the algorithm of Montes y Gómez et al. (2001) to compare their similarity and to obtain the set of similarities SG between graph pairs. We calculate the similarity between the concepts in the two graphs using Dice’s coefficient:

                                 
                                    (7)
                                    
                                       
                                          
                                             S
                                             c
                                          
                                          
                                             (
                                             G
                                             ,
                                             
                                                G
                                                ′
                                             
                                             )
                                          
                                          =
                                          
                                             
                                                2
                                                ·
                                                
                                                   ∑
                                                   
                                                      c
                                                      ∈
                                                      V
                                                      
                                                         (
                                                         G
                                                         )
                                                      
                                                      ∩
                                                      V
                                                      
                                                         (
                                                         
                                                            G
                                                            ′
                                                         
                                                         )
                                                      
                                                   
                                                
                                                w
                                                
                                                   (
                                                   c
                                                   )
                                                
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      c
                                                      ∈
                                                      V
                                                      (
                                                      G
                                                      )
                                                   
                                                
                                                w
                                                
                                                   (
                                                   c
                                                   )
                                                
                                                +
                                                
                                                   ∑
                                                   
                                                      c
                                                      ∈
                                                      V
                                                      (
                                                      
                                                         G
                                                         ′
                                                      
                                                      )
                                                   
                                                
                                                w
                                                
                                                   (
                                                   c
                                                   )
                                                
                                             
                                          
                                          ,
                                       
                                    
                                 
                              
                           

where w(c) is the weight of a concept c (see Section 3.2). Likewise, we calculate the similarity between the relations as:

                                 
                                    (8)
                                    
                                       
                                          
                                             S
                                             r
                                          
                                          
                                             (
                                             G
                                             ,
                                             
                                                G
                                                ′
                                             
                                             )
                                          
                                          =
                                          
                                             
                                                2
                                                ·
                                                
                                                   ∑
                                                   
                                                      r
                                                      ∈
                                                      E
                                                      
                                                         (
                                                         G
                                                         )
                                                      
                                                      ∩
                                                      E
                                                      
                                                         (
                                                         
                                                            G
                                                            ′
                                                         
                                                         )
                                                      
                                                   
                                                
                                                w
                                                
                                                   (
                                                   r
                                                   )
                                                
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      r
                                                      ∈
                                                      E
                                                      (
                                                      G
                                                      )
                                                   
                                                
                                                w
                                                
                                                   (
                                                   r
                                                   )
                                                
                                                +
                                                
                                                   ∑
                                                   
                                                      r
                                                      ∈
                                                      E
                                                      (
                                                      
                                                         G
                                                         ′
                                                      
                                                      )
                                                   
                                                
                                                w
                                                
                                                   (
                                                   r
                                                   )
                                                
                                             
                                          
                                          ,
                                       
                                    
                                 
                              
                           

where w(r) is the weight of a semantic relation r (see Section 3.3). We interpolate
                                 27
                              
                              
                                 27
                                 The original CL-KGA combined Sc
                                     and Sr
                                     with 
                                       
                                          
                                             S
                                             g
                                          
                                          
                                             (
                                             G
                                             ,
                                             
                                                G
                                                ′
                                             
                                             )
                                          
                                          =
                                          
                                             S
                                             c
                                          
                                          
                                             (
                                             G
                                             ,
                                             
                                                G
                                                ′
                                             
                                             )
                                          
                                          
                                             (
                                             a
                                             +
                                             b
                                             ·
                                          
                                          
                                             S
                                             r
                                          
                                          
                                             (
                                             G
                                             ,
                                             
                                                G
                                                ′
                                             
                                             )
                                          
                                       
                                    . However, we observed that the current equation allows to ease the tuning of relevance of concepts and relations without affecting the performance.
                               the two above measures of conceptual (S
                                 c
                              ) and relational (S
                                 r
                              ) similarity to obtain an integrated measure S
                                 g
                              (G, G′) between knowledge graphs:

                                 
                                    (9)
                                    
                                       
                                          
                                             S
                                             g
                                          
                                          
                                             (
                                             G
                                             ,
                                             
                                                G
                                                ′
                                             
                                             )
                                          
                                          =
                                          a
                                          ·
                                          
                                             S
                                             c
                                          
                                          
                                             (
                                             G
                                             ,
                                             
                                                G
                                                ′
                                             
                                             )
                                          
                                          +
                                          b
                                          ·
                                          
                                             S
                                             r
                                          
                                          
                                             (
                                             G
                                             ,
                                             
                                                G
                                                ′
                                             
                                             )
                                          
                                          ,
                                       
                                    
                                 
                              
                           

where a and b, 
                                 
                                    a
                                    +
                                    b
                                    =
                                    1
                                    ,
                                 
                               are the parameters of the relevance of concepts and relations respectively. In Fig. 6
                               we can see the differences among CL-KGA, CL-C3G, and CL-ASA when detecting CL plagiarism. Thanks to the aforementioned characteristics (see Section 3.4), the use of knowledge graphs allows to detect similarity even when the paraphrasing is employed and the languages are not syntactically and semantically related. Note that the procedure described so far is the basic model of the candidate retrieval task (Barrón-Cedeño et al., 2013; Potthast et al., 2011a), which needs a detailed analysis component to detect plagiarism cases.


                              Detailed analysis and postprocessing of similarities. Once we obtain the set SG with the similarities between the text fragments of the documents dL
                               and 
                                 
                                    
                                       d
                                       
                                          L
                                          ′
                                       
                                       ′
                                    
                                    ,
                                 
                               we employ the method introduced in Barrón-Cedeño (2012) and Barrón-Cedeño et al. (2013) to analyse the values and determine which fragments of text are cases of plagiarism. This method was originally designed to process the similarity scores of CL-ASA and CL-CNG and it is described in Algorithm 1
                              . Basically, for each text fragment of dL
                               we obtain PG
                              , i.e., the top 5 most similar fragments of document 
                                 
                                    d
                                    
                                       L
                                       ′
                                    
                                    ′
                                 
                               (line 3). Then, we start an iterative process until convergence that merges the fragments of PG
                               with a distance δ lower than a threshold thres1 (lines 6–7). Finally, we select as plagiarism the cases which combine more than thres2
                              
                                 28
                              
                              
                                 28
                                 In this work we used the original thresholds employed in Barrón-Cedeño (2012) and Barrón-Cedeño et al. (2013): 
                                       
                                          
                                             thres
                                             1
                                          
                                          =
                                          1500
                                       
                                     and 
                                       
                                          
                                             thres
                                             2
                                          
                                          =
                                          2
                                       
                                    .
                               text fragments (line 9). The function offsets( · ) provides with the beginning and end offsets of the plagiarism case. This algorithm has been used for evaluating all the models compared in the evaluation section.

@&#EVALUATION@&#

In this section we compare the different variants of our CL-KGA model with several state-of-the-art approaches in the task of CL plagiarism detection. Given a suspicious document dL
                      in a language L and a collection of source documents 
                        
                           D
                           
                              L
                              ′
                           
                           ′
                        
                      in a language L′, the task is to identity all the plagiarised fragments of dL
                      from the document collection 
                        
                           D
                           
                              L
                              ′
                           
                           ′
                        
                     .

To evaluate our model we selected the datasets employed for the CL plagiarism detection competition of PAN at CLEF.
                           29
                        
                        
                           29
                           
                              http://www.clef-initiative.eu/.
                         The two available datasets, PAN-PC-10
                           30
                        
                        
                           30
                           
                              http://www.uni-weimar.de/en/media/chairs/webis/corpora/corpus-pan-pc-10/.
                         and PAN-PC-11,
                           31
                        
                        
                           31
                           
                              http://www.uni-weimar.de/en/media/chairs/webis/corpora/corpus-pan-pc-11/.
                         contain the used Spanish–English (ES–EN) and German–English (DE–EN) partitions. Both datasets contain plagiarism cases generated using machine translation with Google translate.
                           32
                        
                        
                           32
                           
                              https://translate.google.com/.
                         In addition, PAN-PC-11 contains also cases of plagiarism with manual correction after automatic translation. These cases are CL paraphrasing cases of plagiarism. We selected the complete PAN-PC-10 dataset to perform the comparison of the CL-KGA weighting schemes and the tuning of our parameters. Then, we used the PAN-PC-11 dataset to perform the evaluation of the CL-KGA model and the comparison with the state-of-the-art. In Table 1
                         we can see the statistics of the datasets.

@&#METHODOLOGY@&#

As evaluation metric we selected the measures employed at the PAN shared task: precision, recall, granularity, and plagdet (Potthast et al., 2010b). Let S denote the set of plagiarism cases in the suspicious documents, and let R denote the set of plagiarism detections the detector reports for these documents. A plagiarism case s ∈ S represents a reference to the characters that form that case. Likewise, a plagiarism detection r ∈ R is represented as r. Based on these representations, the precision and the recall at character level of R under S are measured as follows:

                           
                              (10)
                              
                                 
                                    
                                       precision
                                       
                                          (
                                          S
                                          ,
                                          R
                                          )
                                       
                                       =
                                       
                                          1
                                          
                                             |
                                             R
                                             |
                                          
                                       
                                       
                                          ∑
                                          
                                             r
                                             ∈
                                             R
                                          
                                       
                                       
                                          
                                             
                                                |
                                             
                                             
                                                ⋃
                                                
                                                   s
                                                   ∈
                                                   S
                                                
                                             
                                             
                                                
                                                   (
                                                   s
                                                   ⊓
                                                   r
                                                   )
                                                
                                                |
                                             
                                          
                                          
                                             |
                                             r
                                             |
                                          
                                       
                                       ;
                                    
                                 
                              
                           
                        
                        
                           
                              (11)
                              
                                 
                                    
                                       recall
                                       
                                          (
                                          S
                                          ,
                                          R
                                          )
                                       
                                       =
                                       
                                          1
                                          
                                             |
                                             S
                                             |
                                          
                                       
                                       
                                          ∑
                                          
                                             s
                                             ∈
                                             S
                                          
                                       
                                       
                                          
                                             
                                                |
                                             
                                             
                                                ⋃
                                                
                                                   r
                                                   ∈
                                                   R
                                                
                                             
                                             
                                                
                                                   (
                                                   s
                                                   ⊓
                                                   r
                                                   )
                                                
                                                |
                                             
                                          
                                          
                                             |
                                             s
                                             |
                                          
                                       
                                       ,
                                    
                                 
                              
                           
                        where 
                           
                              s
                              ⊓
                              r
                              =
                              s
                              ∩
                              r
                           
                         if r detects s and ∅ otherwise. Note that precision and recall do not account for the fact that plagiarism detectors sometimes report overlapping or multiple detections for a single plagiarism case. To address this issue, we also measured the detector’s granularity:

                           
                              (12)
                              
                                 
                                    
                                       granularity
                                       
                                          (
                                          S
                                          ,
                                          R
                                          )
                                       
                                       =
                                       
                                          1
                                          
                                             
                                                |
                                             
                                             
                                                S
                                                R
                                             
                                             
                                                |
                                             
                                          
                                       
                                       
                                          ∑
                                          
                                             s
                                             ∈
                                             
                                                S
                                                R
                                             
                                          
                                       
                                       
                                          |
                                          
                                             R
                                             s
                                          
                                          |
                                       
                                       ,
                                    
                                 
                              
                           
                        where SR
                        ⊆S are cases detected by detectors in R, and Rs
                        ⊆R are detections of S, i.e., 
                           
                              
                                 S
                                 R
                              
                              =
                              
                                 {
                                 s
                                 |
                              
                              s
                              ∈
                              S
                              ∧
                              ∃
                              r
                              ∈
                              R
                              :
                              r
                           
                         detects s} and 
                           
                              
                                 R
                                 s
                              
                              =
                              
                                 {
                                 r
                                 |
                              
                              r
                              ∈
                              R
                              ∧
                              r
                           
                         detects s}. The three previous measures were integrated together in order to obtain an overall score for plagiarism detection (plagdet):

                           
                              (13)
                              
                                 
                                    plagdet
                                    
                                       (
                                       S
                                       ,
                                       R
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             F
                                             1
                                          
                                          
                                             (
                                             S
                                             ,
                                             R
                                             )
                                          
                                       
                                       
                                          
                                             log
                                             2
                                          
                                          
                                             (
                                             1
                                             +
                                             granularity
                                             
                                                (
                                                S
                                                ,
                                                R
                                                )
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                     

We compared our CL-KGA model with the state-of-the-art CL-ESA,
                           33
                        
                        
                           33
                           We used 10,000 Spanish–German–English comparable Wikipedia pages as document collection. All pages contain more than 10,000 characters and were represented using the term frequency-inverse document frequency (TF-IDF) weighting. The similarities are computed using the cosine similarity and the IDF of the words of the documents to index is calculated from Wikipedia.
                         CL-ASA
                           34
                        
                        
                           34
                           We used a statistical dictionary trained using the word-alignment model IBM M1 (Och & Ney, 2003) on the JRC-Acquis (Steinberger et al., 2006) corpus. Similar performance for Spanish–English is obtained using BabelNet as statistical dictionary (Franco-Salvador et al., 2012), but not for German–English.
                         and CL-C3G models.
                           35
                        
                        
                           35
                           CL-C3G is CL-CNG using character 3-grams, as recommended in Potthast et al. (2011a).
                         We included also the results obtained previously by the original CL-KGA (Franco-Salvador et al., 2013) – CL-KGA (BabelNet 1.0) from here –, and those obtained by the CL-KGA variations introduced in Section 3.4.1: CL-KGA (WSD path filter), CL-KGA (WSD concepts), and CL-KGA (WSD concepts w/o weighting). We showed the results of our model using the distributed concept weighting for the CL-KGA model and also for its better performing variant when employing the classic weighting. We introduced also three baselines: (i) statDict, which used a statistical dictionary – the same used by CL-ASA – to obtain all possible translations of each word. A BOW representation was obtained for each text fragment.
                           36
                        
                        
                           36
                           By generating a BOW with all possible translations, we attempted to counterbalance possible errors introduced when using a statistical dictionary for translating.
                         Text fragments were compared using the Dice’s coefficient; (ii) POS + statDict, same as statDict but using TreeTagger to POS tag and lemmatize words before translation; and (iii) POS + statDict + MFS, which additionally used the Most Frequent Sense (MFS) baseline
                           37
                        
                        
                           37
                           Basically, for each word it provides the first sense suggested by WordNet, which represents the most frequent use of that word.
                         to disambiguate the words before generating the BOW. In Table 2
                         we can find a summary of all the models included in the evaluation.

The experiments were divided into three subsections: (i) in Section 5.3 we used the PAN-PC-10 dataset to perform the comparison and tuning of the CL-KGA weighting schemes of semantic relations; (ii) in Section 5.4 we compared the different variants of CL-KGA and studied the characteristics of the model using the PAN-PC-11 dataset; and (iii) in Section 5.5 we compared our model with the state of the art, evaluating the performance when detecting the CL plagiarism cases of the PAN-PC-11 dataset. In this last section we also studied the performance on exclusively the CL cases with paraphrasing, and compared the computational efficiency of the models.

In this section we compared the classical graph weighting for semantic relations based on Dice’s coefficient (cf. Section 3.3.1) and the new method using distributed representations of concepts (cf. Section 3.3.2). We used these experiments to optimize also the parameters of the CL-KGA model.
                           38
                        
                        
                           38
                           Since all the CL-KGA variants share the same basic structure and graphs, we used the same parameters for all of them.
                         For these experiments we used the Spanish–English and German–English partitions of PAN-PC-10 and measured the overall score of plagiarism detection, i.e., plagdet.

First, for each weighting scheme, we determined the threshold to consider that the concepts of the knowledge graphs are semantically related (cf. Section 3.2). Next, we selected the values of relevance for concepts and relations used with CL-KGA (cf. Section 4) for both weightings.

To determine the threshold of the semantic relations, we tested values between 0.001 and 1.
                           39
                        
                        
                           39
                           We start at 0.001 because a value of zero would suppose using all the relations of BabelNet and would generate too much dense and noisy graphs. For the DCW weighting we started using 0.3 as threshold because lower values were computationally very expensive. In this experiment, we set the values of relevance for concepts and relations to 50–50%.
                         In Fig. 7
                         we can see the results of the experiments. For the model using the classical weighting, we obtained the best results with the minimum threshold: 0.001. Similar results were obtained using 0.005 as in previous works. In this case, because of the low number of weighted edges, augmenting the threshold considerably reduced the connectivity of the graphs and, consequently, the plagdet. In contrast, the CL-KGA model using DCW had 0.5 as optimal value in both language pairs, with close results using values between 0.3 and 0.7. The DCW scheme was less sensitive to the threshold value, probably because the higher number of relations contained in the graphs, and remained stable with a strong decreasing for high thresholds. We assume that the key concepts of the graphs were present and connected until those values were higher than 0.8. In contrast to the results shown in the next section using PAN-PC-11, the PAN-PC-10 dataset provided better results on the German–English partition.

To select the values of relevance of concepts and relations, we modified the percentage of relevance between 0% and 100% for both parameters. Fig. 8
                         shows the results of these experiments. We observed a similar trend using both weighting schemes. The best values were obtained for equal relevance for concepts and relations, with similar values for the close percentages, excluding German–English with the classical weighting, which obtained the best values using a 60–40% distribution. These results show that CL-KGA benefits both from the weight of the concepts and the relations to detect CL plagiarism. Note that our DCW scheme obtained better performance on each language pair in all the tested configurations. The use of distributed representations to model concepts benefited our model with a more accurate and human interpretable
                           40
                        
                        
                           40
                           By “human interpretable” we refer to the values of the weights, that have in 50% the optimal value to consider that a relation is semantically related.
                         semantic relation weights.

Finally, we highlight also the difference in size (number of concepts) of the knowledge graphs using the classical or the DCW schemes. Using the optimal parameters determined in this section, a graph using the first weighting had on average 1384 concepts. In contrast, using DCW graphs were much dense, containing on average 17,495 concepts. This was produced for the high number of weighted edges available when using DCW and may be reduced using a higher relation threshold if the computational speed is a priority.

We used also PAN-PC-10 to tune the threshold employed by CL-ESA to make zero the low similarity scores of a text with a Wikipedia page. The best results were obtained with 0.01. In the next section, we used the best values obtained here for each language pair and model.

In this section we used the Spanish–English and German–English partitions of the PAN-PC-11 to compare the proposed variants (cf. Sections 3.3.1, 3.3.2 and 3.4.1) of the CL-KGA model and study the characteristics of our approach.

In Table 3
                         we show the results for Spanish–English. The new experiments with the CL-KGA variants achieved interesting results. Despite using the same weighting, CL-KGA improved the results obtained using BabelNet 1.0. This difference is due to the new relations between concepts, and the new lexicalizations for WordNet verbs, adjectives, and adverbs in Spanish inside BabelNet 2.5, which were only in English in the previous experiments (Franco-Salvador et al., 2013). Similarly to the results with PAN-PC-10 of Section 5.3, CL-KGA with the new weighting scheme based on distributed representations of concepts, CL-KGA (DCW), obtained higher results with a significant difference,
                           41
                        
                        
                           41
                           In this work, statistically significant results of plagdet according to a χ
                              2 test (p < 0.05) were highlighted in bold.
                         and highlights the quality of the new relation weights for computing semantic relatedness. Despite theoretically providing with cleaner graphs, the version with WSD path filter was not able to improve the results of CL-KGA although its results were close. This difference may be due to the wrong disambiguations and intermediate concepts between them that we are keeping. Note that the use of knowledge graphs to perform WSD offers an accuracy close to 70% (Navigli & Ponzetto, 2012a). The CL-KGA (WSD concepts), which keeps the WSD concepts and removes the vocabulary expansion, reduced considerably the performance. We observed that the problem was due to the weighting of the concepts, which was estimated as a function of the outdegree of the complete graph. The current variant, exclusively weighting the WSD concepts, offered too sparse and unbounded values, which made it more difficult to be successfully compared using Dice’s coefficient (cf. Sections 3.2 and 4). We repeated the experiments without weights for the conceptual similarity measure. That model, CL-KGA (WSD concepts w/o weighting), obtained the best results with the two weighting schemes for knowledge graphs. It seems that the use of knowledge graphs to perform a multilingual WSD produced a especially precise representation of the text fragments. If we analyse the need of vocabulary expansion in knowledge graphs (cf. Section 3.4.2), we note that this WSD exploits the expanded concepts to determine the disambiguations. Therefore, although not using expanded concepts directly in the representation as CL-KGA, the vocabulary expansion is crucial for our model.

The results for German–English were a similar. In Table 4
                         we can observe the overall performance. Note that the best weighting scheme was the DCW, and the best results were again with the WSD concepts w/o weighting variant, which highlights the relevance of WSD in our model.

In this section we compare CL-KGA and its variants with several state-of-the-art approaches and baselines (see Table 2) using the PAN-PC-11 dataset for CL plagiarism detection.

In Table 5
                         we show the results obtained for Spanish–English. The lowest results were obtained by CL-C3G. This is unsurprising if we consider that Spanish and English do not share many lexical and syntactic similarities – indispensable requirement for a high character n-gram overlap. The second worst results were obtained by CL-ESA. The CL-ASA model obtained a similar recall but with higher precision, resulting in a superior plagdet. It seems that CL-ESA, based on similarities with a document collection, gave a higher number of false positives. In fact, ESA was originally meant for tasks of relatedness rather than plagiarism. The CL-KGA results obtained previously using Babelnet 1.0 were the next in the ranking. Because of the knowledge graphs, CL-KGA was able to model the text in a more precise manner and provided better results in all measures. Note that the best possible value of granularity is 1.0. However, the proposed baselines offered higher performance. Despite the simplicity of statDict, even the basic variant – with higher results if we POS tag and lemmatize –, obtained a very competitive performance. The disambiguation step using MFS improved the results although without significant differences. The use of a statistical dictionary to generate a BOW containing all the translations with equal relevance, provided a simple but solid model against wrong translations. The results with the CL-KGA variants provided significant differences and superior performance for the standard version with the proposed DCW scheme, and even higher results for the CL-KGA (WSD concepts w/o weighting) variant. We can observe notable differences – especially with German–English – compared to the other approach using WSD: POS + statDict + MFS. This highlights the quality of the disambiguations using knowledge graphs. Note also the differences in performance between the two models using a multilingual collection of concepts: CL-ESA and CL-KGA. These differences were due to the characteristics of the models, which were studied in Section 3.4.4: aimed at adjusting to the text words, our model has a variable concept inventory. In addition, CL-KGA uses relatedness between concepts and vocabulary expansion.

The differences between the models for German–English were similar but with an overall and small performance reduction. In Table 6
                         we can see the results. There are some interesting aspects to highlight. CL-C3G obtained even lower results than for Spanish–English. Although having the same linguistic roots, these two Germanic languages do not share enough lexical and syntactic similarities to model the content properly using character n-grams. On the other hand, the variants of statDict using POS tagging and lemmatization did not excelled as in Spanish–English. The use of the TreeTagger tool introduced errors, which reduced the quality of the representations. Note that the best results were with CL-KGA using our DCW scheme and the WSD concepts w/o weighting variant. This proves that CL-KGA is a competitive model for Spanish–English and German–English CL plagiarism detection.

As we mentioned in Section 5.1, the PAN-PC-11 dataset contains cases of CL paraphrasing. This type of plagiarism is more difficult to detect because its text has been modified in order to hide the plagiarism action. We were interested in observing the differences of the models when trying to detect only those paraphrasing cases. We performed an additional experiment to consider only paraphrasing cases as instances of plagiarism in the corpus. In Tables 7
                            and 8
                            we can see the results. The differences in the performance of all the models compared to the results obtained previously using the complete dataset were substantial. We observed that most of these paraphrasing cases were very short in length, and probably the use of Algorithm 1, designed for longer cases, was the reason of this global quality reduction. However, we can still appreciate that the differences among the results of the models were similar at a smaller scale. CL-KGA obtained the higher performance using DCW for the relations of the knowledge graphs. In this experiments we did not observe such substantial differences between CL-KGA (DCW) and CL-KGA (DCW) (WSD concepts w/o weighting), although may be still appreciated for German–English.

In order to select a model for CL plagiarism detection, its computational efficiency is a key aspect. The purpose and requirements of the system may require a fast or an accurate model. In Table 9
                            we measured the number of text fragments indexed and compared per second for each evaluated model using the complete Spanish–English partition. These experiments were performed using a Intel-i5@2.8 GHz with 16 GB of RAM. As we can see, CL-KGA required considerably more time to index (or generate the graphs of) text. This is due to the use of the BabelNet multilingual semantic network. The 9,348,287 synsets and the ∼262 relations among them made the graph generation a computationally expensive task. In addition, the use of DCW made the graphs more dense and, consequently, they required more time to be compared in the similarity step. Text indexing is usually part of the preprocessing step, being the indexing of the new documents needed only once. The text similarity step is the most important, and the two weighting schemes using WSD concepts w/o weighting may be a solution. These were the fastest models in calculating similarity because they only contain a BOW of disambiguated words. In contrast, if the speed of indexing is crucial, statDict offered a balance between performance and efficiency. Note that in order to speed up graph indexing, parallel computing can be used, as we did for our experiments.

@&#CONCLUSIONS@&#

In this paper we performed a systematic study of Cross-Language Knowledge Graph Analysis, an approach that represents fragments of text using knowledge graphs as a language independent model of its content. We studied the impact of relevant aspects of the model for the task of cross-language plagiarism detection: word sense disambiguation, vocabulary expansion, language independence and representation by similarities with a collection of concepts. Experimental results showed that WSD is the essential component of the model, being only necessary the use of vocabulary expansion during the WSD processing. The differences between CL-ESA and CL-KGA – the two models that exploit Wikipedia as multilingual collection of concepts – favour the latter model, which thanks to the high coverage of BabelNet, the vocabulary expansion and the concept relatedness employed, offered a higher performance. In addition, we proposed a new weighting scheme of relations between concepts based on the use of distributed representations of concepts. The use of this weighting provided our model with state-of-the-art performance on the Spanish–English and German–English partitions of the PAN-PC-11 dataset. The study of the model with cross-language paraphrasing cases proved also its superiority. However, a comparison of the computational efficiency of the models showed that our model is more adequate when a fast document similarity is required and the indexing is performed in a preprocessing step. In other situations, statDict – also introduced in this paper – is the recommended solution due to its fast indexing and similarity calculation, in addition to its high performance.

For future work we will continue exploring the use of knowledge graphs and multilingual semantic networks for cross-language similarity tasks. The use of semantic signatures allows to create a new type of knowledge graphs which have been successfully used for multilingual WSD (Moro et al., 2014), and will be studied in the future. The use of distributed representations will also be investigated further. The generation of distributed representations of concepts is only in its infancy, and works like SensEmbed, the study of Aletras and Stevenson (2015), or this paper, could be extended for tasks such as similarity analysis, conceptual relatedness or WSD.

@&#ACKNOWLEDGEMENTS@&#

This research has been carried out in the framework of the European Commission WIQ-EI IRSES (No. 269180) and DIANA-APPLICATIONS – Finding Hidden Knowledge in Texts: Applications (TIN2012-38603-C02-01) projects. We would like to thank Tomas Mikolov, Martin Potthast, and Luis A. Leiva for their support and comments during this research.

@&#REFERENCES@&#

