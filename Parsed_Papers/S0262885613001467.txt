@&#MAIN-TITLE@&#Spatially aware feature selection and weighting for object retrieval

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a more accurate similarity measurement for object retrieval.


                        
                        
                           
                           Our method improves two features of the BoW model.


                        
                        
                           
                           Spatial expansion can incorporate more latent visual words into a query.


                        
                        
                           
                           Visual word re-weighting can increase weights of reliable visual words.


                        
                        
                           
                           The combination of them can improve both precision and recall.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Object retrieval

Bag-of-words

Spatial expansion

Visual word re-weighting

@&#ABSTRACT@&#


               
               
                  Many recent image retrieval methods are based on the “bag-of-words” (BoW) model with some additional spatial consistency checking. This paper proposes a more accurate similarity measurement that takes into account spatial layout of visual words in an offline manner. The similarity measurement is embedded in the standard pipeline of the BoW model, and improves two features of the model: i) latent visual words are added to a query based on spatial co-occurrence, to improve query recall; and ii) weights of reliable visual words are increased to improve the precision. The combination of these methods leads to a more accurate measurement of image similarity. This is similar in concept to the combination of query expansion and spatial verification, but does not require query time processing, which is too expensive to apply to full list of ranked results. Experimental results demonstrate the effectiveness of our proposed method on three public datasets.
               
            

@&#INTRODUCTION@&#

Recent years have witnessed increasing interest in the problem of object retrieval [1–7] and its applications [8–12]. Typically, the aim of this task is to select from a collection of images which contain the same object as a query image. Many works have addressed this problem with a standard “bag-of-words” (BoW) framework [1,13,14,2,15] borrowed from text retrieval. In this framework, local features in an image are detected and then represented by D-dimensional descriptors. To reduce memory usage and achieve fast indexing, these descriptors are clustered and then quantized to form a vocabulary of “visual words”. An image is represented by a sparse frequency histogram over the visual vocabulary.

Although the BoW model is simple to implement and robust to some types of image variation, an object in a target image still can fail to be retrieved from the dataset, mainly due to quantization used in this standard pipeline. As noted by many works over the past few years, quantization introduces two main problems:
                        
                           •
                           Loss of recall: information about the raw features is lost, so matching features may be assigned to different visual words. This can decrease the matching score of images that contain the same object and therefore reduce recall.

Loss of precision: features that do not correspond to the same location may be assigned to the same word. This can increase the matching score of images that do not contain the same object, and therefore reduce result precision.

To address the first problem, a number of methods focus on improving the query recall. For example, soft-assignment [4,16] method maps a raw feature to multiple visual words; in [17,18] a more accurate approximate nearest neighbor assignment is implemented by combining k-means clustering and binary vector signatures; the dimension reduction and the indexing algorithm are jointly optimized in [19,20], such that the image representation provides accurate search results with low vector dimensionality. Other methods aim to learn a better metric for feature quantization [5] or group similar features when generating the vocabulary [21,6]. Query expansion [3,22,23] addresses the loss of recall in a post-processing step. It enriches the query model by adding query relevant words collected from the initial retrieval results. To address the second problem, a re-ranking process is required [2,17,24], which filters out highly ranked false positives in the initial retrieval results. These methods mainly rely on the spatial consistency of visual words in pairs of query and dataset images. Precision is increased after images that do not contain a significant number of spatially consistent matches are re-ranked lower.

In this paper, we address loss of precision and recall by proposing a novel spatial co-occurrence measure. We use an image similarity measurement that takes into account the spatial layout of visual words, but can be embedded into the standard retrieval pipeline. We do this by creating a visual thesaurus that records spatial co-occurrence information for each pair of words in a vocabulary. Based on that, we explore two types of image-dependent contextual information to improve the BoW similarity measure: a voting-based method to include latent visual words and an information theory based measurement to re-weight the importance of each visual word. This enables us to improve both recall and precision of the standard BoW object retrieval method.


                     Spatial expansion. A query can be expanded by including latent words, which are highly correlated with those already present in the query. The correlation is deduced from the degree of spatial co-occurrence of the visual words. In this way, the query recall can be improved.


                     Visual word re-weighting. After additional words are introduced to the query, the tf-idf scheme weights each visual word according to its frequency in the individual image as well as in the corpus. However, tf-idf weights do not distinguish between word appearances in the foreground or background of an image. Therefore it might assign high weights to words that are not informative when searching for an object, or underweight words that are informative. Thus we develop a word weight that is more directly based on how often, and in what range of conditions, a word is correctly matched when it appears as part of the foreground object. Using the automatic training data collection method of [25], we select a subset of visual words that frequently occur as inliers to object matches robustly estimated between images. Under the standard tf-idf weighting, these visual words are not necessarily weighted strongly—for example they may occur in many images in the database, and therefore have a low idf weight. Based on these inlier measurements we use entropy to measure the importance of a visual word according to its spatial co-occurrence distribution. A re-weighting scheme is proposed in this paper to encourage these informative visual words. In this way, query precision can be improved.

As illustrated above, and described in more detail in Section 4, our method captures the underlying spatial relationship among the visual words, and their importance as an indicator of a foreground object, and thereby improves both precision and recall. Our method is similar to recent methods [26,3], but inspired by those which also discover the spatial relationships among the visual words [27–30]. As we will explain in the following section in more detail, the key differences between our work and previous methods are that ours focus on improving precision and recall without the use of result re-ranking or issuing multiple queries. This allows a trade-off between effectiveness and efficiency by adjusting the query vector online to incorporate this spatial and relevance information, based on weights computed offline. In addition, our method can be applied wherever the BoW model is used.

The remainder of the paper is organized as follows: Section 2 briefly reviews related works about the retrieval methods. Section 3 outlines our method and Section 4 describes the details of methods, including the definition of the visual thesaurus and two usages of the visual thesaurus: spatial expansion (Section 4.1) and visual word re-weighting (Section 4.2). We report the experimental results in Section 5. The paper is finally concluded in Section 6.

@&#RELATED WORK@&#

The BoW model for representing images has been widely used in computer vision problems [31,13,32,1,8–10]. Here, we focus on methods that incorporate spatial context into the BoW model. These methods can be divided into two groups: i) spatial context derived from related images; or ii) spatial context derived from related visual words.

The first group of methods is based on the spatial verification of images [2]: pairwise images that pass a geometric consistency test are likely to contain the same object. Typically, a RANSAC matching method is used and those images whose inlier match count exceeds a threshold are accepted. Using spatial verification, query expansion methods refine the query model by adding words from spatially verified regions in result images [3,22] or refine the distance measure with a k-reciprocal nearest neighbor test in image space [24]. Alternatively, spatial verification can be used to detect and match visually similar features that have been assigned to different visual words [33].

The second group of methods focuses on a bag-of-phrases structure. The works proposed in [30,34,26] rely on the spatial co-occurrence of visual words in order to assemble those containing a high-order relationship into visual phrases. The visual phrases are selected from a fixed image grid in [30] or K nearest neighbors in a fixed spatial region [26]. The visual phrases are more flexibly organized in [34], which bundles local features in a randomized partition of the image. Alternatively, methods by [27,35] use spatial co-occurrence within feature space. Local features are projected to different directions and the spatial information is encoded as the ordered bag-of-features representation in [27]. In contrast, [35] use neighboring local features to softly down-weight less informative words.

Most existing methods built on the BoW model generate a large vocabulary (e.g. 1million visual words) by clustering features from a large image dataset. Therefore, [28] argue to reduce memory requirements by selecting a small subset of features that exhibit spatial consistency. We use a similar method in this paper to select and re-weight “informative” visual words.

The idea of our method is also similar in spirit to [3,26]. Different from [3], our spatial expansion method is based on relations found in an offline step rather than mining query results online. In [26], the visual context is defined as a set of local points which are in a fixed spatial region of a visual word. These are used to expand the visual words used in the query vector. The method needs to partition the spatial region into K sectors, and for each sector a sub-histogram is constructed to record the visual context. In contrast, our method uses a spatial co-occurrence histogram (termed a visual thesaurus in [36]). As a result, the computation of our method is simpler than [26] because we only need to count votes or calculate entropy in independent visual thesaurus histograms.

@&#OVERVIEW@&#

In this paper, we propose a visual thesaurus data structure to record the spatial relations between visual words, and use it to refine the BoW model. The architecture of our framework is shown in Fig. 1
                     . Our method is composed of an offline process and an online process. The offline process creates visual words and weights them in terms of their occurrence in the whole dataset. In this stage, the weights of the visual words are adjusted according to their importance in the images. The online process stage aims to retrieve the relevant images by matching the visual words vectors. In this stage, the query vector is expanded and re-weighted using the knowledge learned from the offline stage. Algorithm 1 describes the work flow of our method. For descriptive convenience, we first introduce some main notations in Table 1
                     .
                        
                           
                        
                     
                  

The visual thesaurus captures the spatial relationship among pairs of visual words. Assume that there are N visual words 
                        
                           W
                           :
                           =
                           
                              
                                 
                                    w
                                    i
                                 
                              
                              
                                 i
                                 =
                                 1
                              
                              N
                           
                        
                      in the vocabulary. The co-occurrence of a pair of words (wi
                     , wj
                     ) can be expressed as the joint probability of wi
                      and wj
                      occurring in the same spatial region, defined as Pr(wi
                     , wj
                     ). For a given word wi
                     , we can obtain up to n nearest neighbors in a single image as 
                        
                           
                              D
                              i
                           
                           :
                           =
                           
                              
                                 
                                    w
                                    j
                                 
                              
                              
                                 j
                                 =
                                 1
                              
                              n
                           
                        
                     , such that ∥
                     d(w
                     
                        i
                     )−
                     d(w
                     
                        j
                     )∥<
                     ρ, where wj
                     
                     ∈
                     
                        W
                     , ρ is the distance threshold limiting the nearest neighbor region and the function d(·) returns the location of visual words in the image space. For all the images in the dataset, the visual thesaurus comprises of a set of histograms 
                        
                           H
                           :
                           =
                           
                              
                                 
                                    h
                                    i
                                 
                              
                              
                                 i
                                 =
                                 1
                              
                              N
                           
                        
                     , in which the jth entry, hi
                     (j), indicates the frequency of co-occurrence between the word wi
                      and its nearest neighbor wj
                     
                     ∈
                     
                        D
                     
                     
                        i
                      collected from the whole dataset. Thus, hi
                      records the probability of co-occurrence Pr(wi
                     , wj
                     ) in a spatial region. Note that the length of each histogram is N, but in general the histograms are sparse.

By using this structure, we are able to generate two kinds of visual thesaurus: general thesaurus and object based thesaurus, which is used in two different ways. A general thesaurus is built for spatial expansion, and is based on all the visual word in the dataset to find the spatial information among them. An object based thesaurus is built for visual word re-weighting, and only considers a subset of images to explore some discriminative words.

Due to viewpoint changes and quantization errors, matching features are sometimes assigned to different words and therefore do not contribute to the similarity measure. This can cause true positive results to be omitted from the top ranks. In order to address this problem, our method expands the query visual words with their associated words from general thesaurus. Such words commonly occur together in the image domain, for example, a pair of words A and B shown in Fig. 2
                        . In this case, if A does not appear in an image, B can indicate the presence of word A for the same object.


                        
                           
                              
                           
                        
                     


                        
                           
                              
                           
                        
                     

We define a general thesaurus, Fn
                           , as the set of histograms ℋ based on (up to) n nearest neighbors. To build Fn
                           , the thesaurus histograms are computed from all the images in the dataset 
                              
                                 V
                                 :
                                 =
                                 
                                    
                                       
                                          v
                                          i
                                       
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    M
                                 
                              
                           . Each feature in the images is mapped to a visual word id by quantization. The spatial region for a visual word wi
                            in the image space is defined as a K nearest neighborhood 
                              D
                           
                           
                              i
                           . The pseudo code for creating Fn
                            is given in Algorithm 2. >The general thesaurus uses a set of N histograms to record the co-occurrence of pairwise visual words in the nearest neighborhoods, where N is the vocabulary size. The histograms are initialized beforehand, and are incremented if co-occurrence is detected. Since the vocabulary is usually large, the histograms are sparse (most of the entries in the histograms will be zero). However in some cases, pairs of visual words can be found to often occur together, as shown in Fig. 2. This leads to some peaks in the histograms, which indicates frequent co-occurrence of the words in the dataset. The simplest case is an F
                           1 thesaurus which records a pair of visual words that often occur as nearest neighbors (i.e. the words A and B shown in Fig. 2).

A query image contains visual words Q
                           ={q
                           
                              i
                           }
                              i
                              =1
                           
                              n
                           , with qi
                           
                           ∈
                           
                              W
                           . For each visual word qi
                           
                           ∈
                           Q, a thesaurus histogram hi
                            can be retrieved from general thesaurus. The expansion of query words is based on the visual words associated with peaks in the thesaurus histogram. These latent relationships are useful for a query containing a large number of visual words (Hertford in Fig. 3
                           ), and critical for a query where only a small number of visual words are present or the object's appearance changes significantly (All souls in Fig. 3). As shown in Fig. 3, more visual word matches can be found by expanding the query words Q. The image query is then formed by concatenating the query words Q and spatially expanded words WT
                            together:
                              
                                 (1)
                                 
                                    
                                       
                                          Q
                                          ′
                                       
                                       =
                                       
                                          Q
                                          
                                             W
                                             T
                                          
                                       
                                       =
                                       
                                          
                                             q
                                             1
                                          
                                          
                                             q
                                             2
                                          
                                          …
                                          
                                             q
                                             n
                                          
                                          
                                             c
                                             1
                                          
                                          
                                             c
                                             2
                                          
                                          …
                                          
                                             c
                                             k
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

Note that the spatially expanded visual words are chosen based on their co-occurrence with the visual words in the query. Algorithm 3 describes the detail of generating the spatially expanded visual words 
                              W
                           
                           
                              T
                           : for each word qi
                            in a given query Q, if Pr(qi
                           , ck
                           ) is greater than a threshold θ, this word ck
                           
                           ∈
                           
                              W
                            is added to the query. As a result, we obtain an enriched query Q′ to search the dataset. The threshold θ is defined by an automatic mechanism, described in Section 5.

Like query expansion, spatial expansion is designed to improve average recall by adding extra relevant words to a query vector. However, this increased recall can come at the cost of decreased precision, as these additional words can promote the rank of false positive results. For example, Fig. 4
                        ((a) and (b)) shows cases where the spatially expanded word correspondences are mostly false positive results. In contrast, most correspondences are correct in a true positive result, and appear on the object we are trying to find (Fig. 4(c)).

The standard method for eliminating these false positives is spatial verification [2], or one of its variants. These methods post-process the result list to remove results that do not exhibit a high degree of spatial consistency. If the query object is rigid, spatial consistency usually indicates it is present in the result image. In this paper, we use a spatial consistency test to re-weight words rather than filter results. The key advantage is that this does not involve post-processing the result list, and is thus more efficient at query time.

In the BoW model, each image is represented by a vector in which each entry indicates the importance of a corresponding visual word. As proposed in [1], the importance of a visual word wi
                         is defined by tf-idf weight:
                           
                              (2)
                              
                                 
                                    t
                                    
                                       
                                          w
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                n
                                                
                                                   
                                                      w
                                                      i
                                                   
                                                
                                             
                                             
                                                n
                                                d
                                             
                                          
                                          ⏟
                                       
                                       tf
                                    
                                    ⋅
                                    
                                       
                                          
                                             log
                                             
                                                M
                                                
                                                   M
                                                   
                                                      
                                                         w
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                          ⏟
                                       
                                       idf
                                    
                                 
                              
                           
                        in which n(wi
                        ) refers to the frequency of the word wi
                         in document d, nd
                         refers to the number of visual words in document d, M refers to the dataset size and M(wi
                        ) is the frequency of the word wi
                         in the corpus. As a consequence, each dataset image is represented by a vector of tf-idf weights 
                           
                              
                                 v
                                 →
                              
                              =
                              
                                 
                                    t
                                    
                                       
                                          w
                                          1
                                       
                                    
                                    ,
                                    …
                                    ,
                                    t
                                    
                                       
                                          w
                                          i
                                       
                                    
                                    ,
                                    …
                                    ,
                                    t
                                    
                                       
                                          w
                                          N
                                       
                                    
                                 
                              
                           
                        . Under such definition, a visual word will be weighted heavily if it occurs frequently in image d (tf score) while appearing rarely in the dataset (idf score). However, as noticed by many previous methods, this definition does not work all the time due to quantization errors.

Spatial verification overcomes this by basing the result ranking on the number of spatially consistent matches found between a query image and the top-k ranked results. In contrast, our method re-weights words based on their local geometric consistency, without the need for post-processing. Our method collects spatial consistency information in an offline step, and uses an information theory based measurement to re-weight the visual words that have been detected as spatially consistent. The similarity score between pairwise image is computed by the dot-product similarity [1], where the tf-idf vector is refined: 
                           
                              sim
                              
                                 
                                    
                                       
                                          
                                             
                                                v
                                                →
                                             
                                          
                                          q
                                       
                                    
                                    ∗
                                 
                                 
                                    
                                       
                                          
                                             
                                                v
                                                →
                                             
                                          
                                          d
                                       
                                    
                                    ∗
                                 
                              
                           
                        . The details are described as follows.

We use a modified version of automatic training data generation method proposed in [25] to obtain pairwise image matches. This can be seen as an offline spatial verification process. Our method is slightly different from [25] in two aspects:
                              
                                 •
                                 We only use the inliers from the training pairs. Geometric matching roughly locates the object of interest in images; as a result, the outliers returned by the geometric matching (e.g. RANSAC) contain too much uncertainty about the location of foreground or background, and we do not consider the outliers in this paper.

The epipolar geometry is calculated from correspondence of visual words, instead of raw-SIFT matching. That is because visual word level matching is much faster, allowing us to process a larger amount of training data at the expense of slightly lower accuracy.

As a consequence, the foreground visual words are selected as inliers to a dominant epipolar constraint found by RANSAC. Fig. 5
                            shows some training image pairs collected from the Oxford 5K and Paris 6K datasets with a high number of geometric inliers (20 inliers, as set by [5]).

Let 
                              
                                 
                                    W
                                    S
                                 
                                 :
                                 =
                                 
                                    
                                       
                                          w
                                          i
                                       
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                              
                            be the informative words appearing as inliers at least once in the training data. The re-weighting function (wi
                           ) is applied to adjust the score of each inlier visual word wi
                           
                           ∈
                           
                              W
                           
                           
                              s
                           :
                              
                                 (3)
                                 
                                    
                                       
                                          t
                                          ∗
                                       
                                       
                                          
                                             w
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   α
                                                   
                                                      
                                                         w
                                                         i
                                                      
                                                   
                                                   ⋅
                                                   t
                                                   
                                                      
                                                         w
                                                         i
                                                      
                                                   
                                                
                                                
                                                   if
                                                   
                                                   
                                                      w
                                                      i
                                                   
                                                   ∈
                                                   
                                                      W
                                                      S
                                                   
                                                
                                             
                                             
                                                
                                                   t
                                                   
                                                      
                                                         w
                                                         i
                                                      
                                                   
                                                
                                                
                                                   otherwise
                                                   .
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           Intuitively, the simplest re-weighting function is to multiply the inlier visual words by a constant scale factor, (wi
                           )=
                           c, c
                           >1. This treats all inlier visual words as being equally important, and more important than words that do not occur as inliers, but neglects the difference among them. Alternatively, each visual word can be re-weighted according to the frequency with which it occurs as an inlier during the training stage: 
                              
                                 α
                                 
                                    
                                       w
                                       i
                                    
                                 
                                 =
                                 
                                    P
                                    f
                                 
                                 
                                    
                                       w
                                       i
                                    
                                 
                                 =
                                 
                                 
                                    
                                       
                                          #
                                          
                                          of
                                          
                                          
                                             w
                                             i
                                          
                                          
                                          is
                                          
                                          inliers
                                       
                                       
                                          #
                                          
                                          of
                                          
                                          
                                             w
                                             i
                                          
                                          
                                          in
                                          
                                          training
                                          
                                          data
                                       
                                    
                                 
                              
                           . The intuition here is that a word that commonly occurs on an object of interest and is correctly matched is likely to be a good indicator of the object. However, this fails to detect words that commonly occur as inliers, but are largely redundant because they are strongly associated with other inlier words. In this case, the visual thesaurus will include all such words in any query vector that contains one of them. Weighting all of these words strongly overestimates the confidence of the match and can lead to false positives.

To avoid these cases, we devise a measure to strongly weight those words that occur as inliers despite a wide variety of words appearing in their spatial neighborhood. This implies that the word itself is a strong indicator of the presence of the object. In order to choose reliable visual words in this way, we introduce an information theory based method to measure the importance of a visual word based on its neighborhood diversity, which to our knowledge has not been studied in previous methods.

As illustrated in first row of Fig. 4, it is hard to distinguish true from false results based solely on word matches. Our method is motivated by the following observations on spatial neighborhoods. Observation 1: Since the objects of interest are rigid, the spatial structure of these objects is geometrically consistent across images—for example, visual words on the objects satisfy a common epipolar constraint. To reduce the incidence of background matches, we only consider words that occur frequently as spatial inliers in the training data collection described previously. Observation 2: Due to the random sampling used in RANSAC, noisy features are also likely to be selected as inliers. Such visual words are less informative, and should not be weighted higher than the visual words observed above. As shown in Fig. 6
                           , these words are often either isolated from other inliers (i.e. word D has no neighbors) or, in the case of word C, co-occur with few neighbors in their spatial neighborhood.

As a result, the diversity of inliers occurring in their spatial neighborhood is essential in measuring the importance of the inlier visual words. Mathematically, the diversity of a distribution can be characterized as entropy 
                           [37]. A higher entropy indicates that a variable is more uniformly distributed. Such words occur as inliers despite a wide variety of spatial neighbors, and are the ones we need to heavily weight. Fig. 7
                            shows examples of a visual word whose spatial neighborhood has a high entropy. As it occurs frequently in the corpus, its tf-idf weight is low. However, in practice it is a reliable visual word for matching.

To measure entropy, for each visual word wi
                           
                           ∈
                           
                              W
                           
                           
                              S
                           , we obtain its nearest neighbors 
                              D
                           
                           
                              i
                           , with 
                              D
                           
                           
                              i
                           
                           ⊂
                           
                              W
                           
                           
                              S
                           .The diversity of the distribution of 
                              D
                           
                           
                              i
                            is measured by a relative entropy H(
                              D
                           
                           
                              i
                           ):
                              
                                 (4)
                                 
                                    
                                       H
                                       
                                          
                                             D
                                             i
                                          
                                       
                                       =
                                       −
                                       
                                          
                                             ∑
                                             j
                                          
                                       
                                       P
                                       
                                          
                                             w
                                             j
                                          
                                       
                                       log
                                       P
                                       
                                          
                                             w
                                             j
                                          
                                       
                                    
                                 
                              
                           where wj
                           
                           ∈
                           
                              D
                           
                           
                              i
                            and visual words that occur in a fixed spatial region are counted.As a result, the re-weighting function can be defined as:
                              
                                 (5)
                                 
                                    
                                       α
                                       
                                          
                                             w
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   1
                                                
                                                
                                                   if
                                                   
                                                   
                                                      D
                                                      i
                                                   
                                                   =
                                                   ϕ
                                                
                                             
                                             
                                                
                                                   exp
                                                   
                                                      
                                                         δ
                                                         ∗
                                                         H
                                                         
                                                            
                                                               D
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   otherwise
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where the scaling factor δ is used to control the influence of α(wi
                           ), δ
                           ∈(0,1) and is fixed in the experiments.In order to calculate the entropy H(
                              D
                           
                           
                              i
                           ), we build an object based thesaurus to store the spatial co-occurrence found during the training stage.

The object based thesaurus captures the spatial co-occurrence properties of inlier words 
                              W
                           
                           
                              S
                           . It defines a K nearest neighborhood (
                              D
                           
                           
                              i
                           ), as used in general thesaurus. However, the visual words in the neighborhood need to be inliers appearing in the training data collection, which is not required in building the general thesaurus. We use a Gaussian mask to calculate the co-occurrence of visual words in the thesaurus histogram hi
                           
                           ∈ℋ , in which each entry hi
                           (j) is incremented as follows:
                              
                                 (6)
                                 
                                    
                                       
                                          h
                                          i
                                       
                                       
                                          j
                                       
                                       =
                                       
                                          h
                                          i
                                       
                                       
                                          j
                                       
                                       +
                                       exp
                                       
                                          
                                             −
                                             ∥
                                             d
                                             
                                                
                                                   w
                                                   i
                                                
                                             
                                             −
                                             d
                                             
                                                
                                                   w
                                                   j
                                                
                                             
                                             ∥
                                             /
                                             σ
                                          
                                       
                                    
                                 
                              
                           where wj
                           
                           ∈
                           
                              D
                           
                           
                              i
                           , σ is the scaling function and the function d(·) returns the location of visual words in the image space. Thus, the entropy of each word wi
                           
                           ∈
                           
                              W
                           
                           
                              S
                            is calculated by the function H(
                              D
                           
                           
                              i
                           ), where 
                              D
                           
                           
                              i
                            can be found from the thesaurus histogram hi
                           .

Furthermore, we propose a query specification mechanism to encourage the words that appear both in the query Q and the informative words 
                              W
                           
                           
                              S
                           . Such words are important for a query since they can be shared by other images in the dataset, which also contain the same query objects.

Thus, we slightly increase the frequency of such words in computing the similarity:
                              
                                 (7)
                                 
                                    
                                       
                                          t
                                          ∗
                                       
                                       
                                          
                                             w
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             
                                                
                                                   λ
                                                   ⋅
                                                   α
                                                   
                                                      
                                                         w
                                                         i
                                                      
                                                   
                                                   ⋅
                                                   t
                                                   
                                                      
                                                         w
                                                         i
                                                      
                                                   
                                                
                                                
                                                   if
                                                   
                                                   
                                                      w
                                                      i
                                                   
                                                   ∈
                                                   
                                                      Q
                                                      ∗
                                                   
                                                
                                             
                                             
                                                
                                                   α
                                                   
                                                      
                                                         w
                                                         i
                                                      
                                                   
                                                   ⋅
                                                   t
                                                   
                                                      
                                                         w
                                                         i
                                                      
                                                   
                                                
                                                
                                                   if
                                                   
                                                   
                                                      w
                                                      i
                                                   
                                                   ∈
                                                   
                                                      W
                                                      S
                                                   
                                                   ,
                                                   
                                                   and
                                                   
                                                   
                                                      w
                                                      i
                                                   
                                                   ∉
                                                   
                                                      Q
                                                      ∗
                                                   
                                                
                                             
                                             
                                                
                                                   t
                                                   
                                                      
                                                         w
                                                         i
                                                      
                                                   
                                                
                                                
                                                   otherwise
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    Q
                                    ∗
                                 
                                 =
                                 Q
                                 ∩
                                 
                                    W
                                    S
                                 
                              
                           , λ is a scaling factor, and the re-weighting function α(wi
                           ) is predefined. Since Q
                           ⁎ is obtained online according to the given query, this step requires a slight cost in similarity computation. Usually, the length of Q
                           ⁎ is much smaller than the length of query vector Q. The computation complexity will increase very little. Algorithm 4 describes the details of our re-weighting method.


                           
                              
                                 
                              
                           
                        


                           
                              
                                 
                              
                           
                        

We present a total association (TA) scheme to combine the benefits of spatial expansion and visual word re-weighting. The total association scheme is illustrated in Fig. 8
                        . It has two stages:
                           
                              •
                              offline: re-weight the visual words wi
                                 
                                 ∈
                                 
                                    W
                                 
                                 
                                    S
                                  in all the documents according to the entropy H(
                                    D
                                 
                                 
                                    i
                                 ), as shown in Fig. 8(a).

online: given a set of query words Q, it will be first expanded with latent visual words to be Q′←[Q,
                                    W
                                 
                                 
                                    T
                                 ] and then Q′ will be re-weighted according to the entropy H(
                                    D
                                 
                                 
                                    i
                                 ) as well, as shown in Fig. 8(a).

Algorithm 5 describes the details of our total association scheme. This combined scheme can cover more than one situation in which tf-idf based image retrieval fails. First, spatially expanded visual words can be added into query if they are found; second, discriminative words can be weighted effectively. Like the combination of query expansion and spatial verification, the first step improves recall while the second improves query precision.

@&#EXPERIMENTAL RESULTS@&#

In this section, we first describe our experimental setup, including the datasets, the implementation details of our method and the evaluation criterion. Then, we evaluate the retrieval performance of our method in the following three aspects: i) spatial expansion; ii) visual word re-weighting; and iii) total association. Subsequently, we compare the retrieval performance of our scheme with several state-of-the-art methods. Finally, we provide an experimental discussion.

The experiments are conducted on some public image datasets: two small datasets Oxford 5K [38], Paris 6K [39], and a large-scale Oxford 105K (consisting of Oxford 5K images and 100K images from MIRFLICKR-1M
                           1
                        
                        
                           1
                           Images are downloaded from http://press.liacs.nl/mirflickr/dlform.php.
                        ) to test the scalability of our method. The configurations are summarized in Table 2
                        .

The Oxford 5K dataset consists of 5062 images collected from Flickr [40] by searching 11 distinctive Oxford landmarks. Each landmark contains 5 different queries, each of which is within an associated bounding box. The dataset also provides groundtruth for these 11 different landmarks, where each image is annotated with four sorts of labels: Good, OK, Junk, and Bad. The images annotated as “Good” and “OK” (“Bad”) are treated as positive (“negative”) samples, and the “Junk” images are not considered in evaluating the retrieval performance, in the same way as [2]. The Paris 6K dataset consists of 6 412 images collected from Flickr [40] by searching 11 distinctive landmarks in Paris. Each image in the Paris 6K dataset is annotated in the same way as that in the Oxford 5K dataset.

@&#IMPLEMENTATION DETAILS@&#

To ensure the compatibility of the experiments, we directly use the public vocabulary of the Oxford 5K dataset available in [38]. The corresponding visual vocabularies for the Paris 6K and Oxford 105K are created in the same way as [2], where the visual words are generated by using FASTCLUSTER [41] to perform the approximate k-means clustering in the Hessian–Affine and SIFT feature spaces.
                              2
                           
                           
                              2
                              The source code is available from http://www.robots.ox.ac.uk/vgg/research/affine/.
                            The vocabulary size for all the datasets is 1million. After clustering, the feature descriptors are assigned to the closet cluster centers by FLANN [42]. We run our experiments on 2×8-Core Xeon E5-2680 at 2.70GHz with 10GB memory. The visual thesaurus creation is completed offline. For a general thesaurus this takes half an hour while for object thesaurus it takes around 5h including collection of spatial related data. At query time, the computational cost of our scheme is mainly due to the extra query words added by spatial expansion. Visual word re-weighting does not increase the number of words and thus has very low run time cost. Performance details are reported and discussed in Section 5.5.

In order to quantitatively evaluate the retrieval performance of the competing methods, we use the same evaluation criterion as [2] where the retrieval performance of single query is measured by the average precision (AP) (the area under the precision recall curve). In practice, to test the stability of the retrieval performance, we run multiple queries and take the mean Average Precision (mAP), as the final measurement of the retrieval performance. Our baseline is the standard BoW model and tf-idf weighting reported in [2].

In this section, we investigate the improvement of recall with spatial expansion. To illustrate the effectiveness of spatial expansion, we set up the experiments in the following ways:


                           Tables 3 and 4
                           
                            investigate the retrieval performance on six types of general thesaurus: F
                           1,F
                           2,…,F
                           6, which correspond to different numbers of nearest neighbors in a spatial region. The distance threshold is chosen as ρ
                           =10 in building general thesaurus. As the number of nearest neighbors increases, the general thesaurus is able to include more spatial information.

As shown in Tables 3 and 4, the overall mAPs of various general thesaurus Fn
                            increase with the number of nearest neighbors n. However, for individual queries the peak mAP may be obtained for a value of Fn
                            that is less than the maximum. This is because where the object is small or partly hidden in the image, its spatial support will be small. In this case, adding more spatial neighbors adds noisy expanded words which tend to decrease retrieval accuracy. The precision recall curves of two particular cases All souls and Hertford are illustrated in Fig. 9
                           , showing the improvement of the retrieval performance after spatial expansion (corresponding to Table 3). The choice of threshold θ is crucial in spatial expansion. It should ensure that as the number of nearest neighbors becomes larger, the threshold θ also requires a less strong correlation in order to accept words. Initially, the threshold θ is set to 0.1, and then monotonically decreases by 30% as the number of nearest neighbors increases (i.e. 0.1 for F
                           1, 0.07 for F
                           2, 0.049 for F
                           3, and so on).

We first investigate the effect of spatial expansion by evaluating the retrieval performance with the expanded words (C) only, ignoring the query words themselves (Q). On the Oxford 5K dataset, we obtain the mAP score 0.656, compared to the baseline (0.614). Therefore, these spatial related words are effective in improving the retrieval performance.

As reported in Tables 3 and 4, performance with spatial expansion on some landmarks becomes worse as n increases, because noisy spatial information is included e.g. Magdalen (n
                           ≥3). To investigate this, we examine the correspondences between spatially expanded visual words for selected image pairs. The correspondences are obtained by using both the query words Q (blue) and the expanded words C (magenta). As seen in Fig. 10
                           , the All souls images mostly feature the building prominently, and thus the query correspondences occur mostly in image regions that are locally consistent. In these cases, spatial expansion reinforces these correct matches with further consistent correspondences. However, in the case of Magdalen, the building is often small in the image or partly occluded. There are often not enough matches between query words to retrieve the results shown in Fig. 10. In this case, the words obtained by spatial expansion account for most of the matches and are vital in boosting recall. However, some matches of them are incorrect because of the small region of spatial support around some query features. This affects the retrieval performance when n increases.


                           Fig. 11
                            illustrates the effects of the general thesauruses F
                           1–F
                           6 on the retrieval performance and the number of words included in the query. The horizontal axis shows the number of spatially expanded words that are included increases continuously as the thesaurus includes more neighbors (from F
                           1 to F
                           6). However, the improvement in average retrieval accuracy (mAP scores) diminishes as n increases, as discussed above. To compromise between accuracy and efficiency, we set the general thesaurus as F
                           5 by default for the rest of this paper.

In this section, we investigate the improvement of precision with visual word re-weighting. We use F′15 in building the object based thesaurus, where the number of nearest neighbors is fixed, the distance threshold ρ
                        =30 and the scaling factor for Gaussian mask σ
                        =15 (mentioned in Eq. (6)). We first illustrate the effects of training data size and the re-weighting function α(wi
                        ), and then compare with other state-of-the-art methods.


                           Tables 5 and 6
                           
                            investigate the effects of training data size on the Oxford 5K and Paris 6K datasets. To simplify the comparison, we start with the re-weighting function as α(wi
                           )=2, and then generate training data with the method presented in Section 4.2. After collecting 10% of visual words from the vocabulary, we stop the process of training data generation. During this step, the mAPs for both the Oxford and Paris datasets rise for small amount of training data, and then plateau if given more training data, as the red dashed curves show in Fig. 12
                           . This is because our method relies on the spatial correspondence between image pairs. Once there is sufficient data to estimate this, retrieval performance is stable.

We examine the performance of visual word re-weighting with five different re-weighting functions α(wi
                           ), which can be grouped into two categories: non-entropy (α1 and α2) and entropy (α3, α4 and α5).
                              
                                 •
                                 α(wi
                                    )=2, there is no difference between the importance of the selected visual words 
                                       W
                                    
                                    
                                       S
                                    .

α(wi
                                    )=exp(P
                                       f
                                    (wi
                                    )), the importance of each visual word wi
                                    
                                    ∈
                                    
                                       W
                                    
                                    
                                       S
                                     is proportional to Pf
                                    (wi
                                    ), the number of times it appears as an inlier in the training data, as a fraction of the total number of times it appears. Therefore, Pf
                                    (wi
                                    )∈(0,1] and α2
                                    ∈(1,e].


                                    
                                       
                                          
                                             α
                                             3
                                          
                                          
                                             
                                                w
                                                i
                                             
                                          
                                          =
                                          exp
                                          
                                             
                                                δ
                                                ⋅
                                                H
                                                
                                                   
                                                      D
                                                      i
                                                   
                                                
                                             
                                          
                                       
                                    , the importance of each visual word wi
                                    
                                    ∈
                                    
                                       W
                                    
                                    
                                       S
                                     is proportional to the entropy in its neighborhood word set 
                                       D
                                    
                                    
                                       i
                                    .


                                    α
                                    4(w
                                    
                                       i
                                    )=exp(δ
                                    ⋅
                                    P
                                    
                                       f
                                    (w
                                    
                                       i
                                    )H(D
                                    
                                       i
                                    )), α4 incorporates the prior information into the entropy. Thus, α4 is a weighted version of α3.

α5: query specification on α4 (Section 4.2, Eq. (7)), it slightly increases the weight of the words Q
                                    ⁎, based on the results of α4.

In our experiments, the scaling factor δ for the re-weighting functions is set to 0.5,
                              3
                           
                           
                              3
                              This is based on the maximum entropy observed from experiments. Ideally, the maximum entropy occurs if the nearest neighbor 
                                    D
                                 
                                 
                                    i
                                  includes all words 
                                    W
                                 
                                 
                                    S
                                 , and each word wj
                                 
                                 ∈
                                 
                                    D
                                 
                                 
                                    i
                                  has the same probability: 
                                    
                                       H
                                       
                                          
                                             D
                                             i
                                          
                                       
                                       =
                                       −
                                       m
                                       ⋅
                                       
                                          
                                             1
                                             m
                                          
                                       
                                       ⋅
                                       log
                                       
                                          
                                             1
                                             m
                                          
                                       
                                    
                                 , where m is the size of the informative words 
                                    W
                                 
                                 
                                    S
                                 . In our experiments m
                                 =105, the maximum entropy can be 5. However, the highest entropy we observed in our experiments is around 2. As a result, the entropy functions vary in the same range as the non-entropy functions.
                            and the scaling factor λ for query specification is set to 1.15. Note that these re-weighting functions start with no spatial relationship (α1), to weak spatial information (α2), and to stronger spatial information using entropy (α3, α4, and α5). The corresponding retrieval performances of these re-weighting functions are reported in Fig. 12. As is seen by the curves in Fig. 12, we obtain two groups of results, according to the non-entropy functions and the entropy functions. The mAPs for all the re-weighting functions rise rapidly for small amounts of training data (from zero to 20%L). The entropy functions lead to better results over the non-entropy functions when the size of training data increases (from 20%L to 100%L). The mAPs for the entropy functions can still rise after using more than 20% training data, and then plateau with enough training data (about 80%L). On the contrary, the retrieval performance of the non-entropy function does not improve after above 20% of the training data. The best results are obtained by α5, and this will be used in the following experiments.


                           Fig. 13
                            illustrates the distribution of re-weighting scores across different visual words. Note that the tf-idf score adaption does not need to be applied to all the visual words. Instead, it is applied to 10% visual words from the vocabulary collected by training data. According to Fig. 13(a), many visual words in 
                              W
                           
                           
                              S
                            will be heavily weighted if they appear both in query and dataset image (computed offline by α4). During the run time, query specification (α5) increases the tf-idf scores based on α4 to re-weight the visual words. These highly re-weighted visual words (α5
                           >4) are shown in Fig. 13(b), which mainly correspond to the geometric information of the buildings. We also notice that the number of the nearest neighbors influences the performance of visual word re-weighting, but the difference is small.

The full scheme, total association (TA), combines spatial expansion and visual word re-weighting. Table 7
                         reports the details of the mAP results of total association. It is observed that most queries have more than 10% improvement on retrieval performance, compared to the baseline. Exceptions occur in Table 7, where the accuracy of our method is lower than the baseline, due to the noisy spatial information we learnt from general thesaurus, as discussed in Section 5.2. Considering both precision and recall improvement yields in the Oxford 5K and Paris 6K datasets further improvement of retrieval performance over the baseline method, compared to the individual ones (spatial expansion and visual word re-weighting).

In this section, we compare our method with a number of state-of-the-art methods. The comparison is separated based on methods that operate before the query is executed (pre-processing) and after (post-processing).


                           Tables 8 and 10
                           
                           
                            compare the accuracy and run time with commonly used post-process methods (spatial verification [2] and average query expansion [3] methods). The retrieval results in Table 8 are reported in three groups (A–C): Group A compares each step of our method (spatial expansion, visual word re-weighting and total association) with the baseline results, respectively. At each step, our method can outperform the baseline with some extra run time as reported in Table 10. The increase of run time is at most double that of the baseline. In particular, using visual word re-weighting alone requires very little extra computation time during query (almost the same as the baseline). Group B reports our method jointly working together with spatial verification [2]. As seen in Tables 8 and 10, our method in Group A can outperform spatial verification without the need for post-process. We can also jointly use spatial verification with each step of our method as shown in Group B, and obtain further improvement on retrieval accuracy. Group C reports our method jointly working with average query expansion (AQE) [3], which requires spatial verification to refine the query model. This further improves retrieval results compared with the results in Groups A and B. However, the performance gain of methods in this group is small. This is because AQE uses top verified results returned from the retrieval results. The accuracy of these top verified results is high for all methods. Moreover, the collection of top verified results is expensive, as reported in Table 10. It requires more than 2s by using spatial verification. In contrast, our method learns the latent spatially relevant words off-line, which is much faster than AQE. We adopt total association as our proposed method in the following experiments.


                           Table 9 compares our method with those who requiring training/learning stage as a pre-process. We evaluate these methods in two ways: i) computation of training data (offline) and ii) computation during run time (online). Typically, methods without prior training data collection (a–c) have higher query time computation costs than others. The soft-assignment method (a) needs to include n times more visual words than the baseline method to expand the coverage of each individual word, and results in a n
                           2 computational cost multiplier, compared with baseline. Method (b) uses inverted file to store the visual phrases and results in approximately double query time. Our method (c) is more efficient than soft-assignment since only the query words are expanded. As shown in Fig. 11, our method requires about 7 times more query words while soft-assignment requires n
                           2
                           =9 times to reach the same level of retrieval accuracy (mAP score above 0.670 for both). In contrast, methods with a training data collection have the advantage of selecting a subset of visual word (features) (d–g). Methods (d–f) can run the query as fast as the baseline methods. Thus, our method (g) outperforms other offline methods in terms of retrieval accuracy.

This section discusses the retrieval performance of our method. Figs. 14 and 15
                        
                         show some examples of top retrieved facades from the Oxford and Paris images. A short summary of different retrieval methods on three datasets is given by Table 11
                        , where the Oxford 105K is added to test for the scalability of our method. Spatial expansion and visual word re-weighting have different effects on the retrieval performance as follows:

We obtain the mAP as 0.685 and 0.679 for the Oxford 5K and Paris 6K datasets, respectively. The improvement is more evident on large scale dataset. We obtain the mAP as 0.622 on the Oxford 105K dataset, which is 20.8% higher than the baseline method. The spatial expansion can be thought of as an offline “query expansion” method. The effect of spatial expansion is to include query-dependent spatial latent visual words, which can be learned from a voting-based method (with general thesaurus). Recent work [22] has noticed that there are confusing words for a given query image, which should not be used for expansion. In contrast to [22], we refine the query model by encouraging informative words. Again, this is completely computed offline.

Our visual word re-weighting method is computationally cheaper than spatial re-ranking of the baseline results. Moreover, using visual word re-weighting alone is capable of achieving higher accuracy without increasing query complexity. Recent work [35] has proposed a similar method to boost the discriminative ability in a fixed spatial region (spatial contextual weighting). However, the method proposed in [35] only focuses on boosting the precision, and applies to the feature space rather than image space.

As discussed above, spatial expansion helps to find correlated visual words, while visual word re-weighting helps to weigh heavily the discriminative words. To achieve the improvement of both precision and recall, we propose a total association scheme to balance these two methods. The retrieval performance can be further improved with combination of these two methods. Total association achieves 0.708, 0.709 and 0.689 on the three datasets, which is 15.3%, 11.0% and 33.8% increase of the baseline method.


                           Table 11 compares our method with state-of-the-art in three groups. Group A compares methods without post-processing. The results show that total association can outperform previous methods in this group. Group B compares methods with post-processing, where result re-ranking can increase the retrieval accuracy. Group C illustrates methods jointly working with post-processing, in particular query expansion. As the retrieval results reported in Group C show, our method with AQE and DQE has similar performance to other methods based on query expansion, e.g. Contextual synonym dictionary [26], with minimal run time overhead. It is difficult, however, to directly compare the improvement that is due to each of these methods as they are based on different baseline implementations: the baseline BoW method in [26] has approximately 10% higher mAP than the standard used in this paper and [2,3,5].

@&#CONCLUSION@&#

This paper introduces a more accurate image similarity measurement based on the BoW object retrieval model. Our method uses a visual thesaurus to store the spatial information among the visual words. Then, we introduce several ways of improving the standard pipeline. The first one is based on a general thesaurus, in which a query model is refined by including latent spatial related visual words. The second one is a re-weighting scheme, where reliable visual words, learned from an object based thesaurus, have higher weights than other words. Finally, we introduce a total association scheme to combine spatial expansion and visual word re-weighting. We demonstrate our method on some public datasets. The experimental results show that the total association can achieve an average of over 10% increase in mAP scores, on object retrieval performance for standard data sets.

@&#REFERENCES@&#

