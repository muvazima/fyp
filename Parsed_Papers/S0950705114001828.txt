@&#MAIN-TITLE@&#Crisscross optimization algorithm and its application

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This paper introduces a new optimization algorithm called crisscross optimization algorithm (CSO).


                        
                        
                           
                           The horizontal crossover searches the offspring within a half population of hypercubes.


                        
                        
                           
                           The vertical crossover is able to accelerate the stagnant dimensions of population to jump out of local optima.


                        
                        
                           
                           The integration of the dual search mechanisms gifts the CSO algorithm with powerful global search ability.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Crisscross optimization algorithm

Economic dispatch

Optimization problem

Horizontal crossover search

Vertical crossover search

@&#ABSTRACT@&#


               
               
                  How to improve the global search ability without significantly impairing the convergence speed is still a big challenge for most of the meta-heuristic optimization algorithms. In this paper, a concept for the optimization of continuous nonlinear functions applying crisscross optimization algorithm is introduced. The crisscross optimization algorithm is a new search algorithm inspired by Confucian doctrine of gold mean and the crossover operation in genetic algorithm, which has distinct advantages in solution accuracy as well as convergence rate compared to other complex optimization algorithms. The procedures and related concepts of the proposed algorithm are presented. On this basis, we discuss the behavior of the main search operators such as horizontal crossover and vertical crossover. It is just because of the perfect combination of both, leading to a magical effect on improving the convergence speed and solution accuracy when addressing complex optimization problems. Twelve benchmark functions, including unimodal, multimodal, shifted and rotated functions, are used to test the feasibility and efficiency of the proposed algorithm. The experimental results show that the crisscross optimization algorithm has an excellent performance on most of the test functions, compared to other heuristic algorithms. At the end, the crisscross optimization algorithm is successfully applied to the optimization of a large-scale economic dispatch problem in electric power system. It is concluded that the crisscross optimization algorithm is not only robust in solving continuous nonlinear functions, but also suitable for addressing the complex real-world engineering optimization problems.
               
            

@&#INTRODUCTION@&#

In recent decades, many population-based stochastic optimization algorithms have been designed to address different optimization problems. These commonly used algorithms include particle swarm optimization (PSO) [16], ant colony optimization (ACO) [8], genetic algorithm (GA) [10], artificial bee colony (ABC ) [15], gravitational search algorithm (GSA) [32], intelligent water drops (IWD) [43], river formation dynamics (RFA) [33], charged system search (CSS) [18], harmony search algorithm (HSA) [9], invasive weed optimization (IWO) [23], bacterial foraging optimization algorithm (BFO) [28], group search optimizer (GSO) [12], and fruit fly optimization (FOA) [27]. Also, plenty of their variations and combinations are proposed to improve the performance in terms of convergence rate and solution accuracy [1,30,37,39,52,57,59]. These swarm intelligence based algorithms are commonly inspired by observing natural swarming behavior or physical phenomenon. They have exhibited good performance in solving many engineering real-world problems, such as filter optimization[14,40,42], PID parameter optimization [24,34,44], topology structure optimization [20,22], neural network training [11,48,49,56,58], mechanical design [6,17,45], cell formation [19,25], image compression[47], and some other optimization applications [45,46,50,51,55].

Despite many efforts invested so far, the aforementioned algorithms still face some challenges and disadvantages in their utilization. For example, PSO exhibits slow convergence speed and it is liable to suffer from the premature convergence problem when addressing some multimodal problems. GSA and CSS are often time-consuming. Some algorithms like ABC and BFO need to tune several control parameters to maintain the balance between local search and global search. Others manage to achieve the preservation of population diversity at the cost of slow convergence or complicated algorithmic structures. So far, none of the heuristic search algorithms are capable of offering adequately high performance to solve all optimization problems in comparison with other alternatives [54]. Consequently, it remains a challenge to develop a population-based heuristic search algorithm that is able to manage to prevent premature convergence and meanwhile keep the fast-converging feature.

Inspired by Confucian doctrine of gold mean and the crossover operation in genetic algorithm, a novel optimization algorithm called crisscross optimization algorithm (CSO) is introduced. In history, no philosopher has ever affected more individual lives than Confucius in China. One of his important thoughts holds that moderation in all things is the best of rules. Based on this middle-of-the-road concept, we realize the proposed crisscross search strategy by adopting a duo search mechanism including horizontal crossover and vertical crossover, which reproduce a population of moderation solutions at each generation by performing different crossover operations in opposite direction. In the new generation, only those moderation solutions that outperform their counterparts in the parent population can survive. All others are eliminated in the competition. Such competitive mechanism makes the crisscross search always maintain a population in the best position historically in order to accelerate the convergence speed. The innovation of this study consists in three aspects: Firstly, the horizontal crossover in CSO divides the multidimensional problem-solving space into half-population of hypercubes. Each pair of the parent individuals reproduces the offspring in the space of their own hypercube to a greater extent. To reduce the blind spot that cannot be reached, the horizontal crossover also searches the periphery of each hypercube with a smaller probability. This gifts CSO with powerful global search ability. Secondly, the motivation of introducing vertical crossover is to facilitate some stagnant dimensions of the population to escape from the dimension premature convergence. In CSO, vertical crossover is inspired by our observations that the premature convergence of most of the swarm intelligence search algorithms is caused by a few stagnant dimensions of the population. Finally, the combination of horizontal crossover and vertical crossover brings a magical effect on improving the convergence speed and solution accuracy. Once certain stagnant dimension of an individual jumps out of the local minima in the vertical crossover operation, it will spread rapidly through the whole swarm by horizontal crossover. That, in turn, facilitates the other stagnant dimensions to jump out the local minima as quickly as possible in the vertical crossover operation. It is just this crisscross search behavior resembling the chain reaction that enables CSO the competitive advantage over other heuristic algorithms in terms of global search ability and convergence speed.

The performance of the CSO algorithm is verified by twelve well-known benchmark functions. At the end, the CSO algorithm is applied in a complex constrained unit commitment optimization problem with 40 thermal generating units in power system. The satisfactory results demonstrate that the proposed new algorithm is not only appropriate for the optimization of continuous nonlinear functions, but also suitable for addressing complex optimization problems in engineering and science.

The rest of this paper is organized as follows: Some related concepts and procedures of the crisscross optimization algorithm are introduced in Section 2. Section 3 discusses the search behavior of horizontal crossover and vertical crossover in CSO. Section 4 uses twelve benchmark test functions to verify the performance of the proposed algorithm. In Section 5, the performance of CSO is measured by addressing a complex economic dispatch problem in 40-unit power system. The conclusion and future work are included in Section 6.

The crisscross optimization algorithm (CSO) is a new population-based stochastic search algorithm consisting of horizontal crossover as well as vertical crossover, which execute in sequence within iteration. By incorporating a simple competitive mechanism, the horizontal crossover and the vertical crossover are merged perfectly. Similar to other swarm intelligence like PSO, the CSO evolutionary process is based on the population. In CSO, the population is updated twice per iteration by horizontal crossover and vertical crossover, respectively. The updated populations are then reselected twice by the competitive operation. To clarify the procedure of the crisscross optimization algorithm, several related concepts are explained as follows.

For the sake of convenience, the population in the CSO algorithm is represented by matrix X. Where, each row is a potential solution represented by X(i) called individual. X(i,
                     j) is the ith individual with jth dimension. The number of rows and columns in the matrix are the size of population M and the dimensions D in problem-solving space, respectively. The solutions generated by the horizontal crossover and the vertical crossover are called moderation solutions represented by MS
                     
                        hc
                      and MS
                     
                        vc
                     . The solutions updated by the competitive operator are called dominant solutions represented by DS
                     
                        hc
                      and DS
                     
                        vc
                     . The population of moderation solutions generated by horizontal crossover competes with the population of dominant solutions obtained by the competitive operation after vertical crossover. Similarly, the population of moderation solutions generated by vertical crossover competes with the population of dominant solutions obtained by the competitive operation after horizontal crossover.

The procedure of crossover optimization algorithm is summarized as follows:
                        
                           step 1. Initialize the population.

step 2. Perform horizontal crossover with the competitive operator.

step 3. Perform vertical crossover with the competitive operator.

step 4. Terminal Condition: If the number of iterations is larger than the predefined maximum number, the process terminates. Otherwise, go to Step 2 for a new round of iteration.

The detailed explanation for steps 2 and 3 is given in the following sections.

The horizontal crossover is an arithmetic crossover operated on all the dimensions between two different individuals. Suppose the ith parent individual X(i) and the jth parent individual X(j) are used to carry out the horizontal crossover operation at the dth dimension, their offspring can be reproduced through the following equation:
                           
                              (1)
                              
                                 
                                    
                                       MS
                                    
                                    
                                       hc
                                    
                                 
                                 (
                                 i
                                 ,
                                 d
                                 )
                                 =
                                 
                                    
                                       r
                                    
                                    
                                       1
                                    
                                 
                                 ·
                                 X
                                 (
                                 i
                                 ,
                                 d
                                 )
                                 +
                                 (
                                 1
                                 -
                                 
                                    
                                       r
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 ·
                                 X
                                 (
                                 j
                                 ,
                                 d
                                 )
                                 +
                                 
                                    
                                       c
                                    
                                    
                                       1
                                    
                                 
                                 ·
                                 (
                                 X
                                 (
                                 i
                                 ,
                                 d
                                 )
                                 -
                                 X
                                 (
                                 j
                                 ,
                                 d
                                 )
                                 )
                              
                           
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       MS
                                    
                                    
                                       hc
                                    
                                 
                                 (
                                 j
                                 ,
                                 d
                                 )
                                 =
                                 
                                    
                                       r
                                    
                                    
                                       2
                                    
                                 
                                 ·
                                 X
                                 (
                                 j
                                 ,
                                 d
                                 )
                                 +
                                 (
                                 1
                                 -
                                 
                                    
                                       r
                                    
                                    
                                       2
                                    
                                 
                                 )
                                 ·
                                 X
                                 (
                                 i
                                 ,
                                 d
                                 )
                                 +
                                 
                                    
                                       c
                                    
                                    
                                       2
                                    
                                 
                                 ·
                                 (
                                 X
                                 (
                                 j
                                 ,
                                 d
                                 )
                                 -
                                 X
                                 (
                                 i
                                 ,
                                 d
                                 )
                                 )
                              
                           
                        where r
                        1 and r
                        2 are uniformly distributed random values between 0 and 1, c
                        1 and c
                        2 are uniformly distributed random values between −1 and 1. MS
                        
                           hc
                        (i,
                        d) and MS
                        
                           hc
                        (j,
                        d) are the moderation solutions that are the offspring of X(i,
                        d) and X(j,
                        d), respectively.

According to Eqs. (1) and (2), in a multidimensional solution space, the horizontal crossover searches for the new solutions (i.e., MS
                        
                           hc
                        (i)) in a hypercube space that takes the two paired parent individuals (i.e. X(i) and X(j),) as its diagonal vertices with a larger probability. Meanwhile, the horizontal crossover might sample the new positions on the periphery of the hypercube with a smaller probability in order to reduce the blind spot that cannot be searched by the parent individuals. This cross-border search mechanism of the horizontal crossover distinguishes itself from the genetic algorithm. Further explanation can be referred to the search behavior of horizontal crossover in the next section.

The procedure of the horizontal crossover search is given in Fig. 1
                        . Within an iteration, the horizontal crossover search takes a population of dominant solutions (DS
                        
                           vc
                        ) achieved by the vertical crossover as its parent population except for the first iteration. To conduct a horizontal crossover search, it is necessary to make a pair for the individuals in matrix DS
                        
                           vc
                        . This process is implemented by line 3, which performs a random permutation of the integers from 1 to M. The chosen individuals such as X(no1) and X(no2) generate their offspring (i.e. the moderation solutions: MS
                        
                           hc
                        (no1) and MS
                        
                           hc
                        (no2)) according to Eqs. (1) and (2). It is worth noting that the horizontal crossover probability P
                        1 is generally set to 1 at most situations in order to find better solutions as many as possible. Another important parameter is the expansion coefficient c
                        1 or c
                        2 that has a great influence on the search scope of an individual. In this article, c
                        1 or c
                        2 is a uniformly distributed random value between −1 and 1. When the moderation solutions represented by MS
                        
                           hc
                         are generated, the next step is to perform the competitive operation between MS
                        
                           hc
                         and its parent population X (i.e., DS
                        
                           vc
                        ). Only the competition winners can survive and be stored into the matrix DS
                        
                           hc
                        .

The vertical crossover is an arithmetic crossover operated on all the individuals between two different dimensions. Suppose the d1th and d2th dimensions of the individual i are used to carry out the vertical crossover operation, their offspring MS
                        
                           vc
                        (i) can be reproduced by Eq. (3).
                           
                              (3)
                              
                                 
                                    
                                       MS
                                    
                                    
                                       vc
                                    
                                 
                                 (
                                 i
                                 ,
                                 d
                                 1
                                 )
                                 =
                                 r
                                 ·
                                 X
                                 (
                                 i
                                 ,
                                 d
                                 1
                                 )
                                 +
                                 (
                                 1
                                 -
                                 r
                                 )
                                 ·
                                 X
                                 (
                                 i
                                 ,
                                 d
                                 2
                                 )
                                 
                                 i
                                 ∈
                                 N
                                 (
                                 1
                                 ,
                                 M
                                 )
                                 ,
                                 
                                 d
                                 1
                                 ,
                                 d
                                 2
                                 ∈
                                 N
                                 (
                                 1
                                 ,
                                 D
                                 )
                              
                           
                        where r is a uniformly distributed random value between 0 and 1. MS
                        
                           vc
                        (i,d1) is the offspring of X(i,
                        d1) and X(i,
                        d2) (i.e., DS
                        
                           hc
                        (i,
                        d1) and DS
                        
                           hc
                        (i,
                        d2)).

The procedure of the vertical crossover search is given in Fig. 2
                        . During all of the iterative process, the parent population of the vertical crossover search is the population of dominant solutions (DS
                        
                           hc
                        ) from the horizontal crossover. Besides, there are several aspects for the vertical crossover that are apparently different from the horizontal crossover. Firstly, since each dimension of the individual solution may have different upper and lower bounds, it is necessary to undertake the normalization operation according to the upper and lower bound of each dimension to make sure that the offspring reproduced by the vertical crossover can locate within the boundary of each dimension. Secondly, the vertical crossover occurs between different dimensions of the same individual. It seems inconceivable, but it is really an efficient way to prevent the dimensions of the swarm from trapping into the local minima. Finally, each vertical crossover operation only generates a single offspring in order to provide an opportunity for the stagnant dimension to jump out of the local optima and not destroy another dimension that is probably global optimal. Consequently, the vertical crossover probability P
                        2 is less than the horizontal crossover probability P
                        1 (is set to 1 at most cases) based on the fact that only a few dimensions of the swarm are possibly trapped into the local minima at most situations. Numerous experiments show that it is appropriate to set P
                        2 from 0.2 to 08. The competitive operation of the vertical crossover between MS
                        
                           vc
                         and its parent population X (i.e. DS
                        
                           hc
                        ) is similar to that of the horizontal crossover.

The competitive operator provides an opportunity for the competition between the offspring population and its parent population. For example, as far as the horizontal crossover is concerned, only if its offspring individual (i.e. the moderation solutions MS
                        
                           hc
                        (i) outperforms its parent individual X(i) (i.e. the dominant solution DS
                        
                           vc
                        (i) after the vertical crossover), can it survive and be saved in DS
                        
                           vc
                        (i). Otherwise, the parent individual survives. The vertical crossover has a similar operation with regard to the competitive operator. It is the simplicity of this competitive mechanism that makes the population move rapidly to the search region with better fitness and quicken the converge rate to the global optima. The procedure of competitive operation is given in Fig. 3
                        .

Since the CSO algorithm merges both of horizontal crossover and vertical crossover, it is natural that the CSO algorithm inherits their respective advantages. From the perspective of information recognition, the vertical crossover searches the new positions based on self-recognition of the individuals, while the horizontal crossover searches the new moderation solutions based on the social recognition among individuals. The integration of these two cognitive capabilities into the CSO algorithm makes it greatly outperform either the horizontal crossover or the vertical crossover.

We take one dimensional space as an exemplar to analyze the behavior of horizontal crossover. The multidimensional case can be derived similarly.

The horizontal crossover searches for new positions according to Eqs. (1) and (2). We split Eq. (1) into two parts and simplify them in the form of Eqs. (4)–(6).
                           
                              (4)
                              
                                 Z
                                 =
                                 
                                    
                                       X
                                    
                                    
                                       1
                                    
                                 
                                 +
                                 
                                    
                                       X
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    
                                       X
                                    
                                    
                                       1
                                    
                                 
                                 =
                                 
                                    
                                       r
                                    
                                    
                                       1
                                    
                                 
                                 ·
                                 
                                    
                                       x
                                    
                                    
                                       1
                                    
                                 
                                 +
                                 (
                                 1
                                 -
                                 
                                    
                                       r
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 ·
                                 
                                    
                                       x
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 
                                    
                                       X
                                    
                                    
                                       2
                                    
                                 
                                 =
                                 c
                                 ·
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       2
                                    
                                 
                                 -
                                 
                                    
                                       x
                                    
                                    
                                       1
                                    
                                 
                                 )
                              
                           
                        
                     

For the sake of convenience, assume x
                        1
                        =0 and x
                        2
                        =1. In this case, the probability density of X
                        1 and X
                        2 can be expressed by Eqs. (7) and (8):
                           
                              (7)
                              
                                 
                                    
                                       f
                                    
                                    
                                       
                                          
                                             X
                                          
                                          
                                             1
                                          
                                       
                                    
                                 
                                 =
                                 1
                                 
                                 
                                    
                                       X
                                    
                                    
                                       1
                                    
                                 
                                 ∈
                                 (
                                 0
                                 ,
                                 1
                                 )
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    
                                       f
                                    
                                    
                                       
                                          
                                             X
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                 
                                    
                                       X
                                    
                                    
                                       2
                                    
                                 
                                 ∈
                                 (
                                 -
                                 1
                                 ,
                                 1
                                 )
                              
                           
                        Since X
                        1 and X
                        2 are two independent variables, the probability density of Z
                           
                              (9)
                              
                                 
                                    
                                       f
                                    
                                    
                                       Z
                                    
                                 
                                 (
                                 z
                                 )
                                 =
                                 
                                    ∫
                                    
                                       -
                                       ∞
                                    
                                    
                                       ∞
                                    
                                 
                                 
                                    
                                       f
                                    
                                    
                                       
                                          
                                             X
                                          
                                          
                                             1
                                          
                                       
                                    
                                 
                                 (
                                 
                                    
                                       x
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 
                                    
                                       f
                                    
                                    
                                       
                                          
                                             X
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                                 (
                                 z
                                 -
                                 
                                    
                                       x
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 
                                    
                                       dx
                                    
                                    
                                       1
                                    
                                 
                              
                           
                        According to 
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                0
                                                <
                                                
                                                   
                                                      X
                                                   
                                                   
                                                      1
                                                   
                                                
                                                <
                                                1
                                             
                                          
                                          
                                             
                                                -
                                                1
                                                <
                                                Z
                                                -
                                                
                                                   
                                                      X
                                                   
                                                   
                                                      1
                                                   
                                                
                                                <
                                                1
                                             
                                          
                                       
                                    
                                 
                              
                              ⇒
                              
                                 
                                    
                                       
                                          
                                             
                                                0
                                                <
                                                
                                                   
                                                      X
                                                   
                                                   
                                                      1
                                                   
                                                
                                                <
                                                1
                                             
                                          
                                          
                                             
                                                Z
                                                -
                                                1
                                                <
                                                
                                                   
                                                      X
                                                   
                                                   
                                                      1
                                                   
                                                
                                                <
                                                Z
                                                +
                                                1
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        Z Can be divided into three ranges: (−1,0), (0,1), and (1,2).

When z
                        ∈(−1,0)
                           
                              (10)
                              
                                 
                                    
                                       f
                                    
                                    
                                       Z
                                    
                                 
                                 (
                                 z
                                 )
                                 =
                                 
                                    ∫
                                    
                                       0
                                    
                                    
                                       z
                                       +
                                       1
                                    
                                 
                                 1
                                 ×
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                 dx
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 x
                                 
                                    
                                       |
                                    
                                    
                                       0
                                    
                                    
                                       z
                                       +
                                       1
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 (
                                 z
                                 +
                                 1
                                 )
                              
                           
                        
                     

When z
                        ∈(0,1)
                           
                              (11)
                              
                                 
                                    
                                       f
                                    
                                    
                                       Z
                                    
                                 
                                 (
                                 z
                                 )
                                 =
                                 
                                    ∫
                                    
                                       0
                                    
                                    
                                       1
                                    
                                 
                                 1
                                 ×
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                 dx
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                    
                                       |
                                    
                                    
                                       0
                                    
                                    
                                       1
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        
                     

When z
                        ∈(1,2)
                           
                              (12)
                              
                                 
                                    
                                       f
                                    
                                    
                                       Z
                                    
                                 
                                 (
                                 z
                                 )
                                 =
                                 
                                    ∫
                                    
                                       z
                                       -
                                       1
                                    
                                    
                                       1
                                    
                                 
                                 1
                                 ×
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                 dx
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 x
                                 
                                    
                                       |
                                    
                                    
                                       z
                                       -
                                       1
                                    
                                    
                                       1
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 (
                                 2
                                 -
                                 z
                                 )
                              
                           
                        According to Eqs. (10)–(12), the probability density of Z is shown in Fig. 4
                        
                     

For 2-D solution space, the probability density can be derived similarly, and it is shown in Fig. 5
                        .

It can be seen that, for 1-dimension space, the horizontal crossover samples the new positions within the range of [x
                        1,
                        x
                        2] with a larger probability, but outside the range with a decreasing probability. Since the zone of the central square that takes the coordinate (x
                        1,
                        y
                        1) and (x
                        2,
                        y
                        2) as its diagonal vertices holds the highest value of probability density in Fig. 5, it becomes the focus of searching the new moderation solutions. The periphery of the square is sampled with a decreasing probability. For 3-dimension or high dimension space of horizontal crossover, the situation is similar. The sampling area is mainly centralized on a cube or hypercube nearby.

Based on the analysis above, it is concluded that the horizontal crossover search divides the multidimensional problem-solving space into half-population of hypercubes that take the paired parent individuals (e.g. X(i) and X(j)) as their diagonal vertices assuming that the horizontal crossover probability is set to 1. Apparently, each pair of the parent individuals reproduces the offspring in the space of their own hypercube to a greater extent. To reduce the blind spot that cannot be reached, the horizontal crossover also searches the periphery of each hypercube with a smaller probability. Such search mechanism not only makes sure that the horizontal crossover focuses on sampling the better positions in different search spaces (i.e. hypercubes), but also enhances the global search ability in the solution space (see Table 2 and Figs. 7–9).

As a unique feature of the CSO algorithm, the vertical crossover plays an important role in the search process. It is not only a highly efficient search method for complex functions with rotation (see f
                        10–f
                        12 in Table 1
                         and Fig. 9), but also an efficient way to prevent the dimensions of the population from trapping into the local minima. As a matter of fact, an interesting finding is that the main reason that most of the swarm intelligence search algorithms (including the horizontal crossover search) converge to the local optima is because that a few dimensions of the population are possibly struck stagnant. With this in mind, the vertical crossover is implemented by exchanging the useful information between different dimensions of the same individual. Such operation makes possible for the stagnant dimensions to escape from the local minima by exchanging good information (moderation solutions achieved) with other “normal” dimensions. Once certain stagnant dimension of an individual jumps out of the local minima, it spreads rapidly through the whole swarm by the horizontal crossover operation. That, in turn, facilitates the other stagnant dimensions to jump out the local minima as quickly as possible by the vertical crossover operation. It is just because of the crisscross operation on both horizontal and vertical directions that makes the CSO have unique global search ability on addressing multimodal problems with many local minima. It is worthwhile to note that the operations on normalization and reverse normalization are needed to perform vertical crossover, considering different limit bounds for each dimension. Unlike other mutation operators in literature, the vertical crossover search for new positions by updating several whole dimensions of the parent population all at once. This search behavior is especially useful in the CSO algorithm when optimizing the complex functions such as the multimodal function f
                        4 (Rastrigin) and the rotated function f
                        10–f
                        11 (i.e., Rotated High Conditioned Elliptic and Rotated Rastrigin’s) (see Table 1). When addressing such function optimization problems, the horizontal crossover search is often stuck into the local minima, but the vertical crossover is able to facilitate these stagnant dimensions to jump out of the local minimum (see Table 2
                        , Figs. 8 and 10).

The parameters’ setting is a ubiquitous problem for most stochastic optimization algorithms. In CSO, although there are two crossover operators (i.e., horizontal crossover and vertical crossover), only the vertical crossover probability needs to be set. Because CSO always manages to maintain a population of “pbest” solutions (dominant solutions) by using the competitive operator, it is natural for the horizontal crossover operator to reproduce as many potentially competitive offspring (moderation solutions) as possible. Accordingly, the horizontal crossover probability P
                        1 is recommended to be set to 1 for all optimization problems.

As for the vertical crossover probability P
                        2, we investigated the effect of P
                        2 on CSO by using several benchmark functions listed in Table 1, and the results are shown in Fig. 6
                        . In the figure, the ordinate is the average error in 30 independent runs with P
                        2 in the range [0,1]. The results show that, for the unimodal (f
                        1–f
                        3) or the multimodal problems (f
                        4–f
                        6) without coordinate shift or rotation, CSO can always converge to the global minimum point (zero) without any inaccuracy for whatever the value of P
                        2 is set in the range [0,1]. Therefore, for the relatively simple unimodal or multimodal optimization problems, the vertical crossover probability P
                        2 is recommended to be set to 0 in order to save half of the function evaluations (FEs). For more complex optimization problems such as functions with coordinate shift or rotation, if the vertical crossover operation is removed from CSO (i.e., P
                        2
                        =0), we observed that about 10–40% of dimensions in the population might simultaneously suffer from stagnancy while the rest of dimensions seems to be normal. In particular, for the shifted functions f
                        7, f
                        9 and the rotated functions f
                        10, f
                        11, we found that, in all 30 trials, the number of stagnant dimensions accounts for about 10–30% and 20–40%, respectively. In order to facilitate these stagnant dimensions to escape from the premature convergence, the vertical crossover operator in CSO provides a novel mechanism for these stagnant dimensions to mutate. As described in Section 2.2, one vertical crossover operation only reproduces a single offspring, For example, the value of P
                        2
                        =0.8 indicates that 80% of the dimensions participate in the vertical crossover operation, but the actual number of dimensions to be mutated occupies only 40%. Many empirical tests show that a value of P
                        2 in the range [0.2,0.8] offers better performance when optimizing such complex problems as functions with shift or rotation. In particular, we suggest that P
                        2 is to be set in the range [0.2,0.6]. For the rotated functions, P
                        2 is set in the range [0.6,0.8].

To exhibit CSO’s behavior and performance, two groups of experiments on 12 benchmark functions shown in Table 1 
                     [36] are conducted. The first group of experimental tests aims to exhibit the role of horizontal crossover and vertical crossover in CSO. The second one is used to validate the performance of CSO by comparing with other popular heuristic algorithm including particle swarm optimization (PSO) [16,38], quantum-behaved particle swarm optimization (QPSO) [41], and electromagnetism-like mechanism algorithm (ELM) [3].

The test functions (f
                        1–f
                        12) adopted in this paper can be classified into four groups. f
                        1–f
                        3 are unimodal functions. f
                        4–f
                        6 are multimodal functions with many local optima, which can be used to test the global search ability of the CSO algorithm. f
                        7–f
                        9 and f
                        10–f
                        11 are shifted functions and rotated functions, respectively. They are more complicated functions and can be used to validate the comprehensive performance of the CSO algorithm.

In Table 1, the parameter ξ stands for the acceptable error that is defined as the difference between the global optimum and the best value achieved by any algorithm in a trial. The trial is considered successful if and only if the error satisfies error ⩽ξ. The column “domain” defines the lower and upper bounds of the definition domain in all dimensions.

In the experiments, the parameters of all the algorithms are set as follows: In CSO, The horizontal crossover probability is set to P
                        1
                        =1 and the vertical crossover probability is set to P
                        2
                        =0.8. In PSO, the inertia weight is set to ω
                        =0.4, the acceleration coefficients are set to c
                        1
                        =
                        c
                        2
                        =2.0 according to the classic configurations for the original PSO [36]. In QPSO, the creative coefficient beta decreases from 1.0 to 0.5 linearly in all 30 runs according to the suggestion in [31]. In all experimental tests, the number of dimensions of all the test functions is set to D
                        =30. The population size is set to M
                        =40. The maximum number of iterations is set to MaxIter=2000 for f
                        1–f
                        6, and MaxIter=5000 for f
                        7–f
                        12. To reduce statistical errors, each test is repeated 30 times independently. All algorithms are programmed in Matlab2010a, and the simulations are executed on a PC with a Core (TM) 2 Duo CPU P8400 running at 2.26GHz with memory capacity of 1.94GB under Windows XP Professional Operating System.

@&#EXPERIMENTAL RESULTS@&#

The CSO algorithm integrates horizontal crossover with vertical crossover. However, the horizontal crossover or the vertical crossover can be separately performed as a single search algorithm. To exhibit their respective search behavior and search capability, besides the CSO algorithm, the horizontal crossover search and the vertical crossover search (called Horizontal-Crossover and Vertical-Crossover, respectively) are also used to optimize the test functions. The optimization results are given in Table 2 and the typical convergence graphs are illustrated in Figs. 7–10
                        
                        
                        
                        .

In Table 2, the minimum errors, the mean error and the standard deviation obtained by CSO, Horizontal-Crossover and Vertical-Crossover after the predefined MaxIter (2000 for f
                        1–f
                        6 and 5000 for f
                        7–f
                        12) are reported. In the experiments, it can be found that Horizontal-Crossover outperforms Vertical-Crossover on f
                        1–f
                        3, f
                        5–f
                        9. However, Vertical-Crossover outperforms Horizontal-Crossover on f
                        4 and f
                        10–f
                        12. Although Horizontal-Crossover has better performance on most of the test functions compared to Vertical-Crossover (see Figs. 6–8), it might still trap into the local optima when optimizing the multimodal functions (e.g. f
                        4) or the rotated functions (e.g. f
                        10–f
                        12). As an important component of the CSO algorithm, Vertical-Crossover really works well on complex rotated functions such as f
                        10–f
                        12, which is difficult to be addressed by Horizontal-Crossover. By combining the advantages of both horizontal crossover and vertical crossover, the CSO algorithm is endowed with more powerful capability of addressing complex functions optimization problem. From Table 2, we can see that the mean errors for f
                        1–f
                        6 are zero, and the mean errors for f
                        7–f
                        12 are generally far less than the acceptable error ξ. The experimental results show that the integration of Horizontal-Crossover and Vertical-Crossover enables CSO to have the powerful global search ability on almost all of the tested benchmark functions.

To further validate the performance of CSO, we compare it with PSO, ELM, and QPSO in terms of the minimum error, the mean error, the standard deviation, and the CPU time. The comparative results obtained by all the algorithms are reported in Table 3
                        . According to the experimental results, it can be clearly observed that CSO has overwhelming superiority over other algorithms in terms of convergence accuracy. In particular, on the unimodal functions f
                        1–f
                        3, the results show that all the algorithms have a good performance, but only CSO can always converge to the global minimum point (zero) without any accuracy error. For multimodal functions f
                        4–f
                        6 with many local minima, the global minima becomes more difficult to locate. As CSO incorporates with the vertical crossover technique, we expect that CSO can escape from the local minima and enhance the performance on the multimodal functions. The experimental results on f
                        4–f
                        6 prove that CSO does have powerful global search ability when addressing multimodal problems. Indeed, CSO can also converge to the global optimum on all the multimodal functions f
                        4–f
                        6 in all 30 runs. For shifted and rotated functions f
                        7–f
                        12, due to the coordinate shift or rotation, it increases the difficulty for algorithms to search for the global optimum. The results show that, except for f
                        12, CSO outperforms PSO, ELM, and QPSO on f
                        7–f
                        11 significantly. This further demonstrated the effectiveness of CSO in addressing complex function problems.

As far as computing time is concerned, except that ELM consumes less time on f
                        9–f
                        12, CSO spends less computing time on the rest of test functions than all other algorithms. To determine the statistical significance of the difference between two independent samples obtained by CSO and any of the compared algorithms, we use the two samples t-test testing method to compare the results. The results of t-tests shown in Table 3 also show that CSO has overwhelming superiority over the other compared algorithms.

As one of the most important optimization problems in power system, the objective of the economic dispatch (ED) is to minimize the generation cost by the reasonable allocation of power generation among the committed generating units. The ED is a difficult unit commitment optimization problem characterized by non-convexity, non-linearity and non-differentiability and high dimensionality. In recent years, many related studies are conducted for addressing the ED problem by applying various swarm based intelligent search algorithms such as evolutionary programming and its improved version [35,53], modified particle swarm optimization algorithms [7,13,31], evolutionary strategy optimization [31], improved genetic algorithm [4,5,8] and other heuristic algorithms [10,17,21,26,54]. To further verify the performance of CSO, several compared algorithms are used to optimize the complex constrained unit commitment problem with 40 thermal generating units considering the valve-point loading.

The ED problem can be described as an optimization (minimization) process with the objective:
                           
                              (13)
                              
                                 Min
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 
                                    
                                       P
                                    
                                    
                                       i
                                    
                                 
                                 )
                              
                           
                        where F
                        
                           i
                         is the fuel cost function of the ith generating unit, P
                        
                           i
                         is the power generated by the ith generating unit, and n is the number of committed generating units. Subject to the power balance constraint:
                           
                              (14)
                              
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       P
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       P
                                    
                                    
                                       L
                                    
                                 
                                 +
                                 
                                    
                                       P
                                    
                                    
                                       D
                                    
                                 
                              
                           
                        where P
                        
                           L
                         is the system load demand and P
                        
                           D
                         is the transmission loss, and generating capacity constraints:
                           
                              (15)
                              
                                 
                                    
                                       P
                                    
                                    
                                       i
                                       
                                          min
                                       
                                    
                                 
                                 ⩽
                                 
                                    
                                       P
                                    
                                    
                                       i
                                    
                                 
                                 ⩽
                                 
                                    
                                       P
                                    
                                    
                                       i
                                       
                                          max
                                       
                                    
                                 
                              
                           
                        where P
                        
                           imin and P
                        
                           imax are the minimum and maximum power outputs of the generating unit.

The fuel-cost function considering valve-point loadings of the generating units is given as:
                           
                              (16)
                              
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 
                                    
                                       P
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       a
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       P
                                    
                                    
                                       i
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       b
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       P
                                    
                                    
                                       i
                                    
                                 
                                 +
                                 
                                    
                                       c
                                    
                                    
                                       i
                                    
                                 
                                 +
                                 |
                                 
                                    
                                       e
                                    
                                    
                                       i
                                    
                                 
                                 ·
                                 sin
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       i
                                    
                                 
                                 ·
                                 (
                                 
                                    
                                       P
                                    
                                    
                                       i
                                       
                                          min
                                       
                                    
                                 
                                 -
                                 
                                    
                                       P
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 )
                                 |
                              
                           
                        where a
                        
                           i
                        , b
                        
                           i
                        , c
                        
                           i
                         and are the fuel-cost coefficients of the ith unit, e
                        
                           i
                         and f
                        
                           i
                         are the fuel cost-coefficients of the ith unit with valve-point effects.

Different from the unconstrained function optimization problem, the unit commitment optimization needs to satisfy the constraints imposed by the power balance. To deal with such constraint, the penalty coefficient method is adopted. The modified objective function that is used to evaluate the individual solutions in the CSO algorithm is shown as Eq. (17).
                           
                              (17)
                              
                                 f
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          n
                                       
                                    
                                 
                                 
                                    
                                       a
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       P
                                    
                                    
                                       i
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 
                                    
                                       b
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       P
                                    
                                    
                                       i
                                    
                                 
                                 +
                                 
                                    
                                       c
                                    
                                    
                                       i
                                    
                                 
                                 +
                                 |
                                 
                                    
                                       e
                                    
                                    
                                       i
                                    
                                 
                                 ·
                                 sin
                                 (
                                 
                                    
                                       f
                                    
                                    
                                       i
                                    
                                 
                                 ·
                                 (
                                 
                                    
                                       P
                                    
                                    
                                       i
                                       
                                          min
                                       
                                    
                                 
                                 -
                                 
                                    
                                       P
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 )
                                 |
                                 +
                                 
                                    
                                       P
                                    
                                    
                                       f
                                    
                                 
                                 ·
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                
                                                
                                                   n
                                                
                                             
                                          
                                          
                                             
                                                P
                                             
                                             
                                                i
                                             
                                          
                                          -
                                          
                                             
                                                P
                                             
                                             
                                                L
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where P
                        
                           f
                         is penalty factor that is set to 100 [2].

The test case for the ED problem consists of 40 generating units considering the valve-point loading. The data of generating units [35] is shown in Table 4
                        .

To demonstrate the effectiveness of the proposed CSO algorithm in practical applications, the CSO algorithm is used to solve the complex ED problem with 40 thermal generating units considering the valve-point loading effect. The solution quality obtained by CSO is compared with that of PSO, QPSO, and ELM adopted in the function optimization problems. We also compare CSO with some more complicated heuristic algorithms such as IFEP [35], MPSO [29], ESO [31], and IGA-MU [4].

In the experiments, the parameters of the ED problem are set as following: the number of control variables D
                        =40, the fuel-cost coefficients (i.e. a, b, c, e, and f) for each generating unit is shown in Table 4, the system loads demand P
                        
                           L
                        
                        =10,500MW, the penalty factor of objective functions P
                        
                           f
                        
                        =100. For CSO, PSO, QPSO and ELM, the population size is set to M
                        =30, the maximum number of iterations is set to MaxIter=3000, other parameters are set to the same as those used in the function optimization problems. To reduce statistical errors, the test is repeated 50 times independently for each algorithm.

The simulation results including the mean fuel-cost, the maximum fuel-cost, the minimum fuel-cost, and the mean CPU time obtained by CSO and other compared algorithms are shown in Table 5
                        . In terms of solution quality, we see that the generation cost achieved by CSO algorithm is less than that obtained by the original algorithms like PSO and ELM significantly. It is worthwhile to note that CSO also outperforms all the other complicated heuristic algorithms including QPSO, IFEP, MPSO, ESO, and IGA-MU. The results demonstrate the effectiveness and practicality of the proposed CSO algorithm in addressing the large scale ED problems. In stability, we can observe that the standard deviation achieved by CSO is smallest among the compared algorithms, which shows that CSO has more stable performance. With respect to the computing time, except for PSO, CSO consumes less CPU time in all 50 runs. Although the CSO algorithm needs to spend a bit more time than PSO for each run because CSO needs to evaluate the objective functions as many again as PSO per iteration, its global convergence rate is much faster than that of PSO. The fast-converging ability of CSO is clearly illustrated by Fig. 11
                        .


                        Table 6
                         shows the best solutions for the power output allocation obtained by PSO, QPSO, ELM, MPSO, ESO, IGA-MU, and the proposed CSO. The data of MPSO, ESO, and IGA-MU are obtained by the corresponding references. The results show that not only CSO obtains the best solution among all the compared algorithms, but also completely meets the load demand constraint.

@&#CONCLUSIONS@&#

In this study, a novel optimization algorithm called crisscross optimization algorithm is introduced. CSO fuses the horizontal crossover search as well as the vertical crossover search to find the global optima in the problem-solving space. In CSO, the horizontal crossover focus on sampling the better positions in different search spaces (i.e. hypercubes and their respective peripheries), while the vertical crossover is designed to facilitate the stagnant dimensions of population to jump out of local optima. The integration of both crossover operators through the competitive operator gifts the CSO algorithm with obviously competitive advantage over other heuristic algorithms in terms of global search ability and convergence speed. Although CSO needs to evaluate the objective functions as many again as other heuristic algorithms like PSO per iteration, it exhibits a faster converging behavior. In addition, in CSO, there is only one parameter (i.e., the vertical crossover probability) that needs to be tuned. Many empirical tests show that a value of P2 in the range [0.2,0.8] offers better performance when optimizing complex problems such as functions with shift or rotation.

The successful application of the CSO algorithm to addressing the benchmark functions optimization and the ED problems indicates that the CSO has great potential in solving complex optimization problems in engineering and science. Further research remains to be conducted on some concepts underlying the CSO algorithm. At present, we are preparing to apply the CSO algorithm to the optimization of more complex engineering problems with high dimensions for further evaluation of its performance.

@&#ACKNOWLEDGMENTS@&#

This research work is supported by key laboratory project on power energy saving and new energy technology (IDSYS200701) and by the National Natural Science Fund Project of China (51307025).

@&#REFERENCES@&#

