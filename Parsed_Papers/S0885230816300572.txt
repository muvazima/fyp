@&#MAIN-TITLE@&#Sentence alignment using local and global information

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We propose an integer linear programming algorithm to extract parallel sentences.


                        
                        
                           
                           We build an English–Persian parallel corpus from Wikipedia articles.


                        
                        
                           
                           Intrinsic evaluation using gold data shows the effectiveness of the ILP method.


                        
                        
                           
                           Extrinsic evaluation via SMT and CLIR confirms high quality of the created corpus.


                        
                        
                           
                           The extracted parallel corpus is freely available for research purposes.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Parallel corpus

Sentence alignment

Bilingual resource

Global information

Integer linear programming

@&#ABSTRACT@&#


               
               
                  Parallel corpora are essential resources for statistical machine translation (SMT) and cross language information retrieval (CLIR) systems. Creating parallel corpora is highly expensive in terms of both time and cost. In this paper, we propose a novel approach to automatically extract parallel sentences from aligned documents. To do so, we first train a Maximum Entropy binary classifier to compute the local similarity between each two sentences in different languages. To consider global information (e.g., the position of sentence pairs in the aligned documents), we define an objective function to penalize the cross alignments and then propose an integer linear programming approach to optimize the objective function. In our experiments, we focus on English and Persian Wikipedia articles. The experimental results on manually aligned test data indicate that the proposed method outperforms the baselines, significantly. Furthermore, the extrinsic evaluations of the corpus extracted from Wikipedia on both SMT and CLIR systems demonstrate the quality of the extracted parallel sentences. In addition, Experiments on the English–German language pair demonstrate that the proposed ILP method is a language-independent sentence alignment approach. The extracted English–Persian parallel corpus is freely available for research purposes.
               
            

@&#INTRODUCTION@&#

Statistical translation techniques have been shown to perform promising when they are trained using a large amount of high quality data. One essential category of resources for training statistical machine translation (SMT) and cross language information retrieval (CLIR) systems is large and accurate parallel texts (Brown et al., 1991; Vogel and Tribble, 2002). It is shown that training translation techniques via sentence-level aligned corpora, henceforth called parallel corpora, is more effective than using other types of parallel texts (Ma, 2006). Parallel corpora contain several sentence pairs in two (source and target) languages, which are translations of each other (Gale and Church, 1993).

It is known that manual sentence alignment is very expensive in terms of both time and cost. This fact has motivated researchers to automatically extract parallel sentences from the available bilingual data (Gale and Church, 1993; Wu, 1994; Li et al., 2010; Pal et al., 2014). The exponential growth of the Web, and thus the availability of huge amounts of textual data in various languages intensifies the importance of automatic sentence alignment (Resnik and Smith, 2003).

A main part of automatic sentence aligners is computing the similarity between the source and the target sentences. Several methods have been so far proposed to compute length-based, lexicon-based, and hybrid similarities between two given sentences (Ma, 2006; Gale and Church, 1993; Wu, 1994). We refer to this kind of information that can be captured from the source and the target sentences as “local” information. Although local information can show the similarity between two sentences, it cannot consider the information that comes from the other sentences in the documents. We refer to the information related to the other sentences of the documents as “global” information. In this paper, we propose a language-independent method for extracting parallel sentences
                        1
                     
                     
                        1
                        Note that although the sentences aligned by the automatic aligners are not always exact translations of each other, they can be considered as parallel sentences for further learning processes (Gupta et al., 2013). That is why several methods (Resnik and Smith, 2003; Patry and Langlais, 2011; Baratalipour, 2012) have been so far proposed to extract parallel sentences from the Web.
                      from the aligned documents by exploiting both local and global information.

We explore a learning approach to compute the similarity between a given sentence pair to consider local information. To this aim, we consider a number of length-based, lexicon-based, alignment-based, and miscellaneous features and train a Maximum Entropy (MaxEnt) binary classifier. Since MaxEnt is a probabilistic classifier, it enables us to compute the probability of being parallel for any sentence pair.

A number of previous work (Gale and Church, 1993; Ma, 2006; Baratalipour, 2012) assume that the sentences in the target document are translated from the source sentences exactly in the same order. Therefore, they avoid cross alignments during their aligning process. Although this assumption might be true in many situations, there are also several aligned documents, such as comparable corpora, in which documents are not translation of each other. These aligned documents also may contain several parallel sentences. Hence, to avoid this strict assumption, we propose a method to penalize cross alignments, instead of completely ignoring them. In other words, we develop a method that avoids cross alignments unless the aligned sentences are highly similar. To this aim, we design a bipartite weighted graph for each aligned source and target documents. Each vertex in the graph corresponds to a sentence
                        2
                     
                     
                        2
                        We also define super-nodes to be able to extract parallel sentences with more than one sentence in at least one of the source and target languages.
                      and the edge weights between the vertices are computed using the aforementioned MaxEnt classifier. Then, we introduce an objective function to maximize the similarity of aligned sentences and also to penalizes the cross alignments. To optimize the objective function, we propose a novel method based on integer linear programming (ILP).

In the experiments, we focus on the English–Persian language pair, since the available parallel corpora in the Persian language (also known as Farsi) are limited in terms of both domain and size. We extract parallel sentences from the Wikipedia
                        3
                     
                     
                        3
                        
                           http://wikipedia.org/.
                      articles, since they cover wide variety of domains. We extensively evaluate our method in different scenarios. We first intrinsically evaluate the proposed sentence alignment methods using manually tagged test data extracted from English–Persian Wikipedia articles. The intrinsic evaluation shows that the proposed method significantly outperforms competitive baselines. We further create a parallel corpus using the Wikipedia articles, which is freely available for research purposes.
                        4
                     
                     
                        4
                        
                           http://ece.ut.ac.ir/en/project/wikipedia-parallel-corpus.
                      We extrinsically evaluate the proposed method using CLIR and SMT applications. The experimental results show that the quality of the extracted English–Persian parallel corpus is higher than the existing parallel corpora. We also consider the English–German language pair to evaluate the proposed method in an additional language pair. The results demonstrate that the proposed method can perform effectively in other language pairs, such as English–German.

The remainder of this paper is structured as follows: Section 2 reviews related work. We present the characteristics of Wikipedia articles in Section 3. Section 4 introduces the proposed method to exploit both local and global information to extract parallel sentences. The proposed method is evaluated in Section 5. We finally conclude our paper and discuss possible future directions in Section 6.

@&#RELATED WORK@&#

In this section, we first present related sentence alignment methods. We then introduce the existing English–Persian bilingual resources. We finally review the previous work related to integer linear programming.

Building parallel corpora contains four typical steps: (1) collecting appropriate documents in two languages, (2) pre-processing the collected data to convert all documents into a standard format, (3) aligning documents that are in different languages and may contain parallel sentences (i.e., document-level text alignment), and (4) extracting parallel sentences from the aligned documents (i.e., sentence alignment) (Tiedemann, 2011).

It is typically supposed to be one-to-one mappings in document-level text alignments (Tiedemann, 2011). The document matching criterion could be based on the features extracted from the documents’ content and their available meta-data. For instance, STRAND
                           5
                        
                        
                           5
                           Structural Translation Recognition Acquiring Natural Data.
                         (Resnik, 1999) is an architecture proposed for web-based document-level text alignment using the HTML structure with the help of search engines. A later version of STRAND employs web spiders and name matching techniques instead of using search engines (Resnik and Smith, 2003). PTMiner (Chen and Nie, 2000) is another web-based method for document-level text alignment which selects the URL pairs based on their URL prefix and suffix similarities. Esplá-Gomis proposed an structural matching approach which uses the information about tags and text chunks (Esplà-Gomis, 2009). Textual content of documents might be the most important part which could help to find parallel documents. Uszkoreit et al. (2010) cast the problem of document-level alignment in large corpora to the near-duplicate detection task. In other words, they extracted n-gram features from an initial low-quality translation to align parallel documents. Due to the importance of this task, there is an annual workshop on Building and Using Comparable Corpora (BUCC) since 2008. Several related papers have been published in the proceedings of this workshop. For instance, Morin et al. (2015) recently proposed a model to identify comparable documents in Wikipedia. To this end, they simply counted hapax words. Another approach was proposed in Barrón-Cedeño et al. (2015) to tackle the problem of detecting comparable texts from Wikipedia. Social networks are another source of information that were also employed to develop comparable corpora (Hajjem et al., 2014). In this paper, we focus on the sentence alignment task and the document-level alignments are given to the proposed method.

Extracting parallel sentences from aligned documents has been extensively studied since 1990s. Gale and Church (1993) proposed the early sentence alignment methods based on sentence length. They assumed that sentences in target documents are translated from corresponding source documents in the same order as appeared in the documents. Based on this assumption, they proposed a dynamic programming algorithm to align parallel sentences. The assumption of translation in the same order has been frequently used in many previous work. For instance, Ma (2006) proposed a sentence alignment method based on dynamic programming similar to what was introduced by Gale and Church (1993). This method, called Champollion, uses lexical information to compute the similarity between two given sentences in different languages. More recently, the Fast-Champollion method (Li et al., 2010) was proposed to improve the efficiency of Champollion. For obvious reasons, the aforementioned strong assumption is not always true, particularly in extracting parallel sentences from very non-parallel and comparable corpora. In this paper, we relax this assumption by penalizing cross-alignments instead of completely ignoring them.

Machine learning approaches have also been exploited for the sentence alignment task. For instances, the bootstrapping approach was previously used by Fung and Cheung (2004b) and Mújdricza-Maydt et al. (2013) for extracting parallel sentences. Munteanu and Marcu (2005) cast the sentence alignment task to a binary classification problem. It was shown that aligning sentences via binary classification can improve the SMT performance (Tillmann, 2009; Smith et al., 2010). According to the effectiveness of binary classifiers in previous studies, in this paper we also consider a probabilistic binary classifier to compute the local similarity between each two sentences in different languages.

A number of manual and automatic approaches have so far been explored to create English–Persian bilingual aligned corpora. Hashemi et al. (2010) considered two newswire collections (BBC English news articles and Hamshahri Persian news articles) for document-level alignment. They built a comparable corpus whose documents are aligned based on publication date and title similarity.

Shiraz project (Amtrup et al., 2000) was the first attempt to create an English–Persian parallel corpus to train a machine translation system. This small corpus contains 3000 English–Persian sentence pairs. The Persian sentences were extracted from the Hamshahri news articles. These sentences were translated to English by human translators. TEP (Pilevar et al., 2011) is one of the first freely available English–Persian parallel corpora that was automatically extracted from the aligned movie subtitles. Since most of the dialogues in movies contain informal and slang words and phrases, this corpus cannot be sufficiently appropriate for several tasks, such as translating formal texts. ELRA (Mosavi Miangah, 2009) is another English–Persian parallel corpus, whose sentence pairs were extracted manually from noiseless parallel web pages collected using search engines. The Roman
                           6
                        
                        
                           6
                           The word “Roman” is the transliteration of the Persian equivalence of the word “novel” in English. In this article, the word “Roman” always refers to this corpus.
                         parallel corpus (Mansouri and Faili, 2012) was extracted from several translated novel books. Rasooli et al. (2011) extracted parallel paragraphs and sentences from several documents and their translations. They trained a linear model based on length-based similarities, punctuation marks, and dictionary-based alignment features. Jabbari et al. (2012) crawled the Web to gather English and Persian documents and employed the hunalign (Varga et al., 2005) and the Microsoft aligner
                           7
                        
                        
                           7
                           
                              http://research.microsoft.com/en-us/downloads/aafd5dcf-4dcc-49b2-8a22-f7055113e656/.
                         toolkits to extract parallel sentences, automatically. They also translated several English documents to Persian, manually. They put all of the gathered parallel sentences together and release the ITRC English–Persian parallel corpus. More recently, Ansari et al. (2014) studied the characteristics of English and Persian Wikipedia articles and showed that Wikipedia is a valuable resource for extracting parallel sentences. Their observation motivated us to consider Wikipedia articles for extracting parallel sentences.

Linear programming is a mathematical optimization framework in which the objective function and the constraints are linear. In linear programming, all variables are assumed to be continuous real numbers. A linear programming approach in which at least one of the variables are restricted to be integer is called integer linear programming. Although the integer linear programming in general is NP-hard,
                           8
                        
                        
                           8
                           The reason is that the minimum vertex cover problem, a well-known NP-hard problem, can be mathematically modeled as an integer linear programming problem.
                         there are a number of ILP formulations that can be solved in polynomial time. For instance, the assignment problem, the problem of assigning tasks to agents (see Burkard et al. (2009) for more detail), is an ILP problem and the Hungarian algorithm (Kuhn and Yaw, 1955) shows that this problem can be solved in polynomial time.

According to the applications of ILP in various fields, including machine learning, several methods have so far been proposed for optimizing ILP problems. Several heuristics and approximations are also proposed to speed up the ILP solvers. Tabu search, hill climbing, and simulated annealing are examples of optimization methods that can be used to solve ILP problems (Karlof, 2005).

There are several methods for NLP tasks that are formulated by ILP. Roth and tau Yih (2004) proposed an ILP-based method for global inference. They achieved outstanding performance in the context of learning named entities and relations. Punyakanok et al. (2004) introduced an ILP formulation to incorporate both linguistic and structural constraints for the semantic role labeling task. Marciniak and Strube (2005) proposed an ILP-based discrete optimization framework as an alternative to the pipeline of classifiers for different NLP tasks. Clarke and Lapata (2006) formulated the sentence compression task as an ILP optimization problem. DeNero and Klein (2008) proposed an ILP formulation for the phrase alignment task and showed that ILP can perform efficiently in this task which is an NP-hard problem. More recently, Goldwasser et al. (2012) demonstrated that many NLP tasks can be modeled using the constrained conditional models (CCM). CCM is a learning and inference framework based on integer linear programming. Due to the successful development of ILP-based methods in various NLP tasks, we propose a novel ILP formulation for the sentence alignment task.

Wikipedia is a free encyclopaedia in different languages, which is built by the collaboration of volunteer users. It is administrated by a non-profit organization named Wikimedia Foundation. Wikipedia is one of the largest multilingual corpora which is highly popular among Web users (Adafre and de Rijke, 2006; Otero and López, 2010). Wikipedia contains tremendous numbers of documents in more than 300 languages. The document types in Wikipedia vary in different domains and contain wide range of topics. Thus, it is a rich vocabulary corpus (Wang et al., 2014) and thus, creating comparable and parallel corpora based on Wikipedia would be precious. That is why Wikipedia was widely discovered as a general or broad-domain corpus in several previous studies (Smith et al., 2010; Wang et al., 2014; Ronzano et al., 2008).

Wikipedia is a hypertext corpus; each page is a hypertext document with a rich link structure. Most of these links refer to the outside of Wikipedia. There are also several links between similar Wikipedia pages. Different versions of each page in different languages are also hyperlinked to each other. This is a worthwhile property to explore aligned documents in different languages.


                     Fung and Cheung (2004a) investigated different types of non-parallel corpora. They claimed that comparable corpora contain topic aligned documents which are not translation of each other. Noisy parallel corpora contain bilingual documents with many parallel sentences in the same order. Finally, quasi-comparable corpora are very non-parallel-corpora in which documents are not necessarily in the same topic. According to Smith et al. (2010), Wikipedia is a mixture of noisy parallel and comparable corpora; in Wikipedia some of the articles were translated and the other ones were written by volunteer authors.

Due to the fast growth of Wikipedia, many approaches in the last few years exploited Wikipedia for multilingual tasks in natural language processing, information retrieval, and knowledge engineering. For instance, Wikipedia has been considered to improve the quality of multi/cross language information retrieval (Sorg and Cimiano, 2012; Yin et al., 2010), named entity recognition (Nothman et al., 2013), text classification (Ni et al., 2011), textual entailment (Mehdad et al., 2010), and bilingual dictionaries/terminologies (Tyers and Pienaar, 2008; Erdmann et al., 2009). Moreover, the previous studies on the comparable and parallel corpora extracted from Wikipedia (Bharadwaj and Varma, 2011; Skadina et al., 2012; Adafre and de Rijke, 2006; Otero and López, 2010) investigated the importance of using this multilingual encyclopaedia.

@&#METHODOLOGY@&#

In this section, we propose our approach to consider local and global information. To do so, we create a weighted bipartite graph for each aligned document pair, whose vertices represent sentences in those documents. The edge weights in this graph denote the local similarity of the corresponding sentences (vertices). We further optimize a defined objective function using a novel integer linear programming (ILP) approach. The local similarity between sentences in two languages are computed using a learning approach using the Maximum Entropy classifier.

In this subsection, assume that for each k sentences from the source language and k′ sentences from the target language (henceforth, called the source sentences S and the target sentences T), we can compute p(“parallel”|S, T) which represents the local similarity of S and T. We discuss our methodology to compute this probability in Section 4.2. In this subsection, we consider global information to extract parallel sentences from aligned document pairs. The term “global information” refers to the information comes from the whole document. In this phase, unlike many existing sentence alignment methods (e.g., Gale and Church, 1993; Varga et al., 2005; Ma, 2006) that do not allow cross-alignments, our proposed approach accepts cross alignments if the similarity of the sentences are substantially high. In fact, there is a trade-off between the number of cross-alignments and the similarity of extracted parallel sentences. To penalize cross-alignments, we first create a bipartite graph from the source and the target documents and then propose an integer linear programming approach to consider global information to extract parallel sentences.

In this part, we create a bipartite graph from the source and the target documents. Let D and D′ be two aligned documents from the source and the target languages, respectively. Our goal is to extract parallel sentences from these two documents.

Assume that s
                           1, s
                           2, …, s
                           
                              m
                            and t
                           1, t
                           2, …, t
                           
                              n
                            respectively are the sentences appear in the documents D and D′, in the same order as what was written in the documents. We create a bipartite graph G (shown in Fig. 1
                           ) whose first part contains m vertices u
                           1, u
                           2, …, u
                           
                              m
                           , each corresponds to a sentence in the document D. The second part contains n vertices 
                              
                                 v
                                 1
                              
                              ,
                              
                                 v
                                 2
                              
                              ,
                              …
                              ,
                              
                                 v
                                 n
                              
                           , each corresponds to a sentence in the document D′. Between each pair of vertices 
                              〈
                              
                                 u
                                 i
                              
                              ,
                              
                                 v
                                 j
                              
                              〉
                            where 1≤
                           i
                           ≤
                           m and 1≤
                           j
                           ≤
                           n, there is an edge whose weight is equal to p(“parallel”|
                           s
                           
                              i
                           , t
                           
                              j
                           ). Therefore, we have created a complete bipartite graph whose edge weights are in the [0, 1] interval.

It is known that the translation of a sentence from the source language might be more than one sentence in the target language. Therefore, in addition to the singleton parallel sentences,
                              9
                           
                           
                              9
                              Singleton parallel sentences refer to parallel sentences with exactly one sentence in each of the source and the target languages.
                            we would like to detect complex parallel sentences. Complex parallel sentences refer to the (i, j) alignments, where i and j denote the number of sentences in the source and the target languages, respectively. To reduce the time complexity of the proposed method, we set an upper-bound of k and k′ for the values of i and j, respectively. The intuition behind this decision is that it is very rare to have complex parallel sentences with more than k and k′ sentences in the source and the target languages, respectively. In other words, our goal is to detect up to (k, k′) alignments.

To model complex parallel sentences in the created bipartite graph, we define the concept of super-nodes as follows: the super-node u
                           
                              ij
                            corresponds to j
                           +1 sentences s
                           
                              i
                           , s
                           
                              i+1, …, s
                           
                              i+j
                            from the source document D. We can similarly define super-node 
                              
                                 v
                                 ij
                              
                            for the second part of the graph (the target side). Note that the vertices u
                           
                              i
                            and 
                              
                                 v
                                 
                                    
                                       i
                                       ′
                                    
                                 
                              
                            in the created bipartite graph are special-case of the defined super-node concept. In other words, the super-nodes u
                           
                              ij
                            and 
                              
                                 v
                                 
                                    
                                       i
                                       ′
                                    
                                    j
                                 
                              
                            where j
                           =0 can model the vertices u
                           
                              i
                            and 
                              
                                 v
                                 
                                    
                                       i
                                       ′
                                    
                                 
                              
                           , respectively.

Similar to what is explained for the singleton parallel sentences, we can compute the edge weight between each super-node pair u
                           
                              ij
                            and 
                              
                                 v
                                 
                                    
                                       i
                                       ′
                                    
                                    
                                       j
                                       ′
                                    
                                 
                              
                            where 1≤
                           i
                           ≤
                           m, 1≤
                           i′≤
                           n, 0≤
                           j
                           ≤
                           k
                           −1, 0≤
                           j′≤
                           k′−1, i
                           +
                           j
                           ≤
                           m, and i′+
                           j′≤
                           n.

The edge weights in the created graph G demonstrate the similarity between two super-nodes in two parts and by selecting each of the edges in this graph, we select up to (k, k′) alignments from the source and the target languages.

In this part, we propose an integer linear programming approach to extract parallel sentences given the graph G defined in the previous subsection. Selecting each edge between vertices u and 
                              v
                            in graph G is equivalent to detecting the sentences corresponding to those vertices as parallel sentences.

Our goal is to select a number of edges by considering the following constraints:
                              
                                 •
                                 Sentences cannot appear in more than one alignment.

Since in many cases, the whole or a big part of each document is usually translated from another one and human translators often translate sentences in the same order as appear in the source document, the number of cross-alignments should be minimized. Therefore, alongside maximizing similarity of the selected parallel sentences, we should minimize the number of selected edges that cross each other.

To satisfy the aforementioned constraints, we define an objective function which tries to maximize the similarity of the selected parallel sentences as well as to penalize the number of cross-alignments. The objective function is defined as follows:


                           
                              
                                 (1)
                                 
                                    max
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   0
                                                
                                                
                                                   m
                                                   −
                                                   1
                                                
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      j
                                                      =
                                                      0
                                                   
                                                   
                                                      n
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         
                                                            i
                                                            ′
                                                         
                                                         =
                                                         0
                                                      
                                                      
                                                         k
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         ∑
                                                         
                                                            
                                                               j
                                                               ′
                                                            
                                                            =
                                                            0
                                                         
                                                         
                                                            
                                                               k
                                                               ′
                                                            
                                                            −
                                                            1
                                                         
                                                      
                                                      
                                                         
                                                            a
                                                            
                                                               
                                                                  iji
                                                                  ′
                                                               
                                                               
                                                                  j
                                                                  ′
                                                               
                                                            
                                                         
                                                         
                                                            w
                                                            
                                                               
                                                                  iji
                                                                  ′
                                                               
                                                               
                                                                  j
                                                                  ′
                                                               
                                                            
                                                         
                                                         −
                                                         α
                                                         
                                                            c
                                                            
                                                               
                                                                  iji
                                                                  ′
                                                               
                                                               
                                                                  j
                                                                  ′
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           where 
                              
                                 w
                                 
                                    
                                       iji
                                       ′
                                    
                                    
                                       j
                                       ′
                                    
                                 
                              
                            denotes the weight of the edge between super-nodes u
                           
                              ij
                            and 
                              
                                 v
                                 
                                    
                                       i
                                       ′
                                    
                                    
                                       j
                                       ′
                                    
                                 
                              
                           . For instance, as shown in Fig. 1, 
                              
                                 w
                                 1221
                              
                            is the weight of the edge between the super-nodes u
                           12 and 
                              
                                 v
                                 21
                              
                           . 
                              
                                 a
                                 
                                    
                                       iji
                                       ′
                                    
                                    
                                       j
                                       ′
                                    
                                 
                              
                            is a binary variable which shows whether the edge between super-nodes u
                           
                              ij
                            and 
                              
                                 v
                                 
                                    
                                       i
                                       ′
                                    
                                    
                                       j
                                       ′
                                    
                                 
                              
                            is selected or not. The parameter α is a free hyper-parameter whose value is given as an input of the ILP algorithm. This parameter is the penalty coefficient for cross-alignments. The integer variable 
                              
                                 c
                                 
                                    
                                       iji
                                       ′
                                    
                                    
                                       j
                                       ′
                                    
                                 
                              
                            shows the number of selected edges that cross the edge between the super-nodes u
                           
                              ij
                            and 
                              
                                 v
                                 
                                    
                                       i
                                       ′
                                    
                                    
                                       j
                                       ′
                                    
                                 
                              
                           . The parameter α can be optimized using the hyper-parameter optimization techniques, such as grid search and randomized search (Bergstra and Bengio, 2012). To summarize, in the defined formulation we have two groups of variables, a and c. The first group indicates whether an edge between two super-nodes are selected or not. The second group of variables calculates the number of cross-alignments. These variables should satisfy the following constraints:
                              
                                 •
                                 
                                    [Boolean constraint]: This constraint avoids the selection of each edge more than once. In fact, it indicates that each two super-nodes from different parts of the graph G should be selected or not. This constraint can be formalized as follows:
                                       
                                          (2)
                                          
                                             ∀
                                             i
                                             ,
                                             j
                                             ,
                                             
                                                i
                                                ′
                                             
                                             ,
                                             
                                                j
                                                ′
                                             
                                             :
                                             
                                                a
                                                
                                                   
                                                      iji
                                                      ′
                                                   
                                                   
                                                      j
                                                      ′
                                                   
                                                
                                             
                                             ∈
                                             {
                                             0
                                             ,
                                             1
                                             }
                                             .
                                          
                                       
                                    
                                 


                                    [Alignment restriction constraints]: As pointed out in the beginning of this subsection, each sentence should appear at most in one alignment. To this aim, for each sentence, at most one of the super-nodes that include that sentence should be aligned. In other words, the alignment restriction constraints guarantee that each sentence is selected at most once during the sentence alignment procedure. We can formalize these constraints using the two following formulas:
                                       
                                          (3)
                                          
                                             ∀
                                             0
                                             ≤
                                             i
                                             <
                                             m
                                             :
                                                
                                             
                                                ∑
                                                
                                                   x
                                                   =
                                                   max
                                                   {
                                                   0
                                                   ,
                                                   i
                                                   −
                                                   k
                                                   }
                                                
                                                i
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      j
                                                      =
                                                      i
                                                      −
                                                      x
                                                   
                                                   
                                                      k
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         
                                                            i
                                                            ′
                                                         
                                                         =
                                                         0
                                                      
                                                      
                                                         n
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         ∑
                                                         
                                                            
                                                               j
                                                               ′
                                                            
                                                            =
                                                            0
                                                         
                                                         
                                                            
                                                               k
                                                               ′
                                                            
                                                            −
                                                            1
                                                         
                                                      
                                                      
                                                         
                                                            a
                                                            
                                                               
                                                                  xji
                                                                  ′
                                                               
                                                               
                                                                  j
                                                                  ′
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             ≤
                                             1
                                             ,
                                          
                                       
                                    
                                    
                                       
                                          (4)
                                          
                                             ∀
                                             0
                                             ≤
                                             
                                                i
                                                ′
                                             
                                             <
                                             n
                                             :
                                                
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   0
                                                
                                                
                                                   m
                                                   −
                                                   1
                                                
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      j
                                                      =
                                                      0
                                                   
                                                   
                                                      k
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         x
                                                         =
                                                         max
                                                         {
                                                         0
                                                         ,
                                                         
                                                            i
                                                            ′
                                                         
                                                         −
                                                         
                                                            k
                                                            ′
                                                         
                                                         }
                                                      
                                                      
                                                         
                                                            i
                                                            ′
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         ∑
                                                         
                                                            
                                                               j
                                                               ′
                                                            
                                                            =
                                                            
                                                               i
                                                               ′
                                                            
                                                            −
                                                            x
                                                         
                                                         
                                                            
                                                               k
                                                               ′
                                                            
                                                            −
                                                            1
                                                         
                                                      
                                                      
                                                         
                                                            a
                                                            
                                                               
                                                                  ijxj
                                                                  ′
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             ≤
                                             1
                                             .
                                          
                                       
                                    
                                 


                                    [Cross-alignment integer constraint]: This simple constraint indicates that the number of cross-edges during the alignment procedure is an integer. Formally writing, this constraints can be mathematically modeled as:
                                       
                                          (5)
                                          
                                             ∀
                                             i
                                             ,
                                             j
                                             ,
                                             
                                                i
                                                ′
                                             
                                             ,
                                             
                                                j
                                                ′
                                             
                                             :
                                                
                                             
                                                c
                                                
                                                   
                                                      iji
                                                      ′
                                                   
                                                   
                                                      j
                                                      ′
                                                   
                                                
                                             
                                             ∈
                                             
                                                
                                                   ℤ
                                                
                                             
                                             .
                                          
                                       
                                    
                                 


                                    [Cross-alignment bounding constraint]: This constraint denotes that the number of cross-alignments is always non-negative. In other words, the variable group c only holds non-negative values. On the other hand, the total number of selected edges is an upper-bound for each variable c. Therefore, we can formally model this constraint as follows:
                                       
                                          (6)
                                          
                                             ∀
                                             i
                                             ,
                                             j
                                             ,
                                             
                                                i
                                                ′
                                             
                                             ,
                                             
                                                j
                                                ′
                                             
                                             :
                                                
                                             0
                                             ≤
                                             
                                                c
                                                
                                                   
                                                      iji
                                                      ′
                                                   
                                                   
                                                      j
                                                      ′
                                                   
                                                
                                             
                                             ≤
                                             m
                                             *
                                             n
                                             *
                                             
                                                a
                                                
                                                   
                                                      iji
                                                      ′
                                                   
                                                   
                                                      j
                                                      ′
                                                   
                                                
                                             
                                             .
                                          
                                       
                                    
                                 


                                    [Cross-alignment lower-bound constraint]: This constraint plays the main role in computing the number of cross-alignments. In this constraint, we first would like to assign the number of cross-alignments to zero, if the edge is not selected. If the corresponding edge is selected, the number of edges that are selected and also cross the corresponding edge should be computed. This constraint can mathematically formalized as follows:
                                       
                                          (7)
                                          
                                             
                                                c
                                                
                                                   
                                                      iji
                                                      ′
                                                   
                                                   
                                                      j
                                                      ′
                                                   
                                                
                                             
                                             ≥
                                             
                                                ∑
                                                
                                                   x
                                                   =
                                                   i
                                                   +
                                                   
                                                      i
                                                      ′
                                                   
                                                   +
                                                   1
                                                
                                                
                                                   m
                                                   −
                                                   1
                                                
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      y
                                                      =
                                                      0
                                                   
                                                   
                                                      j
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         z
                                                         =
                                                         0
                                                      
                                                      
                                                         k
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         ∑
                                                         
                                                            w
                                                            =
                                                            0
                                                         
                                                         
                                                            
                                                               k
                                                               ′
                                                            
                                                            −
                                                            1
                                                         
                                                      
                                                      
                                                         
                                                            a
                                                            xyzw
                                                         
                                                         −
                                                         (
                                                         1
                                                         −
                                                         
                                                            a
                                                            
                                                               
                                                                  iji
                                                                  ′
                                                               
                                                               
                                                                  j
                                                                  ′
                                                               
                                                            
                                                         
                                                         )
                                                         *
                                                         m
                                                         *
                                                         n
                                                      
                                                   
                                                
                                             
                                             .
                                          
                                       
                                    
                                 

If 
                                       
                                          a
                                          
                                             
                                                iji
                                                ′
                                             
                                             
                                                j
                                                ′
                                             
                                          
                                       
                                     equals to 1, this constraint forces the variable 
                                       
                                          c
                                          
                                             
                                                iji
                                                ′
                                             
                                             
                                                j
                                                ′
                                             
                                          
                                       
                                     to become greater than or equal to the number of edges that cross the edge between the super-nodes u
                                    
                                       ij
                                     and 
                                       
                                          v
                                          
                                             
                                                i
                                                ′
                                             
                                             
                                                j
                                                ′
                                             
                                          
                                       
                                    . Since the defined objective function is maximized and the variable 
                                       
                                          c
                                          
                                             
                                                iji
                                                ′
                                             
                                             
                                                j
                                                ′
                                             
                                          
                                       
                                     has a negative effect on the objective value, this variable will select its minimum value (i.e., the number of cross-alignments). Otherwise, if 
                                       
                                          a
                                          
                                             
                                                iji
                                                ′
                                             
                                             
                                                j
                                                ′
                                             
                                          
                                       
                                       =
                                       0
                                    , this constraint results that the variable 
                                       
                                          c
                                          
                                             
                                                iji
                                                ′
                                             
                                             
                                                j
                                                ′
                                             
                                          
                                       
                                     should be greater than or equal to a negative number. On the other hand, the cross-alignment bounding constraints indicate that this variable should be greater than or equal to zero. Again, since the variable 
                                       
                                          c
                                          
                                             
                                                iji
                                                ′
                                             
                                             
                                                j
                                                ′
                                             
                                          
                                       
                                     has a negative effect on maximizing the objective function, this variable will hold its minimum value which is zero.

Finally, the sentence pairs whose corresponding edges are selected by the above ILP formulation will be reported as parallel sentences. Therefore, we use global information of the whole documents in both source and target languages to extract parallel sentences.

In the aforementioned ILP algorithm, each of the variable groups a and c contains m
                           *
                           n
                           *
                           k
                           2 variables. Hence, by increasing the size of the source/target documents, the efficiency of the proposed algorithm drops. In addition, the ILP algorithm may select some sentences with low local similarities to maximize the objective function. To increase both efficiency and effectiveness of the proposed ILP algorithm, we propose to omit the edges with weight lower than a threshold τ. In other words, before applying the ILP algorithm which uses the global information, we prune the graph G by ignoring sentences with low local similarities. This improves the effectiveness, since it avoids aligning sentences with low local similarities. It also improves the efficiently, because the number of variables in the ILP algorithm decreases and thus, the algorithm is supposed to perform faster.

In this subsection, we introduce our proposed methodology to compute the similarity between the source sentences S and the target sentences T. We first extract several features from S and T. We further exploit a learning method to compute the similarity between S and T.

We extract several features from each sentence pair 〈S, T〉 where S and T denote a few (up to k and k′, respectively) consecutive sentences from the source and the target languages, respectively. The extracted features can be partitioned into four categories: length-based, dictionary-based, alignment-based, and miscellaneous. In the following, we introduce our features, in detail.
                              
                                 •
                                 
                                    Length-based features: these features only considers the length of S and T. They were previously used in the first attempts to extract parallel sentences (Gale and Church, 1993).
                                       
                                          –
                                          Sentence length: there is no much differences between the length (i.e., word count) of aligned sentences. This feature can help to ignore the sentence pairs whose lengths vary a lot. We consider the length of both S and T.

Sentence length difference: this feature, which is more powerful than the previous one, shows how much the candidate sentences differ in terms of the sentence length. This feature is computed by the absolute distance between the length of S and T.

Sentence length ratio: the idea behind this feature is similar to the previous one. This feature is calculated by dividing the length of S by the length of T.


                                    Dictionary-based features: these features are computed using a bilingual dictionary. To compute these features, we use the Google dictionary.
                                       10
                                    
                                    
                                       10
                                       Note that the Google dictionary differs from the Google translator.
                                    
                                    
                                       
                                          –
                                          Word overlap ratio of S (T): the proportion of words in S (T) whose at least one of their translations exists in T (S). The intuition behind this feature is that parallel sentences usually have higher word overlap ratio.


                                    Alignment-based features: these features are obtained by aligning the words in S and T. They are primarily defined based on the IBM Model 1 alignments. However, we use the agreement alignment proposed by Liang et al. (2006), in which the symmetric word alignment is achieved by the agreement between two simple HMM Models. The agreement alignment is more accurate and also as simple as IBM model 1 for computing. Exploiting IBM Model 1 to identify parallel sentences was introduced by Munteanu and Marcu (2005).
                                       
                                          –
                                          Number of aligned words: the total number of aligned words based on the agreement alignment. The value of this feature for parallel sentences usually are higher than those for non-parallel sentences with approximately the same length.

Number of unaligned words: the total number of words that are not aligned by the agreement alignment algorithm. Parallel sentences usually have a low number of unaligned words.

Longest aligned sequence of words: length of the longest continuous sequence of words in S (T) which are aligned to at least one word in T (S). Parallel sentences usually contain a number of complete aligned phrases and thus, this feature can help to recognize parallel sentences.

Longest unaligned sequence of words: Length of the longest continuous sequence of words in S (T) which are not aligned during the alignment process. We consider this feature because parallel sentences usually do not contain long unaligned phrases.

Alignment score: this feature is equal to the score of the alignment of S and T. Parallel sentences usually hold higher alignment scores compared to the non-parallel ones.


                                    Miscellaneous features: in addition to the aforementioned features, we use three other features. The first two features were previously used for detecting parallel documents (Patry and Langlais, 2011).
                                       
                                          –
                                          Numerical entities ratio: this feature is computed by dividing the number of numerical words in S by the number of those in T. Since the number of numerical words in parallel sentences is supposed to be similar, this feature can be useful in detecting parallel sentences.

Punctuation ratio: this feature is computed by dividing the number of punctuations in S by the number of those in T. Parallel sentences often use similar punctuations.

Common words ratio: This feature is computed by dividing the number of common words in S and T by the maximum length of S and T. In some sentences, named entities or part of the sentence that cannot be translated easily are brought in the form of their original language. It should be noted that this feature is more important when the alphabets of the source and the target languages are different, e.g., English and Persian.

Using the introduced features, we train a Maximum Entropy (MaxEnt), also known as Logistic Regression, classifier (Yu et al., 2011) to detect parallel sentences. We consider MaxEnt for two reasons. The first reason is that the MaxEnt classifier has been extensively used in various text mining and natural language processing tasks and it usually achieves outstanding performance in different test collections (Berger et al., 1996; Nigam et al., 1999). The second reason is that the output of the MaxEnt classifier for a given sentence pair 〈S, T〉 is the probability of that pair being member of each of the classes. Hence, we can use these probabilities in our further procedures. We train the MaxEnt classifier as a binary classifier where the labels are either “parallel” or “non-parallel”.

To train the MaxEnt classifier, we consider sentences in an existing parallel corpus as the instances from the “parallel” class. For the non-parallel class, we pair each source sentence randomly with N target sentences from the parallel corpus. Therefore, the number of non-parallel instances are N times more than the number of parallel instances. To tackle with the data imbalance problem we use the sample weighting technique. In other words, we weight parallel instances N times more than the non-parallel ones in the loss function. Using this technique, we can consider more non-parallel instances to increase the generality of the model and address the data imbalance problem at the same time. This idea has previously been used in various machine learning tasks, e.g., de Souza et al. (2015), Zamani et al. (2015) and Montazeralghaem et al. (2016).

@&#EXPERIMENTS@&#

In this section, we perform several experiments to show the effectiveness of the proposed method. We first intrinsically evaluate the proposed method using a manually tagged dataset and compare the achieved results with those obtained by competitive baselines. In the second set of experiments, we create an English–Persian parallel corpus from Wikipedia articles and report the statistical information of the created corpus. In the next two set of experiments, we perform extrinsic evaluation by exploiting the created parallel corpus in CLIR and SMT applications and compare the results with those obtained by a number of existing English–Persian parallel corpora. In the last set of experiments, we evaluate the proposed method on an additional language pair (i.e., English–German) to show that the proposed ILP method is a language-independent approach for sentence alignment. It should be noted that in our experiments we set the parameters k and k′ (the upper-bounds for the size of complex parallel sentences), described in Sections 4.1 to 4. We also set the parameter N (the number of random non-parallel instances) defined in Sections 4.2 to 5. To train the MaxEnt classifier for the experiments related to the English–Persian language pair, we use a combination of four existing English–Persian parallel corpora containing formal texts: the Roman parallel corpus (Mansouri and Faili, 2012), the ELRA Persian–English parallel corpus (Mosavi Miangah, 2009), a part of the Mizan parallel corpus, and the ITRC parallel corpus (Jabbari et al., 2012). This combination, called 20M, was previously used in many research work including (Azarbonyad et al., 2014; Baratalipour, 2012) to train English–Persian SMT and CLIR systems.

In this experiment, we perform an intrinsic evaluation using a manually tagged parallel corpus extracted from a number of Wikipedia articles. In this subsection, we first report statistics of the manually tagged dataset and then present the evaluation metrics. After that, we introduce the competitive baselines that we consider in this study. The experimental results are reported and discussed in the last part.

We exploit a manually tagged dataset extracted from Wikipedia articles, which is created in the Natural Language and Text Processing laboratory of the University of Tehran
                              11
                           
                           
                              11
                              
                                 http://ece.ut.ac.ir/en/lab/nlp.
                            (Baratalipour, 2012). In this dataset, called gold data, parallel sentences are tagged by several reviewers. The statistics of the gold data are shown in Table 1
                           . It is worth mentioning that this data is freely available for research purposes.
                              12
                           
                           
                              12
                              
                                 http://ece.ut.ac.ir/en/node/940.
                           
                        

To evaluate the proposed method, in addition to the precision and the recall metrics, we consider the F
                           
                              β
                            measure (Rijsbergen, 1979) to have a complete comparison between different methods. F
                           
                              β
                            is the weighted harmonic average of precision and recall. It is calculated as:
                              
                                 (8)
                                 
                                    
                                       F
                                       β
                                    
                                    =
                                    (
                                    1
                                    +
                                    
                                       β
                                       2
                                    
                                    )
                                    *
                                    
                                       
                                          precision
                                          *
                                          recall
                                       
                                       
                                          
                                             β
                                             2
                                          
                                          *
                                          precision
                                          +
                                          recall
                                       
                                    
                                    ,
                                 
                              
                           where precision and recall are given by:
                              
                                 (9)
                                 
                                    precision
                                    =
                                    
                                       
                                          #
                                             
                                          of
                                             
                                          correct
                                             
                                          extracted
                                             
                                          aligned
                                             
                                          sentences
                                       
                                       
                                          #
                                             
                                          of
                                             
                                          extracted
                                             
                                          aligned
                                             
                                          sentences
                                       
                                    
                                    ,
                                 
                              
                           
                           
                              
                                 (10)
                                 
                                    recall
                                    =
                                    
                                       
                                          #
                                             
                                          of
                                             
                                          correct
                                             
                                          extracted
                                             
                                          aligned
                                             
                                          sentences
                                       
                                       
                                          #
                                             
                                          of
                                             
                                          correct
                                             
                                          aligned
                                             
                                          sentences
                                       
                                    
                                    .
                                 
                              
                           
                        

Since the main usage of parallel corpora is in the further learning-based cross- and multi-lingual applications, such as SMT and CLIR, the quality of parallel corpora would be very influential in their performance. Therefore, although the size of a parallel corpus is crucial, precision might be more important than recall in this task. We consider both β
                           =1 and β
                           =0.5 in F
                           
                              β
                           , which the former gives equal importance to both precision and recall and the latter pays more attention to the precision value.

In this set of experiments, we consider the following baselines:
                              
                                 •
                                 
                                    Champollion (Ma, 2006): this unsupervised method
                                       13
                                    
                                    
                                       13
                                       Obviously, comparing our supervised method with unsupervised baselines is not fair; but, we consider Champollion as a state-of-the-art unsupervised baseline to have an insight into the differences between supervised and unsupervised methods. The second and the third baselines are supervised.
                                     consists of a dynamic programming approach which avoids cross-alignments and is similar to the hunalign method (Varga et al., 2005). The main idea of Champollion is that all sentences in the source document are translated to the target language in the same order. Champollion uses a bilingual dictionary to compute the similarity between two sentences and pays more attention to low frequency words by considering term frequency (TF) and inverse document frequency (IDF). In our experiments, we exploit the English–Persian Google dictionary as the translation resource.


                                    MaxEnt: we use our learned Maximum Entropy classifier as one of the baselines. This is a binary classifier which determines whether a bilingual sentence pair is parallel or not. This method only considers local information of the sentence pair. Thus, comparing its result with those obtained by the proposed method will demonstrate the impact of using our ILP method to exploit global information. As mentioned above, output of the MaxEnt classifier is a probability for each class which shows the probability of the input being their member. If the probability of the input being a member of the “parallel” class is greater than or equal to a threshold τ′, we consider the input sentence pair as parallel sentences.


                                    Champollion+MaxEnt: this method is similar to Champollion and the only difference is in computing the similarity between two sentences. This supervised method uses our learned MaxEnt classifier to compute the similarity between two sentences. Similar to what we do in the proposed method, in this baseline if the probability of being parallel for two given sentences is lower than a threshold τ′, their similarity would set to zero to avoid selection of sentence pairs with low similarities. Comparing our results with those obtained by this baseline will show the effectiveness and the importance of our cross-alignment assumption and our methodology to consider global information.

We run all baselines on the gold data. The parameter τ′ in the last two baselines is swept between 0.4, 0.5, and 0.6. These values are also considered for the parameter τ, described in Section 4.1.3. This parameter is a threshold for ignoring candidate pairs with low local similarity scores. The parameter α is set to 0.05, 0.1, and 0.2 in different setups to see its impact on the results. Table 2
                            reports the average of precision, recall, F
                           1, and F
                           0.5 values obtained from all the documents in the gold data. According to Table 2, by increasing the parameter τ′ precision is increased and recall is decreased. This was expected, because by increasing the threshold τ′, limited amount of sentence pairs with higher similarity scores are selected. The best recall is achieved by the Champollion baseline that uses bilingual dictionary. Champollion also has the lowest precision among all the baselines. The reason is that Champollion is the only unsupervised method among the baselines and it extracted a huge amount of sentences as parallel ones.

According to Table 2, although Champollion achieves the highest F
                           1, its F
                           0.5 is the lowest one and according to the importance of quality of parallel corpora, it could not be a suitable method for sentence alignment. From Table 2 it can be realized that although the F
                           1 obtained by different methods are around each other, their F
                           0.5 values vary a lot. The best F
                           0.5 is achieved by Champollion+MaxEnt where τ′=0.5.


                           Table 3
                            demonstrates the results achieved by the proposed method with different parameters. According to this table, the best precision obtained by the proposed method is greater than all of those achieved by the baselines. Regarding Table 3, the best F
                           0.5 and F
                           1 are achieved when the parameter τ is equal to 0.5. In addition, where τ
                           =0.5, the proposed method outperforms all the baselines, in terms of precision, F
                           0.5, and F
                           1. It is important to bear in mind that however the size of parallel corpora is important for further usages, the precision is really crucial to have reliable training data. Therefore, the recall itself would not be a good evaluation metric. We performed the statistical significance test (two-tailed pair t-test) and the numbers in Table 3 that are marked with * are significantly better than those obtained by the baselines (p-value
                           <0.001).
                              14
                           
                           
                              14
                              The significance test is performed over the results for each document in the gold data.
                            According to Table 3, in two cases (τ
                           =0.5/α
                           =0.05 and τ
                           =0.5/α
                           =0.1), the F
                           0.5 and the F
                           1 values are significantly higher than those achieved by all the baselines.


                           Table 3 shows that by increasing the value of parameter α, recall and F
                           1 decrease. The reason is that by increasing the value of this parameter, the cross-alignment penalty in the ILP method increases. The proposed method achieves its best precision value, when the parameter α is set to 0.1.

In this experiment, we apply the proposed method on Wikipedia articles and extract parallel sentences using the proposed method. To optimize the hyper-parameters α and τ, we perform randomized search (Bergstra and Bengio, 2012) over the gold data.

We downloaded the dump files of Persian and English Wikipedia articles, and extracted their content and the inter-language links using the wikixmlj API
                           15
                        
                        
                           15
                           
                              http://code.google.com/p/wikixmlj/.
                         and wikiExtractor.
                           16
                        
                        
                           16
                           
                              http://medialab.di.unipi.it/Project/SemaWiki/Tools/WikiExtractor.py.
                         
                        Table 4
                         reports statistics of the English and the Persian Wikipedia articles in the dump files that we used.


                        Table 5
                         investigates information of the created parallel corpus from the Wikipedia articles, henceforth called the Wikipedia parallel corpus. According to Table 5, there are around 280K alignments in the corpus. It should be noted that in each alignment, there are up to 4 sentences in each language. Table 5 also shows that the total number of words in the Persian side of the corpus is higher than those in the English side.


                        Table 6
                         reports statistical information about the number of sentences in each language for each alignment. According to Table 6, the number of (1, 1) alignments is much more than the other alignments, which is expected. Table 6 also shows that 20% of the alignments are related to the super-nodes, which are introduced to handle the alignments with more than one sentence in at least one of the languages. The Wikipedia parallel corpus is freely available for research purposes.

In this section, we extrinsically evaluate the Wikipedia parallel corpus by leveraging it in an English–Persian CLIR ad-hoc retrieval task. In this set of experiments, we used the CLIR method described in Azarbonyad et al. (2014) which exploited the Okapi BM25 retrieval method (Robertson et al., 2000). The Okapi parameters k
                        1 and b are respectively set to the default values of 1.2 and 0.75. Similar to many previous work (Nie, 2010; Azarbonyad et al., 2012, 2013), translations are extracted from the parallel corpus using the IBM Model 1 word alignment method. All the CLIR experiments were carried out using the Lucene toolkit.
                           17
                        
                        
                           17
                           
                              http://lucene.apache.org/.
                        
                     

We exploited the Hamshahri collection (AleAhmad et al., 2009) (the CLEF 2008 and 2009 English–Persian CLIR collection) that contains 166k documents in Persian and 100 queries in Persian and English. We used the English titles as the queries for the CLIR task. The documents of this collection are the articles of the Hamshahri newspaper.

To evaluate the Wikipedia parallel corpus, we compare the CLIR results obtained by different parallel corpora as bilingual resources. To this aim, we consider the TEP parallel corpus (Pilevar et al., 2011) which is extracted from movie subtitles, the Roman parallel corpus (Mansouri and Faili, 2012) which is extracted from several novel books, and the 20M parallel corpus that is a combination of four existing English–Persian parallel corpora containing formal sentences. In addition, we also extract an English–Persian parallel corpus from Wikipedia articles using the Champollion method (Ma, 2006), a state-of-the-art sentence alignment method. This parallel corpus is called Wiki-Champ. Table 7
                            reports the statistics of these parallel corpora.

By comparing Tables 5 and 7, it can be realized that TEP, Roman, and 20M are larger than the created Wikipedia parallel corpus, in terms of the number of alignments and the number of words. TEP contains shorter sentences and Roman contains longer sentences than the Wikipedia parallel corpus. 20M is by far larger than the created parallel corpus.

To evaluate retrieval effectiveness, we use Mean Average Precision (MAP) of the top-ranked 1000 documents as the main evaluation metric. In addition, we also report the precision of the top 5 and 10 retrieved documents (P@5 and P@10). These metrics have been extensively used in literature for the ad-hoc retrieval task. Although MAP is sensitive to the position of relevant documents in the retrieved results, P@5 and P@10 only consider the number of relevant documents in the top retrieved lists. P@5 and P@10 are more important from the users’ perspective, since users would prefer to find their information needs in the top retrieved results. All the aforementioned metrics are in the [0, 1] interval and the higher values represent higher retrieval quality.

According to Table 8
                           , the results obtained by Roman is lower than the other parallel corpora. The reason could be that Roman only contains some very formal words in a specific domain, but Hamshahri (our CLIR test collection) does not. Although the size of parallel corpus used in CLIR is influential in the CLIR results (Nie, 2010), results of the Wikipedia corpus is much higher than those obtained by the TEP and the Roman corpora and very similar to the results achieved by the 20M corpus. The reason is that the domain of Wikipedia articles is more general and it contains lots of named-entities. Keep in mind that 20M is around 4 and 8 times larger than our created corpus in terms of the number of alignments and the number of words, respectively. As mentioned above, 20M is a filtered corpus created by combining four popular English–Persian parallel corpora. The experiments also demonstrate that the results achieved by Wikipedia are higher than those achieved by Wiki-Champ. This shows that quality and generality of the Wikipedia parallel corpus is relatively high compared to the other corpora.

Parallel corpora are essential resources in training statistical machine translation systems. In this set of experiments, we evaluate the created parallel corpus using a phrase-based SMT system (Koehn, 2004). To this aim, we have employed the Moses decoder (Koehn et al., 2007) which includes a set of software and scripts to build a complete SMT system. Moses implements an efficient beam-search algorithm for the decoding step in the statistical translation process.

We compare the results of SMT systems which were learned on Wikipedia, Roman, and TEP parallel corpora, introduced in Section 5.3 (we ignore the 20M parallel corpus in this set of experiments, since it contains some sentences from the first exploited test set). Similar to Section 5.3, we also consider the created Wiki-Champ parallel corpus created by the Champollion method. To tune the SMT parameters, we randomly select half of the test sentences as the development set and consider the remaining sentences as the test set. We repeat this process 10 times and report the average of the results obtained in each shuffling process. We have performed statistical significant test (t-test) over these results. The results which are marked with * are those significantly better than the others.

To evaluate the learned SMT systems, we have exploited two different test sets. As the first one, we used AFEC standard test set (Jabbari et al., 2012). AFEC contains 818 English sentences from news articles and 4 possible Persian translations as references. Since there is no broad-domain English–Persian SMT test set and also because Wikipedia is considered as a general or broad-domain corpus (see Section 3), we used our manually aligned Wikipedia sentences (gold data) as the second test set. As mentioned, this test set contains 1257 English and Persian pair sentences. It should be noted that we have removed the test sentences from our created parallel corpus to have a fair evaluation.

To evaluate the learned SMT systems, we have reported Bi-Lingual Evaluation Understudy (BLEU) measure (Papineni et al., 2002), which is a precision-based metric that compares automatic translations against reference translations by summing the number of matched n-grams (n
                           ∈1, 2, 3, 4) divided by the total number of n-grams in the reference translations. BLEU always produces a score in the [0, 1] interval. Higher scores indicate more accurate translations (Papineni et al., 2002). BLEU is the first and the most popular metric for evaluating machine translation systems, which has a high correlation with human judgements (Callison-Burch et al., 2006).

According to the results reported in Table 9
                           , although the size of both Roman and TEP parallel corpora are much higher than Wikipedia parallel corpus (in terms of the number of aligned sentences), the SMT system learned by the Wikipedia parallel corpus significantly outperforms the same SMT system learned by the Roman and the TEP parallel corpora. Since AFEC sentences were extracted from news articles, it will be considered as an out of domain test set for all the parallel corpora. The results reported for the TEP parallel corpus is far lower than those obtained by the other ones; since, it is extracted from movie subtitles that include informal words and phrases. Because Wikipedia contains documents from various domains, it covers more unique words in both English and Persian sides compared to the Roman parallel corpus. This fact can be recognized by comparing the statistics reported in Tables 4 and 7. That is why the results achieved by Wikipedia parallel corpus in AFEC test set is significantly higher than those obtained by the Roman parallel corpus.

The last column of Table 9 demonstrates the SMT results on the manually tagged parallel sentences from Wikipedia. Hence, the domain of this sentences would be general and more similar to the domain of the created parallel corpus. The results indicate that the Wikipedia parallel corpus is a better training data compared to the other two parallel corpora that are from different domains. We achieved the BLEU of 31.83 using a simple SMT system which has been used as a baseline in the literature. It shows the quality of our created parallel corpus for Wikipedia test set. Keep in mind that the domain of the other parallel corpora are not the same as the Wikipedia corpus. That is why their results are far lower than those obtained by the created parallel corpus. Note that all the improvements are statistically significant. Comparison of the results obtained by the Wikipedia and the Wiki-Champ parallel corpora indicates that the created parallel corpus has higher quality.

In this subsection, we consider another language pair (i.e., English–German) to experiment the proposed method on different languages. To do so, we create an English–German parallel corpus from the Wikipedia articles in these two languages. The experimental setup for extracting English–German parallel corpus is similar to those explained in Section 5.2. We use the well-known Europarl parallel corpus (version 7) (Koehn, 2005) to train the MaxEnt classifier. We train a SMT system using the created dataset. The experimental setup for training the Moses is similar to those presented in Section 5.4.

We compare the results of SMT systems which were learned on the Europarl parallel corpus and the extracted Wikipedia parallel corpus that was added to the Europarl corpus. Similar to Sections 5.3 and 5.4, we also extracted a parallel corpus from English–German Wikipedia articles using the Champollion method. This corpus is called Wiki-Champ. The statistics of the Europarl, Wiki-Champ, and the Wikipedia corpora are reported in Table 10
                        . Similar to Section 5.4, to tune the SMT parameters, we randomly select half of the test sentences as the development set and consider the rest of sentences as the test set. We repeat this process 10 times and report the average of the obtained results.

To evaluate the learned SMT systems, we consider the 2011 test set of the Machine Translation shared-task (WMT 2011).
                           18
                        
                        
                           18
                           
                              http://www.statmt.org/wmt11/translation-task.html.
                         This test set contains 3003 test instances. The sentences are from several news articles. Hence, it can be considered as an off-domain test set for both Europarl and the extracted parallel corpora. In this subsection, we use the BLEU measure which is introduced in Section 5.4.

We report the result of the trained SMT systems in Table 11
                        . According to the results reported in this table, although the size of the created English–German parallel corpus is by far smaller than the size of the Europarl corpus, adding the extracted parallel sentences to the large Europarl corpus improve the SMT performance, in terms of the BLEU score. This improvement is statistically significant based on the t-test in the 95% confidence interval. Experiments also shows the high quality of the created corpus compared to the Wiki-Champ parallel corpus.

In this paper, we proposed a novel approach for extracting parallel sentences from bilingual aligned documents. We exploited a learning approach to compute the local similarity between bilingual sentences. To exploit both local and global information, we proposed an integer linear programming (ILP) approach to identify parallel sentences. The proposed method is able to detect up to (k, k′) alignments; up to k sentences in the source language and up to k′ sentences in the target one for each alignment.

We extensively experimented the proposed method with various experimental setups. Intrinsic evaluation using a manually tagged dataset indicates that the proposed method significantly outperforms competitive baselines. We then focused on a resource-lean language pair, i.e., English-Persian, and extracted a parallel corpus from English and Persian Wikipedia articles using the proposed sentence alignment approach. We further extrinsically evaluated the created parallel corpus in cross language information retrieval (CLIR) and statistical machine translation (SMT) applications. In other words, we employed the created parallel corpus as training data for state-of-the-art CLIR and SMT systems and compare their performance with those obtained by the existing parallel corpora as training data. The CLIR and SMT experiments investigate high quality of the extracted parallel corpus. Since the proposed method does not contain any language-dependent components, we finally experimented the proposed method over an additional language pair, i.e., English–German, to show that that the proposed method is a language-independent sentence alignment approach.

In this paper, we cast the sentence alignment task to a matching problem in bipartite graphs whose objective function is maximizing the similarity of the selected edges as well as penalizing the number of selected edges that cross each other. Although our conjecture is that this matching problem is a NP-hard problem, we did not prove the correctness of this conjecture. An interesting possible future work is to either theoretically prove that this problem is NP-hard, or provide a straightforward polynomial time solution. It is notable that in any case, the proposed ILP formulation would be still valid. Furthermore, future work can focus on studying the influence of various objective functions as well as regularization terms in the experimental results of the proposed method. Moreover, extracting parallel sentences for other language pairs, especially for the resource-lean language pairs, can be considered in future.

This research was in part supported by two grants from Institute for Research in Fundamental Sciences (no. CS1395-4-19 and no. CS1395-4-05).

@&#ACKNOWLEDGEMENTS@&#

The authors would like to thank Hosein Azarbonyad and Razieh Rahimi for their helps to perform extrinsic evaluations. We thank the anonymous reviewers for their thoughtful and constructive comments.

@&#REFERENCES@&#

