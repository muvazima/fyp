@&#MAIN-TITLE@&#Contour-based focus of attention mechanism to speed up object detection and labeling in 3D scenes

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Fast focus of attention mechanism based on 3D contour features


                        
                        
                           
                           Individual contour features cast a vote for the presence of the entire object.


                        
                        
                           
                           Model parts are obtained to generalize across intra-class variations.


                        
                        
                           
                           Considerably speed up is achieved in comparison to a sliding window approach.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

3D contour-based features

Object center voting

Focus of attention

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Over the past few years, the increasing availability of low-cost 3D sensors has made it possible for the problem of detecting and localizing objects in 3D scenes to receive a great deal of attention. Recently, category-level detection and viewpoint estimation were addressed in [1] using 2D images to detect rigid 3D objects. A serious effort has been made to design robust and discriminative 3D features [2–7] with the purpose of finding correspondences between 3D point sets. Depending on their descriptiveness, 3D features may be complex and computationally demanding, sometimes making them unsuitable for real-time applications. While many of these methods use local image patches as basic features both for whole image classification [8–10] and object recognition [11–14], several approaches based on contour features have been recently proposed to address the class-level localization problem [15,16]. When working in complex scenarios, more edges are detected and, consequently, more contour features are extracted. Some of these features correspond to the boundaries of the object of interest and others to the background. The advantage of using contour features is that the information on object contours can be isolated from the background. This is especially true in 3D, where the extraction of contour features is influenced by the physical positioning of objects, making it robust to variations in color or lighting conditions. Image patch-based methods tend to fail due to interactions with the varying background.

In this paper, a 3D object detection pipeline is presented, see Fig 1
                     . The first stage in the pipeline consists of a 3D contour based feature extraction process. The extracted features correspond to a 3D version of the features presented in the work of Ferrari et al. [17]. Since these features are not only fast to compute but also less discriminative than others, which are more complex, they are used to quickly obtain rough estimates for an object's position in the scene.

During the training phase, representative features for each class are extracted to construct a part-based model of the object class. During the testing phase, the extracted features are matched with object models.

The second stage of the pipeline consists of a focus of attention mechanism based on a Hough-style object center voting scheme. Since each feature and object part have their associated local reference frame, the matching of their invariant descriptors induces a translation and a rotation, casting a vote for the presence of the object at a particular position in a 3-dimensional Hough space.

The basic idea of this mechanism consists of obtaining object center hypotheses that, though they may contain many false positives, are fast to compute. These hypotheses will guide the detection carried out by more descriptive features and speed up the overall detection procedure without losing performance.

The last stage of the pipeline performs object detection and labeling in the 3D scene using the excellent algorithm presented by Lai et al. in Ref. [18]. In order to speed up the detection, object center hypotheses obtained in the previous stage of the pipeline are thresholded and taken into account to compute object probability maps. Instead of scanning the entire scene to determine object positions, the HoG based sliding window feature extraction and classification algorithm has been modified. The goal of this modification is to compute features and apply classification filters at the positions where the votes of the previous stage indicate the presence of an object.

This paper is organized as follows: Section 2 gives an overview of different approaches in the field of 2D and 3D object recognition using contour-based methods. Section 3 presents the contour features used in this paper. Section 4 introduces the model part mechanism employed to match features and describes the voting approach that serves as a focus of attention system for the subsequent detection steps. Finally, Section 5 provides some results for the algorithm presented in this paper tested on some scenes. Some of these scenes are taken from the RGB-D dataset [19] while others are newly acquired. These results are also compared with those in Ref. [18].

@&#RELATED WORK@&#

Since the technique presented in this paper uses contour based features to perform a prior object classification and localization, other techniques that also use contour for recognition are summarized below.

The first methods that used contour for object detection performed matching using complete, rigid templates [20]. Then, contours were used to recognize articulated objects like people in Refs. [21,22] and hands in Ref. [23]. Certain techniques, like Ref. [24], depend on large training sets since they match complete contours and need large amounts of data to represent every object configuration.

Other methods rely on contour fragments to add robustness to the detection and do not depend on large datasets of training images. Nelson and Selinger [25] developed a method based on key curves: contour segments comprised between two high curvature points. The size and orientation of a key curve define a square image patch. Each patch is described using the contour points contained in it. These contour fragments are then grouped into a multilevel system for recognizing simple 3D objects. One drawback present in almost all patch-based methods is that they will take into consideration clutter contours lying near the object boundary. In Ref. [26] contour fragments are incorporated into the constellation model. In Ref. [27] pictorial structures are presented as a layered structure of contour fragments and used to detect articulated objects.

Local contour descriptors have also been used to detect objects. Shape contexts [28] describe sampled edge points in a log-polar histogram. In Ref. [11] a Geometric Blur neighborhood descriptor is applied to individual contour segments. Then, a correspondence between pairs of images is established using a non-rigid point matching algorithm. One drawback of this method is that the recognition is reduced to a correspondence between training and testing images. Consequently, a class model containing specific features of the class cannot be generalized.

An alternative to using generic contour features is to learn features adapted to a particular object class. Shotton et al. [16] and Opelt et al. [29] learn class-specific boundary fragments and their spatial arrangement as a star constellation of parts. These fragments store their relative position and orientation with respect to the object center, enabling object localization in test images. Other methods, like Dalal and Triggs [12] and Ferrari et al. [17] achieve this functionality by tiling object windows. Recently, Ferrari et al. [30] incorporated a shape model to generalize across intra-class variations and to increase the performance of their detector, finding not only bounding-boxes but also the actual boundary of objects.

The work presented in this paper is influenced by these last works. This is because the authors believe that although parts are individually less discriminative than entire object contours, in conjunction with each other they provide robustness against clutter and occlusion, being able to generalize across both rigid and articulated objects. The contour features described in this paper are a 3D version of the features presented in Ref. [17]. In order to obtain descriptors which are invariant to translation and rotation, a local reference frame to the feature has been developed.

3D edge detection is performed on the point cloud. Various edges are detected from geometric shapes (boundary, occluding, occluded and high curvature edges). The method assumes that the point cloud is organized as a matrix of points with a known height and width. For boundary edges, it searches for depth discontinuities with a given threshold value that is linearly adapted as a function of the depth values. Since the point cloud is organized, this operation is efficient. In the Kinect sensor, points that are positioned between occluding and occluded points, where no information can be retrieved, are assigned a special value (Not a Number NaN). The method searches for corresponding points across the NaN area. This search is done in an organized fashion, and consequently it is not too time consuming.

The resulting edge points are linked at their discontinuities and straight segments are fit onto them using a RANSAC based approach, see Fig. 2
                        . The edge point cloud is subsampled and a set of representative points are extracted. For each one of these points, their neighbors at different radii are extracted and stored in different point sets. For each point set, RANSAC is performed in order to obtain the coefficients of a line that describe it. Then, inliers are projected into the line. The most distant projected points define the segment. Computed segments are ordered based on their length and the average distance to the points they interpolate, following Eq. 1
                     


                        
                           
                              (1)
                              
                                 
                                    α
                                    ∗
                                    
                                       
                                          1
                                          −
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      n
                                                   
                                                   
                                                
                                                
                                                dis
                                                
                                                   t
                                                   i
                                                
                                             
                                             r
                                          
                                       
                                    
                                    +
                                    β
                                    ∗
                                    
                                       l
                                       
                                          l
                                          max
                                       
                                    
                                 
                              
                           
                        where:
                           
                              •
                              
                                 n is the number of inliers,


                                 dist
                                 
                                    i
                                  is the distance from inlier i to the interpolated line,


                                 r is the search radius used to obtain the neighborhood of points,


                                 l is the segment length,


                                 l
                                 
                                    max
                                  is the maximum segment length allowed, and


                                 α and β are weights used to tune the importance of fitting to the points or to the segment length when ranking the segment.

Segments are picked using this order. Each time a segment is picked, its inliers are marked as visited. Segments that have a certain percentage of inliers not yet visited are eligible. To permit a certain degree of overlapping (which adds robustness to the detection) it may be possible to select a segment with a high number of already visited inliers only if it is sufficiently dissimilar from the segment that is representing these inliers. This approach works somewhat better than a purely greedy one (selecting the best segment that describes the area). For a particular area, it may be possible to extract two dissimilar segments that share a high number of inliers. If these two segments are ranked similarly, it is better to retrieve both so as to take them into account when constructing the model parts. If not, then the retrieved segment in this area may not have a good match during the testing phase.

Following the work of Ref. [17], a variant of their Pair of Adjacent Segments feature to work with 3D points is used in this paper. In the simplified approach presented here, two segments are considered adjacent if one end of the first segment is close enough to one end of the second segment. Each pair of connected segments forms one feature, see Fig. 3
                        . A local feature P
                        =(x
                        
                           c
                        , y
                        
                           c
                        , z
                        
                           c
                        , s, d) is characterized by the location of its center (x
                        
                           c
                        , y
                        
                           c
                        , z
                        
                           c
                        ) (mean over the adjacent ends of the segments), a strength s (which is the mean strength of both segments) and an invariant descriptor d
                        =(p
                        1
                        
                           x
                        , p
                        1
                        
                           y
                        , p
                        1
                        
                           z
                        , p
                        2
                        
                           x
                        , p
                        2
                        
                           y
                        , p
                        3
                        
                           z
                        , l
                        1, l
                        2), Both p
                        1 and p
                        2 are the far ends of the segments. As this is an invariant descriptor, p
                        1 and p
                        2 are expressed in terms of their local reference frame. l
                        1 and l
                        2 are the normalized lengths of the segments with respect to the maximum allowed length for a segment. When working with 3D information, as in this case, there is no need to add a scale term to the feature because it is implicit in the length of its segments. The order in which the segments of the feature are described is important because it has to be done in a repeatable manner, such that similar features have similar order. Using their associated local reference frames, segments are ordered according to their midpoints m
                        1
                        =(x
                        1, y
                        1, z
                        1) and m
                        2
                        =(x
                        2, y
                        2, z
                        2). They are ordered by the x coordinate of their midpoints if (x
                        1
                        −
                        x
                        2)>0.2∗|m
                        1
                        −
                        m
                        2|. If this last condition is not satisfied, they are ordered by the y coordinate of their midpoints if (y
                        1
                        −
                        y
                        2)>0.2∗|m
                        1
                        −
                        m
                        2|. Finally, if that condition is not satisfied, they are ordered by the z coordinate of their midpoints. This mechanism adds robustness to the segment order by not relying on a specific coordinate to arrange them if both of its midpoints are very similar. When constructing the feature descriptor, segments are modified in such a way that their adjacent ends coincide at (x
                        
                           c
                        , y
                        
                           c
                        , z
                        
                           c
                        ).

Using these kinds of features provides a few advantages with respect to other contour detectors. First, by using the RANSAC approach for the segment extraction presented above, robustness is added to the feature matching because segments are connected even across edge gaps. Second, in contrast to patch detectors, the features presented are formed by two segments that can cover a portion of the object contour, leaving out background clutter. Third, these kinds of features offer a good tradeoff between complexity and descriptiveness, meaning that they are fast to compute and, at the same time, complex enough to be informative. In Ref. [17] presents a discussion of why it is better to work with second order-adjacent segments (PAS) rather than other higher order features. Their conclusions are also applicable to 3D data. Finally, since a correspondence between two of these invariant features induces a geometric translation and a rotation, they are adequate for use in a Hough-based voting scheme for object detection.

The descriptors associated with the features presented above have to be translation and rotation invariant. This means that they cannot be described using coordinates in a global reference frame, since this would make them dependent on the specific reference frame of the current 3D point cloud. Hence, we need to compute an invariant Reference Frame for each feature extracted (a Local Reference Frame). This local RF must be efficient to compute, as one of these is computed for each feature extracted, and robust against disturbance factors. Several proposals for local RF for 3D point clouds have been presented [4,7,31–33].

Next, a fully unambiguous local RF method used in this approach is explained. Assuming that the local RF is located at (x,y,z) which, in turn, is the center of the feature, the Z axis of the local RF is obtained as the normal vector to a plane interpolated from the vicinity of points at position (x,y,z). The direction of the Y-axis is the projection of the bisector of the angle formed by the non-adjacent ends of the segments and (x,y,z) on the interpolation plane. Since the feature describes the contour of an object, the Y-axis will be pointing in a direction outside the object. This is done by looking for the direction of Y where the point density of the cloud is lowest. The X-axis is readily obtained as the cross product of the Z- and Y-axes. Each feature descriptor is transformed into its local RF to make it rotation and translation invariant, see Fig. 4
                        .

The distance D(P, Q)∈[0, 1] between two features is obtained using Eq. 2. The first term of this equation measures the difference between segment orientations and the second one accounts for the difference in lengths.


                        
                           
                              (2)
                              
                                 
                                    D
                                    
                                       
                                          d
                                          p
                                       
                                       
                                          d
                                          q
                                       
                                    
                                    =
                                    γ
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                2
                                             
                                             
                                          
                                          
                                          
                                             
                                                
                                                   θ
                                                   i
                                                
                                                
                                                   2
                                                   π
                                                
                                             
                                          
                                       
                                       4
                                    
                                    +
                                    η
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                2
                                             
                                             
                                          
                                          
                                          |
                                          
                                             l
                                             i
                                             p
                                          
                                          −
                                          
                                             l
                                             i
                                             q
                                          
                                          |
                                       
                                       4
                                    
                                 
                              
                           
                        where θ
                        
                           i
                         is the angle formed by the segments in position i of both descriptors. γ∈[0, 1] and η
                        ∈[0, 1] are weights used to tune the importance of the dissimilarity in angles vs the importance of the dissimilarity in lengths in the feature distance calculation.

A codebook C
                        =
                        t
                        
                           i
                         is constructed by clustering the features extracted from the training set. In keeping with the clique-partitioning approximation used in Ref. [17], let G be a complete graph whose nodes are the training features. The arcs are weighted by d
                        −
                        D(a, b), where d is a distance threshold. G is partitioned into cliques so as to maximize the sum of intra-clique weights. Each resulting clique is a cluster of similar features. For each cluster, the centermost feature is retained, minimizing the sum of dissimilarities with respect to all other features of the cluster. The codebook is the collection of this centermost feature. This is convenient because it avoids the need to compare every feature extracted from the test set to every feature from the training set. Instead, an efficient comparison to considerably fewer representative features suffices. The codebook is class-specific and it is constructed from the same data that are used later to obtain the object parts.

Object parts are components of objects that are recognizable across different object instances of the same class. In order to obtain the object model parts, the basic idea is to retrieve features that recur consistently across several training instances with similar locations, sizes and shapes, not taking into account features that are too specific to individual training instances. To accomplish this, each feature extracted from every training instance is compared to the codebook. If the dissimilarity is below a certain threshold ζ, a vote is cast for this specific feature type and position. V
                     
                        i
                      is the voting space associated with a feature of type t
                     
                        i
                     . There are a total of |C| voting spaces, all initially empty. As in Ref. [30], the strength of the vote cast by feature P is s
                     ∗(1−
                     D(d, t
                     
                        j
                     )/ζ), where s is the strength of the feature, d is the feature's descriptor and t
                     
                        j
                      is the feature type associated with voting space V
                     
                        j
                     .

If P is assigned to the most similar codebook entry only, the model becomes too sensitive to exact correspondences to features that were seen during the training phase. Instead of this, P is assigned to multiple codebook entries and its vote is weighted with the feature strength. This makes it so that partial contributions from features based on their dissimilarities are taken into account when computing the overall relevance of a codebook entry. This allows the model to generalize better during the testing phase. Local maxima are computed for every voting space. Each local maximum indicates the presence of a model part in this position. The value of the local maximum describes the confidence that the part belongs to the model.

During the testing phase, the features extracted from the scene are matched to the object parts. This matching is based on the objects' descriptors. More precisely, a feature is matched to an object part if their dissimilarity is below a threshold ζ, the same as that used to obtain the parts for the object model. Since each feature and object part have their associated local reference frame, the matching of their invariant descriptors induces a translation and a rotation, casting a vote for the presence of the object at a particular position and orientation. Votes are weighted by the similarity of the feature and the object part, by the strength of the feature, and by the confidence of that part, see Eq. 3.


                     
                        
                           (3)
                           
                              
                                 vot
                                 
                                    e
                                    weight
                                 
                                 =
                                 s
                                 ∗
                                 conf
                                 
                                    
                                       t
                                       j
                                    
                                 
                                 ∗
                                 
                                    
                                       1
                                       −
                                       D
                                       
                                          d
                                          
                                             t
                                             j
                                          
                                       
                                       /
                                       ζ
                                    
                                 
                              
                           
                        
                     where s is the feature strength, d the feature descriptor and conf(t
                     
                        j
                     ) the confidence of part of type t
                     
                        j
                     . Although this is not a precise mechanism for object detection on its own (it delivers a high number of false positives, see Fig. 5
                     ), it serves its purpose of being a focus of attention for more fine grained and computationally costly methods. 3-dimensional positions of the voted object centers are projected to 2D in order to obtain the positions where the image detector is applied. Instead of dealing with all the possible positions for each object class, this voting scheme gives far fewer center hypotheses of where to look for objects. In this approach, a HoG based method is employed as the next step in the detection. This method is used in conjunction with the center voting mechanism to extract HoG features to specific positions in the image, drastically reducing the problem complexity, see Fig 6
                     .

The RGB-D Object Dataset [19] contains 250,000 segmented RGB-D images of 300 objects in 51 categories. It also includes the RGB-D Scenes Dataset, which consists of eight video sequences of office and kitchen environments. These scenes contain five object categories: bowl, cap, cereal box, coffee mug and soda can. In order to show the results for the remaining object categories, 62 new scenes of kitchen, office, bathroom and dinning room environments were recorded using a Kinect sensor. Each new scene contains objects from 3 to 5 categories, all of them being new instances not present in the original dataset. Each object category appears in ≈5 video sequences. The Kinect Fusion Large Scale 
                     [38] implementation from the Point Cloud Library [37] was used to register the point clouds and to obtain the camera parameters for every scene frame.

The method presented was compared with other 3D object centroid detection methods. Implementations of the Implicit Shape Model [36] and Correspondence Grouping were used. Both implementations were obtained from the Point Cloud Library [37]. In addition to the original implementation of The Implicit Shape Model method, which uses the FPFH descriptor [39,40], SPIN image descriptor [41] was also used. In the same way, the Correspondence Grouping method was extended to use the FPFH descriptor (it uses the SHOT descriptor [42,43] by default). Both variants of the clustering method in the Correspondence Grouping algorithm (3D Hough [34] and Geometric Consistence [35]) were tested.


                     Figs. 7 and 8
                     
                      show the average accuracy for each object and for every method tested. The average accuracy for a particular object was computed across every scene in which that particular object appears. CFG means Correspondence Grouping+FPFH+Geometric Consistency, CFH means Correspondence Grouping+FPFH+3D Hough, CSG means Correspondence Grouping+SHOT+Geometric Consistence, CSH means Correspondence Grouping+SHOT+3D Hough, IF means Implicit Shape Model+FPFH, IS means Implicit Shape Model+SPIN images and OUR is the proposed method.

Both Implicit Shape Model and Correspondence Grouping use non-maxima suppression to obtain the object center predictions. In order to properly compare the results, non-maxima suppression was also implemented in the proposed method by taking into account the object radius. Centroid predictions are compared to pre-labeled ground truth.

Different configurations of clustering parameters and voting thresholds in both Implicit Shape Model and Correspondence Grouping were tested for each object class. The goal was to use a configuration that optimizes accuracy while maintaining all true positives if possible. This was done because for an attention mechanism that is going to be used as a guide for a subsequent HoG-based detector, it is more important to detect the true object position (with a high false positive rate) than to detect few centroid positions that do not include the correct one. This parameter tweaking is especially important when working with highly descriptive features like SHOT or FPFH. The use of these features may improve accuracy when detecting previously seen instances of objects. However, due to their specificity, they may fail to generalize when detecting unseen objects of classes with highly variable colors or shapes.

In general, the accuracy for large objects (e.g. Cereal Box) is higher than for small ones (e.g. Dry Battery). This is mainly because using non-maxima suppression with the object radius causes fewer centroid predictions when detecting large objects than when detecting small ones. Since the SHOT descriptor takes into account the color, Correspondence Grouping with SHOT usually works better in object classes that have a stable color across object instances, like the lime, pear or banana classes. FPFH descriptor works better in object classes that have low intra class geometric variations like the Food Box or the Sponge classes.

If the average accuracy across all object classes is considered, the best methods are the Correspondence Grouping+SHOT descriptor+3D Hough and the Implicit Shape Model+FPFH. As expected, the accuracy is generally lower with the proposed method. This is due to the fact that contour features are less descriptive than SHOT or FPFH features. The accuracy loss in the centroid prediction is mainly caused by the high number of false positives obtained in comparison to the other methods. This high number of centroids will cause the HoG based detector to explore more positions in the image. However, the main strength of the proposed method is its speed, which compensates for the fact that the HoG detector has to explore more positions. As Tables 1 and 2
                     
                      show, the speedup using our proposed method is significantly higher than using the Implicit Shape Model or the Correspondence Grouping methods.


                     Fig. 9
                      shows a sample of centroid predictions using the proposed method on some frames of the recorded scenes.

In order to appropriately compare the results obtained with those presented in Ref. [18], the same experimental setup was used. To evaluate the ability of the method presented to detect and label objects in 3D scenes and to compare computation times, the eight video sequences contained in the dataset were used. Five object categories were used: bowl, cap, cereal box, coffee mug and soda can. Everything else is labeled as background. Each scene contains between 2 and 11 objects from the 5 categories. In order to expand the results, nine more video sequences were used. These scenes contain between 3 and 5 objects from 29 categories.

As in Ref. [18], a linear SVM detector was trained for every object category. The same voxel representation is employed, with 1cm×1cm×1cmvoxels. Each voxel's neighborhood consists of 26 directly adjacent voxels, see Ref. [18] for further details.

In Tables 1 and 2 a comparison between the original sliding window approach (HoG) [18] and three different centroid detection methods is shown. IF means Implicit Shape Model+FPFH descriptor method, CSH means Correspondence Grouping+SHOT descriptor and OUR is the method presented in this paper. The first table contains information on the first eight video sequences and the second table contains information on the remaining nine video sequences.

The first section of each table lists the average accuracy achieved by all of the methods tested. Sometimes the accuracy is higher using attentional mechanisms than with the sliding window approach. This is due to the fact that the sliding window is obtaining recurrent false positives in a specific background position, labeling it as a particular object. When the bounding boxes are pruned with the labeled scene, this wrongly labeled area confirms false positives, causing the accuracy to decrease. The attention mechanism can help in these cases by reducing the number of false positives for the sliding window approach and preventing the scene from being wrongly labeled, thus increasing the accuracy.

The next section in each table shows the savings for the same three methods. In Ref. [18], image pyramidation is carried out to obtain HoG features in every position and scale. Then a sliding window classifier is applied. With the attentional method proposed, the features are extracted only in certain positions. In these positions, classification filtering is also applied. In order to compare the computation times, a percentage is calculated that compares the number of positions where features are extracted and where SVM classifier filtering is applied to the proposed approach and to the sliding window mechanism. The savings are proportional to the number of centroid predictions. In general, savings are greater with both IF and CSH.

The next section in each table shows average computation times per frame for the centroid detection methods tested. The speed of the proposed method is significantly higher than the speed of the other two methods. A single threaded C++ implementation was used. Given that the feature extraction and matching process explained in previous sections are highly parallelizable, the authors believe that a more optimized CUDA version will improve the computation time further. In the method presented, the number of contour features that are extracted depends on the number of edges present in the scene. That means that the computation times in complex, cluttered scenes are higher than the computation times in simple scenes. In the IF and CSH methods, features are extracted at keypoints, which are the points resulting from the downsampled scene point cloud. That means that the computation times are more constant and independent of the scene's complexity.

The last section of each table provides the average total time to process each frame. This is the time needed for the HoG-based detector plus the overhead introduced by the attention mechanism. As we can see, even though the savings in the HoG detection are lower, the much smaller time overhead introduced by the method presented compensates for this.

The total time needed to process each frame is smaller using the proposed method in every video sequence, attaining a level of accuracy that is comparable with the other methods tested. According to Ref. [18], the detection requires 3.6s to process each frame. As shown, savings of 80%–90% are obtained with the attentional mechanism, meaning the detection process can be completed in ≈0.54s. The attentional method proposed introduces an overhead of ≈350ms depending on the video sequence, reducing the total time needed to process each frame to ≈0.8s.

For the sake of comparison with Ref. [18], the same tests were performed as in that reference. In Fig. 10
                         (scenes 1–8) and Fig. 11
                         (scenes 9–17) precision-recall curves are shown for bounding-box proposals with the sliding-window mechanism (green) and with the attentional mechanism proposed in this paper (blue), both with a low score threshold (−0.6). As we can see, the detection performance is quite similar in both methods. As in Ref. [18], detection results are refined by pruning them against a previously labeled scene. Precision–recall curves are shown for the pruned versions of the bounding-box proposals with the sliding window mechanism (cyan) and with the proposed method (red). Note the similarity in the results.


                        Figs. 12,13, 14 and 15
                        
                        
                        
                         show the confusion matrices obtained for all the labeled scenes using the proposed method. The lesser performance in some scenes like 2, 3, 11, 14, 16 and 17 is due to high scoring false positives. In these scenes with complex backgrounds, similar objects to those being detected produce hard false positives. Usually this is solved during the pruning phase and the performance remains high. In other cases, like scenes 2 and 16, recurrent false positives in the same positions cause the scene to be wrongly labeled. In these cases, false positives are not pruned and performance remains low.

Note that, in general, the method performs well and, as described in subsection 5.1, a significant speedup is achieved with the attentional mechanism proposed in this paper.


                        Figs. 16–21
                        
                        
                        
                        
                        
                         provide detailed labeling results for scenes 1–17 using the method presented in Ref. [18] with the attentional mechanism proposed. The MRF-based labeling method exhibits some problems labeling planar objects correctly (like the notebook in scene 17 or the scissors in scene 13) or thin objects (like the toothbrush in scene 11). This is normal because the method is designed to detect other types of objects, namely those that stand out more from the surface where they are located. In general, the labeling results are good in all the object categories tested.

@&#CONCLUSIONS@&#

In this paper, a focus of attention mechanism to speed up the detection and labeling of objects in 3D scenes has been presented. With this method, HoG features extraction and window based classification filters are applied to certain positions of the image where the objects are predicted to be located. Detection performance and computation time results are compared to a sliding window detection approach. The results show that the proposed method considerably speeds up the detection process without sacrificing any performance in terms of object detection.

@&#ACKNOWLEDGMENT@&#

The authors gratefully acknowledge the contribution of the Spanish Ministry of Science and Technology under Projects DPI2010-18349, DPI2013-46897 and the funds from the Agencia Canaria de Investigación, Innovación y Sociedad de la Información (ACIISI).

The following is the supplementary data to this article.
                        
                           
                              Supplementary video.
                           
                           
                        
                     
                  

Supplementary data to this article can be found online at http://dx.doi.org/10.1016/j.imavis.2014.02.013.

@&#REFERENCES@&#

