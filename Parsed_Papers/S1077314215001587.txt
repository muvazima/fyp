@&#MAIN-TITLE@&#An efficient multimodal 2D + 3D feature-based approach to automatic facial expression recognition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a feature-based 2D+3D multimodal facial expression recognition method.


                        
                        
                           
                           It is fully automatic benefit from a large set of automatically detected landmarks.


                        
                        
                           
                           The complementarities between 2D and 3D features are comprehensively demonstrated.


                        
                        
                           
                           Our method achieves the best accuracy on the BU–3DFE database so far.


                        
                        
                           
                           A good generalization ability is shown on the Bosphorus database.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Facial expression recognition

Local texture descriptor

Local shape descriptor

Multimodal fusion

@&#ABSTRACT@&#


               
               
                  We present a fully automatic multimodal 2D + 3D feature-based facial expression recognition approach and demonstrate its performance on the BU–3DFE database. Our approach combines multi-order gradient-based local texture and shape descriptors in order to achieve efficiency and robustness. First, a large set of fiducial facial landmarks of 2D face images along with their 3D face scans are localized using a novel algorithm namely incremental Parallel Cascade of Linear Regression (iPar–CLR). Then, a novel Histogram of Second Order Gradients (HSOG) based local image descriptor in conjunction with the widely used first-order gradient based SIFT descriptor are used to describe the local texture around each 2D landmark. Similarly, the local geometry around each 3D landmark is described by two novel local shape descriptors constructed using the first-order and the second-order surface differential geometry quantities, i.e., Histogram of mesh Gradients (meshHOG) and Histogram of mesh Shape index (curvature quantization, meshHOS). Finally, the Support Vector Machine (SVM) based recognition results of all 2D and 3D descriptors are fused at both feature-level and score-level to further improve the accuracy. Comprehensive experimental results demonstrate that there exist impressive complementary characteristics between the 2D and 3D descriptors. We use the BU–3DFE benchmark to compare our approach to the state-of-the-art ones. Our multimodal feature-based approach outperforms the others by achieving an average recognition accuracy of 86.32%. Moreover, a good generalization ability is shown on the Bosphorus database.
               
            

@&#INTRODUCTION@&#

Affect recognition aims to determine an individual’s emotion by detecting and measuring the emotion related physiological (e.g., bodily symptoms), psychological (e.g., feelings) or behavioral (e.g., facial expression) characteristics [1,2]. As an easily detectable, collectible, and measurable emotion component, facial expression is ideal for affect recognition and for human-computer interaction related applications [3]. However, Facial Expression Recognition (FER) is a very challenging problem mainly because of the diversity and hybridity of human expressions among different subjects in different cultures, genders and contexts.

In the past decades, a large number of FER approaches have been proposed. They can be categorized from three perspectives, namely the data modality, expression granularity, and temporal dynamics. From the first perspective, they are classified into 1) 2D FER (which uses 2D gray or color face images), 2) 3D FER (which uses 3D range images, point clouds, or meshes of faces), and 3) multimodal 2D + 3D FER (which uses both 2D and 3D facial data). From the second perspective, they are divided into 1) six basic facial expression (i.e., anger, disgust, fear, happiness, sadness, and surprise) recognition, 2) facial Action Unit (AU, e.g., brow raiser, lip tightener, and mouth stretch) detection and recognition. From the third perspective, they are categorized into static (still images) and dynamic (image sequences) FER. In this paper, we focus on the problem of recognizing the six basic facial expressions using multimodal 2D + 3D static images.

Appearance-based 2D FER has been widely investigated since 1990s [3]. The main research topics lie in three aspects: face detection, expression related feature extraction and classification. Comprehensive surveys of 2D FER approaches are given in [3,4]. They are mainly classified into two categories, i.e., template-based and feature-based [4]. Template-based approaches usually fit a holistic face model to the input image or track it in the input image sequence. Active appearance model [5], point distribution model [6], mixture of probabilistic PCA [7], and topographic modeling [8] are some typical examples. Feature-based approaches generally localize the features of an analytic face model in the input image or track them in the input sequence. Gabor wavelets [9] and Local Binary Patterns (LBP) [10] based face representations are two popular representatives. Although considerable advancements have been achieved, 2D FER is still very challenging mainly due to its sensitivity to illumination, pose variations, and possible occlusions [3,4].

Recently, with the rapid development of 3D imaging and scanning technologies, it becomes more and more popular to capture 3D face scans. Comparing with 2D face images, 3D face scans contain precise geometric shape information of facial surfaces, which is robust to illumination and pose variations, but more sensitive to facial expression changes. Thus, shape-based 3D FER has attracted increasing attentions. Similar to 2D, 3D FER approaches can also be categorized into template-based and feature-based. Template-based approaches usually build a parametric deformable face model first, and then extract the model parameters as expression features for recognition. 3D morphable model [11], bilinear deformable model [12], shape deformation model [13], and statistical feature model [14] are some famous examples. The main drawback of template-based approaches lies in that they require to establish one-to-one correspondence between 3D face scans, which is still a very challenging issue. Meanwhile, time consuming procedures like dense 3D face registration and model fitting are indispensable. Feature-based approaches generally extract 3D expression cues around facial landmarks using different facial surface geometric or differential quantities. For example, the distances between 3D facial landmarks are widely used in [15–17], and [18]. Moreover, 3D facial curves [19], facial geometry images and normal maps [20,21] facial conformal images [22], facial surface normal [23,24] and curvatures [23–25], and local depth-SIFT features [26] are some popular expression features. Feature-based approaches generally perform better than template-based ones. However, the bottleneck of feature-based approaches lies in accurate and robust 3D facial landmark localization, which is still a very difficult task [27]. More detailed surveys of 3D facial expression recognition are given in [28,29].

Although the effectiveness of multimodal 2D + 3D face recognition has been well presented as in [30,31], the investigation of multimodal 2D + 3D FER is very limited. Wang et al. [25] compared the FER accuracy of 3D primitive surface feature distribution based approach with 2D Gabor-wavelet and Topographic Context based ones on the BU–3DFE database, and found that 3D shape based approach is superior to 2D ones, especially for non-frontal faces. However, the effectiveness of combing 3D and 2D approaches was not discussed. Zhao et al. [14] used both 2D features (RGB values and LBP) and 3D features (3D coordinates and shape index values) in the 3D statistical feature model for prototypical expression recognition. But the results using only 2D features or 3D features were not reported, and thus the complementarity between 2D and 3D features was also not studied. In [32], the authors used both 2D and 3D dynamic data for real-time facial action and expression recognition. More precisely, they first extended the active shape model to handle 3D data for facial feature tracking. Then, they extracted numerous geometric measurements (e.g., the distances between landmarks and the boundary shape of lips) and surface deformation measurements (e.g., image gradient and surface curvature descriptors). Finally, the Rule Classifier was used for recognizing a subset of 11 important AUs and 4 facial expressions (i.e., happy, sad, surprise, disgust) on a dataset consisting of 832 sequences of 52 participants. Their experimental results demonstrated that the proposed 2D+3D algorithm performed much better than the 2D appearance-based algorithm (i.e., 2D ASM + Gabor filters + LDA) for recognizing the four facial expressions. This is a very illuminating approach for 2D+3D multimodal FER. However, they did not report the performance of each modality under their own framework. The importance of each modality is still unclear. Savran et al. [33] utilized multimodal 2D + 3D face data for facial AU detection. They found that 3D data generally perform better than 2D data, especially for lower AUs. Moreover, the fusion of two modalities can improve the detection rates from 93.5% (2D) and 95.4% (3D) to 97.1% (2D+3D). Except for facial AU detection and expression recognition, Wang et al. [34] quantified facial expression abnormality in Schizophrenia by combining 2D and 3D features. Their experimental results demonstrated that the combined features better characterized facial expressions than either individual 3D geometric or 2D texture features.

The above studies have preliminarily proved the fact that the combination of 2D and 3D data is better than either of the single 2D or 3D modality for expression characterization and AU detection, but deep analysis of the superiority for multimodal 2D+3D FER is still missing. An advantage of using 2D data is that it can be used to accurately localize a large set of facial landmarks on 2D face images and further on their 3D face scans due to the 2D–3D correspondence, which is the first contribution of this paper. More precisely, we propose to explore the incremental Parallel Cascade of Linear Regression (iPar–CLR) algorithm [35] to automatically localize 49 landmarks for each 2D face image and its corresponding 3D mesh scan. This large set of expression related landmarks are then used for extracting local texture and shape descriptors for expression classification. To the best of our knowledge, this is the first work which uses such large number of automatically detected landmarks for 2D and 3D multimodal FER. In contrast, the majority of existing feature-based 3D FER approaches reported their results on the BU–3DFE benchmark based on a large set of (typically 83) 3D facial landmarks manually localized by the database providers [15–19,23,25,26]. Therefore, the proposed framework presents a promising way to these landmark-based approaches so that they can be made automatic using the iPar–CLR algorithm in 2D and 3D multimodal face space.

The second contribution of this paper is that a novel second-order image gradient based local texture descriptor (HSOG), a novel first-order mesh gradient (i.e., surface normal) based local shape descriptor (meshHOG), as well as a second-order mesh gradient (i.e., surface curvature) based local shape descriptor (meshHOS) are adapted in FER to comprehensively encode the expression variations in both the 2D and 3D modalities. According to our previous work [36], most of existing popular local image descriptors, such as HOG, LBP, and SIFT, only employ the first-order gradient information related to the slope and the elasticity, i.e., length, area, etc. when the image is regarded as a surface, and thereby partially characterize its geometric properties. By contrast, HSOG captures the curvature related cues of the surface, i.e., cliffs, ridges, summits, valleys, basins, and so on. Thus, HSOG can be applied to describe facial expression deformations (e.g., mouth stretch, lip stretcher, brow raiser). Moreover, in that paper, it was also demonstrated that HSOG outperformed the first-order gradient based local image descriptors (i.e., HOG, LBP, SIFT) when there were not severe scale variations, as in the applications of local image matching and scene classification. In this paper, we give another evidence of the effectiveness and generalization ability of HSOG for FER. Similarly, as general local shape descriptors, meshHOG and meshHOS provide a compact description of the facial surface normal and curvature information, and they have proved very efficient for 3D face identification in our previous works [37,38]. In this paper, we interested in exploring their generalization abilities in 3D FER.

During the FER stage, both the early fusion (i.e., feature-level) and late fusion (i.e., score-level) strategies of 2D descriptors, 3D descriptors, as well as 2D and 3D descriptors are comprehensively demonstrated and their complementary characteristics are well revealed, which is our third contribution. The important findings behind the fusion results can be summarized as: 1) The second-order gradient based local texture or shape descriptor (HSOG or meshHOS) generally have stronger discriminative power than the first-order gradient based ones (SIFT or meshHOG). Moreover, different order 2D or 3D descriptors are complementary in encoding local texture or shape cues. 2) There exist large complementary characteristics between 2D and 3D descriptors of the same order (SIFT and meshHOG, HSOG and meshHOS), different order (SIFT and meshHOS, HSOG and meshHOG), as well as multiple orders (all four 2D and 3D descriptors).

Overall, we present an efficient multimodal (2D and 3D) and multiple-order (first and second) feature-based fully automatic FER approach, and validate it trough comprehensive experiments on the BU–3DFE database. Considerable complementary characteristics between the features of different orders and different modalities are highlighted either by early fusion or late fusion of 2D, 3D, as well as 2D and 3D descriptors. The generalization capability of our approach is further evaluated on the Bosphorus database.

This paper is an extension of our work presented in [23] and is organized as follows. Section 2 introduces the iPar–CLR based 2D and 3D facial landmark localization procedure. Section 3 and 4 give the construction details of the HSOG, meshHOG and meshHOS descriptors. Section 5 lists and compares the accuracies of each single 2D and 3D descriptor, and the ones of their fusion. The generalization capability of the proposed approach is discussed in Section 6. Finally, we conclude the paper and point out the limitations and future directions.

To extract expression related features, a set of key landmarks are required. In this paper, we introduce the incremental Parallel Cascade of Linear Regression (iPar–CLR) [35] for face landmarking in the 2D modality. iPar–CLR is an incremental and parallel version of the Sequential Cascade of Linear Regression (Seq–CLR) algorithm [39]. Given a set of training face images Ii
                      associated with p 2D landmarks 
                        
                           
                              x
                              i
                           
                           ∈
                           
                              R
                              
                                 2
                                 p
                                 ×
                                 1
                              
                           
                        
                     . f is a feature extraction function (e.g., SIFT) and 
                        
                           f
                           
                              (
                              
                                 x
                                 i
                              
                              )
                           
                           ∈
                           
                              R
                              
                                 128
                                 p
                                 ×
                                 1
                              
                           
                        
                      in the case of extracting SIFT features. During training, one assumes that p corrected landmarks are known for each Ii
                     , and denoted as 
                        
                           x
                           
                              *
                           
                           i
                        
                     . To reproduce the testing scenario, one runs the face detector on the training images to provide an initial configuration of the p landmarks 
                        
                           
                              x
                              0
                              i
                           
                           ,
                        
                      which corresponds to an average shape. In this setting, the Seq–CLR algorithm is formulated as:

                        
                           (1)
                           
                              
                                 arg
                                 
                                    min
                                    
                                       
                                          W
                                          k
                                       
                                       ,
                                       
                                          b
                                          k
                                       
                                    
                                 
                                 
                                    ∑
                                    
                                       I
                                       i
                                    
                                 
                                 
                                    ∑
                                    
                                       x
                                       k
                                       i
                                    
                                 
                                 
                                    
                                       ∥
                                       
                                          x
                                          
                                             *
                                          
                                          i
                                       
                                       −
                                       
                                          x
                                          k
                                          i
                                       
                                       −
                                       
                                          W
                                          k
                                       
                                       f
                                       
                                          (
                                          
                                             x
                                             k
                                             i
                                          
                                          )
                                       
                                       −
                                       
                                          b
                                          k
                                       
                                       ∥
                                    
                                    2
                                 
                                 .
                              
                           
                        
                     
                  

In practice, W
                     0 and b
                     0 are first estimated using 
                        
                           
                              x
                              
                                 *
                              
                              i
                           
                           ,
                        
                     
                     
                        
                           
                              x
                              0
                              i
                           
                           ,
                        
                      and 
                        
                           f
                           (
                           
                              x
                              0
                              i
                           
                           )
                        
                     . Then, a sequence of regressions are computed to update 
                        
                           x
                           k
                           i
                        
                      and make it converge to 
                        
                           x
                           
                              *
                           
                           i
                        
                      step by step. iPar–CLR improves Seq–CLR by introducing a parametric 3D shape model for the configuration of p landmarks, and solving Eq. (1) in the parameter space. By assuming that the distribution of the perturbations of shape parameters is Gaussian, iPar–CLR is well suited for the task of incremental update. That is, it can incrementally update the pre-trained shape model according to the newly added face images.

When used for joint 2D and 3D facial landmark localization, the texture map is projected from each textured 3D face scan into a 2D regular grid domain using the interpolation techniques. Then, we apply iPar–CLR [40] to each projected 2D texture face image, outputting 49 2D landmarks (see Fig. 1
                     ). These 2D landmarks are then transferred to 3D texture face space by the inverse of the above projection. Note that since all these 2D landmarks are located at the frontal part of the projected 2D face texture, the one-to-one correspondence between 3D texture and 2D texture can be approximately preserved during the projection mapping. Finally, the corresponding 3D landmarks are directly determined by the one-to-one correspondence between 3D texture and 3D geometry of the 3D face model. We evaluate iPar–CLR on the whole BU–3DFE database and the expressive samples in Bosphorus, and find that it can precisely localize all the pre-defined 49 facial landmarks for all samples even with variations in expression, ethnicity, gender and age etc. (see Fig. 1 for some sampled results).

We extract the SIFT [41] descriptor of each projected 2D texture face image at the locations of the detected 2D landmarks within 16 × 16 patches. The SIFT feature based facial representation of a 2D texture image is generated by concatenating all the SIFT features at the 49 landmarks according to the pre-defined order, resulting in a 
                           
                              128
                              ×
                              49
                              =
                              6
                              ,
                              272
                           
                         dimensional feature vector. This vector is further normalized to the unit length for the following processing.

The HSOG descriptor was originally proposed in [36] and proved very efficient for local image matching, object categorization, and scene classification. In this paper, we explore HSOG for 2D facial expression description. The construction of HSOG is composed of three steps:

(1) Computation of the first order Oriented Gradient Maps (OGMs): The input of HSOG is a R × R image patch around each localized 2D facial landmark. For each image patch I(x, y), it outputs a number of Oriented Gradient Maps (OGMs) 
                           
                              
                                 {
                                 
                                    J
                                    o
                                 
                                 
                                    (
                                    x
                                    ,
                                    y
                                    )
                                 
                                 }
                              
                              
                                 o
                                 =
                                 1
                              
                              L
                           
                         by computing the Gaussian convolution of the positive orientation gradient maps, described as:

                           
                              (2)
                              
                                 
                                    
                                       J
                                       o
                                    
                                    
                                       (
                                       x
                                       ,
                                       y
                                       )
                                    
                                    =
                                    
                                       G
                                       Σ
                                    
                                    *
                                    max
                                    
                                       (
                                       
                                          
                                             ∂
                                             I
                                             (
                                             x
                                             ,
                                             y
                                             )
                                          
                                          
                                             ∂
                                             o
                                          
                                       
                                       ,
                                       0
                                       )
                                    
                                    ,
                                    o
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    ,
                                    L
                                    ,
                                 
                              
                           
                        where o represents a quantized direction, and G
                        
                           Σ
                         is a Gaussian kernel with standard deviation Σ, which is proportional to the size of image patch R.
                        
                     

(2) Computation of the second order gradients: Once these first order OGMs of all quantized directions are generated, they are used as the inputs for computing the second order gradients. Precisely, for each OGM J
                        
                           o
                        (x, y), we calculate its gradient magnitude mago
                        (x, y) and orientation 
                           θ
                        
                        
                           o
                        (x, y) at every pixel location. The orientation value θo
                        (x, y) is then re-scaled from the range of 
                           
                              [
                              −
                              π
                              /
                              2
                              ,
                              π
                              /
                              2
                              ]
                           
                         to [0, 2π], and quantized into L dominant orientations. After quantization, the entry no
                         of each orientation θo
                         is calculated as:

                           
                              (3)
                              
                                 
                                    
                                       n
                                       
                                          θ
                                          o
                                       
                                    
                                    
                                       (
                                       x
                                       ,
                                       y
                                       )
                                    
                                    =
                                    mod
                                    
                                    
                                       (
                                       
                                          ⌊
                                          
                                             
                                                
                                                   θ
                                                   o
                                                
                                                
                                                   (
                                                   x
                                                   ,
                                                   y
                                                   )
                                                
                                             
                                             
                                                2
                                                π
                                                /
                                                L
                                             
                                          
                                          +
                                          
                                             1
                                             2
                                          
                                          ⌋
                                       
                                       ,
                                       L
                                       )
                                    
                                    ,
                                    o
                                    =
                                    1
                                    ,
                                    2
                                    ,
                                    …
                                    ,
                                    L
                                    .
                                 
                              
                           
                        
                     

(3) Spatial pooling: Daisy-style spatial pooling strategy is used in HSOG as illustrated in Fig. 2. It is easy to find that there are four parameters that determine the HSOG descriptor, i.e., the size of the patch (R); the number of quantized orientations (L); the number of concentric rings (CR); the number of circles on each ring (C). The total number of the divided circles can be calculated as 
                           
                              T
                              =
                              C
                              R
                              ×
                              C
                              +
                              1
                           
                        . Within each circle CIRj
                        , and for each OGM J
                        
                           o
                        , a second order gradient histogram is constructed by accumulating the gradient magnitudes mago
                         of all the pixels with the same quantized orientation entry no
                        .

                           
                              (4)
                              
                                 
                                    
                                       h
                                       
                                          o
                                          j
                                       
                                    
                                    
                                       (
                                       i
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          
                                             (
                                             x
                                             ,
                                             y
                                             )
                                          
                                          ∈
                                          C
                                          I
                                          
                                             R
                                             j
                                          
                                       
                                    
                                    δ
                                    
                                       (
                                       
                                          n
                                          
                                             θ
                                             o
                                          
                                       
                                       
                                          (
                                          x
                                          ,
                                          y
                                          )
                                       
                                       =
                                       =
                                       i
                                       )
                                    
                                    *
                                    m
                                    a
                                    
                                       g
                                       o
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              i
                              =
                              0
                              ,
                              1
                              ,
                              …
                              ,
                              L
                              −
                              1
                           
                        ; 
                           
                              o
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              L
                              ,
                           
                        
                        
                           
                              j
                              =
                              1
                              ,
                              2
                              ,
                              …
                              ,
                              T
                              ,
                           
                         and δ is the characteristic function. Then, for each first order OGM J
                        
                           o
                        , its second order gradient histogram ho
                         is generated by concatenating all the histograms from T circles:

                           
                              (5)
                              
                                 
                                    
                                       h
                                       o
                                    
                                    =
                                    
                                       
                                          [
                                          
                                             h
                                             
                                                o
                                                1
                                             
                                          
                                          ,
                                          
                                             h
                                             
                                                o
                                                2
                                             
                                          
                                          ,
                                          …
                                          ,
                                          
                                             h
                                             
                                                o
                                                T
                                             
                                          
                                          ]
                                       
                                       T
                                    
                                    ,
                                 
                              
                           
                        where 
                           
                              o
                              =
                              1
                              ,
                              2
                              ,
                              …
                              L
                           
                        . Finally, the HSOG descriptor is obtained by concatenating all L histograms of the second order gradients as in Eq. (6). Each histogram h
                        
                           o
                         is normalized to a unit norm vector 
                           
                              
                                 h
                                 ^
                              
                              o
                           
                         before concatenation.

                           
                              (6)
                              
                                 
                                    H
                                    S
                                    O
                                    G
                                    =
                                    
                                       
                                          [
                                          
                                             
                                                h
                                                ^
                                             
                                             1
                                          
                                          ,
                                          
                                             
                                                h
                                                ^
                                             
                                             2
                                          
                                          ,
                                          …
                                          ,
                                          
                                             
                                                h
                                                ^
                                             
                                             L
                                          
                                          ]
                                       
                                       T
                                    
                                    .
                                 
                              
                           
                        Similar to SIFT, the HSOG feature based expression representation of a 2D texture image is generated by concatenating all HSOG features of the localized landmarks and then normalized to the unit length. In this paper, we set 
                           
                              R
                              =
                              25
                              ,
                           
                        
                        
                           
                              L
                              =
                              8
                              ,
                           
                        
                        
                           
                              C
                              R
                              =
                              3
                              ,
                           
                        
                        
                           
                              C
                              =
                              4
                           
                         as in [36]. Thus, the dimension of the final HSOG feature vector for a face image is 
                           
                              T
                              ×
                              L
                              ×
                              8
                              ×
                              49
                              =
                              13
                              ×
                              8
                              ×
                              8
                              ×
                              49
                              =
                              40
                              ,
                              768
                           
                        .

The meshHOG and meshHOS descriptors were originally proposed in our previous work [37,38] and proved efficient in 3D face identification. In this paper, we employ them in 3D FER. Similar to HSOG, meshHOG and meshHOS are built by the following three steps:

(i) Computation of facial surface normal and curvature: Each 3D facial surface is represented by a triangular mesh 
                        
                           T
                           =
                           (
                           F
                           ,
                           V
                           )
                           ,
                        
                      where 
                        F
                      and 
                        V
                      are the face and vertex sets. We compute the unit normal vector of each face by the cross product of its two edge vectors. Then the unit normal of each vertex 
                        
                           
                              n
                              v
                           
                           =
                           
                              
                                 [
                                 
                                    n
                                    x
                                    v
                                 
                                 ,
                                 
                                    n
                                    y
                                    v
                                 
                                 ,
                                 
                                    n
                                    z
                                    v
                                 
                                 ]
                              
                              T
                           
                        
                      is achieved by averaging the normal vectors of its one-ring faces. The mesh gradient magnitude magv
                      and orientation θv
                      at each vertex are calculated as:

                        
                           (7)
                           
                              
                                 m
                                 a
                                 
                                    g
                                    v
                                 
                                 =
                                 
                                    
                                       
                                          
                                             (
                                             
                                                n
                                                x
                                                v
                                             
                                             /
                                             
                                                n
                                                z
                                                v
                                             
                                             )
                                          
                                          2
                                       
                                       +
                                       
                                          
                                             (
                                             
                                                n
                                                y
                                                v
                                             
                                             /
                                             
                                                n
                                                z
                                                v
                                             
                                             )
                                          
                                          2
                                       
                                    
                                 
                                 ,
                                 
                                 
                                    θ
                                    v
                                 
                                 =
                                 arctan
                                 
                                    (
                                    
                                       n
                                       y
                                       v
                                    
                                    /
                                    
                                       n
                                       x
                                       v
                                    
                                    )
                                 
                                 .
                              
                           
                        
                     
                  

According to [42], the principal curvatures k
                     max  and k
                     min  are computed by fitting a cubic-order surface:

                        
                           (8)
                           
                              
                                 f
                                 
                                    (
                                    x
                                    ,
                                    y
                                    )
                                 
                                 =
                                 
                                    A
                                    2
                                 
                                 
                                    x
                                    2
                                 
                                 +
                                 B
                                 x
                                 y
                                 +
                                 
                                    C
                                    2
                                 
                                 
                                    y
                                    2
                                 
                                 +
                                 D
                                 
                                    x
                                    3
                                 
                                 +
                                 E
                                 
                                    x
                                    2
                                 
                                 y
                                 +
                                 F
                                 x
                                 
                                    y
                                    2
                                 
                                 +
                                 G
                                 
                                    y
                                    3
                                 
                              
                           
                        
                     and its normal vectors 
                        
                           (
                           
                              f
                              x
                           
                           
                              (
                              x
                              ;
                              y
                              )
                           
                           ,
                           
                              f
                              y
                           
                           
                              (
                              x
                              ;
                              y
                              )
                           
                           ,
                           −
                           1
                           )
                        
                      using both the 3D coordinates and the normal vectors of the associated local neighbor points (two-ring). Once we have two principle curvatures, the shape index values, which describe different shape classes by a single number ranging from 0 to 1, is calculated as:

                        
                           (9)
                           
                              
                                 S
                                 I
                                 =
                                 
                                    1
                                    2
                                 
                                 −
                                 
                                    1
                                    π
                                 
                                 arctan
                                 
                                    (
                                    
                                       
                                          
                                             k
                                             max
                                          
                                          +
                                          
                                             k
                                             min
                                          
                                       
                                       
                                          
                                             k
                                             max
                                          
                                          −
                                          
                                             k
                                             min
                                          
                                       
                                    
                                    )
                                 
                                 .
                              
                           
                        
                     
                     Fig. 3 shows the shape index maps of sampled 3D faces with six prototypical expressions.

(ii) Canonical orientation(s) assignment: Similar to the SIFT feature, to achieve rotation invariance, one or more local coordinate systems (i.e., canonical orientations) should be determined at each localized 3D landmark. This can be accomplished by the following three steps: First, we build an initial local coordinate system, where the landmark v and its normal n
                     
                        v
                      are the origin and the positive z axis, respectively. And two perpendicular vectors in the tangent plane of v are randomly chosen as the x axis and y axis, respectively. Then, the gradient magnitudes and orientations of the vertices around the landmark with a given geodesic distance r
                     0 are computed, Gaussian-weighted by their corresponding gradient magnitudes, and put in a histogram of 360 bins. Dominant gradient orientations, that is, peaks in the histogram, are used to assign one or more canonical orientations to the landmark. Finally, the initial local coordinate system is rotated in the local tangent plane, making each canonical orientation as new x axis, and the new y axis is computed by the cross product of the z axis (i.e., normal vector n
                     
                        v
                     ) and the new x axis (i.e., canonical orientation). Once the canonical orientations are determined, all the neighbor vertices and their normal vectors are transformed to the new local coordinate system for the following processes.

(iii) Spatial pooling: Similar to the HSOG feature, a simplified daisy-style spatial pooling strategy is also used for meshHOG and meshHOS. However, the pooling strategy here is performed on the tangent plane of each 3D landmark and on the local coordinate system determined by the assigned canonical orientations. As illustrated in Fig. 2, for the 3D descriptors, there is only one concentric ring associated with eight circles, resulting in nine sequential circles. Within each circle CIRj
                     , a mesh gradient histogram and a shape index histogram are constructed respectively. The histogram of mesh gradient is constructed by accumulating the gradient magnitudes magv
                      of all vertices with the same quantized orientation entry nθ
                     (v) as:

                        
                           (10)
                           
                              
                                 g
                                 a
                                 
                                    hog
                                    j
                                 
                                 
                                    (
                                    i
                                    )
                                 
                                 =
                                 
                                    ∑
                                    
                                       v
                                       ∈
                                       C
                                       I
                                       
                                          R
                                          j
                                       
                                    
                                 
                                 δ
                                 
                                    (
                                    
                                       n
                                       θ
                                    
                                    
                                       (
                                       v
                                       )
                                    
                                    =
                                    =
                                    i
                                    )
                                 
                                 *
                                 m
                                 a
                                 
                                    g
                                    v
                                 
                                 ,
                              
                           
                        
                     where 
                        
                           i
                           =
                           0
                           ,
                           1
                           ,
                           …
                           ,
                           7
                        
                     ; 
                        
                           j
                           =
                           1
                           ,
                           2
                           ,
                           …
                           ,
                           9
                           ,
                        
                      
                     nθ
                     (v) is entry of the quantized gradient orientation computed the same as 
                        
                           
                              n
                              
                                 θ
                                 o
                              
                           
                           
                              (
                              x
                              ,
                              y
                              )
                           
                        
                      in (3). The histogram of shape index is constructed by accumulating the Gaussian weights G
                     
                        Σ
                     (v) of all vertices with the same quantized shape index value nSI
                     (v)

                        
                           (11)
                           
                              
                                 s
                                 a
                                 
                                    hos
                                    j
                                 
                                 
                                    (
                                    i
                                    )
                                 
                                 =
                                 
                                    ∑
                                    
                                       v
                                       ∈
                                       C
                                       I
                                       
                                          R
                                          j
                                       
                                    
                                 
                                 δ
                                 
                                    (
                                    
                                       n
                                       
                                          S
                                          I
                                       
                                    
                                    
                                       (
                                       v
                                       )
                                    
                                    =
                                    =
                                    i
                                    )
                                 
                                 *
                                 
                                    G
                                    Σ
                                 
                                 
                                    (
                                    v
                                    )
                                 
                                 ,
                              
                           
                        
                     where 
                        
                           i
                           =
                           0
                           ,
                           1
                           ,
                           …
                           ,
                           7
                        
                     ; 
                        
                           j
                           =
                           1
                           ,
                           2
                           ,
                           …
                           ,
                           9
                           ,
                        
                      
                     nSI
                     (v) is the quantized shape index values. Then, for each 3D landmark, its 3D descriptors are generated by concatenating all the histograms from nine circles in a clockwise direction,

                        
                           (12)
                           
                              
                                 
                                    
                                       
                                          H
                                          O
                                          G
                                          =
                                          
                                             
                                                [
                                                g
                                                a
                                                
                                                   hog
                                                   1
                                                
                                                ,
                                                g
                                                a
                                                
                                                   hog
                                                   2
                                                
                                                ,
                                                …
                                                ,
                                                g
                                                a
                                                
                                                   hog
                                                   9
                                                
                                                ]
                                             
                                             T
                                          
                                          ,
                                          H
                                          O
                                          S
                                          =
                                          
                                             
                                                [
                                                s
                                                a
                                                
                                                   hos
                                                   1
                                                
                                                ,
                                                s
                                                a
                                                
                                                   hos
                                                   2
                                                
                                                ,
                                                …
                                                ,
                                                s
                                                a
                                                
                                                   hos
                                                   9
                                                
                                                ]
                                             
                                             T
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     Each sub-histogram (hog
                     
                        i
                      or hos
                     
                        i
                     ) is normalized to the unit length before concatenation to eliminate the influence of non-uniform mesh sampling. Note that, intuitively, HOG describes the point-level bending pattern of the local shape around a landmark while HOS indicates the distribution of different shape categories. The expression representation (based on meshHOG or meshHOS) of a 3D face surface is generated by concatenating all HOG or HOS features of the localized 3D landmarks and then normalized to the unit length. Following [37], the geodesic radius r
                     0 is set to 22.50 mm, the radius of each circle is set to 10 mm, and the distance between the center of the centric circle and the one of each rounding circle is set to 15 mm. As a result, the dimension of the final meshHOG or meshHOS feature is 
                        
                           9
                           ×
                           8
                           ×
                           49
                           =
                           3
                           ,
                           528
                        
                     .

@&#EXPERIMENTAL RESULTS@&#

We make use of the widely used BU–3DFE database [43] to evaluate the proposed multimodal 2D + 3D local feature-based FER approach. This database consists of 2,500 textured 3D face scans of 100 persons in different expression, gender, race, and age. Six prototypical facial expressions (anger, disgust, fear, happiness, sadness, and surprise) with four intensity levels plus a neutral expression are displayed for each person. Examples of some projected 2D texture face images in BU–3DFE database are shown in Fig. 1.

To fairly conduct the identity–independent FER, we use the evaluation protocol in [13]. More precisely, we randomly select 60 persons, and keep the samples with the six prototypical facial expressions of two highest intensity levels. That is, 
                           
                              60
                              ×
                              6
                              ×
                              2
                              =
                              720
                           
                         samples are used for training and testing in total. Then, 648 samples of 54 persons (90%) and 72 of 6 persons (10%) are randomly divided for the training and testing data partition. To achieve stable recognition accuracy, this kind of 10-fold subject-independent cross-validation is conducted 100 rounds for all of our experiments. Based on these data partition strategies and the constructed 2D and 3D features, we utilize the SVM classifier with the Radial Basis Function (RBF) kernel for expression classification. The parameters for SVMs are tuned according to the 10-fold cross-validation in the training sets. To find the complementary characteristics between 2D descriptors, 3D descriptors, as well as 2D and 3D descriptors, we conduct both the early fusion (feature-level) and late fusion (score-level). For early fusion, the fused feature is generated by simply concatenating different descriptors. For late fusion, the mean of the recognition accuracies of different descriptors are used as the final accuracy.

@&#PERFORMANCE EVALUATION@&#


                           Table 1 shows the average expression recognition accuracies achieved using the single 2D descriptors and their fusion. From this table, we can see that: i) The average accuracies of the HSOG descriptor are much better than the ones of SIFT for anger and sadness, and comparable for the other expressions. ii) Early fusion largely improves the average accuracies of anger and sadness for SIFT, but also largely impairs the one of sadness for HSOG. iii) Late fusion generally performs better than early fusion, especially for the fear and sadness expressions. iv) Overall, the average accuracy of HSOG is 84.49%, which is better than SIFT (81.85%), and even slightly better than the ones of early fusion (82.85%) and late fusion (84.29%). We can conclude that the second-order gradient based local texture descriptor (HSOG) has more powerful discriminative ability than the popular first-order gradient based one (SIFT) for local texture-based FER. Moreover, there also exists some complementarity between different order descriptors for some specific expressions (e.g., anger and fear).


                           Table 2
                           
                            shows the average expression recognition accuracies achieved using the single 3D descriptors and their fusion. From this table, we can find that: i) Except anger expression, meshHOS achieves better results than meshHOG, especially for happiness. ii) Early fusion and late fusion generally improve the accuracies of both 3D descriptors for all expressions except happiness with a slight drop in early fusion. iii) Overall, the average recognition accuracy of meshHOS is 80.55%, which is better than meshHOG (77.62%), and late fusion (82.70%) is superior to early fusion (81.23%). We can conclude that the second-order surface gradient-based local shape descriptor (meshHOS) has stronger discriminative capability than the first-order surface gradient-based one (meshHOG). Moreover, they also contain some com plementary information when classifying some specific expressions (e.g., sadness and surprise).

In this section, we indicate that the local 2D texture and 3D shape descriptors contain strong complementary characteristics, and thus their fusion largely improves the expression recognition accuracies.


                           Table 3 lists the average expression recognition results of fusing the same order gradient-based 2D and 3D descriptors lead increase performance. Compared with the results in Table 1 and2, we can see that both early fusion and late fusion of the same order gradient-based 2D and 3D descriptors are very efficient, especially for the case of late fusion, with an improvement up to 3% for SIFT, 7% for meshHOG, 2.3% for HSOG, and 6.3% for meshHOS in the average accuracy. Moreover, the improvement of sadness expression is up to 8% for meshHOG and 10% for SIFT. And the accuracies of happiness are improved about 5% for HSOG and 6% for meshHOS.


                           Table 4
                            shows the average expression recognition results of fusing different order gradient-based 2D and 3D descriptors. Compared with the results in Table 1 and2, we can find that the fusion of the different order gradient-based 2D and 3D descriptors is also very efficient except the case of early fusion of HSOG and meshHOG. Take the results of late fusion as an example, the average recognition accuracies are improved by 3.2% for SIFT, 5% for meshHOS, 1.3% for HSOG and 8.1% for meshHOG. In particular, the improvement of happiness expression is 5.5% in the case of fusing SIFT and meshHOS. And the accuracy of the sadness expression is improved up to 11% when lately fusing HSOG and meshHOG.

As reported in Table 5
                           
                           , when considering the fusion of all the first-order and second-order gradient-based local 2D texture and 3D shape descriptors, our approach achieves an average recognition accuracy of 85.92% for early fusion and 86.32% for late fusion. These scores largely outperform the ones achieved by only fusing 2D descriptors (82.86% and 84.29%) in Table 1 or 3D descriptors (81.23% and 82.70%) in Table 2. More precisely, the confusion matrices of these scores indicate that the 2D descriptors and 3D descriptors have strong complementary characteristics for all the six prototypical facial expressions.

To validate the effectiveness of the proposed method in FER, we compare it with the state-of-the-art methods on the BU–3DFE dataset. To give a comprehensive analysis, four aspects, including the data modality, facial landmark, expression classifier, and recognition accuracy are compared.

From Table 6, we find that all previous methods (except [25] and [14]) reported their FER accuracies on BU–3DFE using only 3D modality data. As mentioned in Section 1, the results of 2D and 3D data are separately reported in [25] and jointly reported in [14]. Complementarity analysis of 2D and 3D data in FER is missing. On facial landmark, early studies such as [16–18,25], and [26] rely on a large number of manual landmarks. Recent studies try to avoid this impractical framework by utilizing global registration algorithms (e.g., [12,13,21,24]), or building general face models (e.g., [14,44]). Our method solves this problem by exploring the iPar–CLR algorithm to jointly detect a large number of 2D and 3D landmarks. For expression classification, SVM is the most popular classifier compared with the others such as Neutral Networks (NN), Sparse Representation-based Classifier (SRC), Bayesian Belief Net (BBN), and Multiple Kernel Learning (MKL).

In the
                           
                            literature, there are three FER protocols on BU–3DFE. Early tasks (e.g., [14,16–18,25]) chose 60 subjects and average the accuracies of one or two rounds of 10-fold cross-validation, totally with 10 or 20 times of train and test sessions (denoted by protocol I). This protocol has proved very sensitive to the identity variations of training and testing samples [13]. Gong et al. [13] later suggested to choose 60 subjects and average the accuracies of 100 rounds of 10-fold cross-validation, resulting in 1000 times of train and test sessions in total (i.e., protocol II). A similar protocol (i.e., protocol III) [26], randomly chose 60 subjects in each round of 10-fold cross-validation and average the accuracies of 100 rounds. From Table 6, we can find that the accuracies of the same methods [15,18,25] dropped more than 20% from protocol I to protocol II. Moreover, the accuracies of the same method achieved by protocol II and protocol III were close to each other as shown in [24] and [45]. Our proposed multimodal 2D+3D local feature-based approach reaches the highest average accuracy (86.32%) in protocol II.

@&#DISCUSSION@&#

In this section, we study the generalization capability of our proposed approach on the Bosphorus database. This database contains 4666 textured 3D face models of 105 subjects in various facial expressions, action units, poses and occlusions. To fairly conduct the identity-independent facial expression recognition, we still use the experimental protocol in [13] (i.e., protocol II). That is, we randomly select 60 persons who display all the six prototypical facial expressions. Totally, there are 
                           
                              60
                              ×
                              6
                              =
                              360
                           
                         samples used for training and testing. And 324 samples of 54 persons (90%) and 36 of 6 persons (10%) are randomly divided for the training and testing data partition. This kind of 10-fold cross-validation is conducted 100 rounds to achieve stable recognition accuracies, and the results are listed in Table 7.

Compare the results in Table 7 with the ones achieved on the BU–3DFE dataset, we can see that: 1) except the SIFT descriptor, the accuracies of other 2D and 3D descriptors are decreased. For example, the performance of meshHOG is dropped from 77.62% on BU–3DFE to 65.39% on Bosphorus. 2) The fusion of 2D and 3D descriptors is still efficient such as the late fusion of SIFT and meshHOG, HSOG and meshHOS. 3) Comparable expression recognition accuracies (86.32% vs. 84.72%) are achieved on the two datasets when fusing all the 2D and 3D local descriptors. 4) Compare with the results in Table 5, the accuracies for happiness and sadness are much better on Bosphorus, while the ones for surprise are much better on BU–3DFE. The possible reasons resulting in 1) and 4) come from the large expression variations of different persons when they displaying the same expression. Noted that all the persons in Bosphorus are professional actors or actress, while the subjects in BU–3DFE are ordinary people such as the university students. As shown in Fig. 4, sadness and anger look very similar for some people, and fear is always with month opening, which makes fear and surprise are largely confused with each other. Moreover, the disgust expression is very special and diversiform, which makes it confusing with sadness, anger and fear. It is probably the reason that most anger samples are misclassified into sadness, and most surprise samples are misclassified to fear and vice versa as shown in the average confusion matrices in Table 7.

To illustrate the complementary characteristics between 2D and 3D multimodal descriptors, we perform the Gentle AdaBoost algorithm [46] on the HSOG and meshHOS descriptors to select the most discriminative 2D and 3D facial landmarks (i.e., local regions used to compute HSOG or meshHOS) on BU–3DEF. More precisely, in each iteration of the Gentle AdaBoost algorithm, each landmark associated descriptor is first fed into a logistic regression weak classier, and the one with the lowest error rate is chosen as the most discriminative one in current iteration. Then, the weights of all the samples (landmarks) are updated, making the algorithm pay more attention on the misclassified samples. Finally, the algorithm stops when the top N discriminative landmarks are selected. Fig. 5 shows the top 15 most discriminative landmarks automatically selected by this algorithm. From this figure, it is not difficult to find that the distributions of the top 15 most discriminative 2D and 3D facial landmarks are largely different from each other for all the six sampled facial expressions. This finding once again indicates that our proposed 2D and 3D multimodal local texture and shape descriptors indeed have strong complementary characteristics.

In this paper, we present an efficient multimodal 2D + 3D feature-based approach for automatic FER. Based on the iPAR–CLR algorithm, we automatically localize 49 2D facial landmarks, and their corresponding 3D facial landmarks. Around each landmark, the HSOG based local texture descriptor and the SIFT descriptor are integrated for local 2D facial texture description. Furthermore, two mesh-based local shape descriptors, which consider both the first-order (surface normal) and the second-order (curvatures) surface gradients, are introduced to describe local 3D facial shapes. Both early fusion and late fusion of 2D, 3D, as well as 2D and 3D descriptors are comprehensively evaluated on the BU–3DFE database. All the experimental results demonstrate the effectiveness of integrating the 2D and 3D descriptors for expression recognition. Furthermore, we also analyze the generalization capability of the proposed approach on Bosphorus, and illustrate the complementary characteristics between the 2D and 3D descriptors.

Considering the limitation of current approach, in the future, we will go deeply in the following directions: i) The iPar–CLR based joint 2D and 3D facial landmark localization algorithm may fail with large pose variations and data missing. To solve this problem, we will investigate more robust algorithms such as [47]. ii) In current work, we use the simplest early and late fusion schemes. To find more intrinsic complementary characteristics between 2D and 3D modalities, we are going to explore better strategies. iii) In this paper, we focus on recognizing six basic expressions using multimodal 2D+3D static images. Following [48,49], this work will also be extended to the problem of 3D action unit recognition and to dynamic 3D face spaces.

@&#ACKNOWLEDGEMENTS@&#

This work was supported in part by the National Natural Science Foundation of China (NSFC) under grant no. 11401464, no. 61202237, no. 61273263 and no. 61303121; the China Postdoctoral Science Foundation (no. 2014M560785); the Specialized Research Fund for the Doctoral Program of Higher Education (no. 20121102120016); the Research Program of State Key Laboratory of Software Development Environment (SKLSDE-2015ZX-30); the French research agency, Agence Nationale de la Recherche (ANR) under grant ANR-07-SESU-004, ANR-2010-INTB-0301-01 and ANR-13-INSE-0004-02; the joint project by the LIA 2MCSI lab between the group of Ecoles Centrales and Beihang University; and the Fundamental Research Funds for the Central Universities.

@&#REFERENCES@&#

