@&#MAIN-TITLE@&#PCA-based polling strategy in machine learning framework for coronary artery disease risk assessment in intravascular ultrasound: A link between carotid and coronary grayscale plaque morphology

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Coronary artery disease risk assessment in intravascular ultrasound.


                        
                        
                           
                           A link between carotid and coronary grayscale plaque morphology.


                        
                        
                           
                           Principal component analysis (PCA) for dominant feature selection.


                        
                        
                           
                           Classification accuracy of 98.43% and reliability index of 97.32%.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Coronary artery

IVUS

Carotid IMT

Machine learning

PCA

Risk assessment

@&#ABSTRACT@&#


               
               
                  Background and objective
                  Percutaneous coronary interventional procedures need advance planning prior to stenting or an endarterectomy. Cardiologists use intravascular ultrasound (IVUS) for screening, risk assessment and stratification of coronary artery disease (CAD). We hypothesize that plaque components are vulnerable to rupture due to plaque progression. Currently, there are no standard grayscale IVUS tools for risk assessment of plaque rupture. This paper presents a novel strategy for risk stratification based on plaque morphology embedded with principal component analysis (PCA) for plaque feature dimensionality reduction and dominant feature selection technique. The risk assessment utilizes 56 grayscale coronary features in a machine learning framework while linking information from carotid and coronary plaque burdens due to their common genetic makeup.
               
               
                  Method
                  This system consists of a machine learning paradigm which uses a support vector machine (SVM) combined with PCA for optimal and dominant coronary artery morphological feature extraction. Carotid artery proven intima-media thickness (cIMT) biomarker is adapted as a gold standard during the training phase of the machine learning system. For the performance evaluation, K-fold cross validation protocol is adapted with 20 trials per fold. For choosing the dominant features out of the 56 grayscale features, a polling strategy of PCA is adapted where the original value of the features is unaltered. Different protocols are designed for establishing the stability and reliability criteria of the coronary risk assessment system (cRAS).
               
               
                  Results
                  Using the PCA-based machine learning paradigm and cross-validation protocol, a classification accuracy of 98.43% (AUC 0.98) with K
                     =10 folds using an SVM radial basis function (RBF) kernel was achieved. A reliability index of 97.32% and machine learning stability criteria of 5% were met for the cRAS.
               
               
                  Conclusions
                  This is the first Computer aided design (CADx) system of its kind that is able to demonstrate the ability of coronary risk assessment and stratification while demonstrating a successful design of the machine learning system based on our assumptions.
               
            

@&#INTRODUCTION@&#

The major cause of morbidity in the world is due to cardiovascular disease (CVD). In 2012 alone, CVDs caused 17.5 million deaths worldwide, out of which, 7.4 million deaths were due to coronary arterial disease and 6.7 million were due to stroke or cerebrovascular disease [1]. A higher occurrence of CVD in the young and middle-aged population is observed in the south-east Asia region. About 35% of all such deaths are between the age group of 35–64 years and are estimated to happen in India [2] between the years of 2000 and 2030.

CVD includes coronary artery disease and cerebrovascular disease. These diseases occur due to atherosclerosis – a progressive and slow process of narrowing the artery, interrupting the flow of blood from the heart or to the brain. In severe cases, plaque deposits inside a vessel of the coronary artery and later ruptures causing myocardial infarction Fig. 1a (left).

The current-state-of-art methods for screening the severity of this disease is: computed tomography (CT), ultrasound (US), and magnetic resonance imaging (MRI). Due to radiation, CT may compromise the patients’ safety, but it is often used because it computes a calcium score in the coronary artery. Even though MRI was earlier not suited to show benefits for soft tissue characterization [3,4], but now has started to be beneficial, but still lacks the concept of real time scanning. On the other hand, IVUS, though invasive, provides real time data, is less time consuming, and is less expensive [5,6]. Though, IVUS is preferred over CT, because of CT radiation risk, both screening tools lack the ability to stratify risk based on plaque characteristics. This paper utilizes the novel idea of coronary artery risk stratification and assessment using the concept of the genetic makeup of the plaque in the coronary and carotid arteries (Fig. 1a (left, right)).

Honda et al. [7] and de Graaf et al. [8] recently pointed out the need for patients’ risk of severity prior to interventional procedures. This severity risk was linked to plaque morphology. Several authors have proposed that plaque in the coronary artery consists of several components such as: fibrous, fibrolipidic, calcified and calcified-necrotic, using modalities like CT, OCT, IVUS and elastography [9–15]. We thus hypothesize that different components offer different risk factors and when combined as a whole using the grayscale wall region image can be used for tissue characterization. We can thus leverage to study the morphological characteristics of these lesions and adapt a machine learning paradigm to predict the risk of severity of CAD leading to myocardial infarction. This paper explores the novel concept of morphological characteristics utilizing the coronary vessel wall region that possesses these plaque components.

It has been recently shown by many researchers that there exists a relationship between plaque burdens in the carotid and coronary arteries. Here we will discuss some of the key studies which correlate cIMT with risk stratification in cardiovascular events. It has also been a biomarker for cerebrovascular events (CVEs) [16–19]. The relationships between coronary artery disease, cIMT and myocardial infarction have been demonstrated in previous studies. Ziembicka et al. [20] showed that there is a 94% chance of having coronary artery disease when cIMT>1.15mm. Ogata et al. [21] showed that maximum cIMT was highly correlated to left main coronary artery disease. Recently, Elias-Smale et al. [22] showed that cIMT>1.26mm can lead to myocardial infarction. Kao et al. [23] had shown that the cIMT>0.80mm (p
                        <0.01) lead to cardiovascular events. Our team lead by Ikeda et al. [24] showed that cIMT>0.9mm had a significant higher SYNTAX score, a risk indicator for coronary artery disease. The association between cIMT and coronary calcium volume has been recently shown by Suri's team [25,26]. Another study by the same group has also shown the correlation between automated cIMT that includes bulb plaque and SYNTAX score was found to be 0.467 (p
                        <0.0001), compared to 0.391 (p
                        <0.0001) between sonographer's cIMT reading and SYNTAX score [27]. Thus, there is a clear relationship between cIMT and coronary artery disease severity. Based on the above analysis, we hypothesize that cIMT can be adapted for developing a link between the coronary plaque burden leading to coronary artery disease or carotid artery disease leading to stroke. Ahead, we will show how to use the cIMT from carotid artery (as ground truth labels) along with grayscale morphology of coronary artery for CAD risk assessment.

Ultrasound ability for real time tissue characterization has recently surfaced for stroke application by Suri's team [28–32]. This stroke risk assessment tool (AtheroRisk™, AtheroPoint™, Roseville, CA, USA) was adapted for the risk of vulnerability to plaque rupture. The AtheroRisk™ tool adapted a machine learning paradigm, which consisted of an offline (training-phase) and online (testing-phase) systems. The morphology-based tissue characterization was performed either on the grayscale cut-section images representing the plaque or the intima-media thickness wall region representing the total plaque area. The training-phase requires the ground truth and this consisted of binary label such as: either asymptomatic or symptomatic or a binary label which can be derived from the cIMT information measured by the sonographer. The training-phase used grayscale features and the ground truth labels to yield the offline training coefficients which were then used to transform the online grayscale features test set to predict the new risk labels. Such learning methods adapt a gold standard whose a priori information was known, such as which plaques were at high risk or low risk. A similar concept of characterization of plaque for classification of plaques into symptomatic and asymptomatic was developed by Suri's team (under the class of Atheromatic™ systems (AtheroPoint™, Roseville, CA, USA). This spirit is being extended to coronary artery disease application in this study for tissue characterization of the coronary artery wall region.

Our system uses coronary grayscale morphology for risk prediction of CAD severity using a machine learning paradigm. Since the numbers of grayscale coronary morphologic features are large, we use dominant feature selection using principal component analysis (PCA) using polling-based method. SVM is adapted for training and testing the machine learning algorithm. The stroke biomarker cIMT is adapted as ground truth for training the machine learning system which is then used for risk prediction of CAD. The cIMT biomarker threshold of 0.9mm is used as a risk label for high/low risk CAD. A cross-validation approach is used for evaluating the efficiency of the machine learning infrastructure. Experiments are performed to study the effect of the data size on the classification accuracy and the role of PCA-based cutoff for feature selection during risk prediction. The overall system is novel and being used for the first time for risk prediction in CAD using PCA-based methods.

The layout of the paper is as follows: Section 2 presents the demographics and data acquisition. Section 3 describes the novel methodology of the proposed system for tissue characterization and risk stratification. Two experimental protocols are presented in section 4. The results are presented in section 5 that shows the optimization of SVM kernel type, dominant feature selection with change in PCA cutoffs and effect of data size on machine learning accuracy. The reliability, stability and feature retaining power are presented in performance evaluation section 6. A discussion on the results of proposed system is presented in section 7 and finally we conclude in section 8.

Nineteen patients were taken from a single-center study [33] between July 2009 and December 2010, with stable angina pectoris who underwent percutaneous coronary interventions (PCI) using iMap (Boston Scientific®, Marlborough, MA) IVUS examination. Among 19 patients, 17 were men and 2 were women with mean age of 66 years (range 36–81 years). Ten patients had a calcified location on the left anterior descending coronary artery (LAD), five on right coronary artery (RCA), three on left circumflex coronary artery (LCX) and one on left main coronary artery (LMT). Ten patients had proximal lesion location, five had at middle and four have at distal location. The mean total Cholesterol, LDL Cholesterol, and HDL Cholesterol was 165mg/dL, 91mg/dL, and 52mg/dL, respectively and mean hemoglobin was 5.81g/dL. Out of 19 patients, five were smokers. The data sets were approved by Institution Review Board and Ethics Committee and consent from patients were also taken before the study. A combination dose of clopidogrel (75mg/d) and aspirin (100mg/d) were given to the patients, prior to performing coronary intervention procedure. Intravenous unfractionated heparin was also given to these patients prior to the procedure. This would obtain a partial thromboplast in time of less than 250s.

Carotid ultrasonography examinations were performed for the same 19 patients with a scanner (Aplio XV, Aplio XG, Xario, Toshiba, Inc., Tokyo, Japan) equipped with a 7.5MHz linear array transducer. All scans were performed by the same experienced sonographer (with 15 years of experience). Subjects were examined in the supine position with the head tilted backwards. After the carotid arteries were located using transverse scans, the probe was rotated 90° to obtain and acquire a longitudinal image of the anterior and posterior walls. The high-resolution images were acquired according to recommendations by the American Society of Echocardiography Carotid Intima-Media Thickness (cIMT) Task Force. Sonographer-based cIMT was recorded. The mean pixel resolution of the carotid database was 0.05±0.01mm/pixel. On the basis of stoke-risk biomarker (cIMT>0.9mm), 11 patients were considered as high risk while 8 were considered low risk for this retrospective study.

We used a 40-MHz IVUS catheter (Atlantis SR Pro; Boston Scientific) for data acquisition. For coronary image cross sectional view acquisition, automated pullback of the catheter, at speed of 0.5mm per second was adapted. IRB had approved the protocol and patient consent was also taken before the study. DICOM format was adapted for image acquisition. DICOM is the proprietary header and the images are 16 bit a pixel. Once the images are converted into AVI movies the resolution of the image are 8 bit a pixel. This compromises the resolution of 16 bits a pixel to 8 bits per pixel. So the image is not as sharp as the original DICOM image. Recently, Kim et. al. [34] has shown the effect of post processing on the quality degradation of ultrasound IVUS images. Currently the study of this paper is focused on the development of new design for the risk assessment of coronary artery diseases using machine learning paradigm. Therefore the current paper doesn’t analyze the effect of quality degradation of image on tissue characterization and machine learning.

Data preparation is very important for risk assessment which is based on two hypothesis and the link between coronary artery disease and carotid artery disease (see Fig. 1b). The coronary risk assessment system (cRAS) is a local system which requires coronary vessel wall region and corresponding coronary risk labels. The coronary risk labels are initiated from the carotid artery based on the cIMT risk biomarker which is derived from second hypothesis. The cRAS system requires the vessel wall region (ring images) of coronary artery which is derived from the coronary video using global shape extraction system. This global shape extraction system uses a feedback system (ImgTracer™, AtheroPoint™, Roseville, CA, USA) for deriving the coronary vessel wall ring images using the expert's knowledge. Prior to the shape extraction of the coronary wall, the system undergoes the data preparation as discussed in the next section.

Our source of the data size comes from 19 patients. Each patient consists on an average of 2500 frames. Since the head and tail frames of the coronary artery movie did not contain clinical information on plaque morphology, we eliminated on an average 200 frames from the head and tail end of the movie leading to 2100 frames. Further, from our observations, the changes in plaque morphology were only observed after a certain interval, we thus took empirically every 10th frame in the coronary artery frame for data size preparation. Thus, on an average, the data size per patient was 210 frames. This resulted in a full data size 4004 frames derived from 19 patients. Note that the data size is not 19 subjects, but approximately 4004 frames as one population pool consisting of 2117 high risk frames and 1887 low risk frames derived from 11 high risk patients and 8 low risk patients respectively. Representative examples of low and high risk patients are shown in Figs. 2 and 3
                           
                           . Note that the data size for machine learning is not 19 subjects but collected frames of 4004 frames which are then used for cross-validation during the performance evaluation.

The tracing protocol consists of the following: IVUS produces a cross-sectional view of the coronary artery. The ImgTracer™ system (courtesy of AtheroPoint™, Roseville, CA, USA) is used for ROI selection and global shape extraction. This vessel wall region is the region between the lumen intima (LI) interface (also called as internal elastic lamina (IEL)) and media-adventitia (MA) interface (also called as external elastic lamina (EEL)). A typical slice of the IVUS frame is shown in Fig. 4
                            (a). As seen in this IVUS frame, the catheter is shown to be dark black in color in the center of the image along with the white calibration marks (shown in orthogonal directions running from east to west and north to south). Typically these marks are 57 pixels apart and represent 1 mm, so called resolution factor (57 pixels/mm). This is standard calibration factor and typically IVUS machines have the calibration factor in this range). The white edge represented in the center of the image is the edges of the catheter. Around the catheter is the blood region (representing low intensity or dark region). Surrounding the blood is the circular coronary wall (higher contrast region). Ideally, this wall is one mm thick starting from IEL (inner interface) to EEL (outer interface). At the IEL and EEL interfaces, the gradient changes are high. The region between the IEL and EEL is the media wall. The outermost wall region is the adventitia region. Since the adventitia region is very bright compared to media wall region, this change in gradient from media to adventitia is a dark band, hence can be easily spotted. Thus, the EEL (or MA) is typically traced first either in clock-wise or anti-clock wise directions. The inner wall (or IEL) is challenging and is harder to delineate. There are two key points which helps locate the IEL gradients. One, typically, this IEL gradient is about one mm from the EEL gradient, and second, the IEL gradient changes from blood (represented by low intensity) to media wall (brighter intensity compared to the blood). Using these two guiding points, one is able to identify the IEL interface. Once spotted, the tracing can follow the clock-wise or counter clock-wise direction by clicking points on the image. During the tracing along the circular pattern, the eye is sometimes kept ahead of the point you are clicking which allows generating the interface correctly and one needs an expert tracer or an experienced imaging who has been trained in IVUS imaging. An example of our tracing protocol is also discussed our recent publication [1]. The tracing protocol leads us to the region of interest (ROI) formation or vessel wall region extraction. The wall region extracted is shown in Fig. 4 (b).

As previously stated in our first hypothesis that components which offer risk in the vessel region are: fibrous, fibrolipidic, calcified and calcified-necrotic of coronary artery. These collectively are the morphological characteristics in the vessel region whose grayscale characteristics need to be computed. These components are represented and segregated by the texture components of the plaque, so called tissue characterization of the vessel wall region. Thus, we clearly need a feature extraction block in our machine learning paradigm. The second most important block comes from our second hypothesis which represents the gold standard and biomarker for cardiovascular diseases, i.e., carotid intima-media thickness (cIMT). Thus we used carotid disease cIMT biomaker for training the machine learning protocol, represented as ground truth. Since the features are large, and dominant and optimal features were necessary to be estimated, we adapted PCA-based polling strategy for feature selection. Thus our risk prediction system must have a feature extraction block, feature selection block and the gold standard for training the machine learning classifier.


                        Fig. 5
                         shows the machine learning system utilizing the above two hypothesis. This consists of two parts: left part consists of training phase and right side consists of testing phase. Both phases use the feature exaction which computes 56 novel features based on various linear and non-linear feature combinations. Both phases utilize the power of novel PCA-based feature selection which selects the dominant features. The training phase uses second hypothesis, where the coronary risk labels are used based on cIMT biomarker for generating the training-based risk coefficients. Thus the ground truth block uses these risk labels for offline machine learning process during training phase. It is these model parameters which are then used for transformation of the online features computed on test coronary images. Example of training phase coronary input images are shown in Figs. 6 and 7
                        
                         which represents two classes: low risk and high risk features. K-fold cross-validation protocol is adapted for evaluating the performance [35]. In this experiment we fixed the fold value (i.e., K
                        =10). Since the features are not linearly separable, we used nonlinear kernel functions, namely Linear, Radial basis function (RBF), Polynomial order 1, 2 and 3 [36] during the SVM-based classification in training and testing phases. This section is has the following parts: Section 3.1.1 is devoted to grayscale feature extraction which in turn has several sub-sections for extraction of grayscale features and Section 3.1.2 discussed the SVM-based classification.

Feature extraction is the heart of the morphological characterization. The feature extraction process computes the a class of seven different feature sets which consists of a total of 56 novel features as shown below:


                           Feature Set 1: Gray level co-occurrence matrix (GLCM): GLCM is a powerful statistical tool for texture feature extraction analysis. GLCM matrix indicates the joint probability of occurrence of gray-level between the neighbor pixel with a given spatial relationship in a defined observation region [37]. Let I
                           
                              g
                            be a given grayscale image, having pixels with different gray level intensities (0, 1, …, G
                           
                              i
                           
                           −1). The co-occurrence matrix is calculated on two parameters: the distance between the pixel pairs measured in number of pixels, and the orientation between the pixel pair. Pixel pair can be specified in four directions (0°, 45°, 90°, 135°). Here P
                           
                              df
                           (x, y) is a joint probability function of pixel pair, one having intensity x and another having intensity y, shown in Table 1
                           .


                           Feature Set 2: Intensity histogram: Intensity histogram [37] measures the characteristic of an image such as brightness and contrast. In intensity histogram, following measurements are computed: mean, energy, variance, entropy, skewness and kurtosis. Mean measures the brightness of an image. Variance measures the contrast of an image. Skew measures the allocation of gray level in an image. The skew is directly propositional to energy. Entropy is the converse of skew and energy measurement. Let L
                           
                              i
                            is the total number of intensity levels in an image and p(x) defines the number of pixel intensities of value x. On the whole, 6 grayscale features are extracted as shown in Table 2
                           .


                           Feature Set 3: Gray level run length matrix (GLRLM): In GLRLM, the gray intensity of the pixel is measured from the reference pixel in a specific direction. The set of pixels having same gray levels in a specific direction is defined as run length [37]. In this grayscale texture feature, S(x,
                           y) defines the set of pixels of successive runs of length y at reference gray level x. Total 13 features were extracted as shown in Table 3
                           . Here, N
                           
                              gr
                            defines the number of gray levels and L
                           
                              R
                            defines the maximum run length.


                           Feature Set 4: Invariant moment: Invariant moment features were introduced by Hu [38]. In invariant moment [37], the raw moments Rm
                           
                              ab
                            and the central moments Cm
                           
                              ab
                            were utilized for the feature extraction. Here, invariant moment is invariant under translation, change in scale, and rotation. Therefore this represents the images regardless of its localization, size and rotation. Total seven features were computed shown in Table 4
                           .


                           Feature Set 5: Neighborhood gray tone difference matrix (NGTDM): The texture features [39] are defined by NGTDM. Essentially, it is a column matrix M(x) wherein each xth value is the magnitude of the difference between the mean of the pixels in the neighborhood and the pixel under assessment. Busyness, contrast, complexity, coarseness and texture length are the five texture features that are derived from the NGTDM. Coarseness describes the variation in pixel from its neighborhood. Larger the Coarseness values, smaller the gray level differences between the pixel under observation and mean pixel in the neighborhood in the region. Busyness defines the rapid change in gray levels between the pixel under examination and its neighborhood. The Small values show the uniformity in intensity in the area under examination. Very high information content shows that texture is complex. If the gray level difference is easily visible that means contrasts are high. We have extracted above texture features as shown in Table 5
                           .


                           Feature Set 6: Statistical feature matrix (SFM): The values of the statistical properties of pixel pairs [39] at various locations were stored in SFM matrix. The CF is the normalization factor considered to be 100 for coarseness computation. m
                           
                              S
                            depicts the displacement vectors, d
                           
                              SS
                            shows the dissimilarity matrix, Con
                           3 expresses the contrast of pixel pair and n
                           
                              S
                            defines the number of elements in the set m
                           
                              S
                           . The mean is denoted by m
                           
                              d
                            and the lowest value of m
                           
                              d
                            is denoted by m
                           
                              dv. The fractal dimensions in x and y directions are denoted by d
                           
                              SF
                           
                           (x) and d
                           
                              SF
                           
                           (y). Table 6
                            shows the four texture features that we have extracted.


                           Feature Set 7: Gray level difference statistics (GLDS): The GLDS algorithm measures the absolute difference between the pairs of the gray levels [39]. Here P
                           
                              GLD
                            is the probability density of an image represented by I
                           
                              GLD
                           (x,
                           y) and n
                           
                              GLD
                            is the dimension vector, representing the number of gray levels. In GLDS, we have extracted 4 texture feature, shown in Table 7
                           .


                           Feature reduction technique using principal component analysis: Principal component analysis (PCA) with polling contribution based feature extraction technique [40] is applied to optimize the extracted features. Feature selection is necessary to select those extracted features which are more significant and which describe the best tissue characteristic of the vessel wall region of coronary artery disease. A total of 56 features were extracted in feature extraction stage for each image, but all extracted feature may not be dominant in nature to describe the best tissue characteristics. Therefore optimization was done in order to eliminate the redundancy within the extracted features. Furthermore, the features which have higher significance were fed to the classifier after applying PCA.

In this section we describe the binary class-based classifier using support vector machine (SVM). It determines the maximum hyper-plane between the two classes using the combination of the extracted features and pre-established ground truth. It is customary to determine the hyper-plane so that the distance from it to the nearest data points on both sides (support vector) is maximized. In this work, the extracted features are not linearly separable so we extend SVM with a kernel function [36] on the dataset to make them linearly separable. For this, we have used five kernel functions namely: linear, RBF, and polynomial order 1, 2, 3. Moreover, we also used K-fold cross validation [35] to reduce variability. Here we have taken 10 fold cross validation with 20 trails (T) per fold.

As discussed in the introduction section regarding the role of the two hypotheses, we here present the two experimental protocols based on that foundation. The two major hypotheses were: (i) risk associated with the components of the coronary artery vessel wall and (ii) ability of carotid IMT to characterize the cardiovascular risk. Our protocol design demonstrates the machine learning paradigm for risk assessment using the combination of (a) grayscale PCA-based dominant features of the coronary artery wall and (b) the gold standard subclinical risk biomarker – the carotid IMT during the learning-phase that generates the offline coefficients. The testing-phase utilizes the PCA-based dominant feature extraction which is then transformed by the offline coefficients. Protocol one consists of selection of best kernel based on classification accuracy during the PCA-based feature selection given the fixed data size. Protocol two consists of effect of data size on classification accuracy based on PCA-based feature selection and its optimization. Both these protocols require that the dominant features to be selected using PCA-based strategy. Since PCA finds the dominant features at different cutoffs without altering the extracted feature values, we thus use these extracted features prior to running the cross-validation system. Here, we choose the cutoff range from 0.90 to 0.99 and adapt the PCA-based polling strategy for best feature combination. Later in the performance evaluation section, we will show (a) ROC curve of the system; (b) what features are retained as the cutoff increases, so called “feature retaining power” of the CADx system and (c) stability analysis for ensuring the stability of the machine learning system.


                     Experiment Protocol 1: Selection of best kernel function based an PCA-based feature optimization with fixed data size (N).

In this experiment we fixed the data size (N) for different PCA-based cutoffs (R) from 0.90 to 0.99 in a step size of 0.01 and run this experiment to calculate all parameters. The goal is to find the best SVM-based kernel choice. This was estimated by computing the classification accuracies at different PCA cutoffs. For this, 4.004 IVUS coronary artery images of both low risk and high risk patients were taken from 19 patients. This data size consists of 2117 high risk patient frames and 1887 low risk patient frames. Moreover, after applying PCA, dominant features were fed to the offline SVM classifier with pre-established ground truth risk label for training. These offline model parameters are then applied to the online test coronary ring images to predict the risk labels on test images. Based on the results of testing, we find the best kernel design among all kernels.


                     Experiment Protocol 2: PCA-based system optimization with varying data size (N) with five kernel functions.

The objective of this experiment is to study the effect on the cRAS performance with varying data sizes from 150 to 1500 in the increment of 150. There is no special requirement in choosing the data set from the giving population of 1500. We choose 150 frames randomly starting from a fixed data set of 150. These 150 random frames are then added to the ongoing pool. Thus the data set consists of 150, 300, 450, …, 1500 image frames. Furthermore, for each data size, dominant features were selected by using PCA, with cutoffs ranging from 0.90 to 0.99 with step size of 0.01. The dataset of different data sizes were fed to the classifier for the offline training with pre-established ground truth to generate parameter model. Online classifier utilized the generated parameter model and online test coronary ring images for prediction of coronary risk. For performance measure, we calculated the classification accuracy of the cRAS for each data size at all cutoffs. Thus, we computed the mean accuracy of each data size for all PCA cutoffs.

@&#RESULTS@&#

Keeping the experimental protocol in mind, we first show the best feature combination using PCA-based polling strategy as discussed above. The section then details which kernel function is best suitable using the PCA-based optimization for a fixed data size N. This is presented in the experiment 1. The results of the PCA-based system optimization of SVM classifier with varying data size (N) for five kernel functions are presented in the experiment 2. Table 8
                      and Fig. 8
                      shows the number of dominant features at different cutoffs value ranging from 0.90 to 0.99. This gives us the best feature combination set for different PCA cutoffs. As can be seen in the table, with the increase in the cutoff values, the dominant feature increases. This means the classifier is able to find more number of dominant features which further boosts the accuracy of the system. It is further important to note that there are certain dominant features which represent this IVUS data sets that are retained while the PCA cutoff values increases. The retaining power can be seen in the color chart of Table 9
                     . For example, the feature #11 (named as: Cluster-Prominence) and feature # 55 (named as: Periodicity ) has always dominated throughout the increase in PCA cutoff values during the machine learning paradigm. Most of the features are shuffled as PCA cutoff increase. For example feature #19 and #27 are lost with the increase in PCA cutoff increases. This is because as the cutoff increases, the system finds a better matching feature which can boost the accuracy of the machine learning system. It can be considered as a greedy approach where the good features are kept and the system is looking for more matching features ensuring that the accuracy is still the highest. For the sake of convenience of the readers, the feature names corresponding the feature numbers in the table are listed as follows: 6- Skewness; 7- Kurtosis; 11- Cluster-Prominence; 12-Cluster-Shade; 19-Sum-Average (SA); 27- Long Run Emphasis (LRE); 29- Run Length Non-uniformity RLN; 33- Low grey level Emphasis (LGRE); 39-Third Invariant Moment (I3); 43- Seventh Invariant Moment (I7); 44-Contrast (from NGTDM); 49- Contrast (From GLDS); 50- Business (BUS); 51- Complexity (Com); 52-Texture Strength (TS); 55- Periodicity Per ; 56- Roughness ( Rog).


                     Experiment 1: Selection of Kernel function based on accuracy using PCA-based feature reduction with fixed data size (N).

In this experiment, on the basis of accuracy, best kernel function is chosen for SVM classifier. Classification has been performed for five kernel functions, Radial basis function (RBF) giving the higher accuracy at different PCA cutoff for fixed data size (N) among all the kernel functions. At cutoff 0.99 RBF kernel function has an accuracy of 98.43%, shown in Table 10
                      and Fig. 9
                     .


                     Experiment protocol 2: PCA-based system optimization with varying data size (N) for five kernel functions.

In this experiment, we evaluated the effect of change of data size (N) on the cRAS accuracy at different cutoffs ranging from 0.90 to 0.99 with a step size of 0.01. The objective of this experiment is to know the overall behavior of the cRAS system. The data size was increased in the step size of 150 for all PCA cutoffs. Table 11
                      shows the variation of the accuracy (η) vs. changing data size (N). This experiment is performed for each of the four kernels represented by letter “k”. Since there are 10 (P) cutoff values (ranging from p
                     =0.90 to 0.99 in an interval of 0.01), we represent an accuracy for cutoff “p”, for kernel “k” and having data size N as: 
                        
                           
                              η
                              N
                              p
                           
                           (
                           k
                           )
                        
                     . Using this notation, we compute the mean for each kernel type (k) using the data size (N) and is given in Eq. (1)
                     
                        
                           (1)
                           
                              
                                 
                                    η
                                    N
                                 
                                 (
                                 k
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             p
                                             =
                                             0.90
                                          
                                          
                                             p
                                             =
                                             0.99
                                          
                                       
                                       
                                          
                                             η
                                             N
                                             p
                                          
                                          (
                                          k
                                          )
                                       
                                    
                                    p
                                 
                              
                           
                        
                     
                  

It is observed that the coronary risk label derived from the cIMT biomarker utilized for training the machine learning system predicts the risk consistency, and there is very small change in classification accuracy with increase in data size for all cutoffs (Fig. 10
                     ). Our results demonstrate that the RBF kernel shows the best performance for all cutoffs.

@&#PERFORMANCE EVALUATION@&#

Any CADx system which stratifies the risk must ensure that it is stable and reliable. The reliability of the system is the behavior of the system which has the normal tendency as expected while ensuring that the hypothesis are met. The best way to estimate the system reliability is to see how accurately cRAS behaves with changes in training data size. Thus, we study the ratio of standard deviation to mean ratios of the accuracies while the data size changes during the training protocols. The stability of the cRAS system is computed if the deviation in mean accuracy corresponding to all the PCA cutoffs for each data size is within the tolerance limit of say 5%. Another methodology by which we can use to understand the performance of the system as to how the PCA is able to retaining the dominant feature during the coronary risk assessment. Thus, we have divided this section into four parts: (a) ROC curve of the system; (b) feature retaining power estimation; (c) cRAS reliability estimation and (d) cRAS stability estimation.

Using the optimized PCA-cutoff of 0.99, we computed the ROC curves corresponding to five kernel functions during SVM-based classification. The optimized PCA-cutoff of 0.99 was chosen as this depicted the most dominant features showing the highest cRAS classification accuracy. Table 12
                         shows the five attributes such as: mean sensitivity, mean specificity, mean positive predictive value, mean accuracy and mean AUC corresponding to the five kernel functions. The mean AUC was highest for RBF kernel with an accuracy of 98.43%. The corresponding ROC curves are shown in Fig. 11
                         where RBF kernel (red dotted line) showing the highest AUC of 0.98. From these graphs, we can see that our cRAS system is able to demonstrate how the machine-learning system can be utilized for risk assessment of plaque buildup in the coronary walls based on the two key assumptions.

We calculated the feature retaining power (FRP) after applying PCA at different cutoffs for fixed data size (N), i.e., 4004 images. In FRP, we found how many features are retained for each successive cutoff. FRP is evaluated as follows:
                           
                              (2)
                              
                                 
                                    F
                                    R
                                    P
                                     
                                    (
                                    %
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                S
                                                F
                                                R
                                                
                                                   P
                                                   
                                                      x
                                                      −
                                                      y
                                                   
                                                
                                             
                                             
                                                F
                                                R
                                                
                                                   P
                                                   x
                                                
                                             
                                          
                                       
                                    
                                    ×
                                    100
                                 
                              
                           
                        where, SFRP
                        
                           x−y
                         is the difference in the number of feature at two consecutive cutoffs: say FRP
                        
                           x
                         (lower cutoff) and FRP
                        
                           y
                         (consecutive cutoff). The denominator FRP
                        
                           x
                         is the number of features at the lower cutoff. An example is discussed here. Feature retaining power (FRP) is calculated at different cutoff ranges from 0.90 to 0.99 in the step size of 0.01. In FRP, successive features at various cutoffs SFRP
                        
                           x−y
                         are calculated. For example, the number of features at FRP
                        0.90 are 3 and, FRP
                        0.91 is 3, then number of similar features in both successive cutoffs (SFRP
                        0.90−0.91) are 3 thus SFRP
                        0.90−0.91 is 100%. Likewise, we have calculated SFRP, for successive features at different cutoffs. Therefore we have found average FRP to be 86.74%, shows in Table 13
                        .

Let the data size set be represented by ℵ consisting of “i” elements, where i takes the values of 1, …, 10. This set ℵ is given by: ℵ={150, 300, …, 1500}. Let N
                        
                           i
                         be the data size of elements i in set ℵ. The reliability index of the data set N is mathematically expressed as:
                           
                              (3)
                              
                                 
                                    R
                                    
                                       I
                                       N
                                    
                                    (
                                    %
                                    )
                                    =
                                    
                                       
                                          1
                                          −
                                          
                                             
                                                
                                                   σ
                                                   N
                                                
                                             
                                             
                                                
                                                   μ
                                                   
                                                      A
                                                      C
                                                      C
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ×
                                    100
                                 
                              
                           
                        where σ
                        
                           N
                         represents the standard deviation and μ
                        
                           ACC
                         represents the mean accuracy for the data size N
                        
                           i
                         under consideration and is computed by taking all the cutoffs of the RBF kernel function. The objective of this analysis is to see how reliable the cRAS system is with an increment in optimal feature population while increasing the data size (N
                        
                           i
                        ). For this analysis we had datasets ranging from 150 to 1500 with increments of 150. The following steps are adopted for the mean reliability for the coronary risk assessment system (cRAS).
                           
                              Step 1.
                              Choose a data size (N) and calculate the cRAS classification accuracy using PCA-based cutoffs (R) varying from 0.9 to 0.99 in the step size of 0.01 i.e., 10 cutoffs.

Compute the mean classification accuracy (μ
                                 
                                    ACC
                                 ) and standard deviation (σ
                                 
                                    N
                                 ) for over all 10 cutoffs.

Compute the reliability index (RI
                                 
                                    N
                                 ) using Eq. (3) for all the 10 data sizes.

Repeat step 1, 2 and 3 for all 10 data sizes (N). Compute the mean reliability index of coronary risk assessment system (
                                    
                                       
                                          
                                             
                                                
                                                   R
                                                   I
                                                
                                                ¯
                                             
                                          
                                          
                                             c
                                             R
                                             A
                                             S
                                          
                                       
                                    
                                 ) corresponding to all data sizes using Eq. (4).
                                    
                                       (4)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         R
                                                         I
                                                      
                                                      ¯
                                                   
                                                
                                                
                                                   c
                                                   R
                                                   A
                                                   S
                                                
                                             
                                             =
                                             
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                            
                                                               N
                                                               =
                                                               1
                                                            
                                                            
                                                               
                                                                  N
                                                                  d
                                                               
                                                            
                                                         
                                                         
                                                            R
                                                            
                                                               I
                                                               N
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            N
                                                            d
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 Here N
                                 
                                    d
                                  is represents the cardinality of set ℵ and N
                                 
                                    d
                                  is total number of elements in set ℵ. Each element is presented by the data size ℵ. The overall reliability index 
                                    
                                       
                                          
                                             
                                                
                                                   R
                                                   I
                                                
                                                ¯
                                             
                                          
                                          
                                             c
                                             R
                                             A
                                             S
                                          
                                       
                                    
                                  of the system is 97.32% shown in Table 14
                                  and Fig. 12
                                 .

The system is said to be stable if the deviation in mean cRAS accuracy corresponding to all the cutoffs for each data size is within the tolerance limit of 5%. The stability of the system is computed in the following way [41,42].
                           
                              Step 1.
                              Compute the risk classification accuracy for each data size (N) for corresponding PCA cutoffs (R) varying from 0.9 to 0.99 in the step size of 0.01.

Compute the mean of all risk classification accuracies corresponding to all cutoffs (R).

Compute the deviation of coronary risk classification accuracy from the mean accuracy at every cutoff (R).

Check if the deviation is under the tolerance limit of 5% of mean accuracy value and declare the system to be stable.

Repeat the step 2 to step 4 for each data size (N) and check against tolerance limit and declare stability.

@&#DISCUSSION@&#

There was several plaque components present in the coronary vessel wall region. These plaque components were hypothesized to consist of fibrous, fibro-lipid, calcified and calcified-necrotic which are responsible for the severity of coronary artery disease and plaque progression. The deposition of plaque in the coronary vessel wall region is due to a complex pathological process. These components contain certain linear and non-linear texture information. In our proposed model, we extracted this plaque morphological information of all the plaque components by computing these features in consecutive frames. Our model assumes that there are subtle changes in plaque morphology due to multifocal disease [44,45], imaging-setup [6] and heart motion [26,46]. Using these features, we adapted a PCA-based feature selection technique to find out the dominant texture features among the extracted features, which can easily characterize these plaque components. Furthermore, dominant texture feature and pre-established ground truth based on the hypothesis were fed to the offline training classifier to generate a machine learning parameter model. This parameter model was further utilized as an online test classifier on test images to predict the risk severity of coronary artery disease.

In the first experiment, RBF kernel function with 10 fold cross validation for 20 trials at cutoff 0.99 gave highest accuracy of 98.43% among all the kernel function and AUC was 0.98. In the second experiment, we analyzed the overall behavior of the system with an increase in data size at all the cutoffs for five kernel functions and found that there was a small change in accuracy for all kernels showed an overall increase in classification accuracy of cRAS with increase in data size starting from 150 frames to 1500 frames in the interval of 150 frames. The reliability index of the proposed paradigm was obtained as 97.32% and stability index was within the tolerance limit of 5% (See Fig. 13).

Any machine learning paradigm relies on the data size of the application. This is because the process of optimization involves relationship between coronary risk assessment's performance (classification accuracy) and the changing data size (N). In Fig. 10, we demonstrated that as the data size increases, most of the kernels showed classification accuracy yielding a plateau. RBF kernel showed the highest accuracy close to 98.5% followed by polynomial order 3, polynomial order 2, linear and polynomial order 1. All kernels showed an overall increase in classification accuracy of cRAS with increase in data size starting from 150 frames to 1500 frames in the interval of 150 frames. This consistency demonstrated stability in machine learning process ensuring the data size maturity using the feature combinations and SVM-classifier. Thus, the population source of 19 coronary videos was strong enough to create a data size on an average of 4004 frames for machine learning characterization of morphological features. Note that this is not always true with every data set. This is also not always true with a different feature combination set. It is beyond the scope of the current study, but the results are truly encouraging demonstrating the role of PCA-based feature selection paradigm in SVM-based training and testing phases (Figs. 13–15
                        
                        
                        ).

Several studies have been attempted for morphological-based carotid plaque classification and risk stratification. This study is the first study of its kind which links the genetic makeup of the atherosclerosis disease, the coronary artery leading to myocardial infarction, and carotid artery leading to stroke. Table 16
                         shows the comparative overview of proposed method and other classification methods with different attributes such as different data sizes, grayscale features, classification methods, feature selection techniques and ground truth adapted. In Christodoulou et al. [43], a modern neural network approach was proposed for carotid plaque classification. The authors had selected ten dominant texture feature sets from 61 texture features and achieved a classification accuracy of 73.10%. Mongiakakou et al. [47] had analyzed B-mode ultrasound images of the carotid artery. In their hybrid neural network approach, carotid plaque classification was used. They extracted 21 features and reported an accuracy of 99.10%. Suri's team [29] designed a CADx system which analyzed the carotid ultrasound images and classified them into symptomatic and asymptomatic plaques based on the textural features. AdaBoost and SVM classifiers were used with accuracies of 81.7% and 82.40%, respectively. Recently, the same team [30] showed another set of feature combination with an accuracy of 83% for stroke prediction. More recently, same team further designed a set of features showing interesting results whose stroke information was a priori known and their CADx system used SVM-based classifier with an accuracy of 85.3% on 492 patients.

This study has a unique morphological-based tissue characterization paradigm on two key hypotheses: (i) for linking coronary artery disease and carotid artery disease for predicting the severity risk of CAD and (ii) coronary risk label was taken as a gold standard based on the carotid biomarker cIMT. We adapted the SVM-based classifier using PCA-based feature selection method. On data size of 4004 coronary ultrasound images, we extracted 56 novel features using PCA-based pooling method. Among all the kernel functions, we achieved a highest accuracy of 98.43% using RBF kernel function, for 14 dominant features at 0.99 cutoffs. This study is the first study of its kind for CAD risk estimation which combines plaque burdens using the same genetic link.


                        IVUS-based characterization: Several studies have been attempted on tissue characterization. In Falk et al. [9], the author found that plaque component like lipid-rich and soft plaques are more unstable and rupture-prone after disruption in comparison to collagen-rich and hard plaques. In another study, Korte et al. [10], utilized an intravascular elastography to characterize different plaque components. In their study, authors found fibrous, fibro-fatty and fatty plaque components in coronary plaque. Okubo et al. [12] developed an online integrated backscatter intravascular ultrasound (IB-IVUS) system for tissue characterization. In their work, IB value classified fibrous, lipid-rich and fibrocalcific plaque components. Taki et al. [14] presented a pixel classification approach for tissue characterization.

Even though, this paper is focused on IUVS, but we wanted to share that OCT is also adapted for tissue characterization for CAD. Yabushita et al. [11] used such model. In this OCT images of coronary, plaque component like fibrous, fibrolipidic, were characterize by homogeitys, signal rich, poor signal with sharp border, while fibrolipidic and lipid-rich were defined by poor signal with diffuse border.

Athanasiou et al. [13] performed tissue characterization using geometric features form virtual histology IVUS image. Here, authors found four plaque components: dense calcium, fibrotic tissue, fibro-fat and necrotic core. Nair et al. [15] showed the usage of autoregressive classification and classic Fourier spectra for plaque characterization, and found fibrous, fibroulipidic, calcified, and calcified-necrotic regions in coronary plaque.

Currently, there are no solid software methods for characterization of coronary plaque wall by taking the entire video of the coronary artery. Secondly, our system is taking a carotid artery plaque as assistance to coronary risk due to same genetic make-up. This is the novelty of the system. Since the plaque in the carotid artery has a genetic link to coronary plaque (present in the coronary walls), it is thus very powerful idea to bridge the two plaques for combined risk assessment. Note that the power of the system lies not just alone in plaque characterization of the coronary artery via IVUS, but the combined power of subclinical atherosclerosis risk biomarker (cIMT) along with the coronary artery plaque. This is a standalone system and an interventional cardiologist does not need to run while he is in operating room (OR). The interventional cardiologist can acquire the data and analyze the risk later before the patient undergoes the bypass surgery or shunting procedure. Such a setup can be adapted as a secondary diagnostic device or tool by which the current visual risk assessment be assured. As new techniques evolve such fusion of OCT and ultrasound (just like CT and PET), we presume that better risk assessment methods will evolve which can combine light and sound and better tissue classification and plaque characterization techniques will emerge.

In our proposed work, coronary risk label was derived using cIMT value of the carotid ultrasound image data and was taken as a ground truth for SVM training system and classification utilizing the PCA-based feature selection strategy. Following are some of the studies which utilize the power of cIMT to link with cardiovascular events. In Kallilazaros et al. [48], authors observed the coronary artery disease risk was based on stroke information derived from carotid B-mode plaque images. The study showed that the carotid disease reduces the performance of left ventricular systolic function of coronary artery. Amato et al. [16] investigated that both intravascular ultrasound (IVUS) and external carotid ultrasound (ECU) were investigated by using cIMT value [49]. It was found that, carotid atherosclerosis correlated better with coronary atherosclerosis, therefore, carotid cIMT was considered as a good biomarker for coronary risk assessment. Bots et al. [17] showed a modest relationship between cIMT and coronary atherosclerosis reflecting uncertainty in atherosclerosis growth between the vascular beds.

Ikeda et al. [24] took data from 496 patients and underwent relationship between Ankle-Brachial Index (ABI) analysis and carotid ultrasound on CAD patients. It was reported that higher SYNTAX scores was obtained for patients with mean cIMT≥0.9mm as compared to patients who had mean cIMT<0.9mm for p
                        <0.0001. Similarly, patients with ABI≥0.9 (p
                        <0.0001) had significantly lower SYNTAX scores than patients with low ABI<0.9. Therefore, it has been found that 75% had coronary artery disease among the patients having both mean cIMT≥0.9mm and ABI<0.9. Recently, Suri's group [27] showed the correlation between coronary atherosclerosis SYNTAX score and B-mode carotid bulb plaque cIMT measurement. This study had a higher correlation of 0.467 (p
                        <0.0001) when compared against 0.391 (p
                        <0.0001) corresponding to sonographer's cIMT reading. This increase in CC was attributed due to plaque build-up in the carotid bulb region. The precision-of-merit for this study was 98.86% while benchmarking automated cIMT against manual reading traced by the radiologist. Another recent study by Suri's group (Araki et al. [49]) showed a correlation of 0.84 between left cIMT and coronary calcium lesion volume while adapting a cIMT threshold of 0.9mm. In the current machine learning paradigm for risk assessment, the subclinical atherosclerotic cIMT biomarker was used at a threshold of 0.9mm as risk criteria.

The strengths of the proposed system are: (i) consideration of the entire coronary artery for risk assessment; (ii) linking the coronary artery disease with carotid disease for estimation of risk severity of coronary artery disease; (iii) adaptation of PCA-based paradigm for dominant feature selection during training and testing phases; (iv) higher reliability and stability of the machine learning system used for coronary risk assessment and stratification. Along with the strengths of proposed system, the main challenge was the adaption of manual method for region of interest (ring images) generation in IVUS frames. Due to the expensive nature of the tracing paradigm and very large number of frames, inter- and intra-observer analysis could not be conducted and this can be accomplished in the future studies. This will allow studying the effect of region of interest selection on machine learning performance. Further, we think more experiments need to be performed with multicenter study using without controls, unlike current study the patients were controlled and diabetic. Lastly, we think that this machine learning paradigm can be adapted for understanding the risk of plaque progression in coronary artery disease patients [50].

We have performed the comprehensive time computation using our hardware system. We adapted the following PC configurations: PC HP Compaq Elite 8300 with Intel Core i7-3770 Processor, 3.40GHz and 2GB RAM, MATLAB 2013b software with Windows-7 Operating System. Table 17
                         shows the relationship between training time (seconds) and change in data size (N), while Table 18
                         the relationship between online testing time per frame (milliseconds) and change in data size (N). The corresponding plots for these tables are shown in Fig. 14 and Fig. 15. As seen in the Fig. 14, with the increase in data size from 150 to 1500, the training time is nearly linear close to 45 degree slope. The online testing time also grows with change in data time but very gradually. Note that the offline training time and online testing times are for 20 trails, which corresponds to 3rd columns of the Tables 17 and 18. Fig. 15 shows the online test times per trial per frame with change in data size in milliseconds. The behavior of offline training and online testing is consistent and as expected.

@&#CONCLUSION@&#

We presented a coronary artery risk assessment system by taking two key hypothesis: (i) there is a coronary artery disease risk associated with vessel wall region consisting of different plaque components such as: fibrous, fibrolipidic, calcified and calcified-necrotic; (ii) coronary risk label is derived using the carotid intima-media thickness biomarker that was used as a gold standard for design and development of machine learning system for coronary artery disease risk assessment and stratification. The computer-aided diagnosis system based on machine learning utilizing PCA-based feature selection criteria is able to classify the high risk and low risk coronary artery disease patients with high accuracy reaching close to 98.50%. We demonstrated that PCA-based feature selection using polling method is highly suitable for dominant features selection. The system showed a high reliability of 97.32% while meeting the stability criteria of 5%. The coronary artery disease risk assessment is automated given the vessel wall region of the coronary artery. The results are promising leading to the prototype design for a clinical setup.

@&#ACKNOWLEDGEMENT@&#

The authors convey their thanks to Harman Suri, Mira Loma, California, USA for proof reading the manuscript.

@&#REFERENCES@&#

