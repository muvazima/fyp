@&#MAIN-TITLE@&#Maximizing lipocalin prediction through balanced and diversified training set and decision fusion

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Unsupervised Kmeans preprocessing for balancing and diversifying training set.


                        
                        
                           
                           Enhanced classification of lipocalins by fusion of classifiers.


                        
                        
                           
                           Superior generalization on blind testing data sets.


                        
                        
                           
                           ReliefF based feature ranking.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Lipocalins

Diverse input patterns

Balanced training set

Boosted random forest

KNN

Classifier fusion schemes

@&#ABSTRACT@&#


               
               
                  Lipocalins are short in sequence length and perform several important biological functions. These proteins are having less than 20% sequence similarity among paralogs. Experimentally identifying them is an expensive and time consuming process. The computational methods based on the sequence similarity for allocating putative members to this family are also far elusive due to the low sequence similarity existing among the members of this family. Consequently, the machine learning methods become a viable alternative for their prediction by using the underlying sequence/structurally derived features as the input. Ideally, any machine learning based prediction method must be trained with all possible variations in the input feature vector (all the sub-class input patterns) to achieve perfect learning. A near perfect learning can be achieved by training the model with diverse types of input instances belonging to the different regions of the entire input space. Furthermore, the prediction performance can be improved through balancing the training set as the imbalanced data sets will tend to produce the prediction bias towards majority class and its sub-classes. This paper is aimed to achieve (i) the high generalization ability without any classification bias through the diversified and balanced training sets as well as (ii) enhanced the prediction accuracy by combining the results of individual classifiers with an appropriate fusion scheme. Instead of creating the training set randomly, we have first used the unsupervised Kmeans clustering algorithm to create diversified clusters of input patterns and created the diversified and balanced training set by selecting an equal number of patterns from each of these clusters. Finally, probability based classifier fusion scheme was applied on boosted random forest algorithm (which produced greater sensitivity) and K nearest neighbour algorithm (which produced greater specificity) to achieve the enhanced predictive performance than that of individual base classifiers. The performance of the learned models trained on Kmeans preprocessed training set is far better than the randomly generated training sets. The proposed method achieved a sensitivity of 90.6%, specificity of 91.4% and accuracy of 91.0% on the first test set and sensitivity of 92.9%, specificity of 96.2% and accuracy of 94.7% on the second blind test set. These results have established that diversifying training set improves the performance of predictive models through superior generalization ability and balancing the training set improves prediction accuracy. For smaller data sets, unsupervised Kmeans based sampling can be an effective technique to increase generalization than that of the usual random splitting method.
               
            

@&#INTRODUCTION@&#

Lipocalins are a part of calycin super-family along with FABPs (fatty acid binding proteins), Triabin, avidins and metalloprotease inhibitors (Bo Akerstrom et al., 2006). Lipocalins have significant diversity at the sequence level and perform a wide variety of biological functions. Apart from their diversity at sequence level as well as in functionalities, they are found in a variety of the organisms ranging from unicellular bacteria to multi-cellular plants and animals. Initially they are identified as the transporters of small hydrophobic molecules. Later they are found to be involved in immune-modulation (Logdberg and Wester, 2000) and are used as biomarkers for various diseases (Xu and Venge, 2000). Lipocalins are also found to have important roles in cell regulation and cancer (Bratt, 2000). Some of the animal lipocalins are found to behave as allergens (Virtanen et al., 1999). Artificial lipocalins are known as Anticalins (Skerra, 2008). Anticalins are being engineered to have highly specific molecular recognition functionality and offer a profitable technology over the conventional antibodies as promising reagents.

The experimental determination of lipocalins is an expensive and time consuming process. Moreover the detection of putative lipocalins using sequence similarity search methods is far elusive as the members of the lipocalin family share very low sequence similarity (Flower et al., 2000) often below the twilight zone (Rost, 1999). However the crystallographic structure of lipocalins reveals a conserved folding pattern that consists of eight beta strands and three structurally conserved regions (SCRs). This conserved pattern is having a close similarity with the one which is found in FABPs (Flower et al., 1993). Hence the presence of the structurally conserved pattern had inspired researchers to use the machine learning based prediction methods such as SVM for identifying the structurally diverse lipocalins by using sequential and structural features (Pugalenthi et al., 2010; Ramana and Gupta, 2009). Basically the sequence similarity scores are obtained by using the computationally significant comparison methods using sequence alignment algorithm, etc. In a nutshell, these methods use the primary sequences as their inputs. Whereas the machine learning methods use the underlying biological significant features that are extracted from the primary sequences as their inputs. So an intelligent pre-processing of input data for extraction of useful biologically significant sequence features is required. The appropriate choice of the extracted features will dictate the degree of success in solving the problems by applying machine learning methods.

In specific both of the methods (Pugalenthi et al., 2010; Ramana and Gupta, 2009) used the features derived from the predicted secondary structure and evolutionary information in the form of position specific scoring matrices (PSSM) along with other sequence based features. A detailed sequence and structural analysis of lipocalins was carried out to deduce the lipocalin fold and assigned LIR2 to lipocalin family by Adam et al. (Adam et al., 2008).

Previously machine learning methods have been successfully used for annotating the protein sequences belonging to various specific protein families (Chou, 2001; Pugalenthi et al., 2007; Shen and Chou, 2007; Kandaswamy et al., 2013). In this paper, we have attempted to enhance the prediction performance by using protocols for diversifying and balancing the training set as well as by applying classifier fusion schemes. Diversified training data set yields greater generalization ability and balanced training data set provides unbiased prediction performance. The classifier fusion schemes were used to achieve improved prediction accuracy in comparison to that of any individual classifier. The unsupervised Kmeans clustering was used to create the balanced and diverse training set and probability based fusion scheme for combining the results from the classifiers. The results of the experiments using these protocols have established that a balanced and diverse training set facilitates the machine learned models to have the superior generalization ability with unbiased performance as compared to that of a randomly created training set.

We have chosen the Ramana and Gupta datasets (Ramana and Gupta, 2009) for immediate comparison and robust analysis. This dataset consist of two parts, the first consists of 136 lipocalins and 166 non lipocalins for training and testing the models. The second part consists of 42 lipocalins, 25 FABPs and 28 Triabins, and is completely separate and mutually exclusive of the first. This second part of the data set is exclusively used for testing the machine learning models in order to get the unbiased prediction metrics and henceforth referred as the test set II.

The selection of apposite input features for any machine learning model plays an important role in accurately classifying the input instances. Discovering the best combination of direct and derived features that are distinctively responsible for accurate classification is an extremely difficult task as there is no standard technique available for it. However, one can try to identify them through trial and error basis. A better combination of input features for a given classification problem can be identified through intelligently experimenting with different combinations of features with the aid of the problem's domain knowledge. For this study, we have used a combination of three sequence-based features, namely: amino acids composition, property group composition and physiochemical n-grams feature. The descriptions about these three features are described as follows:

The percentage composition of each of the twenty different amino acid residues (aa) is used as the first component of the input feature vectors and calculated using the formula:
                              
                                 (1)
                                 
                                    
                                       P
                                       
                                          
                                             C
                                          
                                          
                                             a
                                             a
                                             ,
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             
                                                C
                                                
                                                   a
                                                   a
                                                   ,
                                                   i
                                                
                                             
                                          
                                          
                                             
                                                C
                                                
                                                   r
                                                   e
                                                   s
                                                   ,
                                                   i
                                                
                                             
                                          
                                       
                                       ×
                                       100
                                    
                                 
                              
                           where aa denotes a specific one of the 20 amino acid residues, PCaa,i
                            denotes the amino acid percentage composition of specific type ‘aa’ in the ith sequence. C
                           aa,i
                            denotes the total count of amino acid of specific type aa in the ith sequence. C
                           res,i
                            denotes the total count of all residues in the ith sequence (i.e. sequence length).

The percentage composition of different amino acid property groups is used as the second component in the input feature vector. The eleven different amino acid property groups (Nath et al., 2013) which are chosen for this purpose are given in Table 1
                           . The percentage counts of amino acid property group are calculated using the formula:
                              
                                 (2)
                                 
                                    
                                       
                                          
                                             P
                                             C
                                          
                                          
                                             p
                                             g
                                             ,
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             
                                                C
                                                
                                                   p
                                                   g
                                                   ,
                                                   i
                                                
                                             
                                          
                                          
                                             
                                                C
                                                
                                                   r
                                                   e
                                                   s
                                                   ,
                                                   i
                                                
                                             
                                          
                                       
                                       ×
                                       100
                                    
                                 
                              
                           where pg denotes a specific one of the 11 different amino acid property groups. PCpg,i
                            denotes the percentage composition of specific amino acid property group ‘pg’ in the ith sequence. C
                           pg,i
                            denotes the total count of specific amino acid property group ‘pg’ in the ith sequence. C
                           res,i
                            denotes the total count of all residues in the ith sequence.

This feature is used as the third component in the input feature vector and captures conservation of multiple physicochemical groups along the sequence. We have taken a sliding window of length 2. In a sliding window, if both the amino acids share one or more same physicochemical groups, then the frequency of those physicochemical groups counts are incremented. The same eleven physicochemical groups as mentioned in Table 1 were taken for this purpose.
                              
                                 (3)
                                 
                                    
                                       P
                                       hysicochemical
                                       −
                                       2
                                       grams
                                       :
                                       Tiny
                                       =
                                       
                                          
                                             
                                                
                                                   Σ
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   N
                                                   −
                                                   1
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                C
                                             
                                             T
                                          
                                          (
                                          i
                                          ,
                                          i
                                          +
                                          1
                                          )
                                       
                                    
                                 
                              
                           where N denotes length of the protein sequence. C
                           T denotes tiny count which is equal to one if two consecutive amino acids at positions i and (i
                           +1) belonging to tiny amino acid property group (T) otherwise zero. i denotes the variable position along the amino acid residue in the protein sequence which varies from 1 to N
                           −1.

If the conditions 
                              
                                 (
                                 
                                    
                                       
                                          a
                                          a
                                       
                                       i
                                    
                                    ∈
                                    
                                       T
                                       *
                                    
                                 
                                 )
                              
                            and 
                              
                                 (
                                 
                                    
                                       
                                          a
                                          a
                                       
                                       
                                          i
                                          +
                                          1
                                       
                                    
                                    ∈
                                    
                                       
                                          T
                                       
                                       *
                                    
                                 
                                 )
                              
                            are satisfied simultaneously, then C
                           T(i, i
                           +1) is equal to 1 otherwise 0 where T
                           *
                           ={Ala,Cys,Gly,Ser,Thr}. In the similar way the physicochemical-2g for the remaining ten physicochemical groups are calculated.

Given: sample sequence: ‘AAARNDD’

Sliding window size=2.

Initialize: the initial frequency count for each physicochemical group is set to zero

The possible physiochemical 2-g are: AA, AA, AR, RN, ND, and DD.

AA→increment the counts for tiny, small, non polar and hydrophobic groups by 1.

AA→increment the counts for tiny, small, non polar and hydrophobic groups by 1.

AR→no increment.

RN→increment the counts for polar and hydrophobic groups by 1.

ND→increment the counts for small, polar and hydrophobic groups by 1.

DD→increment the counts for small, polar, charged, acidic and hydrophobic groups by 1.

Training with a diversified data set, which includes every concept and sub-concept of the entire input space belonging to both positive and negatives classes, is very essential for complete learning in any supervised learning model. Both the inter-class imbalance (that causes bias towards the dominating classes) and the intra-class imbalance (that preclude some subclass instances within a particular class) in the training set often tend to produce majority class/subclass classifier which in turn degrades the prediction performance of machine learning models (Jo and Japkowicz, 2004). One of the reasons behind incomplete learning and imbalance data set is the presence of rare cases or less common cases. The creation of training set through random selection will cause the over-representation of the more common cases and under-representation or no-representation of the rare/minority cases in the training set and thereby the model will have less or no opportunity to learn the rare case sub-concepts. Ideally, there should be a balanced representation of both common and rare cases (sub-classes) from all the classes in the training data. So ideally a training set must consist of completely diverse and well balanced input instances which is crucial for perfect learning without any classification bias for achieving the true prediction performance of the classifier. In this paper, we have used the Kmeans clustering algorithm for creating the homogeneous groups in the data. The gist of clustering is given a similarity measure (clustering criterion), it tries to find hidden patterns in the dataset and groups together the more similar entities (Jain et al., 1999; Rui and Wunsch, 2005), and this is useful in creating the diversified data set.

This algorithm partitions the given set of input instances into K clusters, namely C
                           1, C
                           2, C
                           3… CK
                           , and each of which is represented by their centroids. This algorithm starts with an initial number of clusters K, which is to be predetermined using some heuristic procedure in specific to a particular problem. The Kmeans clustering proceeds by minimizing the sum-of-squared distances between patterns to their corresponding cluster centroids (Macqueen, 1967; Larose, 2004). During iterations, each input instance is assigned to its nearest centroid according to the Euclidean distance between them. Then the centroid position is recalculated. This iterative process continues till the convergence criterion is satisfied. The convergence criterion may be a pre-defined number of iterations or no movement of instances between clusters, etc. The pseudo-code of the algorithm is as follows:


                           
                              
                                 
                              
                           
                        

All the feature vectors are normalized before applying the Kmeans clustering and to reduce the chance of sticking to the local minima, the clustering process is repeated for 10 times with a different set of the initial cluster centroid positions. The Kmeans clustering was applied separately on both positive and negative data sets.

The Kmeans algorithm needs the initial value for K and to get adequate diversity the number of clusters for Kmeans clustering ‘K’ must be optimal. To determine the appropriate value for K (separately for both positive class and negative class instances), we have plotted graph with the ratio of intra-cluster variance to the inter-cluster variance along the y-axis and the number of clusters along the x-axis and the cluster number after which there is no significant decrease in the ratio was taken as the optimal value for ‘K’.

An ideal training set constitutes a set of completely diversified and well balanced set of input feature vectors that is essential to achieve perfect learning. We have created our training set by selecting equal number of representatives from each cluster formed by Kmeans clustering on both positive and negative data sets, in order to satisfy the criteria for near perfect training set.
                              
                                 (4)
                                 
                                    
                                       Training Set
                                       =
                                       
                                          
                                             
                                                T
                                             
                                          
                                          +
                                       
                                       ∪
                                       
                                          
                                             
                                                T
                                             
                                          
                                          −
                                       
                                    
                                 
                              
                           
                           
                              
                                 (5)
                                 
                                    
                                       
                                          
                                             T
                                          
                                          +
                                       
                                       =
                                       
                                          
                                             
                                                ∪
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   
                                                      
                                                         K
                                                      
                                                      
                                                         o
                                                         p
                                                         t
                                                      
                                                      +
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             R
                                          
                                          
                                             i
                                             1
                                          
                                          
                                             +
                                          
                                       
                                    
                                 
                              
                           
                           
                              
                                 (6)
                                 
                                    
                                       
                                          
                                             T
                                          
                                          −
                                       
                                       =
                                       
                                          
                                             
                                                ∪
                                                
                                                   j
                                                   =
                                                   1
                                                
                                                
                                                   
                                                      
                                                         K
                                                      
                                                      
                                                         o
                                                         p
                                                         t
                                                      
                                                      −
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             R
                                          
                                          
                                             i
                                             1
                                          
                                          
                                             −
                                          
                                       
                                    
                                 
                              
                           where T
                           +
                           →set of equal representations from +ve class clusters. T
                           −
                           →set of equal representations from −ve class clusters. K
                           +
                           opt
                           →optimal K for the positive class. K
                           −
                           opt
                           →optimal K for the negative class. R
                           +
                           
                              i
                           
                           1
                           →first element of ith cluster of +ve class. R
                           −
                           
                              j1
                           →first element of jth cluster of −ve class.

The remaining representatives from each cluster are used to form the testing set.

Ensemble learning methods first train multiple learners and combine their outcome appropriately to solve the given problem. Ensemble classifiers usually outperform single classifiers and they are robust to the presence of noise in the data and to over fitting of inputs (Polikar, 2006). Boosting, bagging and stacking are three representatives of ensemble methods. 
                           Boosting
                         is an iterative procedure (Freund and Schapire, 1996; Schapire, 2003) that combines many weak base learners linearly to construct a strong classifier with improved accuracy. This is also a kind of sequential ensemble method where the subsequent learners are evolved from the previously experienced learners. During each of the iterations, the incorrectly classified instances from the positive class and negative class data sets are given more weights so that the learning is concentrated on these hard and difficult to classify examples that are present in the training set. 
                           Bagging
                         (Breiman, 1996) is implemented in the random forest classification algorithm. The Random Forest (Breiman, 2001) consist of many individual decision trees. Classifier ensembles promote an optimal trade-off between diversity and accuracy. Different base classifiers making errors in different parts of the hypothesis space give better accuracy when they are properly combined together. In random forest, bootstrap samples from the training set with randomly selected feature subsets are evaluated at each node of the decision tree. The final decision is made by decision fusion of all trees by majority voting. Random forests have been successfully applied to many classification and prediction tasks (Kandaswamy et al., 2011; Nath, 2012). Major steps of the random forest are summarized below:
                           
                              1.
                              A bagged sample is drawn from the training data.

A decision tree is grown without pruning on the bagged sample, where at each node a randomly selected subset of features from the full feature subset is evaluated.

Fusing the decisions from all the individual trees.

The AdaBoost_random forest classifier is the combination of AdaBoost(one of the promising variant of boosting method) and the random forest algorithm (a variant of bagging method). Recently, some of the authors have also successfully applied boosted random forest for classification and prediction (Thongkam et al., 2008; Saravanan and Lakshmi, 2013). The pseudo-code of the AdaBoost (Friedman et al., 2000) algorithm with random forest as a weak learner is given below:


                           
                              
                                 
                              
                           
                        

Real AdaBoost is one of the popular modifications of the AdaBoost algorithm. The major steps are same except that it involves the calculation of real valued class probability estimates. We have performed experiments using both discrete and real Adaboosting algorithms and the one which gave highest sensitivity (real Adaboosting) was chosen as the main classification protocol.

This algorithm calculates the nearest neighbouring instances and assigns the class to the test instances by taking the majority vote of the class of the neighbouring instances (Larose, 2004; Witten and Frank, 2005). It does not learn the relationship between the different attributes and the class attribute. KNN (K nearest neighbour) is a lazy learning method in that it stores all the training examples and no explicit model is constructed. The new test instance is assigned to a class on the basis of the similarity measure between the stored training instances and the test instance. 1NN is the simplest case of KNN in which only the closest neighbour is considered and its class-value is assigned to the test instance. This classifier gave highest specificity and chosen as the second classifying protocol for the purpose of decision fusion. The pseudo-code of KNN algorithm is as follows:


                           
                              
                                 
                              
                           
                        

There are various decision fusion methods proposed in literatures (Kittler et al., 1998; Kuncheva, 2002; Kuncheva, 2004) for further improving the classification results. In this paper, we have analysed the results from the possible combinations of classifiers using different fusion schemes for selecting the best combination. We have used WEKA machine learning platform for implementing all the learning algorithms and decision fusion methods (Hall et al., 2009).

The relative performances of the prediction models are evaluated through four parameters that are derived from the values of confusion matrix, namely TP: true positive (the number of correctly predicted lipocalins), TN: true negative (the number of correctly predicted non-lipocalins), FP: false positive (the number of incorrectly predicted non-lipocalins) and FN: false negative (the number of incorrectly predicted lipocalins). These performance parameters are calculated by using the following formulas.

Sensitivity: expresses the percentage of correctly predicted lipocalins
                           
                              (7)
                              
                                 
                                    Sensitivity
                                    =
                                    
                                       
                                          T
                                          P
                                       
                                       
                                          (
                                          T
                                          P
                                          +
                                          F
                                          N
                                          )
                                       
                                    
                                    ×
                                    100
                                 
                              
                           
                        
                     

Specificity: expresses the percentage of correctly predicted non-lipocalins
                           
                              (8)
                              
                                 
                                    Specificity
                                    
                                       
                                          T
                                          N
                                       
                                       
                                          (
                                          T
                                          N
                                          +
                                          F
                                          P
                                          )
                                       
                                    
                                    ×
                                    100
                                 
                              
                           
                        
                     

Accuracy: expresses the percentage of correctly predicted both lipocalins and non-lipocalins
                           
                              (9)
                              
                                 
                                    A
                                    c
                                    c
                                    u
                                    r
                                    a
                                    c
                                    y
                                    =
                                    
                                       
                                          T
                                          P
                                          +
                                          T
                                          N
                                       
                                       
                                          (
                                          T
                                          P
                                          +
                                          F
                                          P
                                          +
                                          T
                                          N
                                          +
                                          F
                                          N
                                          )
                                       
                                    
                                    ×
                                    100
                                 
                              
                           
                        
                     

ROC (receiver operating characteristic) is a curve between true positive rate and false positive rate at various threshold cut-offs. If this curve is closer to left and top side of the ROC space, then it indicates the prediction model is more accurate. ROC curves can be summarised by a single numerical quantity known as the AUC. It is an important statistical property to compare the relative performance of the prediction methods. AUC can take values from 0 to 1. The value of 0 for the worst case, 0.5 for worthless prediction and 1 indicates the perfect prediction.

It is used as a valuable measure for selecting a model in binary classification problems and is a measure of both sensitivity and specificity. Its value ranges from −1 to +1, where a value of +1 means accurate prediction, a value of zero means random prediction and −1 means total disagreement.
                              
                                 (10)
                                 
                                    
                                       MCC
                                       =
                                       
                                          
                                             (
                                             T
                                             P
                                             ×
                                             T
                                             N
                                             )
                                             (
                                             
                                                
                                                   F
                                                   P
                                                
                                             
                                             ×
                                             F
                                             N
                                             )
                                          
                                          
                                             
                                                
                                                   (
                                                   
                                                      
                                                         T
                                                         P
                                                      
                                                   
                                                   +
                                                   
                                                      
                                                         F
                                                         N
                                                      
                                                   
                                                   )
                                                   (
                                                   
                                                      
                                                         T
                                                         P
                                                      
                                                   
                                                   +
                                                   
                                                      
                                                         F
                                                         P
                                                      
                                                   
                                                   )
                                                   (
                                                   
                                                      
                                                         T
                                                         N
                                                      
                                                   
                                                   +
                                                   
                                                      
                                                         F
                                                         P
                                                      
                                                   
                                                   )
                                                   (
                                                   
                                                      
                                                         T
                                                         N
                                                      
                                                   
                                                   +
                                                   
                                                      
                                                         F
                                                         N
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

@&#RESULTS AND DISCUSSION@&#

The creation of the training data set through random selection generally suffers from incomplete learning and biased classification. This paper is aimed to achieve (i) the high generalization ability without any classification bias through the diversified and balanced training sets and (ii) enhance the prediction accuracy by combining the results of individual classifiers with an appropriate fusion scheme.

The Kmeans clustering algorithm was used to create diversified groups of homogeneous instances. In order to find the optimal number of such clusters, we have drawn the graphs (using the method as mentioned in the previous section) for lipocalin and non lipocalin data sets which are shown in Fig. 1
                     . It was observed from the graphs that 104 is the optimal number of clusters for the lipocalin and 131 is the optimal number of clusters for non lipocalins (distribution of lipocalins and non lipocalin feature vectors in different clusters for optimal K is shown in Fig. 2
                     ). The balanced and diversified training set was created by selecting one instance from each of the lipocalin and non-lipocalin clusters. The remaining instances are kept in the testing set (that we will call as the testing set I). Now the training set is diversified and balanced and consists of 104 lipocalins and 131 non-lipocalins. Subsequently the testing set I consisted of 32 lipocalins and 35 non-lipocalins. The completely blind test set II is used for further validating our results.

We have performed the experiments to evaluate the individual prediction performance of different classifiers on our specifically created training and testing set and chosen six classifiers, namely multilayer perceptron (MLP), support vector machines (SMO), K nearest neighbour (KNN), random forest (RF), AdaBoost random forest (ARF) and real AdaBoost random forest (RARF) for the purpose. After training, each of these classifiers was tested with the testing set I and their individual performance parameters (given in Table 2
                        ) were calculated from the values of the corresponding confusion matrices.

The RF, RARF and MLP all achieved 87.5% sensitivity followed by 81.3% of KNN and 78.1% of SMO. The sorted order of classifiers according to their AUC and accuracy performance parameters is (i) AdaBoost random forest, (ii) real AdaBoost random Forest, (iii) Random Forest, (iv) K nearest neighbour, (v) Support Vector Machines and (vi) Multilayer Perceptron. The ARF classifier achieved 90.6% sensitivity, which is the maximum value among all the classifiers and the RARF was found to be the best performing classifier that achieved 89.6% accuracy 0.791 MCC and 0.957 AUC, which are the maximum results obtained out of all the six tested classifiers.

To obtain an unbiased estimate and to validate the performance of the prediction models, all the six classifiers were further tested with the completely blind test dataset II, which consists of 42 lipocalins, 25 FABPs and 28 Tribians (structurally related to lipocalins). The performance parameters were calculated for all the six trained models and the results are recorded in Table 3
                        .

Here the sorted order of the six classifiers based on the values of AUC is (i) Real AdaBoost random forest, (ii) AdaBoost random forest (iii) multilayer perceptron (iv) K nearest neighbour (v) random forest, and (vi) support vector machines. This shows that the support vector machines which performed at par with KNN and RARF for test dataset I gave the worst performance for test dataset II in terms of accuracy. If some of the subclass patterns of the input features are not present in the training set, then obviously the model could not recognize the input patterns belonging to those subclasses. The main reason behind is that the testing instances in the test dataset I belong to subclasses of the training instances whereas testing instances in the test dataset II are completely blind from the training instances.

The following important observations were made from the performance parameter values recorded in Tables 1 and 2: (i) ARF and RARF gave the result of above 0.9 AUC values, (ii) RARF is found to be superior to the remaining four classifiers and (iii) RARF yields highest sensitivity and KNN yields highest specificity.

The third observation inspires the idea of combining the results obtained from K nearest neighbours (KNN) which is good at predicting negative class (non-lipocalin) and RARF which is good at predicting positive class (lipocalin) through appropriate classifier fusion schemes to achieve better prediction performance than the individual classifiers.

The two pairs of classifiers ARF+KNN and RARF+KNN are selected for fusion because ARF and RARF achieved top two highest sensitivity values that are very close to each other and on the other hand KNN achieved the highest of 88.7% specificity. Five different fusion schemes: viz, average, majority vote, max, min and product rule were applied on the above mentioned pairs of classifiers. Earlier these fusion schemes had been successfully applied for predicting response to anti HIV-1 therapy and protein fold prediction (Altmann et al., 2008; Dehzangi et al., 2011). Except majority vote scheme, all the other fusion schemes yielded similar results which are comparably better than that of the individual prediction model and the performance parameters of the fusion of classifiers (ARF+KNN and RARF+KNN using average fusion scheme) are shown in Table 4
                         (the results of all the fusion schemes are given in Supplementary material). The individual classifier either gives high sensitivity or high specificity, but not both. The combination of classifiers (RARF+KNN) through fusion schemes yielded both high sensitivity as well as high specificity. Hence it was established that balanced and diversified training set and decision fusion maximized the lipocalin prediction rate (the ROC curves for test set I and test set II are given in Supplementary material)

For validating the superior performance obtained by the diversified and balanced training dataset, we have carried out experiments on our model with the conventional unbalanced training dataset (with randomly selected input instances). We created ten random splits of training and testing set with the same number of positive and negative samples as obtained from Kmeans (i.e. 104 lipocalins and 131 non lipocalins) for comparison with Kmeans based splitted training and testing set, the average performance evaluation metrics for the randomly splitted training and testing sets are shown in Table 5
                         (the performance evaluation metrics for each individual randomly generated training and testing sets are given in Supplementary material). It is quite evident from the performance evaluation metrics that the Kmeans based splitted training set is more representative than the ten randomly created training sets. The balanced and diversified training set yielded 91% accuracy and 0.979 AUC on the first testing set and 94.7% accuracy and 0.973 AUC on the second testing set, where as the training set with randomly selected input instances, yielded only 85.9% accuracy and 0.908 AUC on the first testing set and 88.1% and 0.939 AUC on the second testing set, which are comparatively very much less (all the classified and misclassified proteins from test set I and II are provided in Supplementary material)

The random selection of input feature vector in the creation of training set yielded inferior results that are due to the lack of diversity of the input instances in the training set which may cause the learning algorithm to learn sub-concepts that are prevalent in the training set and fail to learn the rare/missing sub-concepts. These findings establish the claim of superior generalization ability that can be achieved through balanced and diversified training data set.

The presence of redundant features affects the training time and the generalization ability of the model. We have used the ReliefF (Kira and Rendell, 1992) feature selection technique to get the best discriminating features. On the analysis of ranked features, we observed that the majority of high ranked features are comprised by physicochemical-2g; this also supports their usefulness as biologically significant sequence features in discriminating lipocalins from non lipocalins. We systematically increased the number of features in the feature set and found that the feature set having 35 features gave the best performance parameters values of 93.8% sensitivity, 97.1% specificity and 95.5% overall accuracy (Table 6
                        ). Fig. 3
                         presents the heatmap representation of the protein sequence features with the corresponding rank of the features in discriminating lipocalins from non lipocalins.


                        Ramana and Gupta (2009) used SVM classifier and reported the results of lipocalin prediction for different combination of input features (amino acid, dipeptide and secondary structure composition with PSSM profiles). We have compared their results with the results obtained through our model for the same data set. The performance metrics of both the methods are shown in Table 7
                        . It is quite visible that diversified and balanced training set by our method improves prediction accuracy much better than previously reported results obtained by random selection of instances in creating the training set. Diversifying and balancing data sets not only provides true performance, but also yields superior prediction accuracy.

A direct comparison cannot be made with the method of Pugalenthi et al. (2010) which used SVM with 119 features (used the feature selection method to remove redundant features) and a very large dataset, we believe that the unsupervised sampling for creation of the training set may help to further improve the accuracy. The dataset as used by Pugalenthi et al. (2010) is exhaustive, but in the current work one of the objectives is the construction of balanced and properly diversified training and testing sets and for achieving this goal, we have implemented unsupervised Kmeans and compared its results with random sampling. Due to high time complexity of the Kmeans algorithm, it is quite cumbersome for large data sets with high dimensional features. Other cluster based sampling techniques like self organizing maps etc can be explored for high dimensional datasets.

@&#CONCLUSION@&#

The identification of lipocalins through experimentation is an expensive, time consuming and labour intensive process, but produces reliable and truly accurate results. The alternative computational model for lipocalin identification is comparatively least expensive and much faster, but the result is not reliable and 100% accurate. In this study, we had attempted to maximize prediction accuracy to a significant level by (i) balancing and diversifying the training data set (to eliminate incomplete learning and prediction bias towards majority class/sub-classes) and (ii) combining the results of the better performing classifiers through fusion schemes. Our proposed methodology employs Kmeans clustering and probability based fusion schemes to implement the above mentioned improvement strategies that had yielded significantly improved lipocalin prediction accuracy. We have also further established that appropriate choice of input feature vectors, good strategies for creating ideal training data set and the choice of a good classifier will lead to the accuracy of identification close to 100% and this is supported by analysing our results with previously reported results in the identification of lipocalins. The present work presents a useful alignment free method for successfully discriminating lipocalins from non-lipocalins with greater accuracy and can complement other methods in important ways.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.compbiolchem.2015.09.011.

The following are Supplementary data to this article:
                        
                           
                        
                     
                     
                        
                           
                        
                     
                     
                        
                           
                        
                     
                     
                        
                           
                        
                     
                  

@&#REFERENCES@&#

