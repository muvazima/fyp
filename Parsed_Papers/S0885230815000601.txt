@&#MAIN-TITLE@&#Speech enhancement based on wavelet packet of an improved principal component analysis

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Integrating the principal component analysis in wavelet packet decomposition.


                        
                        
                           
                           Extended PCA technique for speech enhancement is considered.


                        
                        
                           
                           To obtain a sparse matrix to contain the enhanced speech.


                        
                        
                           
                           Experiments on NOIZEUS data corrupted by Gaussian and four non-stationary noises.


                        
                        
                           
                           Our approach shows superior outcomes in BSS EVAL toolbox, SegSNR, PESQ, and Cov.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Speech enhancement

Wavelet packet analysis

Principal components analysis

Sparse matrix

@&#ABSTRACT@&#


               
               
                  In this paper, we propose a single-channel speech enhancement method, based on the combination of the wavelet packet transform and an improved version of the principal component analysis (PCA). Our method integrates ability of PCA to de-correlate the coefficients by extracting a linear relationship with what of wavelet packet analysis to derive feature vectors used for speech enhancement. This allows us to operate with a convenient shrinkage function on these new coefficients, removing the noise without degrading the speech. Then, the enhanced speech obtained by the inverse wavelet packet transform is decomposed into three subspaces: low rank, sparse, and the remainder noise components. Finally, we calculate the components as a segregation problem. The performance evaluation shows that our method provides a higher noise reduction and a lower signal distortion even in highly noisy conditions without introducing artifacts.
               
            

@&#INTRODUCTION@&#

The goal of speech enhancement in a single-channel recording varies according to specific applications, such as to reduce listener fatigue, to improve the overall speech signal quality, to enhance intelligibility, and to increase the efficiency of the voice communication device (Loizou, 2007).

In general, speech denoising algorithms can be categorized into four broad classes: spectral subtractive algorithms, statistical-model-based algorithms, wavelet transform, and subspace algorithms.

Spectral subtraction (SS) (Boll, 1979) is one of the first algorithms applied to the problem of speech enhancement. It is simple to implement but it distorts the speech signal and introduces additional annoying noise known as “musical noise”. Many algorithms have been proposed to remove this phenomenon including perceptually motivated techniques by Petrovsky et al. (2004) and the aspects of the human auditory system (Lu and Loizou, 2008), but their optimality in a sense of linear estimation is not clear.

Statistical model-based algorithms are one of the most commonly used classes of speech denoising methods. Recovery of the clean speech transform coefficients, or their magnitudes, is treated as a Bayesian estimation problem with known speech and noise statistics. Many estimators have been derived under different assumptions for the noise and speech distributions. The Wiener filter algorithms consist in estimating an optimal filter from the noisy speech spectrum by minimizing the Mean Square Error (MMSE) (Gui and Kwan, 2005). The MMSE estimator is used to evaluate the short-time spectral amplitude (STSA) based on a priori signal-to-noise ratio (SNR) estimation and Gaussian statistics. The prior SNR estimation was performed using “decision estimator” proposed by Ephraim and Malah (1984).

Wavelet transforms (WT) have been applied to various speech applications. The basic principle of speech enhancement in wavelets analysis is based on the thresholding of the discrete wavelet coefficients (DWC) to segregate the components corresponding to the target speech from those of the noise. However, when we apply a fixed threshold for the DWC of speech, some unvoiced speech frames can be eliminated with the additional ranges noise thus degrading the intelligibility of the enhanced speech. To solve this problem, the thresholding must be modified over time. Therefore, diverse adaptive wavelet thresholding procedures are proposed as the universal threshold proposed by Donoho and Johnstone (1995), Stein's Unbiased Risk Estimate (SURE) strategy described in Hu and Loizou (2004), Bayes Shrink exploiting a Bayesian estimate is described in Leporini and Pesquet (2001). The main drawback of the WT is the restricted number of frequency bands. Also, the unvoiced frames of noisy speech have proven to be problematic in terms of the wavelet shrinking. Since the unvoiced parts of speech include many noise-like high frequency components, removing them in the wavelet domain can degrade the quality and intelligibility of the enhanced speech.

The speech subspace algorithms consist in projecting the noisy speech segments onto orthogonal subspaces. The speech subspace is composed of high-energy vectors in the segment's principal component (PC) basis. The first algorithm was introduced by Dendrinos et al. (1991) who proposed to use the Singular Value Decomposition (SVD) technique to eliminate the noise subspace for speech denoising. Therefore, Ephraim et al. (1996) have used the fast Fourier transform (FFT) to approximate PC basis. These methods have removed the “musical noise” artifacts but the subspace approach improves perceived speech quality without increasing speech intelligibility. A well-known method proposed by Hu and Loizou (2003) is based on a joint digitalization of the noise and clean speech covariance matrices leading to the optimal estimators. Unfortunately, an efficient implementation of the subspace based approaches with an optimal choice of parameters is a challenging task and mainly in the case of colored and babble noise. To overcome the limitation, many approaches based on the speech segregation for enhancing the perceptual intelligibility of the speech degraded by additive background noise have been presented such as K-SVD by Sigg et al. (2010) and non-negative matrix factorization (NMF) by Mohammadiha et al. (2011), and Mysore and Smaragdis (2011). However, these methods always demand prior training for supervised segregation, empirical parameters, or particular features.

So, a number of studies using principal component analysis (PCA) are proposed. The essential object is to obtain a set of orthogonal factors that describe the variance of the observations and track the new factors considered to determine the necessary features without prior training. Speech processing by PCA (Joliffe, 2002) is extensively applied as a classical multivariate speech processing tool. For speech segregation, a robust extension of classical PCA by generalizing an eigenvalue decomposition of a pair of covariance matrices is proposed by Benabderrahmane et al. (2010). It may be used in speech denoising by Shinde et al. (2012), speech identification by Abolhassani et al. (2007), and speech recognition by Takiguchi and Ariki (2007).

In this paper, we propose a speech enhancement method based on the combination of the wavelet packet transform (WPT) and an improved version of the principal component analysis (IPCA). In fact, the orthogonal basis functions produced by the WPT, provide satisfactory results at low and high frequencies (Donoho et al., 1995; Ghanbari and Karami, 2006). Projection of a noisy speech onto these basis functions may be realized efficiently by passing the noisy speech through a tree-structured conjugate quadrature filter bank. In the proposed method, we apply the WPT, which allows to obtain appropriate time-frequency resolution, and to adaptively select relative frequency band based on the type of the speech to be estimated. To further improve denoising character of WPT, the paper describes a method, which combines the ability of the PCA tool to decorrelate the variables by extracting a linear relationship with that of wavelet packet analysis to enhance the speech signal. Our basic idea is to construct powerful filters by applying the PCA in the wavelet packet domain, thus getting a compaction of the speech energy into a few principal components (PC's), while the noise is spread over all the transformed coefficients. This allows us to operate with a convenient shrinkage function on these new coefficients, removing the noise without degrading the speech.

Then, we apply our improved version of the PCA at the enhanced speech obtained by the inverse wavelet packet transform. Our extension of PCA technique exploits the benefit of sparse PCA in the context of classification. Based on the fact that the sparse PCA by Zou et al. (2006) is a regression type optimization to PCA with a quadratic penalty, our technique segregates the noisy speech using the sparse matrix, the remainder noise component, and low-rank decomposition technique. In this paper, we propose to optimize a sparse PCA which decomposes the noisy speech into three subspaces corresponding to the speech structure subspace, noise structure subspace, and the remainder noise subspace.

The proposed method is tested on noisy speech under various types of noise conditions including white, babble, factory, car, and street noises. Objective and subjective results show that the system based on this approach has significant improvement over three recent methods.

The rest of the paper is organized as follows. In Section 2, we give a brief description of the PCA technique and its application for speech enhancement. Next, in Section 3, we detail our approach. The experimental results of our method under a variety of real noisy environments are given in Section 4. Finally, we draw conclusions in Section 5.

In this section, we first present fundamental relations and properties of PCA and then review PCA based speech enhancement.

Suppose that the single-channel noisy speech signal observation vector 
                        
                           x
                           (
                           t
                           )
                           ∈
                           
                              ℝ
                              L
                           
                        
                      can be constructed by the sum of the clean speech vector s(t) and the noise vector n(t):
                        
                           (1)
                           
                              
                                 x
                                 (
                                 t
                                 )
                                 =
                                 s
                                 (
                                 t
                                 )
                                 +
                                 n
                                 (
                                 t
                                 )
                              
                           
                        
                     
                  

We consider a set of L centered observations x(t)=[x
                     1, x
                     2, …, x
                     
                        L
                     ]
                        T
                     , and 
                        
                           
                              ∑
                              
                                 k
                                 =
                                 1
                              
                              L
                           
                           
                              
                                 x
                                 k
                              
                           
                           =
                           0
                        
                     .

Then, we arrange the L-dimensional observation vector in a I
                     ×
                     J normalized matrix of the observation X
                     
                        I×J
                     (t), with L
                     =
                     I
                     +
                     J
                     −1.

The time-variable notation is considered implicit. In the remainder of the section, this notation will be left out.

From the vector x, we compute the covariance matrix C. We use the PCA tool to diagonalize the matrix.
                        
                           (2)
                           
                              
                                 C
                                 =
                                 
                                    1
                                    L
                                 
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    L
                                 
                                 
                                    
                                       x
                                       k
                                    
                                    
                                       x
                                       k
                                       T
                                    
                                 
                              
                           
                        
                     
                  

The covariance matrix C can be estimated as:
                        
                           (3)
                           
                              
                                 C
                                 =
                                 
                                    1
                                    
                                       I
                                       −
                                       1
                                    
                                 
                                 X
                                 
                                    X
                                    T
                                 
                                 ∈
                                 
                                    ℝ
                                    
                                       J
                                       ×
                                       J
                                    
                                 
                              
                           
                        
                     
                  

It is about the best covariance matrix estimation having the lowest dimension.

Let 
                        
                           
                              v
                              1
                           
                           ,
                           
                              v
                              2
                           
                           ,
                           …
                           ,
                           
                              v
                              I
                           
                        
                      be the eigenvectors corresponding to the eigenvalues α
                     1, α
                     2, …, α
                     
                        I
                      of the matrix C.

The matrix V is defined as:
                        
                           (4)
                           
                              
                                 V
                                 =
                                 [
                                 
                                    v
                                    1
                                 
                                 ,
                                 
                                    v
                                    2
                                 
                                 ,
                                 …
                                 ,
                                 
                                    v
                                    I
                                 
                                 ]
                                 ∈
                                 
                                    ℝ
                                    
                                       I
                                       ×
                                       I
                                    
                                 
                              
                           
                        
                     
                  

To compute the PCA, we must evaluate the magnitude of the eigenvalue problem
                        
                           (5)
                           
                              
                                 α
                                 V
                                 =
                                 C
                                 V
                              
                           
                        
                     
                  

Then, we can decompose the covariance matrix C into its eigenvalue decomposition (EVD) by Karhunen–Loève transform (KLT) as follows:
                        
                           (6)
                           
                              
                                 C
                                 =
                                 V
                                 χ
                                 
                                    V
                                    T
                                 
                              
                           
                        
                     where χ is a diagonal matrix corresponding to the eigenvalues in a decreasing order represented as follows:
                        
                           (7)
                           
                              
                                 χ
                                 =
                                 d
                                 i
                                 a
                                 g
                                 
                                    
                                       
                                          α
                                          1
                                       
                                       ,
                                       
                                          α
                                          2
                                       
                                       ,
                                       …
                                       ,
                                       
                                          α
                                          I
                                       
                                    
                                 
                              
                           
                        
                     
                  


                     With 
                     α
                     1
                     ≥
                     α
                     2
                     ≥⋯≥0

The aim is to find some orthonormal matrix V in order to extract the feature of the clean speech only. The column of V is the PC's of X.

According to Eqs. (3) and (6), we have:
                        
                           (8)
                           
                              
                                 
                                    V
                                    T
                                 
                                 χ
                                 V
                                 =
                                 
                                    1
                                    
                                       I
                                       −
                                       1
                                    
                                 
                                 X
                                 
                                    X
                                    T
                                 
                              
                           
                        
                     
                  

From Eq. (8), we can observe that 
                        
                           χ
                           =
                           
                              1
                              
                                 I
                                 −
                                 1
                              
                           
                           X
                           V
                           
                              
                                 X
                                 
                                    V
                                    T
                                 
                              
                           
                        
                      is the diagonal covariance matrix.

We can conclude that columns of V are the PCs of X and the eigenvectors of XX
                     
                        T
                     .

The goal of PCA technique is to determine the most significant basis to re-express a noisy speech set. This new basis will filter out the noise and reduce a multidimensional speech to lower dimensions by avoiding redundant data.

Different approaches had been applied to choose the optimal number of PCs like imbedded error function (IEF) (Kaiser, 1960), minimum description length (MDL) (Rissanen, 1978), cumulative percent variance (CPV), average eigenvalue (AE), auto-correlation (AC) (Shrager and Hendler, 1982), parallel analysis (PA) (Zwick and Velicer, 1986), variance of the reconstruction error (VRE) (Qin and Dunia, 1998). The MDL criterion is based on the covariance matrix with solid statistical basis. While, the methods based on the correlation matrix consist in weighting all variables by a variance scaling.

As the distribution of the noisy speech is highly non-Gaussian, Lev-Ari and Ephraim (2003), Wei and Hu (2003) and Li and Lei (2007) propose the use of the Kernel PCA (KPCA) to eliminate the non-stationary and colored noise. In KPCA, the nonlinear structure of the noisy speech is modified into a higher-dimensional space with LPCA by Schölkopf et al. (1998). Besides, we must select the adequate number of PCs to describe the model in an optimal manner.

In order to improve the performance of the speech enhancement methods, the PCA has been incorporated with the human hearing properties (Jabloun and Champagne, 2003), the Hilbert–Huang transform (Khademul and Hirose, 2007), and also the transition from the classical PCA to self-adapting PCA/KLT technique (Rezayee and Gazor, 2001).


                     Vetter et al. (1999) propose a subspace approach based on the KLT/PCA to give an optimum compression of information. The optimal subspace selection is provided by a MDL criterion. The idea is based on the fact that the covariance matrix of a noisy speech can be decomposed into two mutually orthogonal vector spaces: a noise subspace and a signal (+noise) subspace. Noise reduction is achieved by discarding the noise subspace completely, while modifying the noisy speech components in the signal (+noise) subspace.


                     Lee et al. (2001) have applied the PCA in the spectrum domain to estimate the filter-bank coefficients for speech enhancement.


                     Lima et al. (2004, 2005), have applied the KPCA only to the low-order MFCCs for speech enhancement and speech recognition.


                     Takiguchi and Ariki (2007) have proposed to apply the KPCA to the mel-scale filter bank output because the KPCA will project the main speech element onto low-order features, while noise elements will be projected onto high-order ones.


                     Haji-Abolhassani (2007) introduced an optimal subspace partitioning using the variance of the reconstruction error (VRE) criterion.


                     Leitner et al. (2011) used an approach based on the complex coefficients of the FFT using KPCA.

A recent study explores the sparse and the low-rank structures by Candès et al. (2011) to enhance the noisy speech. Candès et al. (2011) have developed meaningful decompositions named “Robust Principal Component Analysis” (Robust-PCA). The authors prove that the sparse and the low-rank components of a matrix can be exactly recovered, if it has a unique and precise decomposition. This unsupervised method gives a blind separation of the sparse speech and the low-rank noise.

In this section, we show how the wavelet-packet transform (WPT) and an improved version of the principal component analysis (PCA) can be suitably combined to simultaneously minimize the speech distortion and maximize the noise reduction.

The principal steps of our method called WP-IPCA (wavelet packet and improved version of the principal component analysis) are shown in Fig. 1
                     .

Wavelet transforms allow the decomposition of the signal into detail and approximation. The approximation is then decomposed into a second level of approximation and details and so on by a quadrature mirror filter (Donoho, 1995). Wavelet packet decomposition (WPD) is a further generalization of the wavelet decomposition (WD). In the WPD, the filtering procedure is iterated on both the high and low frequency components.

In Fig. 2
                        , we show the decomposition tree of the WP.

Each node is defined by (E,n), where E is the decomposition level and n corresponds to a sub-band node index. The root of the tree corresponding to (E,n)=(0,0), makes the reference to the entire signal space. The right and left branches indicate high-pass and low-pass filtering followed by 2:1 down-sampling, respectively.

The application of the WP for speech enhancement is a three step algorithm, applying wavelet decomposition, wavelet coefficient thresholding, and wavelet rebuild.

The different steps of our method are described as follows:
                           
                              Step 1:
                              Divide the noisy speech into F frames of N
                                 =512 samples with half-length overlap. We obtain a matrix 
                                    
                                       X
                                       =
                                       
                                          
                                             
                                                x
                                                1
                                             
                                             ,
                                             
                                                x
                                                2
                                             
                                             ,
                                             …
                                             ,
                                             
                                                x
                                                F
                                             
                                          
                                       
                                    
                                  with dimension F
                                 ×
                                 N. It is the results of the speech framing whose columns host the partially overlapping noisy speech segments.

For each column in the noisy speech data matrix X, we select the wavelet packet (WP) function Ψ
                                 
                                    i,j
                                 (t) and calculate the wavelet packet decomposition (WPD) coefficients matrixes 
                                    
                                       {
                                       
                                          W
                                          
                                             E
                                             ,
                                             0
                                          
                                       
                                       ,
                                       
                                          W
                                          
                                             E
                                             ,
                                             1
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          W
                                          
                                             E
                                             ,
                                             
                                                2
                                                E
                                             
                                             −
                                             1
                                          
                                       
                                       }
                                    
                                  using Daubechies wavelet with level E.

Apply the best WP basis coefficients to operate the WPD tree.

Choice the coefficients as a column vector to get WPD coefficients matrixes with various tree nodes 
                                    
                                       {
                                       
                                          Y
                                          
                                             E
                                             ,
                                             0
                                          
                                       
                                       ,
                                       
                                          Y
                                          
                                             E
                                             ,
                                             1
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          Y
                                          
                                             E
                                             ,
                                             
                                                2
                                                E
                                             
                                             −
                                             1
                                          
                                       
                                       }
                                    
                                 , the column number is F and the row number of these matrix is N/2
                                    E
                                 .

Apply the conventional PCA on the obtained coefficients matrixes to calculate PC's score matrixes 
                                    
                                       {
                                       
                                          O
                                          
                                             E
                                             ,
                                             0
                                          
                                       
                                       ,
                                       
                                          O
                                          
                                             E
                                             ,
                                             1
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          O
                                          
                                             E
                                             ,
                                             
                                                2
                                                E
                                             
                                             −
                                             1
                                          
                                       
                                       }
                                    
                                 , load matrixes 
                                    
                                       {
                                       L
                                       
                                          O
                                          
                                             E
                                             ,
                                             0
                                          
                                       
                                       ,
                                       L
                                       
                                          O
                                          
                                             E
                                             ,
                                             1
                                          
                                       
                                       ,
                                       …
                                       ,
                                       L
                                       
                                          O
                                          
                                             E
                                             ,
                                             
                                                2
                                                E
                                             
                                             −
                                             1
                                          
                                       
                                       }
                                    
                                  in order to reconstruct WP coefficients matrixes 
                                    
                                       {
                                       
                                          
                                             Y
                                             ′
                                          
                                          
                                             E
                                             ,
                                             0
                                          
                                       
                                       ,
                                       
                                          
                                             Y
                                             ′
                                          
                                          
                                             E
                                             ,
                                             1
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             Y
                                             ′
                                          
                                          
                                             E
                                             ,
                                             
                                                2
                                                E
                                             
                                             −
                                             1
                                          
                                       
                                       }
                                    
                                 , and combine the corresponding column vectors to obtain 
                                    
                                       {
                                       
                                          
                                             W
                                             ′
                                          
                                          
                                             E
                                             ,
                                             0
                                          
                                       
                                       ,
                                       
                                          
                                             W
                                             ′
                                          
                                          
                                             E
                                             ,
                                             1
                                          
                                       
                                       ,
                                       …
                                       ,
                                       
                                          
                                             W
                                             ′
                                          
                                          
                                             E
                                             ,
                                             
                                                2
                                                E
                                             
                                             −
                                             1
                                          
                                       
                                       }
                                    
                                 . To get the PC's, first, we compute the Eigen-decomposition of covariance matrices to find out the eigenvector and eigenvalues. Second, we arrange them in a decreasing order. Finally, we select the eigenvalues to determine the number of retained PC's using Kaiser's rule.

As (Rissanen, 2000) showed that the thresholding technique proposed by Donoho and Johnstone (1995) leads to eliminate too many coefficients, we apply the MDL model selection criterion to choose the correct threshold values for denoising. For more detailed information of the MDL model, you can refer to Whitmal et al. (1996).

Reconstruct by an overlap-adding the enhanced speech frames signal of the matrix 
                                    
                                       B
                                       =
                                       
                                          
                                             
                                                
                                                   x
                                                   ′
                                                
                                                1
                                             
                                             ,
                                             
                                                
                                                   x
                                                   ′
                                                
                                                2
                                             
                                             ,
                                             …
                                             ,
                                             
                                                
                                                   x
                                                   ′
                                                
                                                F
                                             
                                          
                                       
                                    
                                  using WP rebuild (WPR) method.

Use the obtained matrix B to reconstruct our final enhanced speech signal 
                                    
                                       X
                                       e
                                       =
                                       
                                          
                                             x
                                             
                                                
                                                   e
                                                   ′
                                                
                                                1
                                             
                                             ,
                                             x
                                             
                                                
                                                   e
                                                   ′
                                                
                                                2
                                             
                                             ,
                                             …
                                             ,
                                             x
                                             
                                                
                                                   e
                                                   ′
                                                
                                                F
                                             
                                          
                                       
                                    
                                  by our improved PCA (IPCA). This step will be detailed in the following sub-section.

Our contribution consists essentially in the partition of the eigenspace of the obtained signal into 3 subspaces to guarantee the maximization of noise reduction and the minimization of the signal distortion.

First, we calculate the FFT of the noisy speech signal and compute the spectrogram to obtain the matrix X′.

The FFT of the result signal x′(t) is given by:
                           
                              (9)
                              
                                 
                                    
                                       X
                                       ′
                                    
                                    (
                                    i
                                    ,
                                    n
                                    )
                                    =
                                    
                                       ∑
                                       
                                          k
                                          =
                                          −
                                          ∞
                                       
                                       
                                          +
                                          ∞
                                       
                                    
                                    
                                       
                                          x
                                          ′
                                       
                                       
                                          k
                                       
                                       w
                                       
                                          
                                             i
                                             −
                                             k
                                          
                                       
                                    
                                    
                                       e
                                       
                                          −
                                          j
                                          2
                                          π
                                          k
                                          n
                                          /
                                          L
                                          f
                                       
                                    
                                 
                              
                           
                        where i refers to the index of the time-frame, n is the index of the discrete frequency, w(i) is an analysis window function, and Lf is the length of the frequency analysis. In the spectral domain, we smooth the speech spectrum magnitude 
                           
                              
                                 
                                    
                                       X
                                       ′
                                    
                                    (
                                    i
                                    ,
                                    n
                                    )
                                 
                              
                           
                         at each frame, and accumulate all frames of the 
                           
                              
                                 
                                    
                                       X
                                       ′
                                    
                                    (
                                    i
                                    ,
                                    n
                                    )
                                 
                              
                           
                         as column vectors to obtain the matrix representation X′.

Second, we apply our improved principal component analysis (IPCA) technique to obtain three matrices S, Re and Lo from the matrix X′. Our partition to the sparse, remainder noise, and the low structure subspaces is based on the hypothesis that the noise spectrum always presents an iterative pattern with limited variation whereas the speech signal has more alteration and is relatively sparse within the noise.

The new formulation of IPCA is described in the frequency domain by the following equation:
                           
                              (10)
                              
                                 
                                    
                                       X
                                       ′
                                    
                                    =
                                    S
                                    +
                                    Re
                                    +
                                    L
                                    o
                                 
                              
                           
                        where X′ is the input coefficients matrix, S refers to sparse matrix, Re refers to the remainder matrix and Lo refers to the low-rank matrix.

In this sub-section, we aim to recover the sparse matrix S, which represents the speech structure matrix, the low-rank matrix Lo, which represents the noise structure matrix, from the input matrix under the perturbation of the remainder matrix Re whose inputs have zero-mean Gaussian distributions. Then, the background noise is identified as the sum of the remainder and the low-rank components. Our method applies the remainder noise matrix to model the alteration of noise while the stable statistics of noise are described in the low-rank matrix. This new assumption should allow us to improve our method.

Now, we try to solve the following well-behaved convex optimization problem using the algorithm proposed by (Zhou and Tao, 2011):
                           
                              (11)
                              
                                 
                                    min
                                    
                                       
                                          
                                             
                                                
                                                   X
                                                   ′
                                                
                                                −
                                                S
                                                −
                                                L
                                                o
                                             
                                          
                                       
                                       *
                                    
                                    +
                                    α
                                    
                                       
                                          
                                             S
                                          
                                       
                                       1
                                    
                                 
                              
                           
                        
                     

||||* makes reference to the nuclear norm of Lo, where ||Lo||* is determined as the sum of the singular values of Lo, which is representative for minimizing the rank of Lo.

The sparsity of S is measured by the L1-norm ||||1, meaning the summation of absolute value of the matrix's elements.


                        
                           
                              α
                              =
                              1
                              /
                              
                                 
                                    max
                                    (
                                    m
                                    1
                                    ,
                                    m
                                    2
                                    )
                                 
                              
                           
                         is a trade-off parameter between the sparsity of S, and the rank of Lo.

Finally, we use a convex relaxation technique by applying the inexact Augmented Lagrange Multiplier (IALM) by Lin et al. (2009), to obtain the enhanced matrix:
                           
                              (12)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         X
                                                         ′
                                                      
                                                      =
                                                      S
                                                      +
                                                      Re
                                                      +
                                                      L
                                                      o
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      g
                                                      
                                                         
                                                            S
                                                            ,
                                                            L
                                                            o
                                                         
                                                      
                                                      =
                                                      Re
                                                      =
                                                      
                                                         X
                                                         ′
                                                      
                                                      −
                                                      S
                                                      −
                                                      L
                                                      o
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The function g describes the remainder matrix Re in terms of the sparse and low components.

As a result, the IALM function is given by the following equation:
                           
                              (13)
                              
                                 
                                    I
                                    (
                                    S
                                    ,
                                    L
                                    o
                                    ,
                                    Re
                                    ,
                                    β
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                L
                                                o
                                             
                                          
                                       
                                       *
                                    
                                    +
                                    α
                                    
                                       
                                          
                                             S
                                          
                                       
                                       1
                                    
                                    +
                                    〈
                                    
                                       X
                                       ′
                                    
                                    ,
                                    Re
                                    〉
                                    +
                                    
                                       β
                                       2
                                    
                                    
                                       
                                          
                                             
                                                
                                                   X
                                                   ′
                                                
                                                −
                                                S
                                                −
                                                L
                                                o
                                             
                                          
                                       
                                       F
                                       2
                                    
                                 
                              
                           
                        
                     

where I() is the IALM function, and β is a positive scalar.

Then, 
                           
                              
                                 
                                    
                                       S
                                       *
                                    
                                    ,
                                    L
                                    
                                       o
                                       *
                                    
                                    ,
                                    
                                       
                                          Re
                                       
                                       *
                                    
                                 
                              
                           
                         of 
                           
                              
                                 
                                    
                                       S
                                       i
                                       *
                                    
                                    ,
                                    L
                                    
                                       o
                                       i
                                       *
                                    
                                    ,
                                    
                                       
                                          Re
                                       
                                       i
                                       *
                                    
                                 
                              
                           
                         is the best solution of our improved PCA and the convergence rate is at least 
                           
                              O
                              
                                 
                                    
                                       β
                                       i
                                       
                                          −
                                          1
                                       
                                    
                                 
                              
                           
                        . It is clear that the choice of β
                        
                           i
                         is essential to obtain a minimum number of SVD. So, we solve the sub-problem 
                           
                              
                                 
                                    
                                       S
                                       
                                          i
                                          +
                                          1
                                       
                                       *
                                    
                                    ,
                                    L
                                    
                                       o
                                       
                                          i
                                          +
                                          1
                                       
                                       *
                                    
                                    ,
                                    
                                       
                                          Re
                                       
                                       
                                          i
                                          +
                                          1
                                       
                                       *
                                    
                                 
                              
                              =
                              arg
                              
                                 
                                    min
                                 
                                 
                                    L
                                    o
                                    ,
                                    S
                                    ,
                                    Re
                                 
                              
                              I
                              (
                              S
                              ,
                              L
                              o
                              ,
                              
                                 
                                    Re
                                 
                                 *
                              
                              ,
                              
                                 β
                                 i
                              
                              )
                           
                         inexactly by the IALM technique.

The detailed procedure is presented in the algorithm below:
                           
                              
                                 
                                 
                                    
                                       Algorithm of the IALM model.
                                    
                                 
                                 
                                    
                                       
                                          Input: data matrix X′, and the parameter α.
                                          Initialization: 
                                          Lo
                                          =0; S
                                          =0; Re
                                          =random; β
                                          >0; ω
                                          >1; i
                                          =0;
                                          while not converged do
                                          
                                          
                                          Update S:
                                          
                                          
                                          S
                                          
                                             i+1
                                          =argmin
                                             S,Re
                                          I(S,Lo,Re*,β
                                          
                                             i
                                          )
                                          
                                          Update Lo:
                                          
                                          
                                          Lo
                                          
                                             i+1
                                          =argmin
                                             Lo,Re
                                          I(S
                                          
                                             i
                                          ,Lo,Re
                                             i
                                          ,β
                                          
                                             i
                                          )
                                          
                                          Update Re:
                                          
                                          
                                          
                                             
                                                
                                                   
                                                      Re
                                                   
                                                   
                                                      i
                                                      +
                                                      1
                                                   
                                                
                                                =
                                                
                                                   
                                                      Re
                                                   
                                                   i
                                                
                                                +
                                                
                                                   β
                                                   i
                                                
                                                
                                                   
                                                      
                                                         X
                                                         ′
                                                      
                                                      −
                                                      
                                                         S
                                                         
                                                            i
                                                            +
                                                            1
                                                         
                                                      
                                                      −
                                                      L
                                                      
                                                         o
                                                         
                                                            i
                                                            +
                                                            1
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                          
                                          
                                          
                                          β
                                          
                                             i+1
                                          =
                                          ωβ
                                          
                                             i
                                          
                                          
                                          
                                          i
                                          +1
                                          end while
                                          
                                          Output: 
                                          S
                                          
                                             i
                                          
                                          ;
                                          Lo
                                          
                                             i
                                          
                                          ;Re
                                             i
                                          
                                       
                                    
                                 
                              
                           
                        
                     

We introduce the remainder matrix in the subspaces to avoid fitting the background noise with simultaneous speech. Also, we would like to cover a wider range of variation as efficiently as possible. Finally, our method is extendable to a more complex model.

The Fig. 3
                         shows respectively the spectrograms of the original speech, the noisy speech, the enhanced speech signal using the wavelet packet and an improved version of the principal component analysis.

In Fig. 3(c), we can observe that the sparse component accords with sparsity of the speech energy in the frequency domain, and the low-rank component is spanned by a few frequency basis segments.

@&#EXPERIMENTS AND RESULTS@&#

We evaluate and compare our proposed method for speech enhancement in this section.

In our simulations, all the speech signals are sampled at 8kHz and were implemented using a frame length of 512 samples and 0.5 overlapped. The FFT length is equal to 256 points. The core test set of the NOIZEUS database (30 sentences) was exploited for the speech enhancement evaluation. For this evaluation, we considered 5 noise types: white, babble and factory noises from the NOISEX-92 database (Varga and Steeneken, 1993), car traffic, and street noises from the NOIZEUS database (Hu and Loizou, 2007). The four noise babble, factory, car, and street noises are considered non-stationary. We mixed the speech signals at three different signal-to-noise ratios (SNRs) from −5 to 5dB.

The results obtained by our method are compared to three methods:
                           
                              –
                              The Geometric Approach (GA) proposed by Lu and Loizou (2008), is an improved method based on the psycho-acoustical model of the spectral density subtraction.

The Robust Principal Component Analysis (Robust-PCA) proposed by Candès et al. (2011) is an unsupervised technique which consists in decomposing a matrix into low-rank and sparse structures, using a convex optimization.

The on-line semi-supervised method is based on Nonnegative Matrix Factorization (on-line-Sup-NMF) and proposed by Joder et al. (2012). This method applies a sliding window technique which separates the spectrum of the recent past into two matrices. An activation matrix is shifted, and the sliding window is also shifted by one frame. The matrices are then revised based on a number of NMF iterations. In our experiments, we choose the number of NMF iterations equal to three.

To test the performance of our proposed speech enhancement method, objective quality or intelligibility tests were applied.

We use the BSS EVAL toolbox developed by Vincent et al. (2006), to measure the performance of the speech enhancement results of our method within an evaluation framework where the original speech signals are available as ground truth. Basically, each estimated speech signal is decomposed into a true speech part and error parts corresponding to interference from the noise signal and artifacts such as “musical noise”. The toolbox gives three metrics: Speech-to-Distortion Ratio (SDR), Speech-to-Interference Ratio (SIR) and Speech-to-Artifacts Ratio (SAR).


                        Figs. 4–6
                        
                        
                         present respectively the SDR, SIR, and SAR measures of the average results over five types of noise permitting to evaluate and compare the different methods.

The SDR metric shows the overall quality of the speech enhancement method. According to SDR results presented in Fig. 4, our method is the best among all methods and essentially at low input SNRs. For high input SNRs the difference between our method and the Robust-PCA is small. It can be explained by the fact that the perturbation of our remainder matrix is slight at an SNR greater than zero, and the introduction of the new constraints on the sparse matrix reduces the speech distortion.

The SIR metric indicates the noise reduction rating. Fig. 5 shows that the on-line-Sup-NMF algorithm obtains the best SIR values. However, we can see that our method provides similar performance for high input SNR. This is because the on-line-Sup-NMF algorithm is achieved in a semi-supervised manner, whereas our method is an unsupervised speech enhancement method. In on-line-Sup-NMF, two noise representations are learned. Thus, the on-line-Sup-NMF algorithm can apply more prior information in the speech enhancement process than our method.

Also, we can see that our proposed method outperforms the Robust-PCA one because it decomposes the noisy speech into two simple subspaces when the sparse matrix contains some part of the low matrix. By contrast, our method decomposes the eigenspace as the summation of a low-rank, sparse, and remainder subspaces to model the change of noise.

The SAR metric reflected the artifacts introduced by the speech enhancement method. As we can see, in Fig. 6, our method outperforms the others. However, the difference between our method and the Robust-PCA is small for high SNR levels. The On-line-Sup-NMF method gets bad performances that can be explained by the fact that this method needs prior training of noises.

According to the three figures above, we can conclude that our method has outperformed all the other approaches, which is closely followed by Robust-PCA at high input SNR. This shows the efficiency of the PCA technique.

The evaluation of the speech enhancement methods based on the objective speech quality measures: SegSNR, PESQ, and Cov are listed respectively in Tables 1–3
                        
                        
                        .

The Segmental SNR (SegSNR) is a practical measure of the speech quality. It is constructed by averaging the frame level SNR estimation.
                           
                              (14)
                              
                                 
                                    SegSNR
                                    =
                                    
                                       
                                          10
                                       
                                       F
                                    
                                    
                                       ∑
                                       
                                          k
                                          =
                                          1
                                       
                                       F
                                    
                                    
                                       
                                           log
                                          
                                             10
                                          
                                       
                                        
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         0
                                                      
                                                      
                                                         N
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         x
                                                         2
                                                      
                                                      (
                                                      k
                                                      ,
                                                      i
                                                      )
                                                   
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         0
                                                      
                                                      
                                                         N
                                                         −
                                                         1
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            [
                                                            x
                                                            (
                                                            k
                                                            ,
                                                            i
                                                            )
                                                            −
                                                            x
                                                            e
                                                            '
                                                            (
                                                            k
                                                            ,
                                                            i
                                                            )
                                                            ]
                                                         
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where N is the length of each frame, and F is the number of frames. xe′ and x are the kth frame of the enhanced and the original speech signal respectively.

The results of the SegSNR values are detailed in Table 1.

The Table 1 shows that the on-line-Sup-NMF gives a good performance for the four non stationary noises at SNR equal to −5 and 0dB compared to our method and Robust-PCA. This table confirms again the results obtained by the SIR improvements measure. The main reason is that our method is totally unsupervised.

The average Perceptual Evaluation of Speech Quality (PESQ) consists in mapping the enhanced and the original speech signals onto an internal representation using a perceptual model.


                        Table 2 gives the averaged PESQ scores for the above-mentioned methods, over all noise conditions.

The high PESQ scores perceived the quality of the enhanced speech. As it can be seen in the Table 2, the proposed method is characterized by the highest PESQ scores showing that the enhanced speech by our method has a better perceived quality. Our method works marginally better than all others methods at low SNR. But, for the high SNR the results are close to that of the on-line-Sup-NMF method with white noise, and for Robust-PCA method at all types of noise.

We can conclude that the PESQ scores exhibit comparable trends to the SDR values.

The composite measures (Cov) results in the combination of the evaluation measures in the frequency-domain, time-domain and perceptual field.

The Cov measure is described as follows:
                           
                              (15)
                              
                                 
                                    Cov
                                    =
                                    1.594
                                    −
                                    0.007
                                    ×
                                    WSS
                                    −
                                    0.512
                                    ×
                                    LLR
                                    +
                                    0.805
                                    ×
                                    PESQ
                                 
                              
                           
                        where, the weighted spectral slope (WSS) is determined by calculating the weighted difference between the adjacent spectral magnitudes in each frequency band. The spectral slope is obtained as the difference between adjacent spectral magnitudes. The log-likelihood ratio (LLR) measure is based on the difference between the all-pole models of the enhanced and clean speech.

The results of the composite measures are detailed in Table 3.

Our method outperforms all other methods with a large margin. It presents the highest Cov values confirming its performance, as shown in Table 3. These results prove the efficiency and the consistency of the combination of the wavelet packet transform and our improved version of the principal component analysis. The main reason of our method outperformance is that:
                           
                              –
                              For the Robust-PCA method, the distinction between low-rank and sparse matrices is very sensitive to the chosen parameters.

For the on-line-Sup-NMF method, we have to define the noise rank. In fact, if it is set too low, it will be incapable to take the noise and if it is set too high, the extra noise dimensions will go to take segments of the speech which explain the signal distortion.

For the GA, the speech is enhanced but the method adds a ringing residual noise that is not appreciated by the listeners.

Informal listening tests are also realized, where a group of 15 listeners (eight women and seven men) are allowed and arranged to perceptually evaluate 15 enhanced speech signals from the NOIZEUS database with three background noises (white, factory, and street) at two SNR levels (0dB and 5dB). The listeners used the mean opinion score (MOS) method to score the difference between the residual noise characteristics of the enhanced speech (1: Bad, 2: Poor, 3: Fair, 4: Good, 5: Excellent).


                        Fig. 7
                         presents the statistic results corresponding to the subjective evaluation of the considered speech enhancement methods.

The enhanced speech using our method is the most favorite. In fact, our approach provides a speech signal containing less musical noise while preserving the speech quality. The quality of the online-NMF and the RPCA methods is also accepted by the listeners. However, the GA presents a ringing residual noise that is not appreciated by the listeners.

@&#CONCLUSION@&#

This work investigated the use of the subspace in speech enhancement systems. We developed a denoising method using a modified version of the principal component analysis (PCA) combined with the properties of the wavelet packet transform (WPT). We proposed a method to enhance the noisy speech without requiring prior training, or empirical parameters. Our basic idea is to design powerful filters in the wavelet packet domain, thus acquiring a compaction of the speech energy into a few principal components, while the noise is distributed over all the transformed coefficients. This allows us to operate with a convenient shrinkage function on these new coefficients, removing the noise without degrading the speech. Then, we apply the improved version of the principal component analysis (IPCA) by decomposing the enhanced speech obtained by the inverse WPT into the sparse matrix, the remainder noise component, and the low-rank matrix.

The evaluation using the BSS toolbox, and the objective speech quality measures showed that a noise enhancement method using an improved version of the principal component analysis combined with a wavelet packet transform outperforms when compared to three recent methods. Our method presents the least distortion in the enhanced speech while suppressing less noise than the on-line-Sup-NMF. However, the on-line-Sup-NMF method decreases the interfering noise greatly with the cost of adding artifacts in the enhanced speech signal. Finally, the listening tests confirmed the performance of our method.

The future work will consist in using a prior training of the noises, which gives few hypotheses about the noise, and in extending this method to speech separation in monaural recordings.

@&#REFERENCES@&#

