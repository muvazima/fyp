@&#MAIN-TITLE@&#Class-specific multiple classifiers scheme to recognize emotions from speech signals

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The emotion recognition performances of AR parameters of different orders are investigated.


                        
                        
                           
                           AR reflection coefficients recognize emotions better than LPC.


                        
                        
                           
                           A new class-specific multiple classifiers scheme is proposed for speech emotion recognition.


                        
                        
                           
                           The proposed method utilizes a feature vector and a classifier for each emotion.


                        
                        
                           
                           The class-specific multiple classifiers scheme improves the recognition accuracy.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multiple classifiers

Class specific classification

Classifier fusion

Speech emotion recognition

AR parameters

@&#ABSTRACT@&#


               
               
                  Automatic emotion recognition from speech signals is one of the important research areas, which adds value to machine intelligence. Pitch, duration, energy and Mel-frequency cepstral coefficients (MFCC) are the widely used features in the field of speech emotion recognition. A single classifier or a combination of classifiers is used to recognize emotions from the input features. The present work investigates the performance of the features of Autoregressive (AR) parameters, which include gain and reflection coefficients, in addition to the traditional linear prediction coefficients (LPC), to recognize emotions from speech signals. The classification performance of the features of AR parameters is studied using discriminant, k-nearest neighbor (KNN), Gaussian mixture model (GMM), back propagation artificial neural network (ANN) and support vector machine (SVM) classifiers and we find that the features of reflection coefficients recognize emotions better than the LPC. To improve the emotion recognition accuracy, we propose a class-specific multiple classifiers scheme, which is designed by multiple parallel classifiers, each of which is optimized to a class. Each classifier for an emotional class is built by a feature identified from a pool of features and a classifier identified from a pool of classifiers that optimize the recognition of the particular emotion. The outputs of the classifiers are combined by a decision level fusion technique. The experimental results show that the proposed scheme improves the emotion recognition accuracy. Further improvement in recognition accuracy is obtained when the scheme is built by including MFCC features in the pool of features.
               
            

@&#INTRODUCTION@&#

Speech is the primary form of communication among human beings. Besides understanding the meaning of speech, the human has the natural ability to estimate the gender, age, speaker and the emotional state of the speaker. The emotional state of a speaker plays an important role because it shapes the real meaning of the spoken language. Emotional states of humans are expressed through changes in speech, facial expression, posture and physiological processes. Recognizing the emotional states by these changes can help us estimate the belief, desire and the likely future behavior of a person (Gratch et al., 2009). Emotion is integral to man's rational and intelligent decisions and expresses feelings and provides feedback (Busso et al., 2009). Anger, boredom, disgust, fear, happiness, sadness and neutral are considered as the seven basic discrete emotions (Yang and Lugger, 2010). The different emotional states of a speaker are associated with different heart rate, skin resistivity, temperature, papillary diameter and muscle activities. These changes result in the production of speech signals that carry the emotional information, making automatic detection of emotions from speech signals possible (Cowie et al., 2001). In the era of increasing human-machine interaction, detection of emotions can make intelligent machines create and understand emotions, like humans. In speech recognition and speaker identification applications, emotions are considered as noise. Therefore the recognition of emotions and its effect on the speech signals can improve the performance of speech and speaker recognition systems. Fear type emotion recognition can be used in audio-based surveillance system (Clavel et al., 2008) in order to gain control of a critical situation. Emotion recognition also finds its application in forensic data analysis and clinical diagnosis.

The four major areas of emotion recognition from speech signals are: acquisition and validation of emotional speech signals, feature extraction, feature selection and classification. In feature extraction and selection, the research community is looking for features which can identify and differentiate one emotion from others. For the features to be universal, it is preferred to be independent of the speaker, language, gender and culture. As the hunt for the best features is not yet complete, some researchers improve the classification accuracy by combining several classifiers (Lee and Narayanan, 2005; Morrison et al., 2007; Albornoz et al., 2011; López-Cózar et al., 2011).

The spectral features MFCC and LPC are the traditional features in automatic speech recognition (Bou-Ghazale and Hansen, 2000). MFCC is also the most investigated spectral feature in automatic speech emotion recognition (Schuller et al., 2011) unlike LPC. But a very few researchers like Nicholson et al. (2000) and Altun and Polat (2009) have used 12th order LPC and 16th order LPC, respectively, as one of the speech features to recognize emotions from speech signals. LPC are the denominator coefficients of an AR model transfer function. AR modeling is a technique which estimates the all-pole transfer function of a system that produces the observed signal. The AR model of speech signal is widely used to estimate pitch, formants, spectra and vocal tract area function. It can accurately model the speech producing system from the vocal cords to the lips as an all-pole linear system. If the number of poles is high enough, the all-pole model can represent the voiced, nasal and fricative of speech signals (Rabiner and Schafer, 2004). Motivated by the performance of LPC in speech recognition applications and its accuracy in modeling the transfer function of speech production system, the present work investigates the emotion classification performance of the features of AR parameters. Instead of concentrating on a single order AR parameters, the paper examines AR parameters of different orders from 3 to 25.

In AR model the combined spectral effect of glottal excitation, vocal tract and radiation are represented by a system function of order P
                     
                        
                           (1)
                           
                              
                                 H
                                 (
                                 z
                                 )
                                 =
                                 
                                    G
                                    
                                       1
                                       +
                                       
                                          ∑
                                          
                                             k
                                             =
                                             1
                                          
                                          P
                                       
                                       
                                          
                                             a
                                             P
                                          
                                          (
                                          k
                                          )
                                          
                                             Z
                                             
                                                −
                                                k
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where G is the gain parameter and a
                     
                        p
                     (k) are the LPC.
                        
                           (2)
                           
                              
                                 G
                                 =
                                 
                                    
                                       
                                          E
                                          P
                                       
                                    
                                 
                              
                           
                        
                     where E
                     
                        P
                      is the minimum prediction error.

The LPC are estimated by a recursive procedure like Levinson–Durbin algorithm in the autocorrelation method (Makhoul, 1975). In the process of estimation of the AR model parameters of order P, we get the reflection coefficients K
                     
                        i
                     
                     =
                     a
                     
                        i
                     (i), i
                     =1, 2, …, P and the prediction error E
                     
                        P
                     . In the AR model of order P, the vocal tract is considered as an acoustic tube with P sections, each reflection coefficient is an indication of the ratio of the cross sectional area of the consecutive sections of the acoustic tube (Makhoul, 1975). Therefore, reflection coefficients may carry more emotional information than LPC. The relation between the reflections coefficients and the section-wise cross sectional area of the vocal tract motivates us to examine the reflection coefficients. In this paper the collection of LPC, gain parameter and reflection coefficients are referred to as AR parameters.

The classification analysis of the features of the AR parameters with standard classifiers shows that a specific feature vector of the AR parameters, coupled with a specific classifier, recognizes a specific emotion with a high recognition rate. This activates us to propose a classification method that makes use of the specific emotion recognition potential of a specific feature vector of the AR parameters with a specific classifier to improve the overall recognition accuracy. The proposed classification method classifies the emotions at two levels. At the first level, an ensemble of class-specific classifiers is utilized and at the second level, the outputs of the first level classifiers are combined using a decision level fusion technique.

The novelty of this paper is twofold. First, in the feature side we examine the features of AR parameters, which include LPC, gain and reflection coefficients with standard classifiers. To the best of our knowledge the AR gain and reflection coefficients are not investigated in the field of emotions recognition from speech signals. Second, in the classification side we propose a new class-specific multiple classifiers scheme to improve the classification accuracy. We call the proposed scheme “class-specific multiple classifiers scheme”, because the individual classifier in the scheme is identified based on its optimum performance in recognizing a specific emotion.

The rest of the paper is organized as follows: Section 2 provides a brief review of the literatures in the field of speech emotion recognition. Section 3 explains the AR features extraction and the emotion classification analysis of AR parameters. Section 4 presents the design and the classification experiments of the proposed class-specific multiple classifiers scheme. Section 5 discusses the experimental results of the standard classifiers classification and the performance improvement obtained by the proposed class-specific multiple classifiers scheme and Section 6 concludes the paper.

@&#RELATED WORKS@&#

The first requirement to design an automatic speech emotion recognition system is the acquisition and validation of emotional speech database. Databases used by the researchers come under two major categories: the acted (simulated) database and the natural (spontaneous) database. The acted databases reported in literature are recorded by professional or semi-professional actors. The acted database is also extracted from movies (Clavel et al., 2008). But the natural databases are extracted from talk show (Grimm et al., 2007), from natural interactions of parents with their infants (Shami and Verhelst, 2007) or from call centers (Lee and Narayanan, 2005; Morrison et al., 2007). Natural database is also recorded by forcing a situation on the speakers without their knowledge in order to elicit natural emotions (Ayadi et al., 2011). The acted databases are most widely used because it is easy to construct them; speech signals with desired emotions can be recorded and an emotional state can be retained by the speaker throughout the recording of a full utterance. Despite the fact that acted emotions cannot emulate the real life emotions, they play a vital role in understanding the vocal cues of different emotions. The inherent advantage of the natural databases is their naturalness but the factors that limit their use are legal restriction and accessibility of a few emotions (for example in call center conversations, the most possible emotions are neutral, anger and happiness). Validation and emotion labeling of databases are done with the help of a listening test.

Once a database is ready, the features are extracted from the speech signals. The standard features which are examined repeatedly in the field are pitch (fundamental frequency), energy and Mel-frequency cepstral coefficients (MFCC). The features that come next to the above-mentioned features are formant (resonance frequency of the vocal tract) (Yang and Lugger, 2010; Lee and Narayanan, 2005; Morrison et al., 2007; Bitouk et al., 2010), duration (Yang and Lugger, 2010; Clavel et al., 2008; Lee and Narayanan, 2005; Morrison et al., 2007), and voice quality features (Clavel et al., 2008; Grimm et al., 2007; Bitouk et al., 2010). Other features which have been examined to detect emotions from speech signals are language and discourse information (Lee and Narayanan, 2005; López-Cózar et al., 2011), zero crossing rate (Yang and Lugger, 2010), sub-band energies, voiced–unvoiced ratio (Altun and Polat, 2009), correllogram features (Chandaka et al., 2009), harmony features (Yang and Lugger, 2010), Bark band energy, spectral centroid (Clavel et al., 2008), spectral shape (Shami and Verhelst, 2007), signal power (Nicholson et al., 2000) and LPC (Altun and Polat, 2009; Nicholson et al., 2000). The extracted speech features are either directly used to classify the emotions or used after the optimization using feature selection algorithms.

The identified features are used to classify the emotions with the help of classifiers. Linear discriminant classifier (LDC) (Lee and Narayanan, 2005), k-nearest neighbor (KNN) (Lee and Narayanan, 2005; Shami and Verhelst, 2007; Morrison et al., 2007), artificial neural network (ANN) (Morrison et al., 2007; Nicholson et al., 2000; Albornoz et al., 2011), hidden Markov model (HMM) (Albornoz et al., 2011), Gaussian mixture model (GMM) (Clavel et al., 2008; López-Cózar et al., 2011), fuzzy logic classifier (Grimm et al., 2007), support vector machine (SVM) (Altun and Polat, 2009; Chandaka et al., 2009; Shami and Verhelst, 2007; Morrison et al., 2007), maximal margin robot (MMR) (Altun and Polat, 2009), Bayesian classifier (Yang and Lugger, 2010) and decision trees (Shami and Verhelst, 2007; Morrison et al., 2007) are the classifiers that are reported in the literatures to classify emotions. Normally a single classifier is used in the classification of emotions, and a combination of classifiers (Lee and Narayanan, 2005; Morrison et al., 2007; Albornoz et al., 2011; López-Cózar et al., 2011) with one or two levels of decision level fusion techniques is also used to improve the classification performance.

The performance of emotion classification method is measured by using different metrics. The most widely used metric by researchers is accuracy. Other metrics used in the field are overall recognition rate (Yang and Lugger, 2010; Nicholson et al., 2000), average recall (Schuller et al., 2009; Steidl et al., 2009; López-Cózar et al., 2011), balanced accuracy (Bitouk et al., 2010), class-wise average classification rate (Batliner et al., 2003), precision (Steidl et al., 2009; López-Cózar et al., 2011), receiver operating region (ROC) (Steidl et al., 2009) and classification error (Grimm et al., 2007; Lee and Narayanan, 2005). Overall recognition rate and accuracy are somewhat sensitive to the imbalance in the number of cases in each class. This problem is rectified in balanced accuracy (Bitouk et al., 2010), average recall and class-wise average classification rate. Even though different names are used for the performance measure; the metric overall recognition rate and the metric accuracy represent the same measure. The metric balanced accuracy, average recall and class-wise average classification rate represent the same measure. In emotion classification problem, the emotions are considered as discrete categories by most of the researchers. But a few researchers consider emotions as points in a two or three dimensional continuous space as in Yang and Lugger (2010) and Grimm et al. (2007).

In this paper, we examine the performance of AR parameters and MFCC of different orders with standard classifiers to recognize seven discrete categories of emotions and propose a new class-specific multiple classifiers scheme to improve the recognition accuracy. For the proposed scheme the optimum class-specific classifiers are identified by the metrics recall, precision and F-measure. The performance of the scheme is measured by the metric accuracy. Similarity and difference in the comparison of the proposed class-specific multiple classifiers scheme with some of the multiple classifiers scheme already used in the field are given in Table 1
                     . Lee and Narayanan (2005) and López-Cózar et al. (2011) used fixed features and classifiers with a fusion technology. The number of classifiers depends on the number of different feature sets. Morrison et al. (2007) combined the same feature set with the five best performing classifiers. Albornoz et al. (2011) combined the best performing features and the classifiers for three emotional groups in a two stage hierarchical structure. The novelty of the proposed method is the selection of one best performing feature vector and a classifier for each emotional class. The proposed method makes use of the best class-wise classification performance of the features as well as the classifiers.

The emotional database used in the present work is the publicly available Berlin Emotional Database (Burkhardt et al., 2005). The speech signals of the database have been recorded in an anechoic chamber of the Technical University of Berlin. Five actors and five actresses, in the age group of 21–35 years, have uttered 5 short and 5 long sentences in seven emotions. The minimum and maximum length of the utterances is 1.5s and 4s, respectively. There are 10 speakers and 10 texts in the formation of the database. Anger (A), boredom (B), disgust (D), fear (F), happiness (H), sadness (S) and neutral (N) are the seven basic emotions covered in the database. The recorded utterances have been validated by a perception test with 20 subjects and the utterances with good emotional quality and naturalness have been retained. There are 535 wave files with a sampling rate of 16kHz and a 16 bits encoding.

Speech signal is a non-stationary signal, but the characteristics are assumed to be stationary in a short time interval. The appropriate frame length for speech signal analysis is 10–20ms. Using Hamming window, the speech signals from the database are divided into 20ms frames with an overlapping of 10ms. From each frame the AR parameters are derived by the autocorrelation method (Makhoul, 1975) and stored in a matrix called coefficient matrix. There is one coefficient matrix for each utterance. In each coefficient matrix there is one row for each frame. Each row consists of P LPC, one gain parameter and P reflection coefficients. A feature matrix is formed by column-wise application of the functionals such as mean, median, maximum, minimum, range, interquartile range, standard deviation, skewness and kurtosis to the coefficient matrix. The functionals define the characteristics of the trajectory of each AR parameter. The sub sets of feature matrix are analyzed for their emotion recognition performance. The following are the different feature vectors that are investigated in the present work:
                           
                              i.
                              Each functional of all LPC, gain parameter and all reflection coefficients.

Each functional of all LPC.

Each functional of all reflection coefficients.

Each functional of all LPC and gain parameter.

Each functional of gain parameter and all reflection coefficients.

Each functional of all LPC and all reflection coefficients.

All functionals of each LPC.

All functionals of gain parameter.

All functionals of each reflection coefficient.

The classification performance of the above feature vectors is investigated in three experiments. In the first experiment, the performance of the feature vectors (i to ix) derived from the combined AR parameters, is analyzed with standard classifiers in order to find out the feature vector that maximizes the emotion recognition accuracy. In the second experiment, feature vectors from the LPC only features (ii and vii) are examined to assess the performance of LPC. In the third experiment, the gain and reflection coefficients only feature vectors (iii, v, viii and ix) are analyzed to assess the performance of the gain and reflection coefficients. The second and third experiments are done to compare the LPC features with the gain and reflection coefficient features. In all the three experiments the classification of emotions is done using discriminant, KNN, GMM, ANN and SVM classifiers. The discriminant classifiers are used with five different types of discriminant functions. They are linear (LDC), quadratic (QDC), Mahalanobis (MDC), diagonal linear (DLDC) and diagonal quadratic (DQDC). The KNN classifiers are used with Euclidean distance metric and k with different values ranging from 1 to 15, the GMM with 3 and 6 Gaussian components and the ANN with three hidden layers. The number of neurons in the first hidden layer is made equal to the number of inputs, in the second hidden layer it is made equal to 0.6 times the inputs and in the third hidden layer it is equal to the number of outputs. As the design of back propagation ANN is one's own experience (Haykin, 2001), we empirically designed the ANN by conducting classification experiments with one, two and three hidden layers and by keeping the number of hidden neurons in different proportions (1–2 in step of 0.1) to the number of inputs and the number of outputs. SVM is used in a three-stage hierarchical structure as shown in Fig. 1
                         to recognize the seven emotions. Each SVM classifier is designed with quadratic kernel to map the training data into kernel space and least square method to find the optimum hyperplane. One-against-the remaining all binary classification approach is used in each SVM classifier.

In order to compare the performance of the AR parameters, especially the AR reflection coefficients with a standard feature, we also analyze 3–24 band MFCC features. The MFCC are extracted from 20ms Hamming windowed speech signals with a window shift of 10ms using the software package Praat (Boersma and Weenink, 2009). MFCC feature matrix is formed in the same way as the AR parameters by applying the nine functionals. The classification performance of each functional of all MFCC and all the functionals of each MFCC are analyzed to find the feature of MFCC that maximizes the emotion recognition accuracy.

In order to ensure the stability and the statistical significance of the results, the classification is performed by m-fold cross-validation methods. In m-fold cross-validation, the database is divided into m mutually exclusive subsets. The classifier is trained and tested m times, each time one set acts as the testing set and the remaining m
                        −1 sets act as the training set. The paper examines the speaker-independent, text-independent and speaker and text dependent classifications by leave-one-speaker-out (LOSO), leave-one-text-out (LOTO) and leave-one-out (LOO) cross-validation schemes.

In LOSO cross-validation, utterances spoken by one speaker is used as the testing set and the remaining utterances as the training set. As the utterances of the database have been spoken by 10 speakers, this is equivalent to 10-fold cross-validation and the classification is speaker-independent. In LOTO cross-validation, the utterances of a same text are used as the testing set and the remaining texts as the training set. The number of texts in the database is 10; therefore, this is also equivalent to 10-fold cross-validation and the classification experiment becomes text-independent. In LOO cross-validation, classification is done by keeping one utterance of the database as the testing sample and the remaining utterances as the training set. This is repeated for each utterance of the database. This is equivalent to 535-fold cross-validation for the Berlin database. LOO is neither speaker-independent nor text-independent.

The proposed class-specific multiple classifiers scheme to recognize seven emotions from speech signals is shown in Fig. 2
                        . The classifiers of the proposed scheme are identified from a pool of features and a pool of classifiers by their optimum performance to a specific emotional class. The outputs of the classifiers are combined by a fusion technique to make a final decision on the detected emotions. The fusion technique utilized in this work is the method proposed by Kuncheva et al. (2001). It is a class-conscious fusion method which combines mutually independent classifiers. The output of each classifier is denoted by the degree of support for each class. The supports from all the classifiers are combined by a class-conscious fusion operator to predict a class. We use the method with a slight modification in the probability estimation. In this method for a C emotional class classification, for each classifier R
                        
                           k
                        , k
                        =1, 2…C, a C
                        ×
                        C confusion matrix is assigned, based on its classification performance with the training set. The (i,j)th element, CM
                           k
                        (i,j) of the confusion matrix CM
                           k
                        , is the number of input patterns that actually belong to emotional class i and detected by the classifier R
                        
                           k
                         as emotional class j. Using the confusion matrix, each classifier is assigned a label matrix LM
                           k
                        . The (i,j)th element, LM
                           k
                        (i,j) of a label matrix of a classifier, is the estimate of the probability of the classifier to detect the emotional class as j, given the actual emotional class of the input pattern X is i.
                           
                              (3)
                              
                                 
                                    
                                       LM
                                       k
                                    
                                    (
                                    i
                                    ,
                                    j
                                    )
                                    =
                                    
                                       P
                                       ˆ
                                    
                                    (
                                    
                                       R
                                       k
                                    
                                    (
                                    X
                                    )
                                    =
                                    j
                                    |
                                    i
                                    )
                                    =
                                    
                                       
                                          
                                             CM
                                             k
                                          
                                          (
                                          i
                                          ,
                                          j
                                          )
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             C
                                          
                                          
                                             
                                                CM
                                                k
                                             
                                             (
                                             i
                                             ,
                                             j
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Corresponding to each input pattern, the output of each classifier will be any one emotional class j
                        ∈{1, 2, …, C}. The output emotional class of each classifier is associated with a soft label column vector LM
                           k
                        (:,j), which is the jth column of the label matrix. The soft label vectors of all the C classifiers are arranged to form a C
                        ×
                        C decision profile matrix DP(X)=[LM1(:,j) LM2(:,j) … LM
                           C
                        (:,j)], j
                        ∈{1, 2, …, C}. The kth column of the decision profile matrix is the soft label vector of the kth classifier. The emotional class i of the input pattern is estimated by applying an aggregation rule (Ψ) to the decision profile matrix.
                           
                              (4)
                              
                                 
                                    i
                                    =
                                    
                                       
                                          arg
                                           
                                          max
                                       
                                       i
                                    
                                    Ψ
                                    [
                                    D
                                    P
                                    (
                                    X
                                    )
                                    ]
                                 
                              
                           
                        
                     

Row-wise application of the aggregation rule to the decision profile matrix produces an emotional score vector. The emotional class, which has the maximum score, is detected as the output emotion i. The multiplication of probability (MP) and average of probability (AP) are used as the aggregation rule in the present work. The defining equations for emotion prediction by MP and AP are given by Eqs. (5) and (6), respectively.
                           
                              (5)
                              
                                 
                                    i
                                    =
                                    
                                       
                                          arg
                                           
                                          max
                                       
                                       i
                                    
                                    
                                       ∏
                                       
                                          j
                                          =
                                          1
                                       
                                       C
                                    
                                    
                                       D
                                       P
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 
                                    i
                                    =
                                    
                                       
                                          arg
                                           
                                          max
                                       
                                       i
                                    
                                    
                                       1
                                       C
                                    
                                    
                                       ∑
                                       
                                          j
                                          =
                                          1
                                       
                                       C
                                    
                                    
                                       D
                                       P
                                       (
                                       i
                                       ,
                                       j
                                       )
                                    
                                 
                              
                           
                        We also investigate the proposed method by implementing the fusion algorithm by the simple unweighted vote (UV). In this case, the predicted class of each classifier is directly used to recognize the emotion of the input speech signal.

The class-specific feature and the class-specific classifier for each emotional class are identified from a pool of AR features and a pool of standard classifiers. The performance of the proposed scheme depends on the performance of the class-specific classifiers. The classifier identified for a specific emotion, to be optimum, should classify the emotion with high true positives, low false negatives and low false positives. To measure the optimum performance for a specific emotion, we use three metrics, they are recall, precision and F-measure. The defining equations of recall, precision and F-measure are given below.
                           
                              (7)
                              
                                 
                                    recall
                                     
                                    (
                                    Rl
                                    )
                                    =
                                    
                                       
                                          #
                                          true
                                          _
                                          positives
                                       
                                       
                                          #
                                          true
                                          _
                                          positives
                                          +
                                          #
                                          false
                                          _
                                          negatives
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (8)
                              
                                 
                                    precision
                                     
                                    (
                                    Pn
                                    )
                                    =
                                    
                                       
                                          #
                                          true
                                          _
                                          positives
                                       
                                       
                                          #
                                          true
                                          _
                                          positives
                                          +
                                          #
                                          false
                                          _
                                          positives
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (9)
                              
                                 
                                    F
                                    -measure
                                    =
                                    (
                                    1
                                    +
                                    
                                       β
                                       2
                                    
                                    )
                                    
                                       
                                          P
                                          n
                                          ×
                                          R
                                          l
                                       
                                       
                                          
                                             β
                                             2
                                          
                                          P
                                          n
                                          +
                                          R
                                          l
                                       
                                    
                                 
                              
                           
                        
                     

When recall is used to identify the feature and the classifier, it ensures high true positives and low false negatives but fails to consider false positives. The use of precision as the selection metric may ensure high true positives and low false positives but does not take into account the false negatives. The metric F-measure can maximize the emotion-specific performance by incorporating all the three factors. In the present work the F-measure is calculated using β values from 0.25 to 1.75 in step of 0.25. The β value determines the weight of precision and recall in the calculation of F-measure. If β
                        =1, the F-measure is calculated by giving equal weight to precision and recall. If β
                        >1, more weight is given to recall and more weight to precision if β
                        <1.

In the design phase of the proposed scheme, each of the classifier from the pool of classifiers is trained with each of the feature from the pool of features. The features and classifiers that maximize the selection metric for each of the emotional class are used to build the scheme. Each of the identified emotion-specific classifier is assigned a label matrix as defined in Eq. (3), based on its classification performance with the training set.

In the classification phase, the scheme is provided with the testing emotional speech signal; the features for each of the emotional class are extracted and applied to the corresponding classifier. The classifiers predict the emotional class of the speech signal. Based on the predicted class and the label matrix of each classifier, the fusion algorithm extracts the soft label vector for each classifier and forms a decision profile matrix as explained in Section 4.1. The aggregation rule is applied to the elements in each row of the decision profile matrix to get an emotional score for each of the emotions. The emotion with the maximum emotional score is declared as the emotion of the input speech signal. In the case of UV fusion, the emotional class i is assigned to the input speech signal that is supported by the majority of the class-specific classifiers.

@&#RESULTS AND DISCUSSION@&#

The maximum accuracies obtained in the classification of emotions by different feature sets of the AR parameters with standard classifiers are shown in Table 2
                        . The experimental results show that the gain and reflection coefficients recognize emotions better than the traditional LPC. When the maximum accuracies in the LPC only features experiments are considered as the baselines, the gain and reflection coefficients register a performance improvement of 9.1% in LOSO, 16.9% in LOTO and 14.8% in LOO experiments. Therefore, in the field of emotion recognition from speech signals, the AR reflection coefficients can be considered as a potential feature. The AR reflection coefficients which have relation to the cross sectional area of the different sections of vocal tract carry more emotional information than LPC. The AR reflection coefficients are capable of classifying emotions with good accuracy in LOTO and LOO paradigms. This indicates the text independence of the AR reflection coefficients to recognize emotions. Conversely, the speaker dependence of the AR parameters is explicitly shown by the low recognition accuracy in LOSO paradigm. In general, the classification analysis of AR parameters with standard classifiers shows that maximum performances are associated with higher order AR parameters and the functionals mean and median carry more emotional information than the other functionals.

The emotion classification performance of MFCC features with standard classifiers is shown in Table 3
                        . Comparison of the performance of AR features in Table 2 with the MFCC features in Table 3 shows that the AR reflection coefficients recognize emotions slightly better than MFCC features in text-independent experiment whereas in speaker-independent and in speaker and text-dependent experiments MFCC features perform better than AR reflection coefficients.

The performance of the emotion-specific multiple classifiers scheme with combined AR feature sets, LPC only feature sets and gain and reflection coefficients only feature sets under the three cross-validation paradigms is shown in Table 4
                        . The first column of Table 4 shows the different metrics that are optimized to identify the emotion-specific classifiers in order to build the proposed scheme. In order to study the performance of the proposed scheme with AR parameters, we consider the maximum accuracies obtained in the classification of the different feature sets of AR parameters with standard classifiers (Table 2) as the baselines.

When the proposed scheme is built and tested by identifying emotion-specific classifiers from the combined AR feature sets; in speaker-independent classification the maximum accuracy of 55.5% is obtained with the selection of the emotion-specific classifiers based on maximum F-measure with β
                        =1.75 and the fusion aggregation rule of unweighted vote. The classification accuracy improvement over the standard classifier classification is 9%. The text-independent classification by the proposed scheme classifies emotions with a maximum accuracy of 77.2% when the emotion-specific classifiers are selected by maximizing F-measure with β
                        =1.75 and achieves a performance improvement of 2.2% over the standard classifier classification. The classification experiment by LOO paradigm classifies emotions with a maximum accuracy of 76.8% and registers 3.9% improvement of classification accuracy with respect to the corresponding single classifier classification. The LOO scheme is neither speaker-independent nor text-independent but considered as a complete cross-validation scheme. When the scheme is designed and tested with LPC only features, maximum classification performance improvements of 8.2%, 6.2% and 8.4% are obtained in LOSO, LOTO and LOO experiments, respectively, as shown in Table 4. The maximum performance improvements over the standard classifier are 10.9%, 1.6% and 3.5% in LOSO, LOTO and LOO experiments, respectively, when the scheme is built and tested by the AR gain and reflection coefficients only features.

The performance of the class-specific multiple classifiers scheme depends on the performance of the class-specific classifiers, which in turn depends on the features in the pool of features and the classifiers in the pool of classifiers. If more features are included in the pool of features, the performance of the proposed scheme will improve. We show the performance improvement by building and testing the proposed scheme by including the MFCC features with the combined AR features in the pool of features. The performance of the proposed scheme with AR+MFCC features is shown in Table 5
                           . The maximum classification performance increases to 57.8% in LOSO, 83.7% in LOTO and 80.2% in LOO experiments. The class-specific features and classifiers identified by the maximum F-measure with β
                           =1.75, which classify the emotions with an accuracy of 83.7% in LOTO experiment, are shown in Table 6
                           . The corresponding confusion matrix of the proposed scheme is shown in Table 7
                           . The performance of the proposed scheme with MFCC only features is shown in Table 8
                           . Comparison of maximum accuracies in AR+MFCC features and MFCC only features shows that AR+MFCC features register an accuracy improvement of 4.2% and 4.8% in LOSO and LOTO, respectively, over MFCC only features. But no significant improvement is obtained in the LOO experiment. In the standard classifiers analysis and in the design of the proposed scheme, the GMM and the ANN classifiers performed below the SVM, discriminant and the KNN classifiers. It could be due to the imbalanced data set and the correlation among the data points.

In the fusion algorithm of the proposed scheme, we assigned the probability estimation 
                              
                                 
                                    P
                                    ˆ
                                 
                                 (
                                 
                                    R
                                    k
                                 
                                 (
                                 X
                                 )
                                 =
                                 j
                                 |
                                 i
                                 )
                              
                            to the emotion-specific classifiers instead of the probability estimation 
                              
                                 
                                    P
                                    ˆ
                                 
                                 (
                                 i
                                 |
                                 
                                    R
                                    k
                                 
                                 (
                                 X
                                 )
                                 =
                                 j
                                 )
                              
                           . First, we compare the performance of the proposed scheme with the two different probability estimations. The comparison is done by designing the scheme with AR+MFCC feature sets. Table 9
                            shows the performance of the proposed scheme with the probability 
                              
                                 
                                    P
                                    ˆ
                                 
                                 (
                                 i
                                 |
                                 
                                    R
                                    k
                                 
                                 (
                                 X
                                 )
                                 =
                                 j
                                 )
                              
                            and the performance of the proposed scheme with the probability 
                              
                                 
                                    P
                                    ˆ
                                 
                                 (
                                 
                                    R
                                    k
                                 
                                 (
                                 X
                                 )
                                 =
                                 j
                                 |
                                 i
                                 )
                              
                            is shown in Table 5. It is clear from Tables 5 and 9 that the maximum accuracy of the proposed scheme with the probability 
                              
                                 
                                    P
                                    ˆ
                                 
                                 (
                                 
                                    R
                                    k
                                 
                                 (
                                 X
                                 )
                                 =
                                 j
                                 |
                                 i
                                 )
                              
                            shows an improvement of 7.1% in LOSO and 1.6% in LOTO experiments over maximum accuracy of the scheme with the probability 
                              
                                 
                                    P
                                    ˆ
                                 
                                 (
                                 i
                                 |
                                 
                                    R
                                    k
                                 
                                 (
                                 X
                                 )
                                 =
                                 j
                                 )
                              
                           . But in the LOO experiment, the proposed scheme produces the same maximum accuracy with both the probability estimations. In order to test the significance of the results, we performed the left-tailed z-test that rejected the null-hypothesis at the 5% significance level against the alternate hypothesis of the mean of accuracies of experiments with the probability 
                              
                                 
                                    P
                                    ˆ
                                 
                                 (
                                 i
                                 |
                                 
                                    R
                                    k
                                 
                                 (
                                 X
                                 )
                                 =
                                 j
                                 )
                              
                            is less than the hypothesized mean in LOSO and LOO experiments. But the null-hypothesis was not rejected in the case of LOTO experiment.

Second, we compare the performance of the proposed scheme with the classifiers aggregation algorithms Bagging and AdaBoost.M1. The Bagging and AdaBoost.M1 algorithms are implemented as given in Kuncheva (2004) with 7, 25 and 50 classifiers. Bagging is implemented with median of gain and all reflection coefficients of AR order 10 as the feature and SVM as the classifier in LOSO experiment, median of all reflection coefficients of AR order 25 as feature and KNN with k
                           =6 as the classifier in LOTO experiment and median of all reflection coefficients of AR order 18 as feature and KNN with k
                           =6 as the classifier in LOO experiment. AdaBoost.M1 is implemented with the same features and the entire pool of classifiers. The performance of Bagging and AdaBoost.M1 algorithms are shown in Table 10
                            and it shows that the performance of the proposed scheme is much better than Bagging and AdaBoost.M1. The Bagging and AdaBoost.M1 algorithms fail to provide performance improvement even over the single classifier classifications. Bagging is not a preferred algorithm to build with stable classifier like KNN; therefore, we tested a 7 classifier Bagging with ANN and median of all reflection coefficients of AR order 25. But it classified emotions with an accuracy of 61.1%, which is much below the performance of building Bagging with KNN classifier.

Finally, we compare the performance of the proposed scheme with that of the standard classifiers classification of simple brute-force fusion of LPC only, G and K only, and MFCC only features. The classification analyses in LOSO, LOTO and LOO experiments are done by combining the corresponding LPC, G and K, and MFCC features as shown in Tables 2 and 3 and with all the standard classifiers mentioned in Section 3.2. The maximum performance in LOSO is 50.3% by KNN classifier with k
                           =12, in LOTO it is 73.1% by LDC and in LOO it is 74.8% by LDC. The maximum performance of the brute-force fusion of all the features is much below the maximum performance of the proposed scheme shown in Table 5.

The performance of the proposed scheme reports high accuracy when emotion-specific classifiers are selected by maximum F-measure and low accuracy by maximum precision. This is because the automatic detection of the maximum value of precision does not guarantee the selection of a best classifier unlike recall. Precision also becomes high for poor classifiers with low true positives and low false positives. For a good classifier, recall and precision should be high and hence the F-measure. The classification accuracy of the proposed method depends on the performance of the emotion-specific classifiers. The performance can further be improved by the features and classifiers selected by maximum F-measure, if the automatic detection of maximum value of precision is shielded from selecting poor classifiers.

In this paper, we have investigated the speech emotion recognition performance of AR parameters, which include the LPC, the gain parameter and the reflection coefficients of different orders. The performance analysis of the AR parameters with standard classifiers shows that AR reflection coefficients classify emotions better than LPC. The AR reflection coefficients also perform slightly better than MFCC in text-independent emotion classification. We have also proposed a class-specific multiple classifiers scheme to improve the classification performance. In the proposed scheme the classification is done in two levels. In the first level, features and classifiers, specific to each emotion is employed and in the second level, the predictions of the first level classifiers are combined using multiplication of probability, average of probability and unweighted vote to take a final decision on the detected emotion. The performance of the proposed scheme has been investigated using AR parameters and MFCC features. The class-specific multiple classifiers scheme improves the classification accuracy over the standard classifier classifications. The novelties of this paper are the investigation of gain parameter and reflection coefficients of AR model of different orders and the selection of an optimum feature and classifier for each emotional class to build the class-specific multiple classifiers scheme.

Our future work includes improving the performance of the proposed scheme by including more features in the pool of features and including more classifiers like HMM in the pool of classifiers.

@&#ACKNOWLEDGEMENT@&#

The authors sincerely thank the anonymous reviewers whose valuable comments have greatly improved the quality of this paper.

@&#REFERENCES@&#

