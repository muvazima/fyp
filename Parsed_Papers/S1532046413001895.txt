@&#MAIN-TITLE@&#Development and evaluation of RapTAT: A machine learning system for concept mapping of phrases from medical narratives

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Annotation can identify critical information within clinical notes.


                        
                        
                           
                           A key annotation step is mapping phrases to concepts of interest.


                        
                        
                           
                           We assess the impact of including token order as a feature on mapping performance.


                        
                        
                           
                           Including token order improves precision and recall over a bag-of-words classifier.


                        
                        
                           
                           The algorithm is scalable and should support near-real-time, automated annotation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

NLP

SNOMED-CT

RapTAT

UMLS

SVM

MCVS

CSV

TP

FP

FN

IQV

Opt

Perf

Natural language processing

Bayesian prediction

Machine learning

Systematized nomenclature of medicine

@&#ABSTRACT@&#


               
               
                  Rapid, automated determination of the mapping of free text phrases to pre-defined concepts could assist in the annotation of clinical notes and increase the speed of natural language processing systems. The aim of this study was to design and evaluate a token-order-specific naïve Bayes-based machine learning system (RapTAT) to predict associations between phrases and concepts. Performance was assessed using a reference standard generated from 2860 VA discharge summaries containing 567,520 phrases that had been mapped to 12,056 distinct Systematized Nomenclature of Medicine – Clinical Terms (SNOMED CT) concepts by the MCVS natural language processing system. It was also assessed on the manually annotated, 2010 i2b2 challenge data. Performance was established with regard to precision, recall, and F-measure for each of the concepts within the VA documents using bootstrapping. Within that corpus, concepts identified by MCVS were broadly distributed throughout SNOMED CT, and the token-order-specific language model achieved better performance based on precision, recall, and F-measure (0.95±0.15, 0.96±0.16, and 0.95±0.16, respectively; mean±SD) than the bag-of-words based, naïve Bayes model (0.64±0.45, 0.61±0.46, and 0.60±0.45, respectively) that has previously been used for concept mapping. Precision, recall, and F-measure on the i2b2 test set were 92.9%, 85.9%, and 89.2% respectively, using the token-order-specific model. RapTAT required just 7.2ms to map all phrases within a single discharge summary, and mapping rate did not decrease as the number of processed documents increased. The high performance attained by the tool in terms of both accuracy and speed was encouraging, and the mapping rate should be sufficient to support near-real-time, interactive annotation of medical narratives. These results demonstrate the feasibility of rapidly and accurately mapping phrases to a wide range of medical concepts based on a token-order-specific naïve Bayes model and machine learning.
               
            

@&#INTRODUCTION@&#

Clinical free text contains a wealth of information that can improve general medical knowledge and patient specific care. For example, it can provide evidence regarding the sources or epidemiologic course of a disease outbreak or delineate patient variables that predict an altered response to a medical intervention [1]. However, the unstructured nature of free text can impede the identification of useful information. Methods to convert unstructured free text into structured data can help the extraction of evidence from medical narratives. Manual annotation, in which a user scans a document and identifies key phrases of text conceptually related to a particular subject of interest, is one way to accomplish this. However, producing high quality annotations may require the extensive labor of multiple, experienced reviewers with sufficient understanding of the domain [2,3]. Furthermore, there can be both intra- and inter-annotator variation in the textual phrases identified during annotation and the concepts associated with those phrases. Even though manual annotations are commonly used as the reference standard when evaluating automated systems, the quality of annotations may vary among reviewers and from one corpus to the next [4].

Natural language processing (NLP) can reduce the burden of manual annotation and may generate more consistent and comprehensive indices of the text [5,6]. However, adapting an existing NLP system to a new task often requires developing novel NLP applications or iterative modification of the existing algorithms to match the document types under review and the environment [7–10]. In addition, even automated systems can require substantial time to classify multiple, complex concepts in large document sets. Such temporal demands could hinder concept identifications if required to be executed in near-real time for prospective clinical applications.

The objective of the present study was to evaluate whether machine learning using a token-order-specific, naïve Bayes classifier could serve as a basis for one aspect of the annotation process, namely mapping of phrases within the free text to coded concepts. Basing the system on efficient, probability-based data structures and algorithms might provide performance advantages over rule-based NLP systems and support near-real-time analysis [11]. In addition, because machine learning provides a methodical procedure for system optimization, such an approach might obviate the need for failure analyses and recoding of software when applying an NLP system to a new task. An optimized system such as this would have a number of potential uses. For one, it could reduce the annotation burden associated with manual indexing of free-text documents by learning and then automatically mapping phrases of text tagged by a reviewer to concepts of interest. By setting the system to map textual phrases to a recognized set of terminologies such as those contained within the Unified Medical Language System (UMLS), clinical providers could rapidly obtain relevant information by simply selecting phrases of interest within a clinical note.

We set out to build an NLP application that could adaptively learn and optimally replicate reference standard performance and demonstrate the accuracy and operational efficiency of the system. We hypothesized that machine learning based on a token-order-specific, naïve Bayes classifier could be used to create a system capable of accurately and efficiently mapping phrases within free text to medical concepts. Herein, we describe the initial development and evaluation of such a system, the Rapid Text Annotation Tool (RapTAT).

@&#BACKGROUND@&#

Multiple studies have established the potential efficacy of NLP for extracting medically relevant information from clinical narratives. Early examples include systems that parse radiograph reports and identify important clinical findings, such as patients at risk of tuberculosis [12–14]. More recent examples include the analyses of discharge summaries to assign risk for development of community-acquired pneumonia [15], automated assessment of quality of care [16,17], identification of patients in need of colon cancer screening [18], description and classification of angiography reports [19], recognition of cases of influenza [1], detection of infectious symptoms [20], detection of post-operative complications [21], and discovery of positive bacterial cultures likely resulting from contamination [22].

Current NLP systems generally require several distinct steps to convert unstructured, free text into structured data. Key steps include detecting document sections and sentence boundaries, tokenizing sentences into separate words and other atomic units such as dosages, tagging tokens with their appropriate parts-of-speech, and “chunking” token sequences into groups to form grammatical phrases. NLP systems also often map identified phrases of text to an identifier representing a common concept, a process referred to as concept recognition, normalization, or grounding [23–25]. The schema may consist of an existing lexicon or ontology such as the Systematized Nomenclature of Medicine – Clinical Terms (SNOMED CT) or a user-defined set of concepts of interest, such as terms related to a particular disease or treatment. Multiple NLP systems have been developed that are capable of mapping clinical free-text to concepts [26]; examples include the Mayo Clinic Autocoder [27], the SNOMED Categorizer (SNOCat) [28], cTAKES [29], IndexFinder [30], KnowledgeMap [31], MedLEE [32], HITex [33], MedEx [34], MetaMap [35], Metaphrase [36], MicroMeSH [37], MTERMS [38], PhraseX [39], SAPHIRE [5], and SENSE [40]. These tools map phrases into a variety of user-defined lexicons, terminologies, and ontologies.

Routine use of NLP in clinical care settings is still rare despite numerous studies demonstrating its potential value [41]. Reasons for this may include the lack of generalizability of NLP tools and study results [41,42]. For example, while systems such as the Unified Medical Language System (UMLS) provide access to an extensive array of terms and concepts [43], some annotation tasks require mapping to concepts not readily expressed within a recognized vocabulary or ontology. Furthermore, the number of potential biomedical concepts and the free text phrases used to express those concepts is unlimited and dynamic, so users must adapt mapping systems to changes in biomedical domains and language use. The present study defines the feasibility of creating a machine-learning tool that can systematically generate an adaptable phrase-to-concept mapping system optimized for the domain being investigated and the concepts of interest.

Multiple computational models exist to support machine learning, including support vector machines (SVMs), logistic regression, neural networks, and Bayesian networks among others. All of these models use a set of features to make a prediction regarding an outcome. In this study, we were interested in predicting the likelihood of a particular concept mapping given a set of features consisting of a sequence of words that form a textual phrase. We used one type of Bayesian network, the naïve Bayes classifier, as our machine learning method. Many of the concept recognition tools described above have relied on matching of text strings to descriptions within an existing database of concepts [5,30–32,35–37,39,40]. A number of rules hand-coded within these systems were then applied to features within the text to select from among the set of matched concepts. One limitation of these approaches is that specific rules and identified features helpful in classifying text in one domain may not be generalizable to other domains. Another is that string matching within large databases can slow analyses and prevent the use of NLP systems for real-time analyses [44].

Probability-based, machine learning methods may provide a way to generate phrase-to-concept mappers tailored to the domain of interest. To the best of our knowledge, only two tools have been described that use probabilistic rather than a largely rule-based approach to map free text to medical concepts [28,45]. The SNOCat tool combines regular expression searches and a vector space model based on term frequency and inverse document frequency to label free text with SNOMED CT concepts [28]. The Autocoder tool uses a database of previous classifications together with a naïve Bayes classifier for medical concept recognition; it maps lists of clinical diagnoses to codes within an ICD-8 based coding system [45]. The implementation treats text as a “bag-of-words” with respect to token frequency and disregards token position. Such an approach could remove important classification information and reduce accuracy. Naïve Bayes classifiers rely on conditional probability distributions of tokens given a particular classification, and those distributions may be position dependent. We therefore included not just the tokens themselves but also their positions within a phrase of text as part of the machine learning process.

An advantage to using the token-order-specific, naïve Bayes model for mapping is that there are only modest computational demands relative to other machine learning methods. Like the bag-of-words naïve Bayes classifier, the token-order-specific naïve Bayes classifier is based on the simplifying assumption that the tokens in a phrase of text are conditionally independent. In other words, if a phrase is used to express particular concept, the presence of a particular token in the phrase does not alter the probability of any other token occurring in the phrase. Given that the tokens are conditionally independent, a system trying to map phrases of text to concepts based on tokens alone does not need to store joint probabilities reflecting the likelihood of two or more tokens occurring in the same phrase. Storing such probabilities would increase spatial demands and could become intractable for corpora with large vocabularies. Under the assumption of independence, the system only needs to store the probabilities of individual tokens in the training corpus occurring in phrases mapped to a particular concept. This assumption thus reduces the spatial requirements for calculating the most likely concept associated with a given phrase. The spatial efficiency afforded by this approach should also improve temporal efficiency. Probabilities can be stored in rapidly accessible, computer memory, potentially capable of supporting real-time concept mapping.

In the present report, we evaluated the impact of adding token order as a feature to a naïve Bayes classifier. Specifically, we determined the impact of this feature on the ability of a machine learning system to accurately reproduce the phrase-to-concept mappings of an existing NLP system from discharge summaries to SNOMED concepts. We also compared the performance of this token-order-specific classifier relative to a bag-of-words-based, naïve Bayes classifier, and the impact of phrase variability and concept ambiguity on performance was defined. Furthermore, we evaluated the accuracy and temporal efficiency of this implementation relative to a more basic system that maps phrases of text to concept-based string look-ups within a disk-based database. Finally, we employed the dataset from the 2010 i2b2 challenge to evaluate the ability of the system to map manually annotated phrases within clinical notes to user-defined concepts.

@&#METHODS@&#

The main document corpus, subsequently referred to as the VA data set, was a random sample of 2860 discharge summaries collected between fiscal years 1999 and 2006 within the Tennessee Valley Healthcare System (TVHS) VA Hospital. The TVHS institutional review board and research and development committee approved the study and granted a waiver regarding the need to obtain informed consent and HIPAA authorization before using patient data. The document corpus had been previously annotated using the Multi-threaded Clinical Vocabulary Server (MCVS) NLP tool to identify noun, verb, adjective, and prepositional phrases and map them to concepts within SNOMED CT [21]. For the named entity recognition part of the task, MCVS gives preference to concepts within SNOMED CT that contain a greater number of content terms (i.e., non-stop words). The method uses word normalization, word and term level synonymy and a word order independent method for concept recognition. Phrase identification is based on a set of heuristics that use concept type as a method to perform a set of rule based combinatorial algorithms. The technique is a backward and forward chaining algorithm and takes into account the assertion value of the concept. An earlier report using a predecessor of the MCVS tool demonstrated that, after accounting for missing synonyms within SNOMED CT, its sensitivity and specificity were 99.7% and 97.9%, respectively, for the mapping of entries within a clinical problem list to the ontology [46]. In a study examining the ability of MCVS to detect symptoms related to tuberculosis, acute hepatitis, and influenza within VA clinical notes, precision and recall over all symptoms evaluated were 0.91 and 0.84, respectively [20].

The MCVS tool was responsible for all pre-processing of the free text, including document parsing, sentence splitting, tokenization, and identification of phrases, which were then mapped to SNOMED CT concepts by MCVS. For the purposes of this study, we defined a phrase as an ordered sequence of tokens formulated by the author of a medical note to express a concept. Tokens were generally words but also included other elements such as numbers, units of measurements, and dosages [46,47]. The MCVS-processed data were provided to RapTAT as an idealized set of phrase-to-SNOMED CT concept mappings for tool development and testing. The aims for this part of the study were to evaluate the ability of RapTAT to learn to reproduce the MCVS mappings and to determine the factors that can affect tool performance.

All sequences were limited to a maximum of 7 tokens, the maximum phrase length identified by MCVS. All token characters were converted to lower case for training and evaluation. There were 567,520 phrases (22,994 unique) within the document corpus, and each phrase was mapped to one of 12,056 unique concepts by the MCVS tool. These annotated documents provided a working environment for training and evaluation of RapTAT, and the phrase-to-SNOMED CT concept mappings generated by the MCVS tool served as the reference standard for the purposes of this study. The data were stored in comma-separated value (CSV) files with each row containing a single phrase and the associated MCVS-mapped concept.

The study also used the data available from the 2010 i2b2 challenge for evaluating the performance of the RapTAT tool with regard to its ability to map manually annotated phrases to concepts within a defined schema [48]. The annotated corpus consisted of discharge summaries and progress notes from 3 institutions, University of Pittsburgh Medical Center, Beth Israel Deaconess Medical Center, and Partners Healthcare. The schema contained three concepts (problem, test, and treatment), and all annotated phrases were mapped to one of those three. The training corpus contained 170 documents, and 16,526 phrases within the corpus were manually annotated and mapped to one of the schema concepts. The test corpus contained 256 documents, and the i2b2 reviewers had mapped 31,162 phrases from that corpus to the schema concepts. RapTAT performance on the test set was evaluated with respect to how closely its mapping of phrases to one of the three i2b2 concepts matched those determined by the i2b2 organization. The assertion and relation classifications of the phrases, which were provided with the i2b2 training and test data, were not used in our study.

We used the Java programming language to generate both bag-of-words based and token-order-specific, naïve Bayes classifiers for mapping free-text phrases to SNOMED CT concepts within the RapTAT application. The system first imported the MCVS-determined phrase-to-concept mappings to establish both prior and conditional probabilities, which were then used to identify the most likely phrase-to-concept mapping within the test data. Prior probabilities for the classifier were determined based on the frequency of concept occurrences within the training data. Conditional probabilities were calculated based on the likelihood of a given token occurring within a phrase given a particular concept. In the case of the token-order-specific implementation, the RapTAT tool generated a separate conditional probability table for each of the 7 potential token positions within a phrase during training. For the bag-of-words, tokens from all positions within a phrase were used to generate a single distribution.

By treating the probability of a particular token as independent of the occurrence of all other tokens in a phrase, we were able to use the naïve Bayes equation to generate the likelihood estimate, P, of the tokens mapping to a given concept, Ci
                         
                        [49]. The form of that equation is
                           
                              (1)
                              
                                 P
                                 (
                                 
                                    
                                       C
                                    
                                    
                                       i
                                    
                                 
                                 |
                                 T
                                 )
                                 =
                                 
                                    
                                       P
                                       (
                                       
                                          
                                             C
                                          
                                          
                                             i
                                          
                                       
                                       )
                                       ·
                                       P
                                       (
                                       T
                                       |
                                       
                                          
                                             C
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       P
                                       (
                                       T
                                       )
                                    
                                 
                              
                           
                        where 
                           T
                         represents the vector of tokens that make up the phrase. Using this equation reduces the task of the system to identifying the particular concept, Ci
                        , which maximizes the right side of the equation. For a given phrase, the denominator, 
                           
                              P
                              (
                              T
                              )
                           
                        , is constant, so that only the numerator needs to be considered when determining the most likely token sequence to concept mapping. For the bag-of-words classifier,
                           
                              (2)
                              
                                 P
                                 (
                                 T
                                 |
                                 
                                    
                                       C
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ∏
                                       
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          n
                                       
                                    
                                 
                                 P
                                 
                                    
                                       (
                                       
                                          
                                             T
                                          
                                          
                                             ij
                                          
                                       
                                       )
                                    
                                    
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                    
                                 
                                 ·
                                 
                                    
                                       [
                                       1
                                       -
                                       P
                                       (
                                       
                                          
                                             T
                                          
                                          
                                             ij
                                          
                                       
                                       )
                                       ]
                                    
                                    
                                       
                                          
                                             x
                                          
                                          
                                             j
                                          
                                       
                                    
                                 
                              
                           
                        where n is the number of unique tokens over all phrases, and xj
                         is one if token Tj
                         occurs in the phrase and zero otherwise [50]. The estimate of P(Tij
                        ) is given by
                           
                              (3)
                              
                                 P
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       ij
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       Occurrences of Token
                                       
                                       
                                          
                                             T
                                          
                                          
                                             j
                                          
                                       
                                       
                                       within Sequence Mapping to
                                       
                                       
                                          
                                             C
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       Occurrences of
                                       
                                       
                                          
                                             C
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The token-order-specific implementation corresponds to a multinomial naïve Bayes model in which positions within a mapped token sequence represent features, and the tokens represent the assigned values of the features. For that model, the conditional probability of the phrase 
                           T
                         of length m is
                           
                              (4)
                              
                                 P
                                 (
                                 T
                                 |
                                 
                                    
                                       C
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 P
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       ij
                                       1
                                    
                                 
                                 )
                                 ·
                                 P
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       ik
                                       2
                                    
                                 
                                 )
                                 ⋯
                                 P
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       ilm
                                    
                                 
                                 )
                              
                           
                        where P(Tijk
                        ) is estimated as
                           
                              (5)
                              
                                 P
                                 (
                                 
                                    
                                       T
                                    
                                    
                                       ijk
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       Occurrences of Token
                                       
                                       
                                          
                                             T
                                          
                                          
                                             j
                                          
                                       
                                       
                                       at Position
                                       
                                       k
                                       
                                       when Sequence Maps to
                                       
                                       
                                          
                                             C
                                          
                                          
                                             i
                                          
                                       
                                    
                                    
                                       Occurrences of
                                       
                                       
                                          
                                             C
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                              
                           
                        One difficulty with using Bayes equation is that conditional probabilities can be zero for rare tokens absent from the training data. We therefore used Laplace smoothing to adjust all probabilities [51].

Hash tables stored the number of times each token was associated with a concept within the training data. In the case of the token-order-specific implementation, there was a separate hash table for each of the 7 potential positions of tokens within a phrase. For example, if the phrase “acute myocardial ischemia” occurred in the test data, RapTAT would use “acute” as a key for the hash table corresponding to the first position in a token phrase. The key would return a set containing all concepts for which acute was the first word in a phrase mapping to one of the concepts. Within the set, each concept would be associated with an integer indicating the number of times a phrase mapped to that concept and contained “acute” as the first token. All input and output data and the data structures used by the tool were maintained in random access memory during processing.

The RapTAT application is available at http://code.google.com/p/raptat/. RapTAT was developed independently and does not contain source code of any form from the Multi-threaded Clinical Vocabulary System or other NLP system. The data structures generated based on the MCVS annotated text were created only for the purposes of testing and evaluation of the RapTAT tool. Those data structures contain potentially identifiable patient health information and cannot be distributed or reused.

Performance was based on the number of true positives (TP), false negatives (FN), and false positives (FP). A TP was attributed to a concept when both the RapTAT system and the reference standard mapped a phrase to that exact same concept. When RapTAT mapped the phrase to a different concept than the one identified by the reference standard, a FP was attributed to the RapTAT concept. A FN was attributed to the reference concept when RapTAT was unable to map the phrase or identified a concept different from the reference standard. The tool itself scored TPs, FPs, and FNs and calculated precision, recall, and F-measure according to the equations
                           
                              (6)
                              
                                 Precision
                                 =
                                 TP
                                 /
                                 (
                                 TP
                                 +
                                 FP
                                 )
                              
                           
                        
                        
                           
                              (7)
                              
                                 Recall
                                 =
                                 TP
                                 /
                                 (
                                 TP
                                 +
                                 FN
                                 )
                              
                           
                        
                        
                           
                              (8)
                              
                                 F-Measure
                                 =
                                 2
                                 ·
                                 Precision
                                 ·
                                 Recall
                                 /
                                 (
                                 Precision
                                 +
                                 Recall
                                 )
                              
                           
                        
                     

For determining the accuracy and efficiency of string matching-based concept mapping, we generated a separate Java tool to sequentially match each of 290,741 randomly selected token sequences from our experimental data to terms in the MCVS SNOMED CT database. The tool used repeated SQL queries for matching phrase strings to terms, and phrases and their matched concepts were cached in memory during processing. Memory caching consisted of dynamically building a hash table mapping each phrase used in a SQL query to the identified concept. This improved the processing rate by eliminating repeated SQL queries on the same phrase; the more efficient hashing process was used if the phrase reoccurred in the data. Matching was carried out using both approximate and exact string matching. Approximate matching was carried out by placing wildcard characters (“%”) at the beginning and end of each evaluated phrase. Queries were of the form “SELECT ‘id’ FROM ‘db_table’ WHERE ‘name’ LIKE ‘phrase’,” where ‘id’ referred to the SNOMED concept identifier, ‘db_table’ was a table in a local database in which SNOMED concept identifiers (‘id’) and fully qualified names of the concepts (‘name’) were columns in the table, and ‘phrase’ was one of the phrases identified by MCVS. When multiple concepts were returned from a query, only the first one was retained and tested for correspondence to the reference standard.

We used bootstrap evaluation to estimate precision, recall, and F-measure of the RapTAT-generated phrase-to-concept mappings over the entire, 2860 document corpus. The evaluation method was automated by the RapTAT system and implemented consistent with bootstrapping methods used in risk prediction modeling [52]. The analysis consisted of 1000 training and testing iterations, and each iteration began with creation of a training set generated by random sampling with the sample size equal to the original number of documents. Sampling was done with replacement; selected documents could be chosen more than once, and each training set might contain 0, 1, or multiple copies of a single document (and its associated phrases and concepts). The system did not include phrase-concept associations from previous iterations in the probability calculations. Estimated performance (PerfEst) with respect to precision, recall, and F-measure was calculated as
                           
                              (9)
                              
                                 
                                    
                                       Perf
                                    
                                    
                                       Est
                                    
                                 
                                 =
                                 
                                    
                                       Perf
                                    
                                    
                                       App
                                    
                                 
                                 -
                                 Opt
                              
                           
                        where PerfApp referred to apparent performance on the entire data set when trained on the entire data set. Optimism (Opt) represents the degree to which PerfApp overestimates performance when training and testing are done on the same data set. It is calculated by training on the bootstrap set and then measuring the difference in performance, averaged for each concept across all iterations, when testing is done on the bootstrap versus the entire data set. In the case of concepts with low prevalence, the training set may have limited or no training on which to base concept mapping. Under these conditions, TP and FP may both be zero so that precision is undefined, or TP and FN may both be zero so that recall is undefined. When this occurs for a given iteration, there is no calculable estimate for optimism. Based on a similar situation that can happen during cross-validation, we evaluated three different approaches for handling this issue during bootstrapping: (1) skip the iteration and do not include it in the calculation for the concept; (2) assume optimism is zero; and (3) assume optimism is one [53]. In addition, because cross-validation is more commonly used than bootstrapping for estimating the accuracy of machine learning-based models, we compared the performance measures estimated using bootstrapping to values obtained using “leave one out” cross-validation. The leave one out evaluation consisted of an iterative process, using one of the documents in the corpus for testing and the remaining documents for training. This was done iteratively until every document had been used once for testing. To minimize bias in the estimated performance measures, TP, FP, and FN were summed over all iterations to give a total precision, recall, and F-measure for each concept [53].

Concepts included in the study were grouped within each of the top-level concepts within the SNOMED CT ontology [53] (Table 1
                        ). These top-level concepts form the roots of 19 hierarchical trees within SNOMED CT, and we refer to these as ‘conceptual groups.’ The grouping was done to illustrate general system performance over the many concepts present in the corpus while still allowing for detection of performance differences among groups. Macro-averages were generated by taking the average performance score for each concept determined by bootstrapping and calculating the ‘average of the averages’ for all concepts within a conceptual group, so both rare and commonly occurring concepts contributed equally.

The phrase tokens within the i2b2 data underwent additional processing before training of RapTAT. Initial pre-processing consisted of phrase tokenization, retention of only the first 7 tokens for each phrase, and conversion of all characters to lower case. Subsequent pre-processing consisted of tagging tokens with their parts-of-speech (POS tagging), removal of stop words (“a,” “an,” “and,” “by,” “for,” “in,” “nos,” “of,” “on,” “the,” “to,” and “with”), token lemmatization, and/or inversion of token order. The influence of each of these pre-processing steps with regard to F-measure of the tool using each of these pre-processing steps was evaluated using the training set and bootstrapping as described above. The combination of steps that produced the highest F-measure was used for pre-processing tokens within the test set.

Tokenization and POS tagging was carried using the OpenNLP libraries (Apache Software Foundation) and trained maximum entropy POS tagger. Lemmatization, which converts multiple inflections of a word into a single form, such as the conversion of the both “runs” and “ran” to “run”, was carried out using the lexical variant generator (LVG) library from the National Library of Medicine [54], Token order inversion consisted of putting the tokens into the hash tables used for probability calculations in reverse order. The last token in an English phrase commonly constitutes the “headword” of the phrase and may strongly influence phrase interpretation [55]. We therefore hypothesized that inverting token order might improve mapping performance by aligning phrases along the last token. For example, without inversion, “acute” and “ischemia” would go into the two hash tables corresponding to positions one and two during training. When testing the system on a phrase such as “chronic myocardial ischemia,” the previous occurrence of the token “ischemia” in the second position would not directly influence the likelihood of the tested phrase mapping to the same concept as “acute ischemia” because of differences in position of “ischemia” in the two phrases. In contrast, if the two phrases were inverted, “ischemia” would be the first token in both.

To evaluate the learning rate of the system, we randomly divided the original VA corpus into training and test set with 50% of the documents in each. The tool was first trained using 12 random subsets ranging in size from 100 phrases to the full training set of 283,760 phrases; precision, recall and F-measure were determined on the entire test set after training on each data subset. To minimize the bias introduced when concepts within the test set are absent from the training set, performance was measured based on the total TP, FP, and FN concept matches summed over all concepts [53]. Evaluation of system speed (phrases processed per second) was accomplished using the same training and test sets. Both training and testing consisted of the system sequentially reading in and processing each row of data from the CSV data file, and the system reported the time to complete every 10,000 rows. We also evaluated the speed of the SQL query-based, string-matching application. For all speed evaluations, text processing did not include tokenization, POS tagging, lemmatization, or token order inversion.

All training and testing of the RapTAT system and baseline evaluation were run using a standard desktop personal computer containing 1.98GB of RAM, an Intel Core 2 Duo processor running at 2.99GHz, and a 7200 RPM hard drive with a SATA-300 interface. The operating system was Windows XP Professional with Service Pack 3. The SQL database used for string matching was created using Microsoft SQL Server 2008 maintained on a server containing 11.9GB of RAM and Intel Xeon CPU running at 2.66GHz with the Microsoft Windows Server 2003 R2 operating system. The server database was maintained on an array of fifteen, 450GB, 15K RPM hard drives maintained in a RAID5 configuration. The SQL table used for querying was sorted and indexed using clustering based on the SNOMED CT term. The application accessed the server through the local VA intranet.

To examine the impact of variations in the set of tokens mapping to a given concept on the corresponding F-measure, phrase variance for each concept was quantified using the index of qualitative variation, IQV, defined as
                           
                              (10)
                              
                                 IQV
                                 =
                                 
                                    
                                       K
                                    
                                    
                                       K
                                       -
                                       1
                                    
                                 
                                 ·
                                 
                                    
                                       
                                          1
                                          -
                                          
                                             
                                                
                                                   ∑
                                                
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   K
                                                
                                             
                                          
                                          
                                             
                                                p
                                             
                                             
                                                i
                                             
                                             
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where K is the number of unique phrases mapping to a given concept, and pi
                         is the proportion of phrases that map to a given concept accounted for by the ith phrase [56]. The IQV provides a measure of the variability of the phrases mapping to a single concept. It reaches a maximal value of one when the phrases mapping to a given concept are evenly distributed among two or more possibilities; its value approaches zero for a concept associated almost exclusively with a single phrase. The IQV is undefined when K is one, so concepts corresponding to a single, unique phrase (2957) were not included in the phrase variance analyses.

We define and computed a measure of “concept ambiguity” to formalize the relationship between mapping uncertainty and tool performance as well as the impact of classifier type on that relationship. This measure quantifies the uncertainty associated with phrase-to-concept mapping due to the use of the same token among phrases used to express distinct concepts. For some phrases, the tokens themselves and/or their sequence may uniquely identify a concept. For example, MCVS mapped the single token phrase, “keflex,” to a single SNOMED CT concept, cephalexin (product). Training a system like RapTAT to accurately reproduce such a mapping is trivial. In contrast, when two or more concepts employ the same tokens for expression, the probability of mapping a given token sequence to a particular concept may be greater than zero for multiple candidate concepts, resulting in greater mapping uncertainty. For example, “abdominal” was a token in the phrases “abdominal hernia,” “abdominal distention,” and “left lower abdominal quadrant” as well as a number of others within our VA document corpus, and each of those phrases was mapped to a different SNOMED CT concept within the MCVS reference standard.

To quantify concept ambiguity (Supplement 2), we first calculated the phrase-to-concept “mapping ambiguity” for each phrase that mapped to a given concept according to the reference standard. Mapping ambiguity for a phrase was based on the number of “similar” phrases in the dataset; its magnitude correlated with the number of concepts to which a phrase might map. In the case of the token-order-specific classifier, similar phrases were defined as all those having the same token at the identical position as the given phrase. In the case of the bag-of-words classifier, they were defined as all phrases having the same token as the one in the given phrase at any position. Potential mapping concepts for a particular token were defined as all concepts associated with the given or similar phrases. Mapping ambiguity for a given phrase was calculated as the size of the set of potential mapping concepts for all tokens within the phrase normalized to phrase length. Concept ambiguity was calculated as the logarithm of the average mapping ambiguity for all phrases associated with the concept based on the reference standard. As a result of using this method of calculation, when concept ambiguity approached zero, the probability of correctly mapping all the phrases associated with that concept approached one.

Simple linear regression was used to evaluate the relationship between precision, recall, and F-measure and the number of documents containing a concept. Paired t-tests were used to compare performance across concepts between the token-order-specific and bag-of-words naïve Bayes classifiers. All statistical analyses were carried out using Stata/IC 11.2 for Mac (Stata Corp., College Station, TX), and p-values of less than 0.05 were considered significant.

@&#RESULTS@&#

When the VA corpus was divided into separate training and test sets, recall and F-measure increased steadily for the first 10,000 training phrases regardless of the machine learning-basis of the classifier (Fig. 1
                     ). For the token-order-specific, naïve Bayes-based classifier, all performance measures were ⩾0.88 after training on phrase-to-concept mappings from 50,000 phrases. With further training, all performance measures appeared to continue to increase, reaching a precision, recall, and F-measure of approximately 0.92 using the entire set of training phrases. In comparison, the performance measures for the bag-of-words-based classifier reached a plateau in the range of 0.80–0.82 after training on 50,000 phrases or more. Increases in performance with additional training were 0.003 or less.

Although cross-validation has been used more frequently than bootstrapping for statistical validation of NLP models, bootstrapping may provide more accurate estimates of precision, recall, and F-measure. To compare performance estimation by bootstrapping to that of cross-validation, our study used both approaches to calculate the average F-measure at the concept level for the token-order-specific classifier. Because low prevalence concepts can bias both approaches [53] and should occur more frequently in smaller document corpora, our study estimated performance as a function of the number of documents containing a concept. When analyzing concepts with low prevalence (present in <5 documents), cross-validation estimated a lower F-measure than the other evaluation methods (Fig. 2
                     ). When the system used bootstrapping and set the optimism to one when it could not otherwise be calculated, it also estimated a lower value of the F-measure for low prevalence concepts than did the other bootstrapping techniques. When the system assumed an optimism of zero or skipped estimation altogether for low prevalence concepts, less bias was apparent, but there did appear to be a slight overestimation of the F-measure relative to higher prevalence concepts. For concepts in 5 or more documents, the F-measure was similar across all methods. Because of this finding, all subsequent analyses were confined to concepts occurring in at least 5 documents to minimize bias, and performance measures were calculated using bootstrapping. This reduced the VA data set to 3302 concepts and 279,088 phrases. When a concept was absent from the bootstrap set, performance measures were calculated assuming an optimism of both one and zero, and the calculated values were averaged to generate a final estimate. To quantify phrase complexity within the reduced VA data set, the number of tokens per phrase was calculated and found to be 1.5±0.8 (mean±SD) across concepts occurring in 5 or more documents with a median of 1 (interquartile range=1–2). The average number of unique phrases mapping to a concept was 1.9±4.3 with a median of 1 (interquartile range 1–2).

Using the bootstrapping technique described above to estimate performance, there was a significant increase in precision, recall and F-measure for both classifier models when the number of documents containing a concept and thus available for training the system to map to the concept increased beyond 5 (Fig. 3
                     ). In the case of the bag-of-words classifier, precision, recall, and F-measure were in the range of 0.80–0.86 after training on 100 or more documents compared to 0.97–0.98 for the token-order-specific classifier (Fig. 3). With respect to variability of the phrases mapping to a concept, as measured by IQV, F-measure decreased for both classifiers with increasing IQV, and the greatest decrease was associated with the bag-of-words classifier (Fig. 4
                     , top). Similar decreases in performance occurred as concept ambiguity increased, and the most substantial decreases were associated with the bag-of-words classifier (Fig. 4, bottom).

The highest scoring system in terms of precision, recall, and F-measure averaged over all concepts was the token-order-specific classifier (Table 2
                     ). The bag-of-words classifier and exact string matching showed intermediate performance, and the approximate string matching had the lowest performance. Histogram-based analysis of F-measures for the naïve Bayes classifiers demonstrated a strong bivariate distribution for phrase mapping using the bag-of-words classifier (Fig. 5
                     ). For that classifier, >29% of the concepts had an F-measure of 0.05 or below, whereas <2% of the concepts mapped with the token-order-specific classifier had such a low F-measure. To determine whether the low performance concepts were alone responsible for the reduced F-measures attained using the bag-of-words classifier (Fig. 3), we restricted the analysis for concepts that had performance measures above 0.5 for both classifiers. Although this restriction did reduce the disparity between the two classifiers, precision, recall, and F-measure were still significantly higher for the token-order-specific (0.99±0.04, 0.99±0.04, and 0.985±0.09, respectively; mean±SD) relative to the bag-of-words classifier (0.94±0.12, 0.96±0.11, and 0.95±0.10, respectively).

Precision, recall, and F-measure for the token-order-specific classifier were generally similar across the 19 SNOMED CT conceptual groups included in the study. Average performance measures were >0.91 for all groups (Table 1), and the majority of groups had an average precision (18 of 19), recall (14 of 19), and F-measure (13 of 19) exceeding 0.95.

When the token-order specific classifier was applied to the i2b2 training set, pre-processing of phrases by removing stop words, POS tagging, or reversal of token order improved F-measure, which was estimated using bootstrapping (Table 3
                     ). Combining all three of these pre-processing steps was associated with the greatest F-measure. Lemmatization had no measurable effect or reduced performance. When these pre-processing steps were used during training of RapTAT on the training set and evaluating it on the test set, measured precision and F-measure were generally in the 0.87–0.94 range depending on the concept, and recall was in the 0.87–0.91 range (Table 4
                     ). Performance increased continuously as the number of training documents increased. This is demonstrated by the strong log-linear relationship between the number of training phrases and precision, recall, and F-measure based on regression analysis (r
                     2
                     >0.96 and p
                     <0.001 for all performance measures) (Fig. 6
                     ).

Training and evaluation rates for the two, naïve Bayes-based classifiers were similar. The token-order-specific and bag-of-words classifiers were able to process 149.4 and 128.0 phrases per millisecond during training (data not shown), respectively, and they mapped 29.6 and 24.4 phrases per millisecond during evaluation (Fig. 7
                     ). This rate did not include any lexical processing, which was done by the MCVS tool before generating the CSV file imported into RapTAT. There were no detectable changes in the rate during training or mapping. With respect to string matching using SQL queries, memory caching gradually increased the rate of phrase processing using exact and approximate matching. Both methods reached a plateau rate after processing approximately 100,000 phrases (Fig. 7). The mapping method used by the naïve Bayes-based classifiers was approximately 5-fold faster than the plateau rate of the exact string matching method. After reaching the plateau, the rate of string matching per millisecond remained within the range of 4–8 for exact phrase matches and 0.10–0.25 for approximate phrase matches.

@&#DISCUSSION@&#

Our study tested the feasibility of using a token-order-specific, naïve Bayes classifier-based, machine learning system to quickly analyze free-text phrases and accurately map them to associated concepts. The generalizability of the tool is suggested by its ability to accurately reproduce the mappings of both automated and manual annotations. In addition, the results demonstrate that the tool can rapidly process phrases both during training and during mapping of phrases to concepts. The discharge summaries used for our study contained, on average, 260 words and 216 mapped phrases. Our findings suggest that our tool requires just 7.3ms to map all the phrases from a single discharge summary. Such a tool should be able to support mapping of such summaries for near-real-time NLP, which could be useful in assisting annotators with regard to concept mapping, enhancing existing NLP tools by improving concept mapping efficiency, or providing rapid, interactive feedback following free-text entry. Temporal performance did not decrease during testing, which suggests that the tool should be scalable for mapping purposes. Also, the tool dealt with over 15,000 unique phrases using a 256MB memory partition for the Java virtual machine, so there is potential for expansion based on current desktop computer configurations.

The use of hash tables likely contributed to system performance. This design choice provided high temporal efficiency during mapping because the approach constrained searches for maximizing equation 1 to only concepts associated with at least one of the phrase tokens. It also reduced spatial requirements because the system only stored actual associations between tokens and concepts in the training data rather than, for example, creating a matrix of all potential associations between every concept and token.

Averaged F-measures for the token-order-specific classifier were generally high with regard to the MCVS phrase mappings, being at least 0.92 for the top-level SNOMED CT conceptual groups and above 0.95 in the majority of cases. The F-measures were somewhat lower for the manual annotations from the i2b2 data set but still above 0.89 overall. The lower performance of RapTAT on the manual annotations was expected given that performance was evaluated on a test set rather than estimated using bootstrapping. Furthermore, it appears that performance was limited somewhat by the size of the training set based on the steady increase of the F-measure learning curves (Fig. 7). Finally, we would expect mapping consistency to be higher for automated than manual annotations, which would likely effect system performance.

The apparently high F-measures achieved by the RapTAT system are somewhat surprising in view of the implicit assumption by naïve Bayes models that all phrase tokens are conditionally independent given the mapped concept. The multinomial naïve Bayes model, such as the token-order-specific classifier used here, maintains this independence assumption; the probability of a given word occurring at a particular position within a sequence is assumed to be independent of the presence of other tokens at other positions within the sequence. A phrase such as “ischemic myocardial infarction,” may violate the assumption, because the occurrence of “ischemic myocardial” would seem to increase the probability of the next word being “infarction.” However, naïve Bayes classifiers often perform well even when the independence assumption is violated [57].

There have been numerous biomedical challenge tasks that evaluate performance with regard to the combination of named entity recognition and concept mapping, and the results of these challenges provide standards for evaluating new computational systems. A report on the 2010 i2b2 challenge indicated that the best system achieved an F-measure of 0.85 for exact matches and 0.92 for inexact matches [48,58], while RapTAT achieved comparable F-measures ranging from 0.87 to 0.94. Because the i2b2 challenge systems were responsible for both named entity recognition and mapping, direct comparison of RapTAT to the systems tested in the i2b2 challenge is not possible. Unfortunately, few challenges, if any, have focused on concept normalization alone [23], but the i2b2 results suggest that RapTAT performs this particular task relatively well. A recent report did focus on normalization of diseases found in the Arizona disease corpus [23,59]. The investigators enhanced two existing biomedical concept mapping systems, Peregrine and MetaMap, with rules to handle issues such as term variation and abbreviations [23]. The maximum F-score achieved in that study with regard to concept mapping was 0.736. However, even though the study focused on concept normalization, mapping still relied on automation of named entity recognition to identify phrases. Named entity recognition performance in that study, which achieved a maximum F-measure of 0.854, undoubtedly limited concept-mapping accuracy.

Our comparison of the token-order-specific to the bag-of-words and string-matching classifiers suggests that the former provided substantial advantages over the other methods. Performance of the token-order-specific classifier was considerably higher after training on phrases within 5 to over 100 documents (Fig. 3). The performance of the bag-of-words classifier reported here actually exceeds that reported by Pakhomov, Buntrock, and Shute, who used a bag-of-words classifier to categorize medical diagnosis and found that precision, recall, and F-measure were 0.59, 0.51, and 0.54, respectively [45]. That study narrowed the use of the bag-of-words classifier to potentially difficult phrases, ones that could not be readily mapped using previous classifications, which may have diminished performance. Differences in the performance measures may also be related to the amount of classifier training, and further training might reduce the differences in precision, recall, and F-measure between the two classifiers (Fig. 3). Nevertheless, our data also indicate that the token-order-specific classifier would likely outperform the bag-of-words classifier if training sets were limited in size.

One reason that performance can be diminished by a bag-of-words classifier is demonstrated by the phrase, “hemodynamically stable,” which was present in 75 documents within the corpus and was mapped to a SNOMED-CT concept of the same name within the conceptual group “clinical finding.” The word “stable” alone occurred much more frequently as a phrase than “hemodynamically stable” and was mapped to a concept within the “qualifier value” conceptual group by MCVS. So, when token position was not included as a feature by the bag-of-words classifier, the mapping of the phrase was strongly influenced by the high probability of association of “stable” with the qualifier value concept. In contrast, when “stable” was the second word in a phrase, it always mapped to “hemodynamically stable,” and when token position was used as a feature by the token-order-specific classifier, it correctly mapped the phrase. Although increased computational accuracy often requires additional calculations leading to a decrease in processing speed, this did not appear to be the case in the present study. Processing speed for the token-order-specific classifier appeared to be similar to and possibly slightly faster than the bag-of-words classifier (Fig. 6). The reason for this is likely that token sequence length is one determinant of classifier speed, and both classifiers must evaluate each of the tokens in a phrase during mapping. However, the bag-of-words classifier must also evaluate the probabilities for tokens that are not in a phrase under evaluation (cf. Eq. (2)
                     
                     versus
                     
                     (4)).

Another advantage of the token-order-specific classifier is that it diminished the impact of phrase variability and concept ambiguity on performance (Fig. 4). This could be important if one was planning to train the tool to reproduce human-generated phrase-to-concept mappings, which might be done if using the tool to automate or assist with manual annotations. Both inter- and intra-reviewer discrepancies during manual annotation can produce substantial variation in the phrases identified and the mappings selected. The amount of variation would likely be greater than that produced by an NLP application such as the MCVS tool used in the current study.

The speed and accuracy of the RapTAT tool is encouraging, but the present system has limitations. For one, the tool only selects the most probable concept mapping when provided with a phrase; it does not identify which raw text phrases should be annotated. However, previous work suggests that such a task is feasible. D’Avolio et al. reported on use of the ARC tool to identify noun phrases within raw text [41]. Using a conditional random field classifier as the basis of machine learning, ARC automated retrieval of three different concepts and their associated phrases from an i2b2 document set, generating micro-averaged F-measures in the range of 0.80–0.83. In addition, there are a number of pre-processing steps that need to be carried out before the phrase-to-concept mapping carried out by RapTAT. These steps will add to the overall document processing time, but our analysis of the time required for database lookups, even when memory caching is used, suggest that the temporal performance of the RapTAT system could help existing NLP systems to move closer to near-real-time processing. Furthermore, because the system is trained using existing phrase-to-concept mappings, it will reproduce any inaccuracies generated by the system that created the initial mappings. Also, the tool needs to process an adequate number of training examples to accurately map phrase to concepts, and the number required increases as phrase variability and concept ambiguity increase. However, synthetic phrases similar to those likely to be found in the domain could be added to boost the training for rare or ambiguous concepts. The tool does not determine the assertion value of concepts, so currently there is no way to distinguish among positive, negative, and uncertain concepts. Similarly, the current version of the tool does not do any semantic analysis. Therefore, once tool training is completed, phrases that might map to more granular concepts and abbreviations whose meaning depends on context only map to a single concept. In addition, it will not map abbreviations that were not included in the training data. Finally, RapTAT cannot combine simple concepts to generate compositional expressions, which are needed to more fully encode documented medical phrases [60]. Despite the limitations of RapTAT, the tool does provide a systematic method for learning and accurately reproducing both established and novel phrase-to-concept mappings, such as might be needed when applying NLP to a new domain. Current efforts to further develop the tool may allow users to train it to determine assertion values and delineate the concepts that might be combined to form compositional expressions.

In addition to addressing the current limitations discussed above, our future plans include using the system to generate an assistive annotation tool. Current manual annotation systems require a reviewer to first identify a phrase of interest and then select from a list of concepts for mapping. By training RapTAT to reproduce concepts selected by an annotator, it should be possible to automate the mapping process or limit the concepts presented to an annotator to only those with high probability. The RapTAT concept-mapping module has been incorporated into plug-ins and components of the GATE and Unstructured Information Management application (UIMA) NLP frameworks. Using this approach, RapTAT can also be used to systematically generate rapid concept mappers that are tailored to different domains and tasks and that can be used within larger, existing NLP systems.

@&#CONCLUSION@&#

Because RapTAT can accurately map phrases to a large repertoire of concepts distributed widely across an existing ontology, it could serve as an alternative mapping system within an existing NLP tool. Given more than 5 training instances, the F-measure exceeds 0.92, which should be sufficient for many tasks. In addition, with a mapping rate of ∼30 phrases per millisecond, the system should be fast enough to readily support phrase-to-concept mapping within a near-real-time NLP system. Using the tool to fully automate the human annotation process will require further development in the form of identifying free text phrases for labeling.

This material is based upon work supported by the Department of Veterans Affairs, Veterans Health Administration, Office of Research and Development, Health Services Research and Development program. The views expressed in this article are those of the authors and do not necessarily reflect the position or policy of the Department of Veterans Affairs or the United States government.

@&#ACKNOWLEDGMENTS@&#

We would like to thank Vincent Messina for his technical assistance. The material in this study is based upon work funded by the Department of Veterans Affairs (VA), Veterans Health Administration, Office of Research and Development, Health Services Research and Development (HSR&D) program. The work was supported with resources and the use of facilities at the VA Tennessee Valley Healthcare System (TVHS). Funding for this study was provided through VA Grant SAF-03-223 and VA HSR&D IIR 11-292. The VA Consortium for Health Informatics Research (CHIR) HIR 09-001 and HIR 09-003 also provided support to Drs. Gobbel, Speroff, and Matheny as well as Ms. Jayaramaraja. The Department of Veterans Affairs Health Administration HSR&D Career Development Award CDA-08-020 also provided support to Dr. Matheny. Dr. Gobbel and Dr. Reeves were supported by the Department of Veterans Affairs Medical Informatics Fellowship Program (Sponsored by Office of Academic Affiliations, Office of Health Information, and HSR&D). Dr. Gobbel and Dr. Reeves were Medical Informatics Fellows within the Department of Veterans Affairs Medical Center, Nashville TN during the core time period of this work. Dr. Matheny is a Physician Researcher at the Geriatrics Research Education and Clinical Center (GRECC) at the Department of Veterans Affairs Medical Center, Nashville, TN. Dr. Brown is a Staff Physician at the Department of Veterans Affairs Medical Center, Nashville, TN and Director of Knowledge-Based System, Health Informatics, Office of Informatics and Analytics, Department of Veterans Affairs. Dr. Speroff is Chief of TVHS Center for Health Services Research, GRECC, Department of Veterans Affairs Medical Center, Nashville TN. The i2b2 data sets used in this study were collected from de-identified clinical records provided by the i2b2 National Center for Biomedical Computing funded by U54LM008748 and were originally prepared for the Shared Tasks for Challenges in NLP for Clinical Data organized by Dr. Ozlem Uzuner, i2b2 and SUNY.

Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jbi.2013.11.008.


                     
                        
                           Supplementary data 1
                           
                              Text processing steps.
                           
                           
                        
                     
                     
                        
                           Supplementary data 2
                           
                              Mapping and concept ambiguity.
                           
                           
                        
                     
                  

@&#REFERENCES@&#

