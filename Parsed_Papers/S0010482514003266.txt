@&#MAIN-TITLE@&#Pre-operative prediction of surgical morbidity in children: Comparison of five statistical models

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We aimed to predict pediatric surgical morbidity using preoperative characteristics.


                        
                        
                           
                           We compared logistic regression models to data mining algorithms.


                        
                        
                           
                           The data mining algorithms performed as well as a simple logistic regression model.


                        
                        
                           
                           A flexible logistic regression model performed best on most model fit criteria.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Data mining

Machine learning

Prediction

Boosting

Random forests

Support vector machines

Logistic regression

Surgical morbidity

Pediatrics

@&#ABSTRACT@&#


               
               
                  Background
                  The accurate prediction of surgical risk is important to patients and physicians. Logistic regression (LR) models are typically used to estimate these risks. However, in the fields of data mining and machine-learning, many alternative classification and prediction algorithms have been developed. This study aimed to compare the performance of LR to several data mining algorithms for predicting 30-day surgical morbidity in children.
               
               
                  Methods
                  We used the 2012 National Surgical Quality Improvement Program-Pediatric dataset to compare the performance of (1) a LR model that assumed linearity and additivity (simple LR model) (2) a LR model incorporating restricted cubic splines and interactions (flexible LR model) (3) a support vector machine, (4) a random forest and (5) boosted classification trees for predicting surgical morbidity.
               
               
                  Results
                  The ensemble-based methods showed significantly higher accuracy, sensitivity, specificity, PPV, and NPV than the simple LR model. However, none of the models performed better than the flexible LR model in terms of the aforementioned measures or in model calibration or discrimination.
               
               
                  Conclusion
                  Support vector machines, random forests, and boosted classification trees do not show better performance than LR for predicting pediatric surgical morbidity. After further validation, the flexible LR model derived in this study could be used to assist with clinical decision-making based on patient-specific surgical risks.
               
            

@&#INTRODUCTION@&#

Data mining algorithms, sometimes called machine learning or statistical learning algorithms, have been increasingly used in biomedical research in recent years. Data mining is broadly defined as the process of selecting, exploring, and modeling large amounts of data to discover unknown and useful patterns or relationships [1,2]. Data mining algorithms arose from the fields of statistics and computer science, and are widely used in marketing, banking, engineering, and bioinformatics. Their application to clinical research, however, has been limited.

In clinical research, logistic regression models are by far the most commonly used algorithm for predicting the probability of occurrence of an event. While these models can provide unbiased estimates of the associations between predictors and the outcome, they have some limitations. First, they assume a particular parametric form of the relationships between the predictors and the outcome; namely, the assumption is made that the logit of the outcome is equal to a linear combination of the independent variables [3]. These models also assume additivity of the predictors’ effects on the outcome. These assumptions are usually incorrect, though the extent to which they are incorrect varies. Furthermore, in small datasets, these assumptions may be necessary to avoid overfitting. In larger datasets, these assumptions can be circumvented by using transformations or splines to model continuous predictors and by including interactions between variables. These techniques can improve model fit, but they are infrequently used, partly because they tend to reduce model interpretability [4]. Another limitation of regression models is that they do not always provide optimal predictive accuracy. In clinical research, these models are typically built to describe the nature of the relationship between specific covariates and the outcome [2]. While estimating such relationships is clearly important in biomedical research, accurate prediction is also very important. In fact, in certain situations in which the primary aim is to achieve optimal predictive accuracy, a reduction in clinical interpretability may be acceptable.

One area of biomedical research in which data mining may be particularly useful is in outcome prediction using large clinical databases, such as the American College of Surgeons’ National Surgical Quality Improvement Program (ACS NSQIP) database [2]. Of the few studies investigating the performance of data mining algorithms for predicting surgical morbidity or mortality, most have been small (several hundred or several thousand patients) [5–10], though a few larger studies have been reported [11–17]. These studies have been inconsistent in their findings, in that some have shown data mining algorithms to perform better than traditional logistic regression in terms of overall accuracy [13,14,16,18], discrimination [13,14,16], or calibration [11], whereas some have reported similar performance according to these measures [11,18–20]. Data from the ACS NSQIP has been used to create risk calculators to predict post-operative outcomes for adult surgery patients overall [21] and for patients undergoing specific procedures [22–25]. Several of these calculators are freely available online, and their use by both physicians and patients has the potential to improve shared decision making and informed consent [21–25]. All of these calculators are based on logistic regression models that are reported to have good discrimination and calibration. However, none of the studies in which these prediction models were derived reported investigating whether other statistical algorithms might perform as well as or better than logistic regression, and none included pediatric patients. The objective of this study was to compare the performance of five different statistical algorithms for predicting surgical morbidity in pediatric surgical patients. The algorithms evaluated were chosen because of their infrequent use in the clinical research literature and their straightforward implementation in freely available software and included (1) a logistic regression model that assumed linearity and additivity (simple logistic regression model) (2) a logistic regression model incorporating restricted cubic splines and interactions (flexible logistic regression model) (3) a support vector machine, (4) a random forest and (5) boosted classification trees.

@&#METHODS@&#

This study used the 2012 NSQIP Pediatric (NSQIP-Peds) Participant Use Data File, which contains patient-level data on 51,008 pediatric surgery cases submitted in 2012 by 50 US and Canadian children’s hospitals. The NSQIP-Peds program is a multi-specialty program with cases sampled from pediatric general/thoracic surgery, pediatric otolaryngology, pediatric orthopedic surgery, pediatric urology, pediatric neurosurgery, and pediatric plastic surgery. Launched in October 2008 with 4 sites, NSQIP-Peds has since expanded, with 50 institutions participating in 2012. The program provides peer-reviewed, risk-adjusted 30-day postoperative outcomes to participating institutions, for the purposes of benchmarking and quality improvement [26–28]. Included cases are selected based on Current Procedural Terminology codes using NSQIP 8-day cycle-based systematic sampling of 35 procedures per cycle. One hundred and twenty-nine variables are collected from the medical records and the patients and their families, including information on demographics, surgical profile, preoperative and intraoperative variables, and postoperative occurrences [26–28]. The conduct of this study was approved by Nationwide Children’s Hospital Institutional Research Board with a waiver of informed consent.

In this study, we considered the question of which model most accurately predicts the occurrence of surgical morbidity within 30 days of surgery. Neonates were excluded, because of the known differences in risk of surgical morbidity between neonates and non-neonates and because of the relatively small number of neonates (N=2919) and larger amount of missing data in neonates compared to non-neonates (N=48089) in the 2012 NSQIP-Peds sample. The 49 preoperative variables in pediatric patients considered for inclusion in each model are shown in 
                     Table 1. This list consists of all preoperative patient characteristics available in the database, though some rare characteristics were eliminated or grouped with other similar characteristics. Of note, procedures that occurred concurrently with the principal operative procedure were not considered as predictors because whether these additional procedures would be performed was not necessarily known preoperatively. In addition, 60 individual procedures (designated by CPT codes) that were performed in the total cohort at least 200 times were also included as indicator variables in the models, resulting in a total of 109 predictor variables. A frequency of 200 times was chosen to maximize the external validity of the models by enabling the risk of surgical morbidity associated with each procedure to be estimated accurately in the training dataset. Many of the procedures performed less frequently had no associated cases of surgical morbidity in the sample, whereas all procedures performed 200 or more times were associated with at least one case of surgical morbidity. No observations were removed from the analyses due to the use of this criterion as each type of procedure included as a predictor was treated as an individual binary variable. The outcome variable was the occurrence of intra-operative or post-operative morbidity within 30 days of the surgery, which was defined as any of the following events: SSI (superficial, deep, or organ/space without open wound), wound disruption, pneumonia without preoperative pneumonia, unplanned intubation, pulmonary embolism, renal insufficiency or failure without preoperative renal failure or dialysis, urinary tract infection, central line associated bloodstream infection, coma >24h without preoperative coma, seizure, nerve injury, any cerebral intra-ventricular hemorrhage, CVA/stroke or intracranial hemorrhage, cardiac arrest requiring CPR, venous thrombosis requiring therapy, bleeding/transfusion, graft/prosthesis/flap failure, or the development or worsening of sepsis [26,29]. Patients who died within 30 days of their surgery (0.1%) were included in all analyses because the outcome under examination, surgical morbidity, could occur either intraoperatively or postoperatively.

In order to avoid overfitting, which occurs when a model has excellent fit to the data used in model fitting but poor fit to external data [4,30], the 2012 NSQIP-Peds PUF dataset was split into training and validation datasets. Seventy percent of the observations were chosen randomly for use as the training dataset, and the other 30% were used as the test (validation) dataset. Each algorithm incorporated all 109 pre-operative variables of interest. The 5 statistical algorithms compared were: (1) a logistic regression model that assumed a linear relationship between each covariate and the log-odds of morbidity, with no interaction terms (simple logistic regression model), (2) a logistic regression model fit with the relationship between continuous variables and the log-odds of morbidity expressed using restricted cubic splines with decile knots and with interactions between any two predictors included if statistically significant at p<0.01 in stepwise selection when added to the model containing all main effects (flexible logistic regression model), (3) a support vector machine (SVM), (4) a random forest (RF) and (5) boosted classification trees [4,30].

The relative performance of the 5 models in the validation dataset was first assessed by examining overall accuracy, sensitivity, specificity, negative predictive value (NPV), and positive predictive value (PPV). In the logistic regression models, a cutoff outcome probability of 50% was used for classification. The McNemar test was used to compare accuracy, sensitivity, and specificity between each of the machine learning algorithms and the logistic regression models. Marginal logistic regression models were used to compare NPV and PPV between the machine learning algorithms and the logistic regression models [31]. This method is analogous to McNemar’s test but for the problem of comparing predictive values, which condition on the test outcome. Because the predictive algorithms do not necessarily all predict the same outcome for a given patient, some patients may contribute zero, one, or two observations to the comparisons, thus the variance estimator for the difference in PPVs and NPVs between algorithms is not straightforward. Marginal regression models provide a natural test statistic to assess these differences. The discriminative ability of the algorithms was determined using the area under the receiver operating characteristic curve (AUROC) [3]. The nonparametric test of DeLong et al. was used to compare the AUROC between the machine learning algorithms and the logistic regression models [32]. The calibration of each model was assessed by first comparing the mean predicted probability of morbidity to the observed probability of morbidity in the validation sample. This provides a measure of the calibration intercept, also known as calibration-in-the-large [33]. Second, the calibration slope was calculated; this slope assesses deviation between observed and expected probabilities of surgical morbidity across the entire range of predicted risk; it equals one if the model is perfectly calibrated. Lastly, a lowess scatterplot smoother was used to graphically describe the relationship between observed and predicted morbidity in the validation sample. Deviation of this curve from a diagonal line with unit slope is indicative of poor calibration [33]. Logistic regression modeling was performed using the logistic procedure in SAS v9.3 (SAS Institute Inc., Cary, NC) and the glm function in the stats package in the R statistical environment (R Foundation for Statistical Computing, Vienna, Austria). SVM models were fit using the svm function in the e1071 package in R [34]. Random forest models were fit using the randomForest function in the randomForest package in R [35]. Boosted classification tree models were built using the gbm function in the gbm package in R [36]. AUROC was calculated and compared between models using the roc.test function in the pROC package in R [37]. Assessment of model calibration was performed using the val.prob function in the rms package in R [38].

Logistic regression is the most common statistical algorithm employed in clinical research studies to assess associations between patient characteristics and binary outcomes. These models are a type of generalized linear model, and are fit using maximum likelihood estimation. In generalized linear models, the expected value of the outcome is a function of a linear combination of the predictors; in logistic regression the logit function is used [3]. Logistic regression models yield odds ratios for the associations between the dependent and independent variables. They also generate a risk score, or an estimated probability of the outcome, that can be used for classification and prediction.

The idea behind SVMs is the construction of an optimal separating hyperplane between two classes [4,39,40]. Each observation is treated as a point in high-dimensional feature (predictor) space, with the dimension of this space determined by the number of predictors. The SVM model uses mathematical functions (kernels) to project the original data into higher-dimensional space in order to improve the separability of the two classes. The SVM model also uses a ‘soft margin’ around the separating hyperplane, the size of which is chosen using cross-validation. This margin allows some observations to violate the separating hyperplane in order to achieve better overall performance [4,40]. Radial kernels often deliver excellent results in high dimensional problems [4], and these were used in this study. SVMs with radial kernels require the specification of two parameters: C, which controls the overfitting of the model, and γ, which controls the degree of non-linearity of the model [30]. To optimize these parameters, 10-fold cross-validation of the training data was performed; the C and γ values that minimized the overall misclassification rate were chosen using a grid search in the intervals [1;1000] and [0.001; 100], respectively. Finally, the output values of the SVM were converted into probabilities using the sigmoid function as described by Lin et al. [41].

A random forest is a collection, or “ensemble”, of classification trees [42] with the predictions from all trees combined to make the overall prediction by “majority vote” [43]. A series of classification trees is built, with each tree being fit using a random bootstrap sample of the original training dataset and a random subset of the predictors that maximize the classification criterion at each node. An estimate of the misclassification rate is obtained without cross-validation by using each classification tree to predict the outcome of the observations not in the bootstrap sample used to grow that particular tree (“out-of-bag” observations), then taking a majority vote of the out-of-the-bag predictions from the collection of trees. Random forests typically have substantially greater predictive accuracy than single classification trees, which have very high variance [43,44]. Random forests require just two parameters to be defined: the number of random trees in the forest, and the number of predictive variables randomly selected for consideration at each node [43]. In this study, these parameters were optimized by a grid search in the intervals [400;1000] and [6,16], respectively; the parameters yielding the lowest out-of-bag misclassification rate were selected for the final models.

Similar to the random forest algorithm, boosting involves the combining of predictions from a large number of ‘weak’ classifiers, each with error rates only slightly better than random guessing, to produce a final more accurate prediction [4]. Boosting can be applied with any base algorithm, but is most often used with classification and regression trees. Unlike random forests, boosted classification trees are grown sequentially using information from previously grown trees. Boosting does not involve bootstrap sampling or the random selection of predictors to be considered at each node. Rather, the boosted classification tree algorithm fits a small tree to a sequence of reweighted versions of the data. In the building of each new tree, patients whose outcomes were incorrectly classified by the previous tree are weighted more heavily than patients who were correctly classified. In this study, we will fit a gradient boosted model with binomial deviance loss function as this algorithm is particularly robust to overlapping class distributions [4,45]. Boosted classification trees fit with the gradient boosting algorithm and incorporating regularization through shrinkage require the defining of three parameters: the number of component trees (M), the shrinkage parameter (ν), and the maximum interaction depth or number of terminal nodes of each tree [45]. To optimize the first two of these parameters, 10-fold cross-validation of the training data was performed; M and ν values that minimized the overall misclassification rate were chosen using a grid search in the intervals [1;10,000] and [0.001; 0.1], respectively. Although there is no consensus on the optimal tree depth, interaction depths between 3 and 7 are often found to yield similar results, with cross-validation error rates being relatively insensitive to particular choices in this range [4]. We chose to use classification trees of depth 4.

@&#RESULTS@&#

Comparisons of pre-operative characteristics between patients with and without surgical morbidity in both the training and validation datasets are shown in Table 1. Forty-six of the 49 evaluated pre-operative characteristics differed significantly between patients with and without surgical morbidity in both datasets, and these differences were fairly consistent across the two datasets. Substantial heterogeneity was found across procedures in the rates of surgical morbidity. The 10 most common procedures in the study cohort are shown in 
                        Table 2. Spinal fusion (arthrodesis) procedures had much higher rates of surgical morbidity than other common procedures.

Patients in the training and validation samples had similar characteristics, with only a few exceptions. Children in the validation sample were slightly more likely to be of Hispanic ethnicity (12.8 vs. 12.0%, 
                           p
                        =0.02) and to have had SIRS, sepsis, or septic shock within 48h before surgery (5.5 vs. 4.9%, p=0.007), and they were slightly less likely to have an open wound at the time of surgery (0.9 vs. 1.1%, p=0.02). The proportions of patients who had each procedure were similar in the two samples when considering procedures performed in at least 200 cases in the overall cohort. The proportion of patients who experienced surgical morbidity was also similar in the two samples (
                        Table 3).

The classification accuracy, sensitivity, specificity, PPV, and NPV of the different models fit to the training sample and evaluated in the validation sample are shown in 
                        Table 4. Accuracy was highest in the flexible logistic regression model. This model incorporated restricted cubic regression splines for the two continuous pre-operative variables and also included eight statistically significant interactions between preoperative variables. The included interactions were those between surgical specialty and age at surgery, inpatient status, blood transfusion within 48h before surgery, and wound classification, as well as those between baseline patient characteristics and particular procedures; namely, between spinal fusion of 7–12 vertebral segments and the presence of a structural central nervous system abnormality, palatoplasty for cleft palate and inpatient status, laminectomy with the release of a tethered spinal cord and inpatient status, and adjacent tissue transfer of 10cm2 or less and steroid use in the 30 days preceding surgery (p<.01 for all). Importantly, the accuracies of the ensemble tree-based algorithms, random forests and boosted classification trees were not statistically significantly different from that of the flexible logistic regression model, and all models had accuracies within 0.5% of each other. Sensitivity was poor but varied substantially across the models, with the flexible logistic regression model performing best. Specificity was excellent and varied by less than 1% across all models. PPV was lower in the simple logistic regression model compared to all other models. NPV differed by less than 1% among all models but was highest in the flexible logistic regression model. The flexible logistic regression model performed at least as well as or better than the other models on all classification accuracy criteria except specificity, for which the support vector machine and random forest were slightly superior.

All models showed good discrimination, with areas under the receiver operating characteristic curve (c-statistics) ranging from 0.818 for the support vector machine to 0.880 for the boosted classification trees. The flexible logistic regression model and boosted classification trees had statistically equivalent c-statistics (Table 5). The calibration of each model is described in 
                        Table 6 and 
                        Fig. 1. The support vector machine demonstrated the worst calibration, and the logistic regression models demonstrated the best calibration of all models. Boosted classification trees also showed good calibration but tended to slightly underestimate the probability of surgical morbidity.


                        
                        Fig. 2 shows marginal odds ratios from the flexible logistic regression model for several important predictors. The marginal effects of predictors that were considered to be both clinically important and either statistically significant at p<.001 or involved in interactions statistically significant at p<.001 are shown. The predictors with the highest estimated odds ratios were two particular procedures, namely spinal fusions and craniectomy. As expected, many individual procedures were found to be significant independent predictors of surgical morbidity. Other factors strongly associated with an increased risk of surgical morbidity in the overall study cohort included higher ASA class, which is a measure of a patient’s preoperative physical state, and having a procedure as an inpatient, which is the setting in which higher risk procedures are typically performed. In addition, particular gastrointestinal, nutritional, or oncologic comorbidities were associated with an increased risk of surgical morbidity. These comorbidities included esophageal, gastric, or intestinal disease, history of or current malignancy, hematologic disorders, and structural central nervous system abnormalities, all of which, in addition to being potential indications for high risk procedures, are often present in high risk complex patients who are at increased risk for postoperative bleeding and infections. Patients who required preoperative nutritional or inotropic support, which are often required to keep complex and critically ill patients stable, were also at greater risk for surgical morbidity. Being younger was also associated with an increased risk of surgical morbidity, as these patients are more likely to undergo high risk procedures for serious congenital anomalies or conditions related to prematurity than older patients; however, the effect of age was mainly evident when comparing children older vs. younger than 2 years of age.

@&#DISCUSSION@&#

This study compared five different statistical algorithms for the prediction of surgical morbidity in pediatric patients. The primary finding was that a flexible logistic regression model that incorporated restricted cubic regression splines and statistically significant and clinically meaningful interactions had the highest out-of-sample accuracy and sensitivity of all algorithms examined. This model also had excellent discrimination and calibration. Given the large number of patients and hospitals included in both the training and validation samples, as well as the highly standardized and validated method of data collection used in NSQIP-Peds, the model derived in this study is likely to be applicable to the general population of pediatric surgery patients. However, as with any statistical model, this model may not extrapolate well to populations not included in its derivation. For example, patients having cardiac, ophthalmologic, obstetric, or transplantation procedures, and patients with traumatic injuries are excluded from the NSQIP-Peds program, and thus the derived models in this study would not apply to these patients [26].

Given the large number and variety of data mining algorithms that have been developed for classification and prediction in the fields of statistics and computer science [2], it may seem somewhat surprising that logistic regression, which has been used for clinical prediction modeling for decades, would perform as well or better than the more recently developed data mining techniques. In fact, the flexible logistic regression model performed better than or equivalently to the data mining algorithms on all model fit criteria except specificity, though the differences in some criteria were very small. There are several reasons for these findings. First, no statistical or data mining algorithm will perform best in all settings. Second, the performance of various algorithms depends not only on the population and outcome under examination, but also on the availability and dimensionality of predictors [46] as well as the criteria chosen to evaluate each method’s performance [47]. Numerous studies have compared logistic regression models to data mining algorithms for the prediction of surgical outcomes, and their findings have been heterogeneous. Several studies have compared logistic regression models to artificial neural networks (ANN), which are nonlinear statistical models that extract linear combinations of the input variables as derived features then model the outcome as a nonlinear function of these features [11,13,14,16,48]. Most of these studies reported that ANNs had superior performance for predicting surgical outcomes, such as mortality after surgery for heart disease, traumatic brain injury, or ascending aortic dissection [13,14,48]. A recent study that examined the performance of logistic regression, classification tree, random forest, and SVM models for predicting sentinel lymph node status in patients with cutaneous melanoma found that all four algorithms had similar predictive accuracy [18]. Another study that compared the accuracy and discrimination of nine different statistical and data mining algorithms, including logistic regression, to that of the TNM staging system for predicting survival in colorectal cancer patients, found that all nine algorithms had similar and slightly better performance than the TNM staging system, but that differences among all algorithms were small [20]. More often than not, studies comparing “new” computational algorithms to older techniques claim that the new method performs better than the older method [49]. Many of these claims have come from small studies of fewer than 1000 patients, a sample size far too small to both develop and validate a model in which dozens of predictor variables are evaluated [48]. Thus, random variation certainly explains some of the heterogeneity in findings across studies. In addition, many comparative studies provide insufficient detail on the method used for choosing parameters to train the algorithm, and many compare models’ performance using only classification accuracy or the area under the receiver operator characteristic curve. There are in fact a number of other characteristics that should be considered when comparing methods, such as the method’s handling of missing data, noise, and highly correlated predictors, variable selection, computational cost, and the generalizability and interpretability of the model. For example, in the present study, the amount of missing data was small and missing values occurred in categorical variables only; thus, a category for patients with unknown values of these variables was simply created and could be easily accommodated by all algorithms. However, in general, logistic regression and SVM models cannot accommodate missing values unless imputation is first performed, whereas the tree-based methods can accommodate missingness through the process of surrogate splitting [4]. The present analysis did not contend with highly correlated predictors or perform variable selection, but regarding computational cost, the data mining methods were more computationally costly to fit than the logistic regression models. This cost would limit the utility of the data mining algorithms in a clinical setting.

Importantly, in predictive modeling, there is always a tradeoff between achieving high accuracy, which is accomplished by constructing a model that is as flexible as necessary without overfitting, and interpretability, which is achieved by constructing a model with parameters that describe the relationships between predictors and the outcome in an understandable way [4]. In the present study, we focused on achieving optimal prediction of the outcome of 30-day surgical morbidity rather than on describing the nature of the relationships between patient and procedure characteristics and surgical morbidity. While the latter task is clearly important in surgical outcomes research, accurate prediction is also critical. By incorporating many characteristics into a global risk prediction score, some of the subjectivity that results from physician overreliance on one or a few patient characteristics can be eliminated. In addition, risk prediction that takes into account all available, relevant preoperative information can assist both surgeons and patients in deciding between different types of procedures and in determining what precautionary measures might be important to take for a patient at high risk of a poor outcome. On the other hand, when interpretability is a primary goal, a single classification tree or a logistic regression model with only linear terms might be the best option, provided that its predictive accuracy and discrimination are sufficiently high. It should be pointed out, however, that although a simple logistic regression model or classification tree may be most easily interpretable, the importance of individual predictors can be evaluated in SVM, random forest, and boosted classification tree models as well. These measures can be derived via recursive feature elimination for SVM [46] and via the sum of the improvements in the split-criterion over all trees for random forests and boosted classification trees [42]. The former technique is also useful for feature selection in SVM; however, we chose not to perform feature selection for any models in the present study because of the large sample size available for analyses and also because all preoperative variables were chosen for inclusion in the NSQIP-Peds database by surgeon experts due to their clinical importance.

This study had several limitations. First, we evaluated only a small sample of existing data mining algorithms, focusing on algorithms that have been infrequently used in the clinical research literature and that could easily be implemented in freely available software. Many other predictive algorithms that have been more frequently explored by clinical researchers or that involve more parameters and thus greater complexity than the algorithms chosen for this study are available, such as individual classification trees, bagging of classification trees, artificial neural networks, Bayesian networks, and the recently developed “superlearner” ensemble machine learning algorithm [4,50]. It is possible that one of these algorithms would have performed better than the flexible logistic regression model derived in the present study. Second, we cannot guarantee that globally optimal parameter values were found for all of the data mining algorithms explored. For example, in the boosting model, we explored only trees of depth 4 since previous studies indicate that interaction depths between 3 and 7 typically yield similar results. For all parameters used to train the other data mining algorithms, we used a grid search method, but we did not perform an exhaustive search of all values within the grids. Third, because only one year of NSQIP-Peds data was available for this study, many surgical procedures were performed in few cases; this prevented us from including every procedure as an indicator variable in the models. Once a larger, multi-year dataset is available, it will be possible to more effectively model the heterogeneity in the risk of surgical morbidity across all types of surgical procedures. However, despite the limited size of the NSQIP-Peds dataset for the number of procedures included, one of its strengths is the variety of standardized and validated preoperative, operative, and postoperative variables it contains. This allowed for higher accuracy and refinement of the statistical models examined compared to those that would be possible using administrative data. Despite the high accuracy of all models explored in this study, all models also unfortunately showed poor sensitivity. This is not an uncommon problem when outcome data are highly imbalanced, and in such cases overall classification accuracy may not be the ideal metric by which to evaluate algorithm performance [47,51]. Finally, in contrast to the ACS NSQIP surgical risk calculator developed to predict surgical morbidity in adult surgery patients [21], the flexible logistic regression model derived in the present study could not account for the clustering of patients within hospitals due to the absence of a site indicator in the available dataset. However, our model included flexible effects, namely splines and interaction effects, which were not reported to have been evaluated in deriving the adult NSQIP surgical risk calculator. In addition, the area under the curve of our model was slightly larger than that reported for the adult NSQIP surgical risk calculator (0.877 vs. 0.816) [21], indicating that model discrimination was likely not substantively worsened by the nonuse of hierarchical logistic regression modeling in this study.

In conclusion, support vector machine, random forest, and boosted classification tree models do not provide superior prediction of 30-day surgical morbidity in pediatric patients compared to logistic regression. A flexible logistic regression model that includes regression splines for continuous variables and statistically significant and clinically meaningful interactions offers improved accuracy, sensitivity, specificity, negative and positive predictive value, and discrimination for predicting surgical morbidity in children compared to a simpler logistic regression model that assumes linearity and additivity. After further validation, the flexible logistic regression model derived in this study could be used to assist with clinical decision-making based on patient-specific surgical risks in pediatric patients.

None declared.

@&#ACKNOWLEDGEMENTS@&#

This study was supported by intramural funding from the Research Institute at Nationwide Children’s Hospital and by Award Number UL1TR001070 from the National Center For Advancing Translational Sciences. However, the study design, analysis and interpretation of data, writing of the manuscript, and decision to publish the manuscript belonged entirely to the authors.

@&#REFERENCES@&#

