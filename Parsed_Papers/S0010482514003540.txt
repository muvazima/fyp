@&#MAIN-TITLE@&#An automated lung segmentation approach using bidirectional chain codes to improve nodule detection accuracy

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel lung segmentation algorithm is proposed, focusing on juxtapleural nodules.


                        
                        
                           
                           A bidirectional chain coding method is proposed to detect border inflection points.


                        
                        
                           
                           A support vector machine classifier is used to selectively smooth the lung border.


                        
                        
                           
                           The method is evaluated on 233 CT studies with 403 juxtapleural nodules.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Lung segmentation

Juxtapleural nodule

Chain code

Support vector machine

Computer aided diagnosis

@&#ABSTRACT@&#


               
               
                  Computer-aided detection and diagnosis (CAD) has been widely investigated to improve radiologists׳ diagnostic accuracy in detecting and characterizing lung disease, as well as to assist with the processing of increasingly sizable volumes of imaging. Lung segmentation is a requisite preprocessing step for most CAD schemes. This paper proposes a parameter-free lung segmentation algorithm with the aim of improving lung nodule detection accuracy, focusing on juxtapleural nodules. A bidirectional chain coding method combined with a support vector machine (SVM) classifier is used to selectively smooth the lung border while minimizing the over-segmentation of adjacent regions. This automated method was tested on 233 computed tomography (CT) studies from the lung imaging database consortium (LIDC), representing 403 juxtapleural nodules. The approach obtained a 92.6% re-inclusion rate. Segmentation accuracy was further validated on 10 randomly selected CT series, finding a 0.3% average over-segmentation ratio and 2.4% under-segmentation rate when compared to manually segmented reference standards done by an expert.
               
            

@&#INTRODUCTION@&#

Computed tomography (CT) is the de facto imaging modality used to diagnose and characterize pulmonary nodules. Compared to conventional chest radiography, CT generates high resolution, volumetric datasets that are able to resolve small and/or low-contrast nodules [1]. However, reading CT images can require the review of a large, often complex volumetric dataset, with the potential to overlook some nodules [4]. In addition, less experienced radiologists may have increased variability in detecting (subtle) lung cancers, as interpretation heavily relies on past experience. In response, computer-aided diagnosis (CAD) [25,30–35] systems have been explored, establishing the potential to improve diagnostic accuracy. Previous studies have shown that CAD increases lung nodule detection rates [44]; decreases false-positive rates [13]; and compensates for deficient reader performance in the detection of the smallest lesions and of nodules without vascular attachment [5]. Indeed, over the past two decades, the ability to accurately and consistently detect lung nodules has been an active area of research in medical image analysis [5,6,15,30–37,46]. The development of automated CAD for lung nodules is now further motivated by the imminent launch of lung cancer screening programs given the findings of the landmark national lung screening trial (NLST), which demonstrated a 20% mortality reduction for individuals with lung cancer who underwent screening using low-dose CT relative to plain chest radiography [2]. Based on this evidence, the United States preventive services task force (USPSTF) recently gave a Grade B recommendation that annual screening for lung cancer with low-dose CT be performed in adults aged 55–80 who have a 30 pack-year (number of packs of cigarettes smoked per day multiplied by the number of years an individual has smoked) smoking history and currently smoke or have quit within the past 15 years [11]. The American Society of Clinical Oncologists (ASCO) suggests similar guidelines [3].

Lung segmentation is an important preprocessing step occurring before nodule detection and the generation of a region of interest (ROI) for subsequent analysis (i.e., the lung field). Pulmonary nodules can be grouped into three categories (
                     Fig. 1): isolated, juxtapleural, and juxtavascular. Isolated and juxtavascular nodules lie within the center of the ROI and are typically segmented without issue. But when lung segmentation fails to correctly define the lung boundaries, juxtapleural (or pleura-connected) nodules can be missed, and normal chest tissue outside of the lung can be included incorrectly as part of the ROI. In point of fact, recent evaluation of a CAD system found that 17% of all true nodules are missed due to poor lung segmentation [7]. Accurate lung segmentation is thus imperative in ensuring accurate CAD system performance: the ability to identify true nodules ultimately sets the upper bound on CAD performance [6].

In this paper, a novel method is proposed for delineating the lung field ROI by automatically segmenting the lung lobe, correcting the border to avoid excluding nodules close to the lung boundary while minimizing possible over-segmentation. The focus of this algorithm is to address issues related to juxtapleural nodules. A bidirectional chain encoding method is used to detect both vertical and horizontal critical point pairs. A support vector machine is then employed to predict whether the concave region formed by a point pair should be corrected based on positional information, concavity rate, and distance information. To test the proposed method, 233 CT scans from the Lung Imaging Database Consortium (LIDC) dataset were used. This paper is divided as follows. Section 2 reviews previous related works, comparing and contrasting our proposed approach with earlier research. In Section 3, the methodology is presented in three steps: image preprocessing, inflection point detection, and border correction. Results of the evaluation are presented in Section 4. Finally, we conclude in Section 5 with a comparison to other works, discussion of the limitations of our method, and future work.

@&#RELATED WORK@&#

While many algorithms [7–10,18–20,23,24,26–29,47] perform automatic lung segmentation of thoracic CT images, only a few explicitly handle juxtapleural nodules; however, evaluation is often lacking [8] or employs a small test set not fully representative of the range of appearance of such nodules (e.g., variations in size, shape). Semi-automated segmentation methods [12] have been previously employed to overcome under-segmentation problems caused by juxtapleural nodules, but the process of having an individual review each study is time consuming and arguably not scalable.

Hu et al. [14] present a fully automated lung segmentation method using combinations of morphological operations to ensure the inclusion of juxtapleural nodules. The effectiveness of the morphological operations is dependent on the shape and the size of the selected structuring element. As juxtapleural nodules vary in size and shape, selecting an optimal size and shape that works well in all cases is difficult. For instance, a smaller sized structuring element will fail to capture larger-sized juxtapleural nodules; conversely, a large structuring element will cause over-segmentation and distortion of the local region. Similarly, a “rolling ball” method has also been used [6,7,15–17], comprising a morphological close operator with a round-shape structuring element. Likewise, the size of the ball is hard to optimize across the variation observed in juxtapleural nodules, as noted in [8,21].

Pu et al. [8] propose a point-wise lung segmentation algorithm, called adaptive border marching (ABM), designed to address juxtapleural nodules. An inclusion criterion is defined based on the ratio between the Euclidean distance of two points on the boundary and the maximum height perpendicular to their connecting line segment. This ratio is used to adaptively adjust the size of a search step for choosing point pairs by comparing itself to a fixed threshold. For all point pair candidates inside one concave region, only the outermost one (which forms the biggest convex hull) is connected (this process is equivalent to the gift-wrapping algorithm [45]). Selecting a threshold parameter to control over-segmentation across all cases is problematic. Varshini et al. [42] extends ABM by merging two lung segmentations obtained using a small threshold and a large threshold separately, but provide no evaluation.

Kim et al. [21] also present a contour-marching method to avoid peripheral nodule exclusion near the lung boundary. This method tracks the lobe boundary to detect suspicious areas with texture features similar to a true nodule. A region growing method is applied to each identified area to re-include it as part of the lung region. Markedly, texture features alone are unable to detect all juxtapleural nodule regions along a boundary. Defining the search window and threshold for region growing method are also inherent challenges to this approach.

Ye et al. [37] use a Freeman chain code to correct the contour of a lung lobe. For all pixels along the boundary, chain codes are used to detect critical points by examining the transition between concave and convex points, determined by a predefined threshold value. All critical point pairs are then connected to form a revised border. Using a preset threshold to define concave/convex regions may not be effective across the natural variation seen in imaging studies and anatomy; and connecting all detected critical point pairs may lead to over-segmentation. Choi et al. [36] detail a similar chain code method, but rather than detecting transitions to find critical points, gradient information is extracted from the chain code and only connecting point pairs whose change in gradient value is below a given threshold value are considered. Both methods detect convexity changes in only the horizontal (or vertical) direction, which is in general not sufficiently robust to correct under-segmentation (as illustrated in Section 3.2). Ko et al. [22] use a curvature-based method to correct the initial lung mask. The curvature for each point on the boundary is calculated to detect a rapid change and a segment is inserted to correct such regions.

As can be seen from the above methods, all rely upon one (or more) predefined parameters (which may vary between CT scans), making algorithm performance sensitive to the normally observed distribution in lung nodule shape and size. Moreover, little validation is given on the effectiveness of the proposed border correction methods on clinical data. Our proposed method, presented subsequently and assessed over a large dataset, eliminates the need for predefined parameters (i.e., it is parameter-free) in order to operate on the full spectrum of nodule sizes/shapes.

@&#METHOD@&#

The proposed method mainly consists of three steps (
                     Fig. 2): (1) preprocessing to generate an initial lung lobe mask using adaptive thresholding (Fig. 2d, e); (2) detecting inflection points (both horizontally and vertically) to obtain all major concave and convex points along the lung lobe boundary (Fig. 2f, g); and (3) correcting the lung boundary border using a support vector machine (SVM) to identify relevant pairwise connections (Fig. 2h) based on extracted features. The details for each step are described as follows.

Preprocessing uses Otsu׳s adaptive thresholding [39] method to automatically obtain an initial lung mask based on the pixel intensity distribution of the input CT image. This method uses discriminate analysis to exhaustively search for a threshold value that minimizes the intra-class variance between two regions of an image. For a given image, let L represent the grey level of all the pixels [1, 2, …, L]. By choosing a threshold at grey level k, the pixels are divided into object class C
                        0 and background class C
                        1. Let w
                        0 and w
                        1 be the probabilities of C
                        0 and C
                        1 separated by a defined threshold and let 
                           
                              
                                 σ
                              
                              
                                 0
                              
                              
                                 2
                              
                           
                         and 
                           
                              
                                 σ
                              
                              
                                 1
                              
                              
                                 2
                              
                           
                         be the variances of these two classes. The intra-class variance is defined as the weighted sum of these two variances:
                           
                              (1)
                              
                                 
                                    
                                       σ
                                    
                                    
                                       I
                                       n
                                       t
                                       r
                                       a
                                    
                                    
                                       2
                                    
                                 
                                 
                                    (
                                    k
                                    )
                                 
                                 =
                                 
                                    
                                       ω
                                    
                                    
                                       0
                                    
                                 
                                 
                                    (
                                    k
                                    )
                                 
                                 ⁎
                                 
                                    
                                       σ
                                    
                                    
                                       0
                                    
                                    
                                       2
                                    
                                 
                                 
                                    (
                                    k
                                    )
                                 
                                 +
                                 
                                    
                                       ω
                                    
                                    
                                       1
                                    
                                 
                                 
                                    (
                                    k
                                    )
                                 
                                 ⁎
                                 
                                    
                                       σ
                                    
                                    
                                       1
                                    
                                    
                                       2
                                    
                                 
                                 
                                    (
                                    k
                                    )
                                 
                              
                           
                        
                     

The optimal threshold T is calculated as the value minimizing 
                           
                              
                                 σ
                              
                              
                                 I
                                 n
                                 t
                                 r
                                 a
                              
                              
                                 2
                              
                           
                           (
                           k
                           )
                        :
                           
                              (2)
                              
                                 T
                                 =
                                 
                                    
                                       
                                       arg
                                       
                                       min
                                       
                                    
                                    
                                       k
                                       ∈
                                       [
                                       1
                                       ,
                                       
                                       L
                                       ]
                                    
                                 
                                 
                                 
                                    
                                       σ
                                    
                                    
                                       I
                                       n
                                       t
                                       r
                                       a
                                    
                                    
                                       2
                                    
                                 
                                 
                                    (
                                    k
                                    )
                                 
                              
                           
                        
                     

After thresholding, a flood filling method combined with 3D labeling is adopted to produce an initial lung lobe mask (
                        Fig. 3b–e). During initial segmentation testing, the CT imaging studies were found to have extremely low background pixel values (Fig. 3a) that formed a peak pattern (Fig. 3b) influencing the optimal threshold calculation. The calculated optimal threshold will not be able to differentiate the lung region from the background due to the influence of this peak pattern, leading to segmentation failure. Therefore, background pixel values are removed before calculating the intra-class variance.

Preprocessing generates a binary mask of the lung lobe region. To selectively revise the initial lung segmentation to re-include juxtapleural nodules, the boundary is first characterized using a bidirectional differential chain (BDC) encoding method to help identify inflection points. Inflection points are defined as the points where the convexity of the boundary changes. Concavities are then detected based on these inflection points. This step maximizes the sensitivity in detecting areas with juxtapleural nodules. A process for selecting critical point pairs is then followed to reduce the false positives and minimize over-segmentation.

The original application of chain codes was for lossless compression of grey-scale images [40]. The basic principle is to separately encode the boundary coordinates (chains of pixels) for each connected component in an image. The chain is a sequence of direction codes from one pixel to the adjacent one. There are eight possible directions between two adjacent pixels. The code word c(i) for a BDC is the number corresponding to the direction from one pixel (i) to the next (i+1) in a chain, c(i)∈{0, 1, −1}, where i represents the index value for the pixel. The assigned code word for each direction is based on the encoding coordinate system (
                        Fig. 4). To detect both horizontal and vertical inflection points, this method uses two different coordinate systems for horizontal and vertical encoding. The detection of the inflection points from the BDC encoding proceeds based on the following steps:
                           
                              1.
                              
                                 Initial boundary generation. The lung lobe boundary pixels are extracted from the binary mask for the left and right lobes, separately.


                                 Boundary encoding. Per lobe, both vertical and horizontal code words are obtained using the corresponding encoding coordinate systems. The encoder moves along the boundary following a (counter)clockwise path, and at each step the direction of this movement is transformed into a code word. The encoding process is illustrated in Fig. 4:
                                    
                                       a.
                                       
                                          Horizontal code word generation. Fig. 4b–d depicts the process of generating horizontal code words. In Fig. 4b, the blue boxes represent pixels along the lung lobe boundary. If A is the starting point, the encoder moves along the boundary in a clockwise way.


                                          Arrow map generation. An arrow map is generated to represent the direction that the encoder moves. For instance, in Fig. 4c the encoder moves in the northwest direction when traversing from point A to B.


                                          Code word assignment. A code word is assigned to each arrow according to the encoding coordinate system given in Fig. 4a. As shown in Fig. 4d, the arrow that points in the northwesterly direction from A to B is assigned a code word of ‘1’ based on the encoding coordinate system.


                                          Vertical code word generation. The vertical code word is generated in a similar manner, but using a vertical encoding coordinate system (Fig. 4f). Fig. 4g–i depicts the process of generating vertical code words.


                                 Inflection point calculation. A differential operation is used to generate the horizontal and vertical differential chain codes, separately. Non-zero points in the differential chain are identified as inflection points. As presented in Fig. 4e and j, the differential code is calculated using a clockwise differential operation based on the generated code words (i.e., from Steps 2a, 2d). For instance, the differential code at point A is 0; the differential code at D is −2. As can be seen, pixels D and E are the only points with non-zero differential codes; therefore, D is detected as a horizontal inflection point and E is detected as a vertical inflection point.

To overcome the influence of the small perturbations in the lobe boundary, a seven-tap Gaussian low-pass filter [40] is applied to smooth code words prior to the inflection point calculations in Step 3. An operator is then applied to round the smoothed code word to the nearest integer (0, 1, or −1). 
                        Fig. 5 gives an example of the detected inflection points for a right lung lobe using the proposed method with and without the low-pass filter. Fig. 5b shows the detected inflection points on the right lung lobe boundary without applying a low-pass filter. The white circles on the boundary represent the vertical inflection points, and the yellow squares represent the horizontal inflection points. Fig. 5b has many more inflection points that add unnecessary noise to the inflection detection process. After applying the Gaussian low-pass filter (Fig. 5c), only the remaining points are deemed inflection points.


                        
                        Fig. 6 illustrates the effectiveness of the vertical and horizontal inflection point detection process. Fig. 6a shows an original CT slice, and the yellow contour in Fig. 6a, b indicates the nodule in the right lung that has been manually outlined by one radiologist. After preprocessing, the segmented right lung lobe ROI does not include this nodule region; Fig. 6c illustrates the under-segmentation problem, from which it can be observed that the convexity changes in the nodule region along the boundary are vertical. Using our approach, the detected critical horizontal and vertical inflection points are shown in Fig. 6d–f using yellow squares and white circles, respectively. By comparing Fig. 6d–f, the nodule region is only captured by vertical inflection points, corresponding to the earlier observation. This observation suggests that detecting both vertical and horizontal changes simultaneously are necessary to robustly correct for under-segmentation. The final segmentation result (after border correction described in the next section) is shown in Fig. 6g.

Rather than connect all inflection point pairs, only critical point pairs are connected to correct the boundary, thereby minimizing over-segmentation. Three features are used to select critical point pairs: boundary segment concave degree, relative boundary distance, and relative position information.

Let EuclideanDistance(A,B) represent the Euclidean distance between two inflection points, A and B. Let SegmentLength(A,B) represent the shortest boundary segment length between these two points. As shown in 
                        Fig. 7a, ED represents EuclideanDistance(A,B) and SL the Segment length (A,B). Let BoundaryLength be the total length of the lung lobe under consideration. The concave feature, 
                           
                              
                                 f
                              
                              
                                 c
                                 o
                                 n
                                 c
                                 a
                                 v
                                 e
                              
                           
                        , and the length feature, 
                           
                              
                                 f
                              
                              
                                 l
                                 e
                                 n
                                 g
                                 t
                                 h
                              
                           
                        , are defined as
                           
                              (3)
                              
                                 
                                    
                                       f
                                    
                                    
                                       c
                                       o
                                       n
                                       c
                                       a
                                       v
                                       e
                                    
                                 
                                 =
                                 
                                    
                                       S
                                       e
                                       g
                                       m
                                       e
                                       n
                                       t
                                       L
                                       e
                                       n
                                       g
                                       t
                                       h
                                       (
                                       A
                                       ,
                                       B
                                       )
                                    
                                    
                                       E
                                       u
                                       c
                                       l
                                       i
                                       d
                                       e
                                       a
                                       n
                                       D
                                       i
                                       s
                                       t
                                       a
                                       n
                                       c
                                       e
                                       (
                                       A
                                       ,
                                       B
                                       )
                                    
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    
                                       f
                                    
                                    
                                       l
                                       e
                                       n
                                       g
                                       t
                                       h
                                    
                                 
                                 =
                                 
                                    
                                       S
                                       e
                                       g
                                       m
                                       e
                                       n
                                       t
                                       L
                                       e
                                       n
                                       g
                                       t
                                       h
                                       (
                                       A
                                       ,
                                       B
                                       )
                                    
                                    
                                       B
                                       o
                                       u
                                       n
                                       d
                                       a
                                       r
                                       y
                                       L
                                       e
                                       n
                                       g
                                       t
                                       h
                                    
                                 
                              
                           
                        
                     

From Eq. (3), f
                        
                           concave
                         increases as the boundary segment increases given a fixed geometric distance, which indicates a larger degree of concavity (Fig. 7a). This observation implies that critical points will have larger values of f
                        
                           concave
                        . In Eq. (4), f
                        
                           length
                         increases as the boundary segment increases for a given lung lobe (with fixed total boundary length). The ratio (i.e., f
                        
                           length
                        ) should be smaller to avoid over-segmentation. By way of illustration, a large f
                        
                           length
                         is shown in Fig. 7b, where connecting the two points will cause significant over-segmentation. A third feature, f
                        
                           position
                        , indicates the relative position information of the point pair, and is defined as
                           
                              (5)
                              
                                 
                                    
                                       f
                                    
                                    
                                       p
                                       o
                                       s
                                       i
                                       t
                                       i
                                       o
                                       n
                                    
                                 
                                 =
                                 
                                    
                                       E
                                       u
                                       c
                                       l
                                       i
                                       d
                                       e
                                       a
                                       n
                                       D
                                       i
                                       s
                                       t
                                       a
                                       n
                                       c
                                       e
                                       (
                                       M
                                       i
                                       d
                                       P
                                       o
                                       i
                                       n
                                       t
                                       
                                          (
                                          
                                             A
                                             ,
                                             B
                                          
                                          )
                                       
                                       ,
                                       C
                                       e
                                       n
                                       t
                                       r
                                       a
                                       l
                                       P
                                       o
                                       i
                                       n
                                       t
                                       )
                                    
                                    
                                       A
                                       v
                                       e
                                       r
                                       a
                                       g
                                       e
                                       D
                                       i
                                       s
                                       t
                                       a
                                       n
                                       c
                                       e
                                       2
                                       C
                                       e
                                       n
                                       t
                                       r
                                       a
                                       l
                                       P
                                       o
                                       i
                                       n
                                       t
                                    
                                 
                              
                           
                        where MidPoint(A,B) is the midpoint between two inflection points, A and B; and CentralPoint is the center of two lung lobe regions. AverageDistance2CentralPoint is the average of all distances from lung lobe boundaries to the center.

Based on these three features, an SVM classifier is used to identify critical point pairs (instead of a threshold value or parameter). SVMs are supervised, non-parametric learning models that perform efficient non-linear classification tasks. SVMs map their inputs into higher dimension feature space to separate categories based on decision boundaries learned through training data. To train the SVM, 172 point pairs were manually selected from 42 LIDC studies, labeled as being positive examples (n=91, point pairs that capture a concave region of juxtapleural nodule) or negative examples (n=81, point pairs that capture a non-lung-tissue region). Finally, point pairs classified as critical are connected, resulting in the lung boundary. In our experiment, a three order polynomial kernel is chosen for the SVM classifier and a 10-fold cross validation is applied to assess the model performance using different subsets of features. Cross validation indicates that the highest accuracy (97.7%) is achieved when all three features are employed in the training task. It is also shown that 
                           
                              
                                 f
                              
                              
                                 c
                                 o
                                 n
                                 c
                                 a
                                 v
                                 e
                              
                           
                         and 
                           
                              
                                 f
                              
                              
                                 l
                                 e
                                 n
                                 g
                                 t
                                 h
                              
                           
                         are more important compared to 
                           
                              
                                 f
                              
                              
                                 p
                                 o
                                 s
                                 i
                                 t
                                 i
                                 o
                                 n
                              
                           
                        . This last observation results from the fact that 
                           
                              
                                 f
                              
                              
                                 c
                                 o
                                 n
                                 c
                                 a
                                 v
                                 e
                              
                           
                         provides important information that increases sensitivity as the inflection point pairs of a juxtapleural nodule usually have larger 
                           
                              
                                 f
                              
                              
                                 c
                                 o
                                 n
                                 c
                                 a
                                 v
                                 e
                              
                           
                         value. 
                           
                              
                                 f
                              
                              
                                 l
                                 e
                                 n
                                 g
                                 t
                                 h
                              
                           
                         helps to reduce false positive rate due to the limited possible size of juxtapleural nodules. To generalize for different datasets, training samples should be collected to retrain the classifier. Positive training data should be collected mainly for the juxtapleural nodules and a small portion of vessels that attach to the lung wall. Negative training data should comprise non-lung-tissue regions with a large concave rate and a moderate 
                           
                              
                                 f
                              
                              
                                 l
                                 e
                                 n
                                 g
                                 t
                                 h
                              
                           
                         value (as these point pairs can easily become false positives).

@&#RESULTS@&#

The proposed method was validated using data from LIDC [38], available through The Cancer Imaging Archive (TCIA). LIDC comprises thoracic imaging studies gathered from five sites across the United States; for each contributed study, four experienced radiologists drew complete outlines for all nodules between 3 and 30mm in diameter. As part of the creation of the LIDC dataset, a two-step review process was conducted to establish ground truth annotations [41].

275 Studies with at least one juxtapleural nodule were identified from LIDC by a trained graduate student. The entire set of 275 CT scans were divided into two subsets: 42 studies for training of the SVM; and 233 studies for testing. A total of 406 juxtapleural nodules were found in the test set, serving as the basis for evaluating the method׳s ability to correctly include juxtapleural nodules in the lung lobe region (i.e., re-inclusion rate [8]). Additionally, 10 CT studies were randomly selected from the test set and the lung contours were manually segmented under the guidance of a practicing thoracic radiologist. The results of the manually segmented contours were used as references to validate overall segmentation accuracy. To aid in the manual segmentation task, an annotation tool was developed to enable the radiologist to automatically generate lung lobe contours first by thresholding, and then correcting any inaccuracies in the contour by adjusting the boundary.

Five metrics were used to measure segmentation performance: (1) the re-inclusion ratio of juxtapleural nodules; (2) the over-segmentation rate; (3) the under-segmentation rate; (4) the volumetric overlap error ratio; and (5) the cumulative error distance distribution [13,8]. The re-inclusion ratio is used to assess per nodule sensitivity. Similar to [8], a trained graduate student was tasked with reviewing each study to determine if a juxtapleural nodule was correctly included (or not) by identifying errors in the segmentation caused by juxtapleural nodules. For voxel-based segmentation accuracy, the volumetric overlap ratio difference, over-segmentation, and under-segmentation rates were computed to characterize differences between boundaries generated by the proposed approach and the reference boundary generated by the manual annotator.

Over-segmentation rate is defined as the number of voxels in a segmented image region that are included as part of the ROI but that are not in the reference standard [8]. Let V
                        
                           auto
                         represent the volume of the binary mask generated using our approach and let V
                        
                           reference
                         be the volume of the reference standard. The over-segmentation rate OR(V
                        
                           auto
                        ,V
                        
                           reference
                        ) is
                           
                              (6)
                              
                                 O
                                 R
                                 
                                    (
                                    
                                       
                                          
                                             V
                                          
                                          
                                             a
                                             u
                                             t
                                             o
                                          
                                       
                                       ,
                                       
                                          
                                             V
                                          
                                          
                                             r
                                             e
                                             f
                                             e
                                             r
                                             e
                                             n
                                             c
                                             e
                                          
                                       
                                    
                                    )
                                 
                                 =
                                 
                                    |
                                    
                                       
                                          
                                             
                                                
                                                   V
                                                
                                                
                                                   a
                                                   u
                                                   t
                                                   o
                                                
                                             
                                             \
                                             
                                                
                                                   V
                                                
                                                
                                                   m
                                                   a
                                                   n
                                                   u
                                                   a
                                                   l
                                                
                                             
                                          
                                          
                                             
                                                
                                                   V
                                                
                                                
                                                   m
                                                   a
                                                   n
                                                   u
                                                   a
                                                   l
                                                
                                             
                                          
                                       
                                    
                                    |
                                 
                              
                           
                        where 
                           
                              
                                 V
                              
                              
                                 a
                                 u
                                 t
                                 o
                              
                           
                           \
                           
                              
                                 V
                              
                              
                                 m
                                 a
                                 n
                                 u
                                 a
                                 l
                              
                           
                         represents the relative complement of 
                           
                              
                                 V
                              
                              
                                 a
                                 u
                                 t
                                 o
                              
                           
                         in 
                           
                              
                                 V
                              
                              
                                 m
                                 a
                                 n
                                 u
                                 a
                                 l
                              
                           
                        . Similarly, the under-segmentation rate 
                           U
                           R
                           
                              (
                              
                                 
                                    
                                       V
                                    
                                    
                                       a
                                       u
                                       t
                                       o
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       V
                                    
                                    
                                       r
                                       e
                                       f
                                       e
                                       r
                                       e
                                       n
                                       c
                                       e
                                    
                                 
                              
                              )
                           
                         is defined as the relative lung volume amount that is regarded as lung tissue in the reference standard but not in our method:
                           
                              (7)
                              
                                 O
                                 R
                                 
                                    (
                                    
                                       
                                          
                                             V
                                          
                                          
                                             a
                                             u
                                             t
                                             o
                                          
                                       
                                       ,
                                       
                                          
                                             V
                                          
                                          
                                             r
                                             e
                                             f
                                             e
                                             r
                                             e
                                             n
                                             c
                                             e
                                          
                                       
                                    
                                    )
                                 
                                 =
                                 
                                    |
                                    
                                       
                                          
                                             
                                                
                                                   V
                                                
                                                
                                                   m
                                                   a
                                                   n
                                                   u
                                                   a
                                                   l
                                                
                                             
                                             \
                                             
                                                
                                                   V
                                                
                                                
                                                   a
                                                   u
                                                   t
                                                   o
                                                
                                             
                                          
                                          
                                             
                                                
                                                   V
                                                
                                                
                                                   m
                                                   a
                                                   n
                                                   u
                                                   a
                                                   l
                                                
                                             
                                          
                                       
                                    
                                    |
                                 
                              
                           
                        
                     

The volumetric overlap ratio measures the relative overlap between the two binary segmentation masks by computing the two volumes׳ intersection divided by their union [29]:
                           
                              (8)
                              
                                 R
                                 
                                    (
                                    
                                       
                                          
                                             V
                                          
                                          
                                             a
                                             u
                                             t
                                             o
                                          
                                       
                                       ,
                                       
                                       
                                          
                                             V
                                          
                                          
                                             m
                                             a
                                             n
                                             u
                                             a
                                             l
                                          
                                       
                                    
                                    )
                                 
                                 =
                                 
                                    |
                                    
                                       
                                          
                                             
                                                
                                                   V
                                                
                                                
                                                   a
                                                   u
                                                   t
                                                   o
                                                
                                             
                                             ∩
                                             
                                                
                                                   V
                                                
                                                
                                                   m
                                                   a
                                                   n
                                                   u
                                                   a
                                                   l
                                                
                                             
                                          
                                          
                                             
                                                
                                                   V
                                                
                                                
                                                   a
                                                   u
                                                   t
                                                   o
                                                
                                             
                                             ∪
                                             
                                                
                                                   V
                                                
                                                
                                                   m
                                                   a
                                                   n
                                                   u
                                                   a
                                                   l
                                                
                                             
                                          
                                       
                                    
                                    |
                                 
                              
                           
                        
                     

Lastly, to make the overlap ratio measurement consistent with the over-segmentation and under-segmentation rates, the volumetric overlap error ratio (
                           D
                           R
                           
                              (
                              
                                 
                                    
                                       V
                                    
                                    
                                       a
                                       u
                                       t
                                       o
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       V
                                    
                                    
                                       m
                                       a
                                       n
                                       u
                                       a
                                       l
                                    
                                 
                              
                              )
                           
                        ) is used:
                           
                              (9)
                              
                                 D
                                 R
                                 
                                    (
                                    
                                       
                                          
                                             V
                                          
                                          
                                             a
                                             u
                                             t
                                             o
                                          
                                       
                                       ,
                                       
                                          
                                             V
                                          
                                          
                                             m
                                             a
                                             n
                                             u
                                             a
                                             l
                                          
                                       
                                    
                                    )
                                 
                                 =
                                 1
                                 −
                                 R
                                 
                                    (
                                    
                                       
                                          
                                             V
                                          
                                          
                                             a
                                             u
                                             t
                                             o
                                          
                                       
                                       ,
                                       
                                          
                                             V
                                          
                                          
                                             m
                                             a
                                             n
                                             u
                                             a
                                             l
                                          
                                       
                                    
                                    )
                                 
                              
                           
                        
                     

To measure the spatial similarity between the lung boundaries generated by our approach and that of the reference standard, the cumulative error distance distribution [8] is computed to provide a global statistical measurement of the fitting between the lung surfaces generated by our method and the lung surfaces in the reference standard. The shortest distance between a point on the lung surface obtained by our algorithm and the lung surface of the reference standard is used to generate the error distance distribution.

@&#RESULTS@&#

Using the 233 test studies from LIDC dataset, our experiment shows that 373 juxtapleural nodules out of total 406 juxtapleural nodules were correctly included as part of the ROI, achieving a 92.6% inclusion rate. After an error analysis, 83.3% of the missing juxtapleural nodules were found sitting in between segments of lung tissues, as shown in 
                        Fig. 8. In this situation, the proposed method fails because each segment is processed separately.


                        
                        Fig. 9 shows the volume-based segmentation error as assessed by over-segmentation ratio, under-segmentation ratio, and overlap ratio difference. The average over-segmentation rate is 0.3%, while the average under-segmentation ratio is 2.4% and the average overlap ratio difference is 2.7%. 
                        Fig. 10 shows the cumulative error distance distribution to assess the border positioning accuracy. The error bars in Fig. 10 represent the standard deviation corresponding to each distance. 93% and 96% lung surfaces obtained by the proposed method are within 2–3mm of the reference standard, respectively. The largest error distance is 22.5mm. Relatively larger under-segmentation and error distances are mainly due to the presence of atelectasis (
                        Fig. 11a, b) or consolidation (Fig. 11c, d).

@&#DISCUSSION@&#

Although many methods have been developed to perform automatic lung segmentation, only a few explicitly handle juxtapleural nodules and evaluate the method on actual patient data. 
                        Table 1 compares the proposed method to other lung segmentation algorithms that handle juxtapleural nodules. Our method achieves better average overlap ratio compared to Wei׳s [47] method. This difference is attributed to the fact that our approach implements a point pairs selection technique, which reduces the risk of over-segmentation. Our method has similar average over-segmentation ratio and average under-segmentation ratio compared to Pu׳s [8] method. Although both Pu [8] and Wei [47] report a 100% re-inclusion rate, their test sets contain a limited set of juxtapleural nodules, 67 and 32 respectively, compared to our set of 406 nodules. Similar to the limitation of our approach in detecting missing nodules that are between lung segments, Pu׳s and Wei׳s methods also process each isolated lung region separately and thus will likely fail in similar situations. One should note that the performance of the oft-cited rolling ball method is highly dependent on the specified parameters, which are not consistently included in publications [8]. For example, Kim [21] highlights the difficulty in selecting the appropriate fixed ball radius because of the large variance in juxtapleural nodule sizes. Therefore, a comparison of our method against algorithms that utilize the rolling-ball method would be inconclusive, given that the original implementation cannot be replicated effectively. Also, Stelmo et al. [43] point out that a fair comparison between two methods would only be possible if the works use the same images and acquisition standards (resolution, bits per pixel, etc.); given the unavailability of others׳ test data, such a formal comparison is not possible. But in general, and as compared to these other methods, our described method can accurately segment the lung tissues while robustly and correctly including the juxtapleural nodules.

A seven-tap Gaussian low-pass filter is employed to smooth the chain code to overcome the influence of the small perturbations in the lobe boundary as described in Section 3.2. This step is used to reduce computational complexity as fewer inflection point pairs are inputted to the SVM classifier afterwards. Removing this step will not reduce the re-inclusion rate of the juxtapleural nodules as the classifier would still identify the same points for re-inclusion but would need to consider more points. In our experiment, a Gaussian kernel with variance equal to two is employed to perform the smoothing task and a 92.6% re-inclusion rate is achieved. By examining the failure cases in our experiment, we found out that none of the failure cases were caused due to inflection point detection failure. As such, we believe the adoption of this Gaussian low-pass filter did not include additional errors. However, to adapt this approach to perform on other datasets, the degree of smoothing strength may be adjusted based on preference and dataset standards.

@&#CONCLUSION@&#

Lung segmentation is an important precursor to many quantitative analysis applications for pulmonary disease. While a large number of lung segmentation methods have been proposed in the past few years, we present a novel approach to segment the lung using a bidirectional differential chain code combined with a machine learning framework. An evaluation involving 233 CT studies containing 403 juxtapleural nodules and manual annotations of a sample of 10 cases by a radiologist has been conducted to demonstrate the effectiveness of our method. The results show that our method is able to correctly include the juxtapleural nodules into the lung tissue while minimizing over and under-segmentation.

The average computation time of the proposed method is 0.53s per CT slice tested on MATLAB implemented software using a laptop with Intel Core i7 3GHz and 8 GB RAM. One limitation of the proposed method is that it sometimes fails to re-include the juxtapleural nodules sitting in consolidation regions (between lung tissue segments); to overcome this problem, future work will integrate some region connection techniques as a precursor step for detected consolidation regions before border correction. The proposed lung segmentation approach is being explored as part of a novel computer-aided lung nodule detection pipeline for lung cancer screening. The screening context presents additional challenges such as the presence of smaller nodules and reduced image quality (grainier appearance of images due to the low-dose nature of image acquisitions). Preliminary exploration of low-dose studies have shown that our proposed approach is capable of handling these issues. In addition, the method is generalizable to any tasks that involve concave/convex detection. For example, in magnetic resonance angiography, detection of concave/convex regions may be able to identify and accurately segment incidental aneurysms to assess their risk of rupture.

@&#SUMMARY@&#

Computer-aided detection and diagnosis (CAD) has been widely investigated to improve radiologists׳ diagnostic accuracy in detecting and characterizing lung disease, as well as to assist with the processing of increasingly sizable volumes of imaging. Lung segmentation is a requisite preprocessing step for most CAD schemes. This paper proposes a parameter-free lung segmentation algorithm with the aim of improving lung nodule detection accuracy, focusing on juxtapleural nodules. A bidirectional chain coding method combined with a support vector machine (SVM) classifier is used to selectively smooth the lung border while minimizing the over-segmentation of adjacent regions. This automated method was tested on 233 computed tomography (CT) studies from the lung imaging database consortium (LIDC), representing 403 juxtapleural nodules. The approach obtained a 92.6% re-inclusion rate. Segmentation accuracy was further validated on 10 randomly selected CT series, finding a 0.3% average over-segmentation ratio and 2.4% under-segmentation rate when compared to manually segmented reference standards done by an expert.

@&#ACKNOWLEDGEMENTS@&#

The authors would like to acknowledge Dr. Sutida Singharuksa for her assistance in the evaluation of the proposed method. This research is supported in part by the Center for Domain-Specific Computing (CDSC) funded by the NSF Expedition in Computing Award CCF-0926127.

@&#REFERENCES@&#

