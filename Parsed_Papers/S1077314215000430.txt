@&#MAIN-TITLE@&#Light field distortion feature for transparent object classification

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We introduce the light field camera for transparent object classification.


                        
                        
                           
                           We model the distortion of the light field caused by a transparent object.


                        
                        
                           
                           Background-invariant light field distortion (LFD) feature is proposed.


                        
                        
                           
                           Classification for a transparent object can be done by a single light field image.


                        
                        
                           
                           Experimental results show our method works well under various conditions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Light field

Object classification

Distortion feature

Bag of features

@&#ABSTRACT@&#


               
               
                  Local features, such as scale-invariant feature transform (SIFT) and speeded up robust features (SURF), are widely used for describing an object in the applications of visual object recognition and classification. However, these approaches cannot apply to transparent objects made of glass or plastic, as such objects take on the visual features of background scenes, and the appearance of such objects dramatically varies with changes in the scenes. Indeed, transparent objects have the unique characteristic of distorting the background by refraction. In this paper, we use a single-shot light field image as input and model the distortion of the light field caused by the refractive property of a transparent object. We propose a new feature which is called the light field distortion (LFD) feature. The proposed feature is background-invariant so that it is able to describe a transparent object without knowing the texture of the scene. The proposal incorporates this LFD feature into the bag-of-features approach for classifying transparent objects. We evaluated its performance and analyzed the limitations in various settings.
               
            

@&#INTRODUCTION@&#

Visual object recognition and classification are useful and important to robotics and computer vision applications. Statistical learning methods such as bag-of-features (BoF) are very active currently in the research of image annotation and object recognition. These methods commonly use local features, such as scale-invariant feature transform (SIFT) [1] and speeded up robust features (SURF) [2], for visual object recognition. However, these features and learning algorithms cannot apply to transparent objects. Our daily environments, kitchen, living room, and office, are filled with many transparent objects, such as glasses, bottles, bowls, jars, vases, and windows, to name a few. The appearance of a transparent object is highly depending on the backgrounds, and it drastically changes when the object moves or background varies. The transparent objects do not have features entirely of their own, but rather transmitted background images. Using the standard approaches requires modeling local features not of the transparent object but the background. In this paper, we propose a novel object feature that can describe a transparent object in an image, and we call it light field distortion (LFD) feature. We discuss how the LFD feature models and visually classifies transparent objects, which to date have been ignored as exceptions in applications of visual object classification or annotation.

Transparent objects are made of refractive materials, such as glass or plastics, and distort rays emanating from the background scene. Different objects produce different distortions, each carrying intrinsic characteristics of the transparent object, namely the refractive index of material and the shape of object, both of which influence the distortion. The LFD feature is calculated from a light field image, a single shot captured by a light field camera, and describes this distortion of the field caused by the refraction in transparent object. Compared with conventional cameras, which capture 2D images from a single perspective, light field cameras obtain richer 4D images that include both the angular and positional information of the light ray space. The LFD feature models the distortion from differences in corresponding points between viewpoints in the 4D light field, whereas common features, such as gradients or edges, model the appearance. This is an entirely original concept for feature description with the advantage that LFD is less affected by background changes, as it uses patterns of ray distortions caused by the transparent objects, not patterns from the appearance of objects.

The light field camera was originally proposed for image-based rendering for use in the graphics community, and has been used for a variety of different visualization applications, such as generating free-view images, 3D graphics, and digital refocusing. Early light field camera systems made by multiple cameras, e.g. Stanford multi-camera array [3], were usually huge and quite expensive. Fortunately, the latest light field cameras consisting of a micro-lens array between the sensor and main lens are becoming inexpensive and compact [4,5], some of these are available in the commercial market [6–8]. We can also emulate a light field camera by moving a conventional camera in a plane, in order to capture the light field image when the light field camera is not available. But this method can only capture the static scenes while real light field cameras can capture dynamic scene by one shot. Because the light field image records richer information than that captured by conventional cameras, and the equipment is easy to get nowadays, we believe that light field is useful in computer vision applications, and such kind of applications are becoming popular. In our task, we need relatively large disparities to describe the distortion of the backgrounds, and the system can be working under dynamic scenes. Therefore, relatively large baseline is preferred, and we use a camera array as input device in our experiments.

The contribution of this paper is (1) in tackling a difficult computer vision problem, transparent object classification with a single-shot image, (2) in proposing a new feature, called the LFD feature, which can describe a transparent object, and (3) in applying a light field image to object classification application. We implemented our method based on the BoF approach and performed laboratory and real experiments, using eighteen objects and various different backgrounds, to assess the effectiveness of the LFD features. Our result shows that the LFD feature can classify transparent objects without explicit physics-based refraction analysis and refraction models.

This paper is the extended version of our previous conference publication [9]. We describe the detail of the background filtering technique in Section 4. The dataset for performance evaluation is enlarged from 7 objects with 5 backgrounds to 18 objects with 10 backgrounds. We utilize an optical flow algorithm with more accuracy [10] for tracking the corresponding points, and get better classification performance for the larger dataset. Moreover, we explain how to get the optimal parameters, and analyze how the rotation in different directions, the number of the viewpoints, and the noise affect the classification accuracy.

@&#RELATED WORK@&#

In recent years, the BoF-based approach has been attracting much attention in the research of image annotation and object recognition. Local features, such as SIFT, are widely used owing to their invariance to scaling, rotation and illumination [11–13]. Local features are divided into several clusters and a representative feature in each cluster is assigned by vector quantization. Objects in the same category are expected to have similar frequency within this representative feature. This approach implicitly assumes that the majority of local features are extracted from an object’s surface rather than the background. Therefore, if local features are drawn from a more dominant background than an object’s surface, existing learning and recognition methods perform poorly. A transparent object yields less information about its appearance. Its actual appearance depends largely on the visible background as viewed through the object. In consequence, extracting background-independent local features from a transparent object area is difficult. Thus, these approaches find local transparent structure by applying a latent factor model before quantizing into a visual word representation [14]. Although such approaches recognize a transparent object without any knowledge of background scenes at test time, the learning step requires many training images in which the transparent object is captured under various environments.

From a different perspective, there has been much research on measuring refraction responses in transparent objects using cameras to obtain physical parameters, such as surface curvature or refractive index. It is well known that refraction polarizes light. Miyazaki et al. measured light intensities from transparent objects through polarizing filters [15,16]. Schlieren photography [17,18] has also been used for fluid, gas flows, and shock wave analysis. This method visualizes the refraction response in a scene as a gray-scale or color image by using special optics, although it requires high-quality optics and precise alignment. Hence, its applicability is restricted to laboratory environments, and not for common practical use. Multiple video cameras are used to build passive tomography system to measure turbulence strength of the invisible fluid [19]. Underwater cameras are often suffer from the distortion caused by random waves in the water–air interface. A special sensor is designed to deal with such kind of distortion for the underwater imaging [20].

Recent years, light field plays an important role in reconstruction of transparent surfaces. Wetzstein et al. [21] proposed light-field background-oriented Schlieren photography that obtains Schlieren photos using a common hand-held camera and a special-purpose optical sheet which is called light field probe (LF-probe). By using this technique, the transparent surface can be reconstructed [22]. Similarly, Ji et al. [23] also utilized the LF-probe and multiple viewpoints to reconstruct the invisible gas flow. Although this technique can reconstruct the transparent surface and invisible gas flow, it also has restricted practical use as the LF-probe is always required as a background object. Ding et al. [24] proposed a method to acquire the dynamic 3D fluid surfaces. They used camera array as the capture device and checkerboard pattern as the background. Ye et al. [25] proposed an approach to acquire the dynamic 3D fluid surfaces with a single camera, but they utilized a special background which is called Bokode to emulates a pinhole projector for capturing ray–ray correspondences.

Approaches, similar to our own, obtain shape from optical flow caused by refraction. In particular, Ben-Ezra and Nayar [26] proposed a model-based method to recover shape and pose from video taken with known camera motions. Similarly, Aagrwal et al. [27] recovered shape from video acquired while the background behind the object moves. Morris and Kutulakos [28] used two calibrated cameras to estimate the refractive indices over time-varying liquid surfaces from distortions of known grid patterns at the bottom of a tank. In contrast to these approaches, the novelty of our work is to apply refraction to transparent object classification, realized from a single shot image, using a light field camera as an input device. Unlike previous methods, there are no constraints on background texture, camera motion or known parameters.

A transparent object can deform the background scene by refraction. Because refraction by objects is affected by shape and refractive index, different objects produce different images of the same scene as shown in Fig. 1
                     . We utilize the background distortion caused by refraction to classify different types of transparent objects in our proposed method. In fact, we modeled the background distortion to the appearance difference from different perspectives (Fig. 2
                     ). Theocratically, the modeled distortion itself is independent of background texture. Although the background determines image appearance, the distortion for corresponding points from different viewpoints is maintained. Therefore, our proposal is to model the refraction of transparent objects as a distortion of multiple viewpoints captured by the light field camera. In this section, we define the LFD feature and outline its use in transparent object classification. The light field is a function that describes the amount of light emitting in every direction from every point in a scene. Conventional cameras integrate light field along the angular domain, and lose the information about the light distribution entering from the world [4]. In contrast, a light field camera obtains a 4D light field image which includes both the angular and positional information of the light rays. There are various representations of the light field. Here, we use the 4D-ray representation of the light field L(s, t, u, v) determined by the intersection of two parallel planes, the viewpoint plane (s, t) and the image plane (u, v) (see Fig. 3
                     ). Fig. 4
                      illustrates the functioning of a camera array and shows the relation between light field and phase space representations. Fig. 4 shows only a 2D slice of the light field and phase space for ease in understanding.
                  


                     Fig. 4(a) depicts a scene where there is no object between background and camera; i.e., light propagates in free-space with no refraction, reflection, scattering, or absorption. As illustrated, if rays emitted from a point in the background are straight, the observed light field has constant disparities over the images for the different viewpoints. The rays from the same point are distributed on a line in the su-phase space (Fig. 4(a)), and the slope of the line depends upon the distance between camera and background. In fact, these rays are distributed on a hyperplane in stuv-space because the actual light field and phase space is in a 4D space. In contrast, if a transparent object intervenes between background and camera, the ray distribution deviates from the line or the hyperplane (Fig. 4(b)). This LFD is caused by refraction occurring within the transparent object, which is characterized by the material (refractive index) and the shape. We call this LFD feature in that it is to be used as a feature in transparent object classification.

Here, we denote an arbitrary point in the image taken from the center viewpoints (0, 0) as 
                        
                           
                              p
                              
                                 0
                                 ,
                                 0
                              
                           
                           =
                           
                              (
                              u
                              ,
                              v
                              )
                           
                        
                      and the corresponding point in the image taken from another viewpoint (s, t) as 
                        
                           
                              p
                              
                                 s
                                 ,
                                 t
                              
                           
                           =
                           
                              (
                              
                                 
                                    u
                                 
                                 ′
                              
                              ,
                              
                                 
                                    v
                                 
                                 ′
                              
                              )
                           
                        
                     . Actually, we denote the ith feature point (i
                     =0,…,
                     N
                     −1) and the corresponding points as p
                     0, 0(i) and p
                     
                        s, t
                     (i) respectively, with the image having N feature points. To make the LFD feature independent of the position of the point (u, v), we use relative differences defined by the following expression,

                        
                           (1)
                           
                              
                                 
                                    
                                       
                                          
                                             Δ
                                          
                                          
                                             p
                                             
                                                s
                                                ,
                                                t
                                             
                                          
                                          
                                             (
                                             i
                                             )
                                          
                                          =
                                          
                                             p
                                             
                                                s
                                                ,
                                                t
                                             
                                          
                                          
                                             (
                                             i
                                             )
                                          
                                          −
                                          
                                             p
                                             
                                                0
                                                ,
                                                0
                                             
                                          
                                          
                                             (
                                             i
                                             )
                                          
                                          ,
                                          
                                             (
                                             s
                                             ,
                                             t
                                             )
                                          
                                          
                                             
                                                0.25
                                                e
                                                m
                                             
                                             
                                                0
                                                e
                                                x
                                             
                                          
                                          ≠
                                          
                                             
                                                0.25
                                                e
                                                m
                                             
                                             
                                                0
                                                e
                                                x
                                             
                                          
                                          
                                             (
                                             0
                                             ,
                                             0
                                             )
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     
                  

Finally, the ith LFD feature is defined as the set of relative differences,
                        
                           (2)
                           
                              
                                 
                                    
                                       
                                          LFD
                                          
                                             (
                                             i
                                             )
                                          
                                          =
                                          {
                                          
                                             Δ
                                          
                                          
                                             p
                                             
                                                s
                                                ,
                                                t
                                             
                                          
                                          
                                             (
                                             i
                                             )
                                          
                                          |
                                          −
                                          m
                                          ≤
                                          s
                                          ≤
                                          m
                                          ,
                                          −
                                          n
                                          ≤
                                          t
                                          ≤
                                          n
                                          }
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     where 
                        
                           2
                           m
                           +
                           1
                        
                      and 
                        
                           2
                           n
                           +
                           1
                        
                      are the numbers of viewpoints.

In this section, we describe an algorithm of our transparent object classification. Fig. 5 shows the overview of our algorithm. We used a commercial light field camera, Pro Fusion25 (ViewPlus Inc.), which has 25 VGA resolution (640
                     
                        
                           ×
                           
                        
                     
                     480pixel) cameras. This camera system can simultaneously capture images from 25 viewpoints (5 horizontal
                     
                        
                           ×
                           
                        
                     
                     5 vertical). We transformed the 25 captured images to a rectified light field image (s, t, u, v) as shown in Fig. 6
                      by the camera array calibration method [29].

In the LFD feature acquisition stage, we obtain the correspondence between the image of the center viewpoint and those of the other viewpoints. A disparity Δp
                     
                        s, t
                     (i) can be calculated from p
                     0, 0(i) and its corresponding point p
                     
                        s, t
                     (i). An LFD feature is composed of these disparities, which is represented by Eq. (2). We use the colors representation for indicating LFD features in this paper. Fig. 7
                     -top shows the examples of the correspondences between the center viewpoint and the neighboring viewpoints. We describe the 2D disparity vectors to color representations as shown in Fig. 7-bottom. Each cell corresponds to a specific (s, t) coordinate; hue and saturation of each cell represent the direction 
                        
                           arctan
                           (
                           
                              Δ
                           
                           u
                           /
                           
                              Δ
                           
                           v
                           )
                        
                      and the length 
                        
                           
                              
                                 Δ
                              
                              
                                 
                                    u
                                 
                                 2
                              
                              +
                              
                                 Δ
                              
                              
                                 
                                    v
                                 
                                 2
                              
                           
                        
                      of the vector, respectively. We use the large displacement optical flow (LDOF) method [10,30] to obtain the correspondences between the center viewpoint and the other 24 viewpoints. As a result, we represent the LFD feature as a 48-dimensional vector for each point which describes the disparities between the center viewpoint and the other views. The LFD features are pixel-wise extracted in an image. The LFD features coming from the transparent object have larger distortion than these from background, since the disparities containing refraction effect deviate from hyperplane assumed as lambertian reflection in the phase space as described in Fig. 4. The hyperplane in stuv-space can be described as,
                        
                           (3)
                           
                              
                                 
                                    
                                       
                                          α
                                          s
                                          +
                                          β
                                          t
                                          +
                                          γ
                                          
                                             Δ
                                          
                                          u
                                          +
                                          ω
                                          
                                             Δ
                                          
                                          v
                                          =
                                          0
                                          ,
                                       
                                    
                                 
                              
                           
                        
                     (α, β, γ, ω) is the unit normal vector 
                        
                           
                              n
                           
                           →
                        
                      of the hyperplane. The unit normal vector 
                        
                           
                              n
                           
                           →
                        
                      is estimated by fitting (s, t, Δu, Δv) values from all M viewpoints.

                        
                           (4)
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   [
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     (
                                                                     s
                                                                     ,
                                                                     t
                                                                     ,
                                                                     
                                                                        Δ
                                                                     
                                                                     u
                                                                     ,
                                                                     
                                                                        Δ
                                                                     
                                                                     v
                                                                     )
                                                                  
                                                                  0
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     (
                                                                     s
                                                                     ,
                                                                     t
                                                                     ,
                                                                     
                                                                        Δ
                                                                     
                                                                     u
                                                                     ,
                                                                     
                                                                        Δ
                                                                     
                                                                     v
                                                                     )
                                                                  
                                                                  1
                                                               
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               ⋯
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               
                                                                  
                                                                     (
                                                                     s
                                                                     ,
                                                                     t
                                                                     ,
                                                                     
                                                                        Δ
                                                                     
                                                                     u
                                                                     ,
                                                                     
                                                                        Δ
                                                                     
                                                                     v
                                                                     )
                                                                  
                                                                  
                                                                     M
                                                                     −
                                                                     1
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   ]
                                                
                                                ︸
                                             
                                             
                                                A
                                             
                                          
                                          
                                          
                                             
                                                
                                                   [
                                                   
                                                      
                                                         
                                                            
                                                               α
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               β
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               γ
                                                            
                                                         
                                                      
                                                      
                                                         
                                                            
                                                               ω
                                                            
                                                         
                                                      
                                                   
                                                   ]
                                                
                                                ︸
                                             
                                             
                                                
                                                   n
                                                
                                                →
                                             
                                          
                                          =
                                          
                                             0
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     Then we calculate the SVD of 
                        
                           
                              
                                 A
                              
                              T
                           
                           A
                           =
                           
                              
                                 UDU
                              
                              T
                           
                        
                     , and the least linear square solution to 
                        
                           
                              n
                           
                           →
                        
                      is the column of U associated with the smallest eigenvalue in D, where the smallest eigenvalue is the least square error E. And the deviation from the hyperplane is defined as the root mean square error,
                        
                           (5)
                           
                              
                                 
                                    
                                       
                                          dev
                                          =
                                          
                                             
                                                E
                                                /
                                                M
                                             
                                          
                                          .
                                       
                                    
                                 
                              
                           
                        
                     
                  

We filtered out the background LFD features by a certain threshold value th. When dev < th, we consider the LFD features are from the background; when dev ≥ th, the LFD features come from the transparent object. The background LFD features are filtered out in this way. As a result, we can obtain a set of N′ LFD features from the single light field image. The selection of the threshold value th is described in Section 6.1.

Training and classification processes are performed by a typical BoF approach. We use the LFD features as visual words. In the training phase, the LFD features are quantized by k-means clustering for obtaining visual words. We represent classes of transparent objects as patterns of histograms of the visual words. In the testing phase, we extract LFD features from input image as a same manner described above, and calculate the similarities of the distances by histogram matching for classification. Finally, we determine the class of the object as a minimum distance of the matching.

We performed some experiments in laboratory setting as well as real scenes to evaluate the performance of our proposed method.
                  

We evaluated our proposed method by classification of transparent objects in a laboratory setting and real environments under the following assumptions;
                           
                              •
                              There is one transparent object as a classification target in a scene.

The target object appears in all of the viewpoints of the LF camera.

Relative positions and poses of the camera and target object are almost same between a training and testings.

Background is reasonably far away from the object.

Background scenes have sufficient textural information.

In the experiments, we used 
                           
                              5
                              ×
                              
                              5
                           
                         viewpoints to acquire the LFD features, and a reference position for learning the bag of LFD features. The camera position is 40cm in front of the transparent object for both lab setting and real scenes. The background is 150cm behind the object for lab setting, and farther than 100cm for real scenes.

The optimal threshold value for filtering out the LFD features come from the background and the number of clusters K for the BoF approach are determined based on the parameter optimizations experiments (Section 6.1), and we choose 
                           
                              th
                              =
                              0.25
                           
                         and 
                           
                              K
                              =
                              1000
                           
                         for the evaluation. And our task is classifying 18 various shapes of the objects (Fig. 8) into the 18 classes under the various background textures.

We performed some classification experiments in a laboratory setting. We used a projector and screen for a backdrop of a scene. There are 10 different scenes, and 5 of which are depicted in Fig. 9.

We calculated recognition ratio among the 18 objects using leave-one-out cross validation for scenes, 1 scene is used for training and the other 9 scenes for testing, so we have 
                           
                              10
                              ×
                              
                              9
                              =
                              90
                           
                         test images for each object. The classification result is shown in Fig. 10
                        . We can see the classification accuracy for each transparent object from Fig. 10(a), most of the objects can be well classified by the proposed method based on LFD feature, while some of them got lower classification results. The confusion matrix shown in Fig. 10(b) can tell us some objects are easy to be misclassified as other objects, such as the Object A is easy to be confused to Object O since their front view is similar, and Object Q and Object E are confused to each other. So these objects got lower recognition ratios than others. The proposed method achieved 84% of average classification accuracy over the 18 objects in front of 10 different backgrounds, although it realized transparent object classification from a single-shot image.


                        Fig. 11
                         shows the differences of the 4 of 1000 (
                           
                              K
                              =
                              1000
                           
                        ) frequent visual words as primal LFD features described by color representation. Fig. 11(a) shows the frequent LFD features obtained by different objects with the same background. The patterns of the LFD features are different for the different objects, despite these objects look similar visually as they were placed in front of the same background. Also the LFD features come from the different regions of the objects. It means that each object was uniquely modeled by the LFDs. Fig. 11(a) shows our method utilizes not only the silhouette LFD features but also the inside region of the object.

In contrast, Fig. 11(b) shows the LFD features from the same object placed in front of different backgrounds. It shows that these LFD patterns are the similar and coming from similar regions of the object, although the visual appearance so different among the background differences. We confirmed that LFD feature is irrespective of the background difference, since the LFD feature does not model the intensity pattern but the geometrical distortion caused by object refraction.

We also performed real experiments in indoor and outdoor settings (Fig. 12
                        ). Objects were placed about 40cm from the camera against real backgrounds of structures at various depths, i.e., distances sufficiently far (more than 1m) from the objects. The training data set is same to the Lab setting. Table 1 shows the average recognition ratios for different number of transparent objects under three different scenes.

We also used a similar classification method in using the SIFT feature. Numbers of clusters K for the SIFT approach was also set to 
                           
                              K
                              =
                              1000
                           
                        . Table 1 shows that the proposed method achieved an average 76.6% accuracy for 6 transparent objects and 53.3% for 18 transparent objects. On the other hand, the ratios of the standard SIFT are the almost chance rates. It is not discriminative to the object patterns at all, since the SIFT features mainly come from the background patterns.

As describe in the overview of our algorithm (Section 4), the number of LFD features is depend on the threshold value for filtering, and the number of visual words is decided by K-means clustering. In order to find the optimal parameters in our system, we conducted a serial of experiments under the same condition described in Section 5.3 with different threshold values and number of clusters.

The threshold value is used to filter out the LFD features from the background. When this value is small, the features from the background cannot be filtered out. And when it is getting larger, the LFD features inside the transparent object will be filtered out as well. We use 
                           
                              K
                              =
                              1000
                           
                         in this experiment. Fig. 13(a) shows the classification results under different threshold values. When the threshold value is 0, the features from the background will be dominant and the classification results is near 25% decreased from the best accuracy. The result shows that the best threshold value is 
                           
                              th
                              =
                              0.25
                           
                        , and we use this for all of the other experiments.

The number of clusters is related to the number of visual words in our system. Regarding to the computational cost, this number is very important. We should carefully choose the number of clusters in order to get the balance of classification speed and accuracy. Fig. 13(b) shows that the classification accuracy is getting better during the number of clusters increase. The recognition ratio almost saturated
                         when the cluster number is over 1000, so the best choice of the cluster number for 18 different objects is 
                           
                              K
                              =
                              1000
                           
                        .

We evaluated how the various camera and object settings affect the performance of our proposed method in this section. We used the same conditions to the experiments for the laboratory setting as described in Section 5.2. We investigated the effects on the classification accuracy under various conditions, including camera positions, background distance, object rotations in three directions, additional lighting conditions, noise levels of the images and the number of viewpoints.

We moved the camera over a range of ± 10cm from the reference position 40cm. Fig. 14
                        (a) shows that as the camera moves away from the reference position, recognition ratios worsen because the LFD features are distorted when the distance between the camera and object changes. Moving away from the reference position incurs greater error than moving closer to the object. We would consider the margins for object deviation to be about 5cm if we accept a 10% decrease in the recognition ratio.

We also moved the background position over the range of 50–250cm from the object, while the reference position of the background was 150cm. Fig. 14(b) shows the recognition ratio decreased when the background displaced from the reference position. The direction of ratio decrease is opposite to that of the camera position change as shown in Fig. 14(a). The ratio is not so changed when the background is away from the object, while it is steeply decreased when the background position is approaching to the object. This is because that the LFD difference caused by the depth disparity is nonlinearly occurred, near position is larger and far is small. The background position does not affect much about the recognition ratio when it is far away enough, and we can apply this method to more realistic no planer scene background, if we can assume that the background objects of the scene are placed reasonably far distance, e.g. more than 100cm.

We analyzed how the rotations affect the performance by rotating the objects or camera in three different directions as illustrated in Fig. 15. We rotated the objects up to 40 degrees along their central axes (z-axis). Fig. 14(e) shows the results splitting to a symmetric group (Objects A-I) and an asymmetric group (Objects J-R) of the objects, as well as overall ratio. As we expected, the ratio of the symmetric group is invariant to poses, since the shape and its LFD features would not be changed. The ratios of asymmetric group decrease gradually and the limitation on object pose variation is within 20 degrees if we accept a 20% degradation in recognition ratios. Because it is difficult to rotate the object along x and y axes, we relatively rotated the camera instead. We rotated the camera over a range of ± 10 degrees along x-axis from the reference position, and the results are shown in Fig. 14(c). We can get more than 70% accuracy if the rotation is within ± 5 degrees. The rotation along y-axis was up to 40 degrees, and the accuracy gradually decreases as shown in Fig. 14(d). The recognition ratio can stay over 70% if the rotation is within 10 degrees along this direction.

We also evaluated effects of illumination change for classification. We placed an additional point light source to the global illumination that was used in the all of the experiments. We changed the direction of the light source from above (0 degree) to the side (90 degrees) with respect to the target object. There were inter-reflections and specular reflections from the light source and these effects were changed as we moved the light source. Fig. 14(f) shows the recognition ratios across the lighting directions. The left most label indicates the recognition ratio without the additional point light source which is same condition of learning setting. This figure shows that the internal and specular from the light source contaminated the LFD features and decreases averagely 15% of the classification accuracy. It is not strongly related to the directions of the settings.

We conducted some experiments to analyze how the input noise affects the performance. We first estimated the input noise of the light field image by capturing several images with constant intensity. The average intensity was calculated as the constant intensity, and the standard deviation of all the pixels was calculated from the mean intensity. The average standard deviation of input noise is 0.032, and we intentionally increased the noise by adding zero-mean Gaussian noise to the original light field image. The additional noise standard deviation was up to 0.3. The classification results can be seen in Fig. 14(g). If we want to get the accuracy over 70%, the noise standard deviation should be controlled under 0.08.
                        
                        
                     

In order to evaluate how the number of cameras affects the classification results, we decreased the viewpoints to 5 views (Fig. 16(a)) and 
                           
                              3
                              ×
                              
                              3
                           
                         views (Fig. 16(b)). By moving the 
                           
                              5
                              ×
                              
                              5
                           
                         viewpoints light field camera on a robot arm, we can increase the viewpoints to 
                           
                              9
                              ×
                              
                              9
                           
                         (Fig. 16(d)) and 
                           
                              13
                              ×
                              
                              13
                           
                         (Fig. 16(e)) as well. We kept the same horizontal and vertical baseline for all the settings, and only increased or decreased the density of the viewpoints. Fig. 14(h) shows that the recognition ratio, not as we expected, is not so affected by the number of the viewpoints. This is because there are only 18 objects in the experiments, and the variation of the LFD for discriminating 18 objects is limited.

The background patterns used in the experiment have complex textures (see Fig. 9) from which correspondence detection can be easily performed. Meanwhile, LFD features cannot be appropriately extracted in certain background scenes (Fig. 17). Because textural information is minimal, correspondences between the viewpoints are difficult to find. In Fig. 17(a), the LFD features were extracted from only the edges of the transparent object, with no LFD feature taken interior to the object. For another background (Fig. 17(b)), LFD features were wrongly extracted exterior to the transparent object (see the top-left part of the figure). Therefore, the performance is affected by the accuracy in correspondence detection.

We evaluated how many LFD feature points are needed for accurate classification in simulation. First, to obtain ideal feature points, a dot pattern was displayed as a background to the transparent object for easy to detect the correspondence of the LFD features. A total of N dots were captured; note that the number of N corresponds to the whole number of pixels. Second, a percentage d% of LFD features were randomly selected. Then, leave-one-out cross validation was performed to acquire the classification accuracy. This procedure was repeated 100 times if d was less than 1%, otherwise, just ten times.

The recognition ratio curve is plotted in Fig. 18
                        . This figure shows that our approach requires at least 3% of the LFD features to obtain almost 100% classification accuracy. In terms of practical uses, extracting LFD features for at least 3% of the image size is not such a difficult problem. Therefore, our proposed LFD feature is considered effective in transparent object classification.

We also evaluated mistracking for estimating the LFD vectors. We used the same simulated features above and randomly selected 3% of the features. We added zero-mean Gaussian noise with different standard deviations to the LFD features to simulate tracking noise. The recognition ratios across different standard deviation of noise (Fig. 19
                        ) show that ratios decrease when error levels increase. We confirmed that less than 5.0pixels of the error is required if we desire 70% recognition ratio.

@&#CONCLUSION@&#

This paper propose a novel feature called light field distortion (LFD) feature, which models refraction in objects as distortions between multiple views captured by a light field camera. We also propose the transparent object classification method using the LFD feature as an application. The proposed transparent object classification method achieved on average 84% accuracy with 18 objects in lab setting and 53.3% in the real scene setting, while standard SIFT is not working at all. We discussed about parameter optimizations and limitation analysis of the method in some experiments, such as: threshold value of the LFD, number of clusters, density of the texture, camera and background positions, object rotations, lighting conditions and the number of cameras. In conclusion, we have been successful in: (1) producing a transparent object classification approach based on a single-shot image, (2) employing a novel feature, namely refraction, for transparent object classification without knowing the background texture, and (3) introducing the light field camera array in training and classification applications. For the future work, we are interested in solving current limitations of the method and extending to multi-object classification.

@&#ACKNOWLEDGMENTS@&#

This research was partially supported by Konica Minolta Science and Technology Foundation, Grant-in-Aid for Scientific Research on Innovative Areas “Shitsukan” No. 23135524 and Grant-in-Aid for Scientific Research (A) No. 25240027.

@&#REFERENCES@&#

