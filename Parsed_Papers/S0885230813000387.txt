@&#MAIN-TITLE@&#Semantic spaces for improving language modeling

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           The unsupervised techniques of word clustering are investigated in this article.


                        
                        
                           
                           Semantic spaces (HAL, COALS, BEAGLE, etc.) are used for clustering of words.


                        
                        
                           
                           Class-based language models are built from clusters and are interpolated with the base-line model.


                        
                        
                           
                           Final models improve performance on Czech, Slovak and English corpora.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Class-based language models

Semantic spaces

HAL

COALS

BEAGLE

Random Indexing

Purandare and Pedersen

Clustering

Inflectional languages

Machine translation

@&#ABSTRACT@&#


               
               
                  Language models are crucial for many tasks in NLP (Natural Language Processing) and n-grams are the best way to build them. Huge effort is being invested in improving n-gram language models. By introducing external information (morphology, syntax, partitioning into documents, etc.) into the models a significant improvement can be achieved. The models can however be improved with no external information and smoothing is an excellent example of such an improvement.
                  In this article we show another way of improving the models that also requires no external information. We examine patterns that can be found in large corpora by building semantic spaces (HAL, COALS, BEAGLE and others described in this article). These semantic spaces have never been tested in language modeling before. Our method uses semantic spaces and clustering to build classes for a class-based language model. The class-based model is then coupled with a standard n-gram model to create a very effective language model.
                  Our experiments show that our models reduce the perplexity and improve the accuracy of n-gram language models with no external information added. Training of our models is fully unsupervised. Our models are very effective for inflectional languages, which are particularly hard to model. We show results for five different semantic spaces with different settings and different number of classes. The perplexity tests are accompanied with machine translation tests that prove the ability of proposed models to improve performance of a real-world application.
               
            

@&#INTRODUCTION@&#

Language modeling is a crucial task in many areas of NLP. Speech recognition, optical character recognition and many other areas heavily depend on the performance of the language model that is being used. Each improvement in language modeling may also improve the particular job where the language model is used.

Research into language modeling started more than 20 years ago and has evolved into a very mature discipline. Now it is very difficult to outperform the state of the art. Our research is focused on inflectional languages as we believe that these languages offer some room for improvement. We however also provide experiments for English (which is not a very inflectional language). Even in the case of English, we were able to obtain positive results.

Czech and Slovak belong to the Slavic language group. These languages are highly inflectional and have a relatively free word order. Czech has seven cases and three genders. Slovak has six cases and also three genders. The word order is very variable from the syntactic point of view: words in a sentence can usually be ordered in several ways, each carrying a slightly different meaning. These properties of the languages complicate the language modeling task. The great number of word forms and more possible word sequences lead to a greater number of n-grams. Data sparsity is a common problem of language models. In Czech, Slovak and other Slavic languages, this problem is more evident.

Class-based modeling is the most popular technique used for reducing the huge vocabulary-related sparseness of statistical language models (Brown et al., 1992). Individual words are clustered into a much smaller number of classes. As a result, less data are required to train a robust class-based language model. Both manual and automatic word-clustering techniques are being used. Standalone class-based models usually perform poorly, which is the reason why they are usually combined with other models. Many researchers have demonstrated that the combination of a standalone class-based language model and a standard word n-gram model reduces the model perplexity (Maltese et al., 2001; Whittaker, 2000; Whittaker and Woodland, 2003).

An effective solution for language modeling is to use information about the morphology of the language. In Oparin (2008) experiments with morphological random forests in the Czech and Russian language are shown with the conclusion that they can be used effectively for inflectional languages. Authors of Vaiciunas et al. (2004) describe the language modeling of Lithuanian by means of class-based language models derived by word clustering and morphological word decomposition and their linear interpolation with the baseline word n-gram model. The authors present a perplexity reduction of 8–13% depending on the size of the corpora. A similarly effective solution is to use class-based language models where classes are derived from lemmas and morphological categories (Brychcín and Konopík, 2011). The article shows a perplexity reduction of 10–30% in corpora in the Czech and Slovak languages. A comparative study of several methods using morphological information for modeling conversational Arabic can be found in Kirchhoff et al. (2006). The usage of morphological information seems to be very effective for inflectional languages; however, it requires a huge number of manually annotated texts.

In Brown et al. (1992) the MMI (Maximum Mutual Information) clustering algorithm was introduced. This algorithm is based upon the principle of merging a pair of words into one class according to the minimal mutual information loss principle. The algorithm gives very satisfactory results and it is completely unsupervised. Its complexity is however very problematic. This method of word clustering is possible only in very small corpora and is not suitable for large vocabulary applications. The authors in Yokoyama et al. (2003) used the MMI algorithm to build class-based language models. Their linear interpolation with the word n-gram model was applied to speech recognition of Japanese. The authors showed a 2% absolute improvement in word accuracy but only in very small corpora.

Several authors have tried to approximate the MMI algorithm to reduce computational requirements and to make it more suitable for large vocabulary language models (Bai et al., 1998; Yamamoto and Sagisaka, 1999). Automatically derived clusters have been used for class-based language models of Japanese and Chinese (Gao et al., 2002). The authors concentrated on the best way of using the clusters; however, they did not focus on how to get them.

Another way of improving language models is to use semantic information. This idea is based on the assumption that words with lexically different forms usually share similar meanings in cases where they frequently occur in similar contexts. The semantic information can be calculated using the Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dumais, 1997; Landauer et al., 1998) method or its probabilistic variant, the PLSA (Hofmann, 1999). A similar method to the PLSA is the Latent Dirichlet Allocation (LDA) (Blei et al., 2003) which is essentially the Bayesian version of the PLSA model. Bellegarda and his team were the first to introduce LSA into language modeling (Bellegarda et al., 1996). Their approach consisted in using LSA to derive word clusters for class-based language models.

The approach then evolved to focus on documents instead of focusing on words. It is assumed that documents may vary in domain, topic and styles, which means that they also differ in the probability distribution of n-grams. This assumption is used for adapting language models to the long context (domain, topic, style of particular documents). LSA (or similar methods) are used to find out which documents are similar and which are not. This long context information is added to standard n-gram models to improve their performance. A very effective group of models (sometimes called topic-based language models) work with this idea for the benefit of language modeling. In Bellegarda (2000) a significant reduction in perplexity (down to 33%) and relative reduction in WER
                        1
                     
                     
                        1
                        The Word Error Rate (WER) measure is often used in speech recognition.
                      (down to 16%) in the WSJ (Wall Street Journal) corpus was shown. Many other authors have obtained good results with PLSA (Gildea and Hofmann, 1999; Wang et al., 2003) and LDA (Tam and Schultz, 2005, 2006) approaches.

In Liu and Liu (2007, 2008), the named entity recognition technique was applied to topic modeling. The topic modes were based upon LDA and clustering. The authors tested the hypothesis that named entities carry valuable information which can be useful for latent topic analysis. The authors presented a 14% perplexity reduction as their best result.

Some comparisons between PLSA and LDA as well as some clustering methods can be found in Hahn et al. (2008). The authors present their results in English and Arabic broadcast news.

An investigation into Topic Tracing Language Models (TTLM) and their application in speech recognition is presented in Watanabe et al. (2011). The TTLM is based on LDA and PLSA and integrates the ability to dynamically track changes in topics. The tracking is based upon focused text information and previously estimated topics.

To put our approach into the context of the above-mentioned methods, we can state that our method also focuses on inflectional languages similarly to methods that use morphology. Our approach, however, is unsupervised. Our method also uses semantic information. We do not rely however on LSA, PLSA or LDA but on different methods (HAL, COALS, BEAGLE and others) that were not tested in the language modeling task before. These methods do not require the text to be partitioned into documents as in the case of LSA, PLSA or LDA. Our approach is in many ways similar to the approach of MMI clustering but the methods we use can deal with much larger data.

The rest of the article is organized as follows. In the following section, we give an overview of the statistical modeling of semantic information. In Section 3 we explore the application of semantic information in language modeling. Section 4 shows various results of our experiment in detail. In the last sections we discuss the results and we conclude the article.

The semantic models investigated in this work are based upon the idea that the word meaning is related to the context in which the word is usually used. The assumption is that two (lexically) different words share a similar meaning if they occur in similar contexts. Some studies (Rubenstein and Goodenough, 1965; Charles, 2000) have confirmed this assumption by empirical tests carried out on human test groups. The implication of the studies is that it is possible to compute the semantic similarity of words by a statistical comparison of their contexts. In this article we use the assumption and its implication to improve language models. Before we introduce the method of incorporating semantic similarity into language modeling we briefly explain several methods for calculating word similarity.

In semantic spaces, each word is represented as a highly dimensional vector. The vectors are derived from the statistical properties of words and their contexts in a plain text corpus. The vectors are constructed in such a way that words similar in meaning should have a similar vector. The methods to calculate the vectors differ. In the following sub-sections we briefly explain the methods that were tested in this article.


                        Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996; Burgess and Lund, 1997) creates a semantic space from word co-occurrences. Each word in the training data is examined and those words nearer than a fixed distance are recorded as co-occurring. Such a group of words is called a “window”. The words in the window are weighted according to the distance from the examined word. It is assumed that the closer the word is, the greater the impact it has on the focused word semantics. The co-occurring words are therefore inversely weighted according to their distance from the examined word.

These windows are used to construct the |W|×|W| co-occurrence matrix 
                           
                              M
                           
                         (|W| is the number of words being analyzed) in the following way. When a word 
                           
                              w
                              j
                           
                         is found in the window of the examined word 
                           
                              w
                              i
                           
                         then a value is added to the m
                        
                           i,j
                         element in the matrix 
                           
                              M
                           
                        . The value depends on the distance of the word 
                           
                              w
                              j
                           
                         from the word 
                           
                              w
                              i
                           
                        . The exact formula to calculate the value is defined in Lund and Burgess (1996). The row and column vectors of the matrix 
                           
                              M
                           
                         contain co-occurrence information on words that appeared before and after respectively. HAL therefore also records simple word-ordering information. Naturally, many words do not appear in the vicinity of each other and the matrix 
                           
                              M
                           
                         tends to be very sparse.

For each word (meaning), certainly not all columns (co-occurred words) provide an equal amount of information. The entropy can be used to retain only a given number of significant columns. In this way, the dimensionality of the matrix 
                           
                              M
                           
                         can be reduced.


                        Correlated Occurrence Analogue to Lexical Semantic (COALS) (Rohde et al., 2004) is a semantic space model based upon HAL and LSA ideas. The process of building the matrix starts almost identically to the HAL methods but COALS adds some tweaks.

The algorithm constructs the matrix 
                           
                              M
                           
                         in a similar way as HAL does. It however does not distinguish whether a co-occurred word comes before or after the focused word. The window that is used has the same length in both directions. The matrix is also normalized using correlation. Any negative values are set to zero and all other values are replaced by the square root.

The final part of the algorithm is inspired by the LSA method. Singular Value Decomposition (SVD) is applied to the matrix 
                           
                              M
                           
                         in order to reduce the dimensionality of the vector space (typically, dimension about 800 is used). The SVD reduction has the effect of bringing out the latent semantic relationships between words (as in LSA) so it can discover transitive relations between words. This final stage is not mandatory for the COALS method and is sometimes skipped.


                        Random Indexing (RI) (Sahlgren, 2005) is based on the process of the accumulation of context vectors of words that are co-occurring. This incremental technique is used to construct the semantic space in a completely different way from the above-described models. Instead of constructing a word by word matrix and then deriving the context vectors, the process is reversed. First, vectors are generated and then the matrix is calculated.

The Random Indexing method can be described in two step. In the first step, a randomly generated high-dimensional vector is assigned to each word. The dimensionality of vectors typically reaches thousands of dimensions. The vectors consist of a small number of randomly distributed nonzero vector values (−1, +1). In this way it is ensured that two vectors do not overlap very often. The generated vector is known as the index vector. During the second step, the algorithm scans the text and updates the context vectors by summing up all the index vectors of co-occurring words.

This method does not require the dimension reduction phase as in the case of HAL and COALS. Here the dimension is set at the beginning and is much lower that in the case of HAL and COALS.

In Sahlgren et al. (2008) the Random Indexing method is extended to keep the word-order information. The modification is inspired by the BEAGLE method (Section 2.4), but instead of using convolution operation this method is based upon the permutation of vector coordinates. While both methods are approximative, the permutation is more simple to calculate.


                        Bound Encoding of the AggreGate Language Environment (BEAGLE) (Jones and Mewhort, 2007) is a computational model that builds a semantic space in a similar way to Random Indexing (Section 2.3). During the first phase, a high-dimensional index vector is also randomly generated; however, the values are given according to the Gaussian distribution. The mean value is set to 0 and the variance is set to 1/D, where D is the dimension (by default, D
                        =1024).

The meaning of words in the final semantic space is compounded from the co-occurrence information and word order information. The co-occurrence information is calculated as in Random Indexing by summing the vectors of the co-occurring words to the vector of the focused word. The word order information is calculated by convolution of the n-gram vectors that contain the focused word. The final semantic vector is then constructed as a combination of the co-occurrence vector and the word order vector and is sensitive to both the neighboring words and word order.


                        The Purandare and Pedersen (P&P) (Purandare and Pedersen, 2004) model is another form of word sense induction, in which word meaning is inducted from different usages in training data.

The process of building the semantic space is divided into two stages. First, the training data are processed and features most likely correlated with focused words are identified. The model uses two kinds of features: co-occurring words (similarly to HAL (Section 2.1) or the COALS model (Section 2.2)) and co-occurring bigrams. The features are selected from a close distance to the focused word (e.g. five-word distance). Features which are statistically significant are kept others are removed. This leads to the removal of words which possibly frequently co-occur with the focused word but which do not have a significant impact on the semantics of the focused word.

In the second stage, the algorithm tries to construct the meaning of words from longer contexts (e.g. 20 words on both sides of the focused word). Only words that are in the features of the focused word are included in the longer context vectors, while others are removed. It is expected that these context vectors represent different usages of the focused word in the corpus. These vectors (usages) are then clustered into a predefined number of clusters. Each of the final clusters receives its own semantic vector and represents one of the meaning of the word. In the final semantic space, each word is described by n meanings. Its final semantic vector is created as a combination of clustered vectors.

The fact that a word is characterized by a vector opens up the opportunity to easily compare two words. The more similar two words are in meaning, the more similar their vectors should be. The ability to compare two words enables us to use a clustering method. Similar words are clustered into bigger groups of words (clusters).

The distance (similarity) between two words can be calculated by a vector similarity function. Let 
                              
                                 a
                                 →
                              
                            and 
                              
                                 b
                                 →
                              
                            denote the two vectors to be compared and 
                              S
                              (
                              
                                 
                                    a
                                    →
                                 
                                 ,
                                 
                                    b
                                    →
                                 
                              
                              )
                            denote their similarity measure. Such a metric needs to be symmetric: 
                              S
                              (
                              
                                 
                                    a
                                    →
                                 
                                 ,
                                 
                                    b
                                    →
                                 
                              
                              )
                              =
                              S
                              (
                              
                                 
                                    b
                                    →
                                 
                                 ,
                                 
                                    a
                                    →
                                 
                              
                              )
                           .

There are many methods to compare two vectors in a multi-dimensional vector space. Probably the simplest vector similarity metrics are the familiar Euclidean (r
                           =2) and city-block (r
                           =1) metrics


                           
                              
                                 (1)
                                 
                                    
                                       S
                                       mink
                                    
                                    (
                                    
                                       
                                          a
                                          →
                                       
                                       ,
                                       
                                          b
                                          →
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          ∑
                                          
                                             |
                                             
                                                
                                                   a
                                                   i
                                                
                                                −
                                                
                                                   b
                                                   i
                                                
                                             
                                             
                                                |
                                                r
                                             
                                          
                                       
                                       r
                                    
                                    ,
                                 
                              
                           that come from the Minkowski family of distance metrics.

Another often-used metric characterizes the similarity between two vectors as the cosine of the angle between them. The cosine similarity is defined as follows:


                           
                              
                                 (2)
                                 
                                    
                                       S
                                       cos
                                    
                                    (
                                    
                                       
                                          a
                                          →
                                       
                                       ,
                                       
                                          b
                                          →
                                       
                                    
                                    )
                                    =
                                    cos
                                    (
                                    θ
                                    )
                                    =
                                    
                                       
                                          
                                             a
                                             →
                                          
                                          ·
                                          
                                             b
                                             →
                                          
                                       
                                       
                                          ∥
                                          
                                             
                                                a
                                                →
                                             
                                          
                                          ∥
                                          ·
                                          ∥
                                          
                                             
                                                b
                                                →
                                             
                                          
                                          ∥
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             
                                                a
                                                i
                                             
                                             
                                                b
                                                i
                                             
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   
                                                      a
                                                      i
                                                      2
                                                   
                                                
                                                ∑
                                                
                                                   
                                                      b
                                                      i
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        

In statistics, the Pearson product-moment correlation coefficient (sometimes referred to as the PPMCC) is a measure of the correlation (linear dependence) between two variables, X and Y, giving a value between +1 and −1 inclusive. Pearson's correlation coefficient between two variables is defined as the covariance of the variables divided by the product of their standard deviations


                           
                              
                                 (3)
                                 
                                    
                                       S
                                       corr
                                    
                                    (
                                    
                                       
                                          a
                                          →
                                       
                                       ,
                                       
                                          b
                                          →
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          E
                                          [
                                          
                                             (
                                             
                                                
                                                   a
                                                   →
                                                
                                                −
                                                
                                                   μ
                                                   a
                                                
                                             
                                             )
                                             (
                                             
                                                
                                                   b
                                                   →
                                                
                                                −
                                                
                                                   μ
                                                   b
                                                
                                             
                                             )
                                          
                                          ]
                                       
                                       
                                          
                                             σ
                                             a
                                          
                                          
                                             σ
                                             b
                                          
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                          
                                             (
                                             
                                                
                                                   a
                                                   i
                                                
                                                −
                                                
                                                   μ
                                                   a
                                                
                                             
                                             )
                                          
                                          (
                                          
                                             
                                                b
                                                i
                                             
                                             −
                                             
                                                μ
                                                b
                                             
                                          
                                          )
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   
                                                      (
                                                      
                                                         
                                                            a
                                                            i
                                                         
                                                         −
                                                         
                                                            μ
                                                            a
                                                         
                                                      
                                                      )
                                                   
                                                   2
                                                
                                                ∑
                                                
                                                   
                                                      (
                                                      
                                                         
                                                            b
                                                            i
                                                         
                                                         −
                                                         
                                                            μ
                                                            b
                                                         
                                                      
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           where μ
                           
                              a
                            is the mean value of the vector 
                              
                                 a
                                 →
                              
                            and σ
                           
                              a
                            is the standard deviation of the vector 
                              
                                 a
                                 →
                              
                           .

This metric is often used in semantic spaces for dense matrices, while the cosine metric is used for sparse matrices.

The clustering of words with similar meaning is the most important and simultaneously the most time consuming part in our class-based language models.

The goal of clustering is simple; to find an optimal grouping in a set of unlabeled data. There are, however, two problems. Firstly, the optimality criterion must be defined. This criterion depends on the task that is being solved. The second problem is the complexity of the problem. The number of possible partitioning rises exponentially
                              2
                           
                           
                              2
                              To be exact, the number of possible partitioning of a n-element set is given by the Bell number which is defined recursively: 
                                    
                                       B
                                       
                                          n
                                          +
                                          1
                                       
                                    
                                    =
                                    
                                       ∑
                                       
                                          k
                                          =
                                          0
                                       
                                       n
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      n
                                                   
                                                
                                                
                                                   
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       B
                                       k
                                    
                                 .
                            with the number of elements in the set. It is therefore impossible to examine every possible partitioning of even a decently large set. The task is then to find a computationally feasible algorithm that would be as close to the optimal partitioning as possible.

In our case, the optimality criterion is supplied from a semantic space and an appropriate similarity metric. The clustering in our case is complicated even more by the fact that we are dealing with really large quantities of data (in the order of hundreds of thousands).

The type of algorithm plays a key role. Hierarchical methods go from the bottom (they start with many classes and join them together) and that is problematic in our case. We need relatively few classes and so a lot of joining operations must be executed. On the contrary, the partitioning methods go from the top (they start with one class and split it repeatedly). These methods are more suitable for us since much fewer operations of splitting than joining is required.

Even with the partitioning clustering method we struggled with the computation complexity of the algorithms. It was soon apparent that a very efficient algorithm would be necessary. Our task was to experiment with approximative partitioning clustering methods. A useful guide was found in the article by Zhao and Karypis (2002) where a comparison of clustering methods was presented. We were able to conclude that the Repeated Bisection algorithm gave satisfactory results with acceptable computational requirements. We use the implementation of the algorithm from the CLUTO software package (Karypis, 2003).

Class-based language models are the state-of-the-art approaches for language modeling. The main task of the approach is to replace the statistical dependencies between words with dependencies between a much lower number of word classes, thus reducing the data sparsity problem.

Let W denote the set of possible words (word vocabulary) and C denote a class vocabulary. Then we can define a mapping function m
                        :
                        W
                        →
                        C, which maps every word 
                           
                              w
                              i
                           
                           ∈
                           W
                         to some c
                        
                           i
                        
                        ∈
                        C. In our case, the classes are the word clusters derived from particular semantic spaces.

The probability estimation of word 
                           
                              w
                              i
                           
                         conditioned by its history 
                           
                              w
                              
                                 i
                                 −
                                 n
                                 +
                                 1
                              
                              
                                 i
                                 −
                                 1
                              
                           
                         (where n is the length of the n-gram) is given by the following formula


                        
                           
                              (4)
                              
                                 P
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       w
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 =
                                 P
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       c
                                       i
                                    
                                 
                                 )
                                 ·
                                 P
                                 (
                                 
                                    
                                       c
                                       i
                                    
                                    |
                                    
                                       c
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 .
                              
                           
                        
                     

We are using the Modified Kneser-Ney interpolation (introduced in (Chen and Goodman, 1998)) which at present is the state-of-the-art approach for smoothing methods. The formula for smoothing of word probabilities is


                        
                           
                              (5)
                              
                                 P
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       w
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       cnt
                                       (
                                       
                                          
                                             w
                                             
                                                i
                                                −
                                                n
                                                +
                                                1
                                             
                                             i
                                          
                                       
                                       )
                                       −
                                       D
                                       (
                                       
                                          cnt
                                          (
                                          
                                             
                                                w
                                                
                                                   i
                                                   −
                                                   n
                                                   +
                                                   1
                                                
                                                i
                                             
                                          
                                          )
                                       
                                       )
                                    
                                    
                                       
                                          ∑
                                          
                                             
                                                w
                                                i
                                             
                                          
                                       
                                       
                                          cnt
                                          (
                                          
                                             
                                                w
                                                
                                                   i
                                                   −
                                                   n
                                                   +
                                                   1
                                                
                                                i
                                             
                                          
                                          )
                                       
                                    
                                 
                                 +
                                 γ
                                 (
                                 
                                    
                                       w
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 P
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       w
                                       
                                          i
                                          −
                                          n
                                          +
                                          2
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 ,
                              
                           
                        where P() is the probability given by the Modified Kneser-Ney interpolation model and cnt() is the count of n-gram. The goal of discounting function D(cnt) is to save some probability mass for lower-order models. The normalization function 
                           γ
                           (
                           
                              
                                 w
                                 
                                    i
                                    −
                                    n
                                    +
                                    1
                                 
                                 i
                              
                           
                           )
                           ∈
                           (
                           0
                           ,
                           1
                           )
                         makes the probability distribution sum up to 1. The definition and derivation of this function can be found in the original paper.

The main advantage of Modified Kneser-Ney smoothing is the clever way it calculates the unigram probability distribution


                        
                           
                              (6)
                              
                                 P
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          N
                                          
                                             1
                                             +
                                          
                                       
                                       (
                                       
                                          •
                                          
                                             w
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       
                                          N
                                          
                                             1
                                             +
                                          
                                       
                                       (
                                       
                                          •
                                          •
                                       
                                       )
                                    
                                 
                                 ,
                              
                           
                        where symbol • means an arbitrary word (class) and 
                           
                              N
                              r
                           
                           (
                           
                              w
                              
                                 i
                                 −
                                 n
                                 +
                                 1
                              
                              i
                           
                           )
                         is the number of n-grams with frequency r (i.e. the number of such n-grams, where 
                           cnt
                           (
                           
                              w
                              
                                 i
                                 −
                                 n
                                 +
                                 1
                              
                              i
                           
                           )
                           =
                           r
                        ). In different words, the unigram probability of 
                           
                              w
                              i
                           
                         is given by the number of different bigrams ending in 
                           
                              w
                              i
                           
                         divided by the total number of different bigrams.

We use a simple but very effective linear interpolation for combining different language models


                        
                           
                              (7)
                              
                                 
                                    P
                                    LI
                                 
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       w
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    K
                                 
                                 
                                    
                                       λ
                                       k
                                    
                                    ·
                                    
                                       P
                                       k
                                    
                                    (
                                    
                                       
                                          w
                                          i
                                       
                                       |
                                       
                                          w
                                          
                                             i
                                             −
                                             n
                                             +
                                             1
                                          
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                    
                                    )
                                 
                                 ,
                              
                           
                        where λ
                        
                           k
                         is the weight of the kth language model P
                        
                           k
                        (). We use the Expectation Maximization (EM) algorithm described in Dempster et al. (1977) to calculate optimal weights λ
                        
                           k
                         by way of maximization of the probability of the held-out data.

The linear interpolation can be extended to a method called bucketed linear interpolation, where weights become the function of the frequency of word history (Bahl et al., 1983). The main idea is that the weights λ
                        
                           k
                         should be different for words with histories of varying frequencies. The formula then transforms to


                        
                           
                              (8)
                              
                                 
                                    P
                                    BLI
                                 
                                 (
                                 
                                    
                                       w
                                       i
                                    
                                    |
                                    
                                       w
                                       
                                          i
                                          −
                                          n
                                          +
                                          1
                                       
                                       
                                          i
                                          −
                                          1
                                       
                                    
                                 
                                 )
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    K
                                 
                                 
                                    
                                       λ
                                       k
                                    
                                    (
                                    
                                       
                                          w
                                          
                                             i
                                             −
                                             n
                                             +
                                             1
                                          
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                    
                                    )
                                    ·
                                    
                                       P
                                       k
                                    
                                    (
                                    
                                       
                                          w
                                          i
                                       
                                       |
                                       
                                          w
                                          
                                             i
                                             −
                                             n
                                             +
                                             1
                                          
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                    
                                    )
                                 
                                 .
                              
                           
                        
                     

The weights λ
                        
                           k
                        () certainly cannot be different for each possible frequency of history. Too much of weights would be created and too little data would be available to train them. Instead, the whole frequency spectrum is divided into buckets, where each bucket holds some range of frequencies. Histories in buckets have the same weights. The number of buckets can be tuned in but it generally depends on the amount of training data available. The more training data are available, the more buckets can be used.

@&#EXPERIMENTAL RESULTS@&#

In this section we closely describe various results of our experiments. The first Section 4.1 gives an overview of the corpora we use. Section 4.2 describes the configuration of the tested language models. Perplexity results, as the most commonly used metric for language models, are shown in Section 4.3. Finally, two additional tests of the performance of the language models are presented (Sections 4.4 and 4.5).

Language models in our experiments were trained on three unlabeled corpora in Czech, Slovak and English. Czech and Slovak belong to the group of Slavic languages and are representatives of highly inflectional languages. We also show tests on an English corpus as a representative of a language with low inflection. English is also used to compare the state-of-the-art approaches with our model.
                           
                              •
                              
                                 CZ: Contains news on many topics such as political, business, sports, international and other news gathered from 1 year in the Czech language. Data in this corpus are provided by the Czech News Agency (CNA).


                                 SK: A huge number of texts from the Slovak National Corpus
                                    3
                                 
                                 
                                    3
                                    The prim-5.0-public-all subcorpus of Slovak National Corpus available at http://korpus.juls.savba.sk.
                                  oriented also toward the artistic, publicist and professional area.


                                 EN
                                 
                                    4
                                 
                                 
                                    4
                                    Material is available from NIST Standard Reference Data Products.
                                 : The data represent a sampling of approximately 40% of the articles published by the Los Angeles Times in the 2-year period from January 1, 1989–December 31, 1990.

The data from all corpora were divided into training, held-out and testing sets in proportions of 65%, 15% and 20%. The parameters of these corpora are shown in Table 1
                        .

All of testing algorithms for building semantic spaces are implemented with the open source package called S-Space (Jurgens and Stevens, 2010).

As a baseline model we use the 4-gram word language model smoothed by the Modified Kneser-Ney smoothing described in Section 3.1. The higher order models prove not to be computationally feasible due their high memory requirements. The baseline language model as well as the semantic spaces are trained on the training parts of corpora. Only words with a reasonable frequency in the training data (5 or more for all corpora) are taken into account during building all of our models. The vocabulary size for each language is shown in Table 1. Corpora are tokenized with our language-independent tokenizer based upon regular expressions. The word dictionary is generated also from the training parts and it is kept fixed for all tests.

To build a class-based model, we use Eq. (4) and the Modified Kneser-Ney smoothing for the classes. The output of semantic spaces (the vectors for each word in the vocabulary) is kept unchanged. We do not use normalization because the principle of semantic space methods ensures that all vectors are normalized. The classes are generated by the Repeated Bisection clustering algorithm (described in Section 2.6.2) with the cosine metric (Eq. (2)) – COS and the Pearson product-moment correlation coefficient (Eq. (3)) – CORR. The data are clustered into 5 different numbers of clusters: 1000, 5000, 10,000, 20,000 and 50,000. Clustering into more than 50,000 classes would be computationally too expensive. Moreover such a high number of classes would mean that the classes would be very small containing often one word per class. The input data for clustering are the vectors from semantic spaces. We test 5 different semantic spaces with two settings for each – see Table 2
                        . We used the recommended settings given by authors of particular methods. In addition to the recommended setting, one more setting per method was added. The settings were based upon our understanding of method principles.

The final language models are created as the linear interpolation of the baseline model and class-based models. We always interpolate all five class-based models (different number of classes, same semantic space) and the baseline n-gram model. To improve the interpolation, we use 20 buckets. The weights of interpolation were estimated on the held-out parts of corpora.

During our experiments we found out that the HAL method is better for dense clusters (1k, 5k and 10k classes) and the COALS method performs better for sparse clusters (20k and 50k classes). We therefore tried to interpolate HAL for dense clusters and COALS for sparse clusters together. The results are denoted by the following notation HAL_w4+COALS_noSVD and presented at the end of all tables.

In this subsection, we present the perplexities of several class-based language models created from semantic spaces and their linear interpolations with the baseline model. The results for 4-gram language models are presented in Table 3a–c. The numbers in bold show the best result for the given number of classes. The numbers in brackets are the relative improvements against the baseline. For comparison we present the results given by both the linear interpolation as well as the bucketed linear interpolation.

The numbers in the tables above show that almost every semantic space gives at least some improvement. In particular, we can see that for inflectional languages (CZ and SK corpora) the improvements in perplexities are more significant than for the English corpus (EN) (a representative of a low inflectional language).

As we indicated before, the HAL-based models are efficient in case of dense clusters (1000 or 5000 classes) and the COALS model looks more suitable for sparse clusters (20,000 or 50,000 classes). The combination of both (the last row of the tables) provides the best perplexity results for each corpus. In following subsections, the HAL+COALS-based language model will be tested in order to verify its performance on other tests.

It is also interesting that the linear interpolation of class-based models created from the P&P model does not improve perplexity against baseline, however, each of the class-based language models alone gives very low perplexity in comparison to other semantic spaces. In many cases these class-based models even give the lowest perplexity when they are not interpolated with the standard n-gram model.

In this subsection we show the interpolation weights of particular sub-models that form the final model. Only the simple interpolation is depicted (single weight for each sub-model). Figures with bucketed interpolation would not be readable due to high number of weights. Weights are depicted in Fig. 1
                           a–c. The impact of each sub-model on the final probability distribution can be easily seen from the length of particular sub-bars.

Figures show a trend where the sparse clusters (20k or 50k classes) received bigger weights (immediately after the baseline) and the weights decrease with decreasing number of classes. A direct correlation between weights and perplexity results (Section 4.3) can be seen. The bigger the weights allocated by the EM algorithm are, the higher the improvement in perplexity is achieved.

The perplexity sometimes does not correspond with results from real-world applications. Here we introduce another test of our language models called the word estimation test.

During the test, our models try to find a missing word in the sentence. Since the selection of a correct word from a full vocabulary would be computationally very expensive, we use the list of the most frequent words (in our tests 1000 words) from which the language model tries to find the correct word. Of course, the list is constructed in such a way that it always contains the correct word (the word that was originally in the focused place in the sentence). The stop words were skipped during this test.

During the test, the words are sorted according to the probability given by the language model. The test then measures the average order of where the model placed the correct word. The best result is when the model places the word as the first word. If the wrong word is chosen, it matters how far the correct word from the top of the list is. We assume that the closer to the top the correct word is the easier the error can be corrected by the system.

The test is formalized as follows. During the mth test, let O
                        
                           m
                         denote the order of the correct answer in the list of possibilities, which is sorted according to the language model. The expectation value of order E(O) is then defined as


                        
                           
                              (9)
                              
                                 E
                                 (
                                 O
                                 )
                                 =
                                 
                                    1
                                    M
                                 
                                 
                                    ∑
                                    
                                       m
                                       =
                                       1
                                    
                                    M
                                 
                                 
                                    
                                       O
                                       m
                                    
                                 
                                 ,
                                 
                                 1
                                 ≤
                                 m
                                 ≤
                                 M
                                 ,
                              
                           
                        where M means the total number of words in progress.

Similarly to the previous subsection, the numbers in bold in Table 4
                        a–c are the best from all semantic spaces, and the numbers in brackets are relative improvements against the baseline.

Although this is not a standard test, we believe that it may provide more fine-grained performance evaluation than other tests. The test distinguishes between really wrong answers (the correct word is far from the beginning of the list) and almost correct answers (the correct word is close to the beginning). The expectation value helps to create a reasonable idea about the probability distributions of different models.

From the tables above it is easy to see that the HAL and COALS models absolutely lead the word estimation test for all tested languages. Their combination created the best improvement when compared to the baseline (for Czech and Slovak more than 10% improvement in the average order of the correct word).

This section describes the performance of proposed language models in a machine translation task. The success in this task should verify the ability of the models to improve performance of a real-world application. The system used in this test is based upon the statistical machine translation toolkit called Moses,
                           5
                        
                        
                           5
                           Available at http://www.statmt.org/moses/.
                         briefly described in Koehn et al. (2007). To train the system the instructions for building a baseline system were accurately followed. The parallel corpora used for training consisted of texts in Czech, Slovak and English languages from European Parliament proceedings corpus (EuroParl)
                           6
                        
                        
                           6
                           Available at http://www.statmt.org/europarl/.
                         version 6. Statistics of used corpora are shown in Table 5
                        . Training parameters are in Table 6
                        . Machine translation experiments with the EuroParl corpus are presented e.g. in Koehn (2005).

In our experiments the translation decoding was done in two steps. In the first step the Moses n-best decoder was used for generating 500 best hypotheses. The Moses standard 3-gram language model was used for speeding up the decoding phase. In the second step the hypotheses were re-scored by our 4-gram language models by replacing the Moses language models scores with the scores of our models.

The results are evaluated by the widely used BLEU metric (Papineni et al., 2002), which measures n-gram overlap with reference translations. Results are presented in Tables 7
                        a–c.

The results clearly show a significant performance gain in machine translation from English into Czech and Slovak languages. Even translation from Czech and Slovak into English showed some increase in BLEU points. The HAL and COALS combined model is again the best performing one. The performance of other models indicates the same trends as in previous tests. This test is a solid proof that the proposed language models are usable in a real-world application.

@&#DISCUSSION@&#

In the previous subsections we showed several experiments that tried to compare and test our language models in different tasks. Perplexities, machine translation performance and average word orders describe the behavior of language models and help to give us an idea about the probability distribution of language models. However, the semantic spaces tested in this work may perform differently in different applications in NLP; the results of their usage in language modeling are summarized below.

During our experiments, we found that semantic spaces have different behavior when we cluster words into differing numbers of clusters. Some of the semantic spaces (e.g. HAL model) are better for dense clusters (1000 or 5000 classes), some of them (e.g. COALS model) are more suitable for sparse clusters (20,000 or 50,000 classes). The combination of HAL-based language models for small number of clusters together with COALS-based models for greater number of clusters gave the best results.

Some thoughts were given into the analysis of why HAL model performs better on dense clusters and COALS is better for sparse clusters. It has to be noted that the following ideas can be hardly proven and have to be considered only as speculations. The basic difference between HAL and COALS in our experiments is that HAL creates vectors with dimension 50,000. COALS uses dimension only 14,000 and modifies vectors by correlation, zeroing the negative numbers and enhancing the positive ones. We believe that the impact of these differences is that COALS can capture more precisely very similar words. The HAL on the other hand benefits from a larger vector and describes more precisely no so much similar words. The results are that COALS is better to create sparse clusters (consisted of more similar words) and HAL is better for dense clusters (not so similar words).

As expected we achieved better results for inflectional languages. For Czech and Slovak, the best improvement in perplexity was 17.9% and 16.1% respectively. For the average order of the correct word in the word estimation test, we achieved a 13.6% and 15.6% relative reduction. The improvement in the machine translation was 0.72 and 0.66 of BLEU points. For English, the improvement was not so significant. Perplexity was reduced by about 10.1%. The word estimation test was improved by about 8.3% and the increase in BLEU metrics of 0.37 points was achieved.

Our explanation is as follows. We discovered that during word clustering, the semantic spaces have a tendency to connect together words with similar morphological forms. In an ideal case, the semantic space should merge together those words that have the same meaning and simultaneously the same morphological form (the same morphological tag). According to several studies in morphological-based language models (Vaiciunas et al., 2004; Kirchhoff et al., 2006; Oparin, 2008; Brychcín and Konopík, 2011), the impact of morphological analysis for modeling of inflectional languages is much more significant than for modeling of low inflection languages. We believe that this is the main reason why the techniques described in this article are more suitable for inflectional languages.

In our experiments we also uncovered several interesting facts. The COALS model with SVD reduction performs very badly for Czech and Slovak. We believe this is caused by the data sparsity problem. For English, the results of the model are better, however, its combination with the baseline model gives no improvement. Similarly, the P&P model performs very well when we investigate the stand-alone class-based language models, but improvement against baseline is negligible. What is noteworthy is that the Random Indexing gives each of the tested languages some improvement but it is worse than the HAL or COALS model. Finally, the BEAGLE model (a similar approach to word meaning modeling as Random Indexing) also produces negligible improvement when compared to the baseline.

@&#SUMMARY@&#

@&#FUTURE WORK@&#

In this work, we concentrated solely on a way to improve the probability estimates with the information that is already in the data. We wanted to use no external information. We see yet another possibility to improve the models without external information. The work will focus on an analysis of OOV
                           7
                        
                        
                           7
                           Out of vocabulary (OOV) word means an unknown (previously unseen) word for language model.
                         words. The idea is based on an estimation of the semantic vectors of OOV words from their contexts and by using an unsupervised approach to morphological analysis (morpheme segmentation, stemming, etc.). Our findings lead us to believe that OOV words are the central weakness of many language models. We believe that by focusing on OOV words we may achieve a very significant performance boost.

The experiments with the Moses machine translation framework revealed that it is an extraordinary framework with great extension capabilities. In the future we plan to experiment with factored translation allowing the usage of class information directly in the training/decoding phase. We expect that by the tighter link between Moses and our language models, even better results can be achieved.

@&#CONCLUSION@&#

In this article we tried to reach a really hard target. We tried to beat n-gram language models merely with better probability estimates and without any external knowledge (morphology, syntax, partitioning into documents). We choose not to do this in a simpler way where n-gram models are weak, we ignored low occurring words and we instead concentrated on the modeling of words with reasonable frequency in the data (five or more). By using semantic spaces and clustering, we were able to improve the probability estimates and achieve significant improvements. The improvement was detected in both synthetic and real-world tests.

To be completely honest, we must note that our models in their current state are not very suitable for practical use. The clustering phase (the most computationally intensive phase) may take up to one week in a powerful computational unit. The models are also very memory demanding since the final language model is a combination of the 4-gram model and several class based models. We believe that there is great room for optimization as there is much redundancy in all the models. By clever pruning, a huge memory saving may be achieved. This was however not the point of our efforts. Our work was rather the proof-of-concept effort. We concentrated on finding a new possibility for improving language modeling. We can state that the semantic class based language models are suitable for building large corpora language models.

We think that our article may also be useful in another way. The semantic spaces presented in the article are quite a new branch of corpus statistics. It is challenging to measure the performance of semantic spaces since it is hard to find an objective criterion. We believe that application in language modeling may be used as an objective criterion that helps to compare semantic spaces with one another. Taking into account the results and our findings during testing, we can recommend the HAL and COALS models. Especially the combination HAL and COALS model seems to provide very good results. We can also recommend the RI models even though they did not win any test. They were, however, able to provide consistently very good results and are computationally very undemanding.

@&#ACKNOWLEDGEMENTS@&#

This work was supported by Grant No. SGS-2010-028, by Grant No. SGS-2013-029 Advanced computing and information systems, by the European Regional Development Fund (ERDF) and by project “NTIS - New Technologies for Information Society”, European Centre of Excellence, CZ.1.05/1.1.00/02.0090. Access to the MetaCentrum computing facilities provided under the program “Projects of Large Infrastructure for Research, Development, and Innovations” LM2010005, funded by the Ministry of Education, Youth, and Sports of the Czech Republic, is highly appreciated. The access to the CERIT-SC computing and storage facilities provided under the program Center CERIT Scientific Cloud, part of the Operational Program Research and Development for Innovations, reg. no. CZ. 1.05/3.2.00/08.0144 is acknowledged. We also thank the Czech News Agency (CNA) for providing a huge number of texts in Czech. We would like to thank David Jurgens and Dr. Keith Stevens for the implementation methods for building semantic spaces (Jurgens and Stevens, 2010) we use in this work.

@&#REFERENCES@&#

