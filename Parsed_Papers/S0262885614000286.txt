@&#MAIN-TITLE@&#Online parameter tuning for object tracking algorithms

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present a new control approach to adapt trackers to scene condition variations.


                        
                        
                           
                           Tracking context is defined as six features describing scene condition.


                        
                        
                           
                           Best tracker parameters are learned offline for tracking contexts.


                        
                        
                           
                           Trackers are then controlled by tuning online their parameters.


                        
                        
                           
                           Experimental results are compared with several recent state of the art trackers.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Object tracking

Online parameter tuning

Controller

Self-adaptation

Machine learning

@&#ABSTRACT@&#


               
               
                  Object tracking quality usually depends on video scene conditions (e.g. illumination, density of objects, object occlusion level). In order to overcome this limitation, this article presents a new control approach to adapt the object tracking process to the scene condition variations. More precisely, this approach learns how to tune the tracker parameters to cope with the tracking context variations. The tracking context, or context, of a video sequence is defined as a set of six features: density of mobile objects, their occlusion level, their contrast with regard to the surrounding background, their contrast variance, their 2D area and their 2D area variance. In an offline phase, training video sequences are classified by clustering their contextual features. Each context cluster is then associated to satisfactory tracking parameters. In the online control phase, once a context change is detected, the tracking parameters are tuned using the learned values. The approach has been experimented with three different tracking algorithms and on long, complex video datasets. This article brings two significant contributions: (1) a classification method of video sequences to learn offline tracking parameters and (2) a new method to tune online tracking parameters using tracking context.
               
            

@&#INTRODUCTION@&#

Mobile object tracking plays an important role in an increasing number of computer vision applications (e.g. home care, sport scene analysis and visual surveillance). The object trajectories are useful for activity recognition, learning of interest zones or paths in a scene and detection of events of interest. Unfortunately the tracking quality depends on many factors: the quality of vision tasks performed at lower levels such as object detection, object classification, and by some video features such as complexity of object movements, scene illumination intensity, low contrast, high density and occlusion frequency of mobile objects. In particular, for a long video sequence (i.e. several hours) in which the variations of these properties happen frequently, the tracking quality is still an issue. The problems we focus on are the following: How can an automatic system robustly track mobile objects in different conditions and situations such as the ones cited above. And in those complex cases, how can the user regulate the tracking parameters to get an optimal tracking quality?

In order to answer these two questions, we propose in this article a new method for controlling tracking algorithms. The objective of the proposed method is to define an automatic control algorithm which is able to adapt online the tracking task to the scene variations in a video sequence by tuning the tracking parameters over time. We aim to build a control algorithm which is: generic, flexible and intelligent. The term “generic” means that our method can handle different tracking algorithm categories. In this work, our objective is to control tracking algorithms which rely on object appearance or points of interest. These algorithms are selected because their approaches are largely studied in the state of the art. The term “flexible” implies that the structure of the proposed control algorithm can be adapted for handling other tracking algorithm category (e.g. object silhouette-based tracking). The term “intelligent” means that this approach requires less human interaction than the control methods in the state of the art.

The control method presented in this manuscript relies on the two following hypotheses:
                           
                              1.
                              The considered tracking algorithms have at least one tunable parameter which influences significantly the tracking quality.

There exist a number of contexts which have an impact on the tracking quality. Let g be a function mapping a video v
                                 
                                    i
                                  to its context. For a tracking algorithm 
                                    T
                                 , we suppose that there exists a function 
                                    
                                       f
                                       T
                                    
                                  mapping a video context to satisfactory tracking parameter values (i.e. parameter values for which the tracking quality is greater than a predefined threshold 
                                    s
                                 ):

Let ϵ1 and ϵ2 be predefined thresholds. The function f is assumed to satisfy the following property if the temporal lengths of v
                        1 and v
                        2 are short enough (lower than 50 frames):
                           
                              (2)
                              
                                 
                                    ∀
                                    
                                       v
                                       1
                                    
                                    ,
                                    
                                       v
                                       2
                                    
                                    :
                                    if
                                    |
                                    g
                                    
                                       
                                          v
                                          1
                                       
                                    
                                    −
                                    g
                                    
                                       
                                          v
                                          2
                                       
                                    
                                    |
                                    
                                    <
                                    
                                       ϵ
                                       1
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    =
                                    >
                                    |
                                    Q
                                    
                                       
                                          
                                             O
                                             T
                                          
                                          
                                             
                                                
                                                   f
                                                   T
                                                
                                                ∘
                                                g
                                                
                                                   
                                                      v
                                                      1
                                                   
                                                
                                             
                                          
                                          ,
                                          
                                             G
                                             
                                                v
                                                1
                                             
                                          
                                       
                                    
                                    −
                                    Q
                                    
                                       
                                          
                                             O
                                             T
                                          
                                          
                                             
                                                
                                                   f
                                                   T
                                                
                                                ∘
                                                g
                                                
                                                   
                                                      v
                                                      2
                                                   
                                                
                                             
                                          
                                          ,
                                          
                                             G
                                             
                                                v
                                                2
                                             
                                          
                                       
                                    
                                    |
                                    
                                    <
                                    
                                       ϵ
                                       2
                                    
                                    .
                                 
                              
                           
                        
                     

This hypothesis means that if the contexts of two videos v
                        1 and v
                        2 are close enough, the tracking performances for v
                        1 and v
                        2 corresponding to their satisfactory tracking parameter values are also close enough.

The hypothesis 2 is given for two objectives. First, we can compute the satisfactory tracking parameter values for a video context cluster using satisfactory parameters of contexts (see Section 3.4.2). Second, the satisfactory tracking parameters for context clusters can be used for tuning online the tracking parameters (see Section 4.2).

This article is organized as follows. Section 2 presents a state of the art on control methods. Section 3, entitled “offline learning phase”, details a scheme to learn satisfactory tracking parameters for each video context cluster. Section 4 describes the online parameter tuning process. Section 5 is dedicated to the experimentation and validation of the proposed method. Section 6 presents concluding remarks as well as future work.

Many approaches have been proposed to track mobile objects in a scene [37]. Depending on taxonomy criteria, the trackers can be classified into different categories. Fig. 1
                      presents a taxonomy example (the red ellipses mark the tracker categories controlled by the proposed method). However the quality of tracking algorithms always depends on scene properties such as: mobile object density, contrast intensity, scene depth and object size. The selection of a tracking algorithm for an unknown scene becomes a hard task. Even when the tracker has already been determined, it is difficult to tune online its parameters to get a high performance.

The idea about an automatic control to adapt the performance of a system to the problem of scene variations has already been studied. However some methods limit their studies to static image and not to video processing. For example the authors in Ref. [34] present a framework which is able to integrate expert knowledge and uses it to control the image processing programs. The framework is experimented on three different applications: road obstacle detection, medical imaging and astronomy. By considering both context and evaluation criteria, the system can find the best algorithm among a predefined algorithm set and tune its parameters to obtain the best possible performance. However, the construction of a knowledge base for this system requires a lot of time and data.

The authors in Ref. [17] present a controlled video understanding system based on a knowledge base. The system is composed of three main components in which the control component performs several steps for managing all the online processes of the system (e.g. program execution and automatic parameter tuning). Different rules are defined in this component based on user goal, contextual information and evaluation results. However their approach does not address directly the tracking task.

Some methods have addressed the tracking parameter tuning, however their approaches require too strong hypotheses and expert knowledge. For example, the author in Ref. [30] proposes an approach to tune automatically tracking algorithm parameters. In this approach, the tracker quality is represented as a function of tuned parameters. The author supposes that this function has no local optimal solutions. Using this hypothesis, for each parameter and a training video, the author determines its optimal value thanks to expert knowledge. Then the parameter tendency (i.e. increase or decrease) for converging to the optimal value is learned in function of the tracker input and output. This learned parameter tendency is used in the online phase to tune automatically the corresponding parameter to improve the tracking performance. In Ref. [9], the authors compare the tracker results with corresponding ground-truth data to determine the importance of each parameter for each context and to exploit the influence of each parameter variation on tracker performance. The authors suppose that parameter variations are independent. This is a strict hypothesis because the parameters are usually dependent on each other. In Ref. [10], the authors propose a tracking algorithm whose parameters can be learned offline for each tracking context. However the authors suppose that the context within a video sequence is fixed over time. Moreover, the tracking context is manually selected.

Some approaches have been proposed to decrease the need of expert knowledge [19,29], however they are expensive in terms of processing time and their performance is dependent on an automatic tracking evaluation. For example, in Ref. [19], the author proposes two strategies to regulate the parameters for improving the tracking quality. In the first strategy, the parameter values are determined using an enumerative search. In the second strategy, a genetic algorithm is used to search for the best parameter values. This approach does not require human supervision and parameter knowledge for controlling its tracker. However, it is computationally expensive because of the parameter optimization performed in the online phase. Moreover, this approach requires an online tracking evaluation (without ground-truth data) to verify the performance of the tracker when using the found parameters. This can decrease the approach performance. In Ref. [29], the authors present a tracking framework which is able to control a set of different trackers to get the best performance. The system runs three tracking algorithms in parallel: normalized cross-correlation (NCC), mean-shift optical flow (FLOW) and online random forest (ORF). FLOW is used as a main tracker. If the tracker quality of ORF is better, FLOW is replaced by ORF. When NCC quality is better than the one of ORF, it takes the main role. The approach is interesting but the authors do not mention how to estimate online the tracker quality. Also, the execution of three trackers in parallel is very expensive in terms of processing time.

@&#DISCUSSION@&#

As analyzed above, many approaches whose objective is to control the tracking process have been studied in the state of the art. These methods have the following issues.

The first issue relates to the context notion. While some methods study context for static image applications [34], to our knowledge, there are no approach which proposes a formal definition for object tracking context.

The second issue is about the generic level of the control methods. Some approaches need too strong hypotheses on the relation between the tracking quality and tracking parameters [30] or on the independence between tracking parameters [9]. Some other methods require expert knowledge [34,10] for building knowledge base or for tuning parameters. These requirements reduce the genericity of these approaches.

The third issue pertains to the feasibility of these studies. Some approaches are expensive in terms of processing time [19,29].

In this article, we propose a control method for object tracking algorithms addressing these issues. In this article, the control trackers belong to “Appearance tracking” or “Point tracking” (see Fig. 1). These tracker categories are selected because they are the most popular ones and are largely studied in the state of the art. Our proposed method includes two phases: an offline learning phase and an online parameter tuning. The next sections present in detail the steps of these two phases.

The objective of the learning phase is to create a database which supports the control process of a tracking algorithm. This database contains satisfactory parameter values of the tracking algorithm for various scene conditions.

This phase takes as input training video sequences, annotated objects, annotated trajectories, a tracking algorithm including its control parameters. The term “control parameters” refers to parameters which are considered in the control process (i.e. to look for satisfactory values in the learning phase and to be tuned in the online phase). In this work we consider only numerical parameters, however the proposed method can be applied also on symbolic parameters. At the end of the learning phase, a learned database is created (if this is the first learning session) or updated (if not). A learning session can process many video sequences. Fig. 2
                      presents the proposed scheme for building and updating the learned database.

The notion of “context” (or “tracking context”) in this work represents elements in the videos which influence the tracking quality. More precisely, a context of a video sequence is defined as a set of six features: density of mobile objects, their occlusion level, their contrast with regard to the surrounding background, their contrast variance, their 2D area and their 2D area variance. For each training video, we extract these contextual features from annotated objects and then use them to segment the training video in a set of consecutive chunks. Each video chunk has a stable context. The context of a video chunk is represented by a set of six code-books (corresponding to six features). An optimization process is performed to determine satisfactory tracking parameter values for the video chunks. These parameter values and the set of code-books are inserted into a temporary learned database. After processing all training videos, we cluster these contexts and then compute satisfactory tracking parameter values for context clusters.

In the following, we describe the four steps of the offline learning process: (1) contextual feature extraction, (2) context segmentation and code-book modeling, (3) tracking parameter optimization and (4) clustering (composed of two sub-steps: context clustering and parameter computation for context clusters).

For each training video, the context feature values are computed for every frame.

The density of mobile objects influences significantly the tracking quality. A high density of objects may lead to a decrease of object detection and tracking performance. The density of mobile objects at instant t is defined by the sum of all object areas over the 2D camera view.

The occlusion level of mobile objects is the main element which influences the tracking quality. An occlusion occurrence makes the object appearance partially or completely not visible. It decreases the object detection and tracking performance. In particular, the variation of object occlusion level over time is even more challenging because the coherence of object appearance changes significantly. Given two objects i and j at instant t of respectively 2D areas a
                           
                              t
                           
                           
                              i
                            and a
                           
                              t
                           
                           
                              j
                           , we compute their occlusion level based on their area overlap as follows:
                              
                                 (4)
                                 
                                    
                                       o
                                       
                                          l
                                          t
                                          k
                                       
                                       =
                                       
                                          
                                             a
                                             t
                                             ij
                                          
                                          
                                             min
                                             
                                                
                                                   a
                                                   t
                                                   i
                                                
                                                
                                                   a
                                                   t
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where k denotes the index value of this occlusion in the set of occlusions occurring at time t and a
                           
                              t
                           
                           
                              ij
                            is the overlap area of objects i and j at t. Two objects i and j are considered in an occlusion state if ol
                           
                              t
                           
                           
                              k
                            is greater than a predefined threshold. Let 
                              
                                 N
                                 t
                              
                            be the number of object occlusion occurrences at instant t and ol
                           
                              t
                           
                           
                              k
                            the occlusion level of case k (
                              
                                 k
                                 =
                                 1
                                 ..
                                 
                                    N
                                    t
                                 
                              
                           ). The occlusion level of mobile objects in a scene at instant t, denoted o
                           
                              t
                           , is defined as follows:
                              
                                 (5)
                                 
                                    
                                       
                                          o
                                          t
                                       
                                       =
                                       min
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         k
                                                         =
                                                         1
                                                      
                                                      
                                                         N
                                                         t
                                                      
                                                   
                                                   
                                                
                                                o
                                                
                                                   l
                                                   t
                                                   k
                                                
                                                ×
                                                2
                                             
                                             
                                                n
                                                t
                                             
                                          
                                          1
                                       
                                    
                                 
                              
                           where n
                           
                              t
                            is the number of mobile objects at t. The multiplication by 2 in the formula is explained by the fact that an occlusion occurrence is related to two objects.

The contrast of an object is defined as the color intensity difference between this object and its surrounding background. Let B
                           
                              i
                           
                           ={C
                           
                              i
                           ,W
                           
                              i
                           ,H
                           
                              i
                           } be the 2D bounding box of object i where C
                           
                              i
                           , W
                           
                              i
                            and H
                           
                              i
                            are respectively its 2D center, width and height. We define an extra bounding box of object i: B
                           
                              i
                           
                           +
                           ={C
                           
                              i
                           ,W
                           
                              i
                           
                           +
                           γ
                           
                              M
                           
                           
                              i
                           ,H
                           
                              i
                           
                           +
                           γ
                           
                              M
                           
                           
                              i
                           } where 
                              M
                           
                           
                              i
                           
                           =min(W
                           
                              i
                           ,H
                           
                              i
                           ), γ is a predefined value in interval [0,1]. The surrounding background of object i is defined as the area 
                              
                                 
                                    B
                                    i
                                 
                                 =
                                 
                                    B
                                    i
                                    +
                                 
                                 ∖
                                 
                                    B
                                    i
                                 
                              
                           .

An object with low contrast reduces first the object detection quality. Second, this decreases the discrimination of the appearance between different objects. So the quality of tracking algorithms which rely on object appearances decreases in this case. The contrast of an object can vary due to the change of its spatial location (see Fig. 3a.) or over time (see Fig. 3b. and c.). The contrast of mobile objects at instant t is defined asthe mean value of the contrasts of objects at instant t.

When different object contrast levels exist in the scene (see Fig. 3a.), a mean value cannot represent correctly the contrast of all objects in the scene. Therefore we define the variance of object contrasts at instant t as their standard deviation value:
                              
                                 (6)
                                 
                                    
                                       
                                          
                                             
                                                c
                                                ^
                                             
                                             t
                                          
                                       
                                       =
                                       
                                          
                                             
                                                1
                                                
                                                   n
                                                   t
                                                
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         c
                                                         t
                                                         i
                                                      
                                                      −
                                                      
                                                         
                                                            
                                                               c
                                                               ¯
                                                            
                                                            t
                                                         
                                                      
                                                   
                                                
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           where c
                           
                              t
                           
                           
                              i
                            is the contrast value of object i at t and 
                              
                                 
                                    
                                       c
                                       ¯
                                    
                                    t
                                 
                              
                            is the mean value of all object contrasts at t.

2D area of an object is defined as the number of pixels within its 2D bounding box. Therefore, this feature also characterizes the reliability of the object appearance for the tracking process. The greater the object area is, the higher the object appearance reliability is. The 2D area feature value at t is defined as the mean value of the 2D areas of mobile objects at instant t.

Similar to the contrast feature, we define the variance of object 2D areas at instant t as their standard deviation value.

The contextual variation of a video sequence influences significantly the tracking quality. Therefore it is not optimal to keep the same parameter values for a long video. In order to solve this issue, we propose an algorithm to segment a training video in consecutive chunks, each chunk is defined as having a stable context (i.e. the values of a same context feature in each chunk are close to each other). This algorithm is described as follows.
                              
                                 1.
                                 The training video is segmented in parts of l frames. The last part can have a temporal length lower than l. The value of l should be low enough (e.g. 50 frames) so that each video part has a stable enough context.

The contextual feature values of the first part is represented by a context code-book model (see more details in Section 3.2.2).

From the second video part, we compute the context distance between the current part and the context code-book model of the previous part (see more details in Section 3.2.3). If their distance is lower than a threshold Th
                                    1 (e.g. 0.5), the context code-book model is updated with the current video part. Otherwise, a new context code-book model is created to represent the context of the current video part. The higher Th
                                    1 value, the less stable the obtained context code-book models are.

At the end of the context segmentation algorithm, the training video is divided into a set of chunks (of different temporal lengths) corresponding to the obtained context code-book models. There are two open problems: How to represent a video context with a code-book model? and how to compute the distance between a context code-book model and a context. The following sections answer these two questions.

During the tracking process, low frequent feature values play an important role for tuning tracking parameters. For example, when mobile object density is high in few frames, the tracking quality can decrease significantly. Therefore, we decide to use a code-book model [24] to represent the values of contextual features because this model can estimate complex and low-frequency distributions. In our approach, each contextual feature is represented by a code-book, called feature code-book, and denoted cb
                           
                              k
                           ,k
                           =1..6. So a video context is represented by a set of six feature code-books, called context code-book model, and denoted CB, CB
                           ={cb
                           
                              k
                           ,k
                           =1..6}. A feature code-book is composed of a set of code-words describing the values of this feature. The number of code-words depends on the diversity of feature values.

A code-word represents the values and their frequencies of a contextual feature. A code-word i of code-book k (k
                              =1..6), denoted cw
                              
                                 i
                              
                              
                                 k
                              , is defined as follows:
                                 
                                    (7)
                                    
                                       
                                          c
                                          
                                             w
                                             i
                                             k
                                          
                                          =
                                          
                                             
                                                
                                                   μ
                                                   i
                                                   k
                                                
                                                ¯
                                             
                                             
                                                m
                                                i
                                                k
                                             
                                             
                                                M
                                                i
                                                k
                                             
                                             
                                                f
                                                i
                                                k
                                             
                                          
                                       
                                    
                                 
                              where 
                                 
                                    
                                       μ
                                       i
                                       k
                                    
                                    ¯
                                 
                               is the mean of the feature values belonging to this code-word; m
                              
                                 i
                              
                              
                                 k
                               and M
                              
                                 i
                              
                              
                                 k
                               are the minimal and maximal feature values belonging to this word respectively; f
                              
                                 i
                              
                              
                                 k
                               is the number of frames when the feature values belong to this word.


                              
                                 
                                    –
                                    At the beginning, the code-book cb
                                       
                                          k
                                        of a context feature k is empty.

For a value μ
                                       
                                          t
                                       
                                       
                                          k
                                        of a contextual feature k computed at time t, verify if μ
                                       
                                          t
                                       
                                       
                                          k
                                        activates any code-word in code-book cb
                                       
                                          k
                                       . μ
                                       
                                          t
                                       
                                       
                                          k
                                        activates code-word cw
                                       
                                          i
                                       
                                       
                                          k
                                        if both conditions are satisfied:
                                          
                                             +
                                             
                                                μ
                                                
                                                   t
                                                
                                                
                                                   k
                                                 is in range [0.7×
                                                m
                                                
                                                   i
                                                
                                                
                                                   k
                                                ,1.3×
                                                M
                                                
                                                   i
                                                
                                                
                                                   k
                                                ].

The distance between μ
                                                
                                                   t
                                                
                                                
                                                   k
                                                 and cw
                                                
                                                   i
                                                
                                                
                                                   k
                                                 is smaller than a threshold ϵ3. This distance is defined as follows:
                                                   
                                                      (8)
                                                      
                                                         
                                                            distance
                                                            
                                                               
                                                                  
                                                                     μ
                                                                     t
                                                                     k
                                                                  
                                                                  ,
                                                                  c
                                                                  
                                                                     w
                                                                     i
                                                                     k
                                                                  
                                                               
                                                            
                                                            =
                                                            1
                                                            −
                                                            
                                                               
                                                                  min
                                                                  
                                                                     
                                                                        μ
                                                                        t
                                                                        k
                                                                     
                                                                     
                                                                        
                                                                           μ
                                                                           i
                                                                           k
                                                                        
                                                                        ¯
                                                                     
                                                                  
                                                               
                                                               
                                                                  max
                                                                  
                                                                     
                                                                        μ
                                                                        t
                                                                        k
                                                                     
                                                                     
                                                                        
                                                                           μ
                                                                           i
                                                                           k
                                                                        
                                                                        ¯
                                                                     
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                where 
                                                   
                                                      
                                                         μ
                                                         i
                                                         k
                                                      
                                                      ¯
                                                   
                                                 is the mean value of code-word cw
                                                
                                                   i
                                                
                                                
                                                   k
                                                .

If cb
                                       
                                          k
                                        is empty or if there is no code-word activated, create a new code-word and insert it into cb
                                       
                                          k
                                       .

If μ
                                       
                                          t
                                       
                                       
                                          k
                                        activates cw
                                       
                                          i
                                       
                                       
                                          k
                                       , this code-word is updated with the value of μ
                                       
                                          t
                                       
                                       
                                          k
                                       .

The code-words whose value f
                              
                                 i
                               is lower than a threshold are eliminated because they are corresponding to too low frequency feature values. The “contextual feature extraction” and “code-book modeling” steps of a video chunk play the role of the function g (mapping a video sequence to its context) presented in hypothesis 2 (Section 1.1).

This section presents how to compute the distance between a context c and a context code-book model CB
                           ={cb
                           
                              k
                           ,k
                           =1..6}. This distance is defined as a function of sub-distances between context c and code-books cb
                           
                              k
                           . This sub-distance is expressed by the number of times where matching code-words are found. Table 1
                            presents the algorithm to compute the context distance in which the function distance(μ
                           
                              t
                           
                           
                              k
                           ,cw
                           
                              i
                           
                           
                              k
                           ) is defined as in formula in Eq. (8).

The objective of the tracking parameter optimization task is to find the values of the control parameters which ensure the best possible tracking quality for each video chunk. This quality has to be greater than the threshold 
                           s
                         presented in hypothesis 2, Section 1.1. These parameters are called “satisfactory parameters”.

This task takes as input annotated objects, annotated trajectories, a tracking algorithm, a video chunk and control parameters for the considered tracker. The annotated objects are used as object detection results. This task provides as output satisfactory parameter values. For each control parameter, its name, value range and step value are needed. The step value of a parameter is defined as the minimal variation which causes a significant change on the tracking quality. This value helps to avoid scanning the entire parameter space when searching for its satisfactory values.

Depending on the search space size and the nature of the control parameters, we can select suitable optimization algorithm. If the control parameter space is small, an exhaustive search [28] or an enumerative search can be used to scan the values of these parameters. Otherwise, we can use a genetic algorithm [18] for searching satisfactory values. In some cases, an optimization problem can be converted to a classification problem whose objective is to optimize the weights of weak classifiers. In this case, the AdaBoost algorithm [15] can be used to determine the best values of these weights (see example in Ref. [10]). More than one optimization algorithm can be performed if the search space or the nature of the control parameters is different.

In order to represent the reliability of the found parameter values, we associate them to two values. The first one is the number of frames of the training video chunk in which mobile objects appear (called “number of training frames”). The second one is a F-Score value representing the tracking quality of the considered video chunk while using the found tracking parameter values. Satisfactory parameter values, their reliability values and the context code-book model corresponding to this video chunk are stored into a temporary learned database.

The clustering step is done at the end of each learning session when the temporary learned database contains the processing results of all training videos. In some cases, two similar contexts can have different satisfactory parameter values because optimization algorithm only finds local optimal solutions. Moreover, the context of a video sequence is not sufficient for determining the best satisfactory tracking parameter values. A clustering step is thus necessary to group similar contexts and to compute satisfactory parameter values for the context clusters. The clustering step is composed of two sub-steps: context clustering and parameter computation for context clusters (see Fig. 4
                        ).

This step takes as input the training videos, the annotated objects, tracking algorithm and annotated trajectories. It also requires the data stored in the temporary learned database and in the learned database resulting from the previous learning sessions. These data include a set of contexts or context clusters associated to their satisfactory tracking parameter values and the reliability values of these parameters. This step gives as output the new context clusters which are associated to their satisfactory parameter values and the reliability values of these parameters.

For the context clustering step, we use the Quality Threshold Clustering (QT clustering) algorithm [21] due to the following three reasons. First, only clusters that pass a user-defined quality threshold can be returned. Second, this algorithm does not require the number of clusters as input. Third, all possible clusters are considered. However, a diameter threshold d is needed to consider whether two contexts can be grouped. The higher this threshold, the more easily contexts are clustered. This threshold can be estimated by defining the distance metric value between two context code-book models in the interval [0, 1].

The distance between a context and a context cluster is defined as the complete linkage (i.e. the maximum distance from the context to any context of the cluster) [14] to ensure a high reliability for the clustering process. In the following, we present how to compute the distance between two context code-book models.

In order to compute the distance between two context code-book models CB
                              
                                 α
                               and CB
                              
                                 β
                              , each feature code-book cb
                              
                                 k
                               (k
                              =1..6) of a context is transformed into a histogram whose bin i corresponds to feature value 
                                 
                                    
                                       u
                                       i
                                       k
                                    
                                    ¯
                                 
                               of code-word i, and value of bin i is defined as f
                              
                                 i
                              
                              
                                 k
                              /N where N is the number of training frames of the code-book, f
                              
                                 i
                              
                              
                                 k
                               is the number of frames in which code-word i is activated (see Section 3.2.2).

The distance between two feature code-books is defined as the Earth Mover Distance between the two corresponding histograms in which the ground distance between bins i and j is defined as 
                                 
                                    
                                       
                                          
                                             μ
                                             i
                                             k
                                          
                                          ¯
                                       
                                       −
                                       
                                          
                                             u
                                             j
                                             k
                                          
                                          ¯
                                       
                                    
                                 
                              . The distance between two context code-book models is defined as the mean value of the six distances between the six feature code-books.

The objective of the “Parameter computation for context clusters” sub-step is to compute satisfactory parameter values for the context clusters. This sub-step includes two stages: “Parameter computation” and “Parameter verification”.

Once contexts are clustered, all the code-words of these contexts become the code-words of the created cluster. The satisfactory tracking parameters of cluster j, denoted 
                                 
                                    
                                       
                                          p
                                          →
                                       
                                       j
                                    
                                 
                              , are computed as follows:
                                 
                                    (9)
                                    
                                       
                                          
                                             
                                                
                                                   p
                                                   →
                                                
                                                j
                                             
                                          
                                          =
                                          
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      
                                                         Θ
                                                         j
                                                      
                                                   
                                                   
                                                
                                                
                                                   
                                                      
                                                         p
                                                         →
                                                      
                                                      i
                                                   
                                                
                                                ×
                                                
                                                   w
                                                   i
                                                
                                             
                                             
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      
                                                         Θ
                                                         j
                                                      
                                                   
                                                   
                                                
                                                
                                                   w
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                              where Θ
                              
                                 j
                               is the number of contexts belonging to cluster j, 
                                 
                                    
                                       
                                          p
                                          →
                                       
                                       i
                                    
                                 
                               is satisfactory parameter values of context i belonging to this cluster, w
                              
                                 i
                               is the weight of parameters 
                                 
                                    
                                       
                                          p
                                          →
                                       
                                       i
                                    
                                 
                               and is defined in function of the two reliability values of 
                                 
                                    
                                       
                                          p
                                          →
                                       
                                       i
                                    
                                 
                              : number of training frames N
                              
                                 i
                               and F-Scorei
                              :
                                 
                                    (10)
                                    
                                       
                                          
                                             w
                                             i
                                          
                                          =
                                          
                                             
                                                
                                                   N
                                                   i
                                                
                                                /
                                                
                                                   N
                                                   j
                                                
                                                +
                                                
                                                   
                                                      
                                                         F
                                                         −
                                                         Score
                                                      
                                                      i
                                                   
                                                
                                             
                                             2
                                          
                                       
                                    
                                 
                              where N
                              
                                 j
                               is the total number of training frames of all contexts belonging to context cluster j.

The reliability of context cluster j is also represented by two values: number of training frames N
                              
                                 j
                               and a tracking quality score defined as a weighted combination of F-Scorei
                              .

The objective of the parameter verification stage is to check whether the parameters of context clusters resulting from the previous stage (“Parameter computation”) are satisfactory. For each cluster, this stage takes all training videos belonging to this cluster and computes the tracking performance with the parameters resulting from the previous stage. For each training video, if the obtained tracking performance is greater or equal to the one computed by its own satisfactory parameters, these parameters are considered “verified”. Otherwise, this video is removed from the considered cluster. It is then stored separately in the learned database. The context cluster and its satisfactory parameters are re-computed and re-verified.

At the end of the clustering process, we obtain in the learned database a set of context clusters represented similarly as a context: a context model of six code-books associated to satisfactory tracking parameter values, number of training frames and tracking quality score.

The training phase cost represents the time needed for the training phase. This cost depends on the costs from the contextual feature computation, code-book modeling, tracking parameter optimization and clustering. The contextual features are low computational using 2D bounding box features. The code-book modeling and clustering tasks are also not expensive in terms of processing time. Therefore, the training phase cost mostly depends on the complexity of the tracking parameter optimization task. More precisely, it depends on the number of control parameters and their search space size. The cost reduction is twofold. First, we only control parameters which significantly influence the tracking quality. Second, we select a suitable optimization algorithm in function of the search space size and of the nature of control parameters as analyzed in the tracking parameter optimization task (see Section 3.3).

We should note that the training phase requires annotated objects and trajectories as input. This can be done using public annotated datasets (e.g. ETISEO, Caviar) or a graphical tool (e.g. Viper
                           1
                        
                        
                           1
                           
                              http://viper-toolkit.sourceforge.net/.
                        ).

In this section, we describe the proposed controller which aims at tuning online the tracking parameter values for obtaining satisfactory tracking performance. The parameter adaptation task takes as input the video stream, the list of detected objects at every frame and the learned database and gives as output the adaptive tracking parameter values for every new context detected in the video stream (see Fig. 5
                     ). In the following sections, we describe the two steps of this task: the context detection and parameter tuning steps.

An open problem is to detect online the variation of the tracking context. The contextual features are computed from the result of the object detection task. In complex cases such as object occlusions, strong or low illumination intensity, the detection quality can decrease significantly. Also, in some cases, due to the mobile object locations, some wrong detections can happen within a small number of frames. Fig. 6
                         illustrates such case. Therefore, in order to detect the context at current time, we need to collect the values of the contextual features in a large enough number of frames. However if this value is too large, the contextual variation is slowly detected and thus decreases the speed of the parameter adaptation.

This step takes as input for every frame, the list of the current detected objects and the image. For each video chunk of l frames, we compute the values of the contextual features. A contextual change is detected when the context of the current video chunk does not belong to the context cluster (clusters are learned in the offline phase) of the previous video chunk. In order to ensure the coherence between the learning phase and the testing phase, we use the same distance defined in the learning phase (Section 3.2.3) to perform this classification. If this distance is lower than threshold Th
                        1, this context is considered as belonging to the context cluster. Otherwise, the “Parameter adaptation” task is activated.

The parameter adaptation process takes as input the current context from the “context detection” task, an activation signal and gives as output adaptive tracking parameter values. When this process receives an activation signal, it looks for the cluster in the learned database the current context belongs to. Let 
                           D
                         represent the learned database, a context c of a video chunk of l frames belongs to a cluster C
                        
                           i
                         if both conditions are satisfied:
                           
                              (11)
                              
                                 
                                    contextDistance
                                    
                                       c
                                       
                                          C
                                          i
                                       
                                       l
                                    
                                    <
                                    T
                                    
                                       h
                                       1
                                    
                                 
                              
                           
                        
                        
                           
                              (12)
                              
                                 
                                    ∀
                                    
                                       C
                                       j
                                    
                                    ∈
                                    D
                                    ,
                                    j
                                    ≠
                                    i
                                    :
                                    
                                    contextDistance
                                    
                                       c
                                       
                                          C
                                          i
                                       
                                       l
                                    
                                    ≤
                                    contextDistance
                                    
                                       c
                                       
                                          C
                                          j
                                       
                                       l
                                    
                                 
                              
                           
                        where Th
                        1 is defined in Section 3.2.1. The function contextDistance(c, Ci
                        , l) is defined in Table 1. The expression in Eq. (11) represents the condition (2) of hypothesis 2 (Section 1.1). If a such context cluster C
                        
                           i
                         is found, then according to this hypothesis, the satisfactory tracking parameters associated with C
                        
                           i
                         are good enough for parameterizing the tracking of the current video chunk. Otherwise, the tracking algorithm parameters do not change, the current video chunk is marked to be learned offline later.

During the online phase, the processing time depends on the context detection and parameter tuning tasks. The context detection task is fast because the computation of context features and context distance is not time consuming. The parameter tuning task complexity is low because it depends linearly on the number of clusters belonging to the offline learned database.

In the literature, we mention two articles [9] and [30] which have proposed parameter tuning approaches for object tracking algorithm. In this section, we present a qualitative comparison between both approaches and the proposed one. Our comparison relies on the following criteria:
                        
                           •
                           
                              Online execution: As object tracking plays an important role in camera surveillance as well as other online applications, this criterion is very important for the object tracking approaches. The approach in Ref. [9] needs ground-truth data to analyze offline the influence of tracking parameters for tracking quality. This approach cannot be done online whereas our proposed approach and Ref. [30] can be performed online.


                              Parameter tuning should be applicable to a large variety of tracking algorithms: For this criterion, we consider two sub-criteria as follows.
                                 
                                    –
                                    
                                       Requirement of tracking parameter independence: The approaches from Refs. [9] and [30] require that the tracking algorithm have independent tracking parameters. During the training phase, both approaches optimize independently the tracker parameters. The proposed approach does not require this independence.


                                       Requirement of unimodality for the tracking performance on parameters: This means that the performance of the tracking algorithm has a unimodal distribution (i.e. no local optimum) in function of each parameter. The approach in Ref. [30] needs this hypothesis to simplify the search of the best parameter value. Its parameter tuning method is inspired by the first derivative computation of the tracking performance function. Our proposed approach does not need this hypothesis as we use more generic and global optimization techniques (e.g. exhaustive search, enumerate search).


                              Requirement of ground-truth data in the training phase: All the three approaches require ground-truth data to train offline the tracking parameters. However in the training phase of Sherrah [30], the best tracking parameter value is determined by hand, and this needs expert knowledge.


                     Table 2
                      summarizes the qualitative comparison of the proposed approach with the approaches in Refs. [9] and [30]. The column “ideal approach” shows the expectations of an ideal approach. We find that our proposed parameter control approach is more generic and practical than the approaches [9] and [30].

The objective of this experimentation is to measure the effect and robustness of the proposed control method. We experiment this method with three object trackers: an appearance tracker [10], a tracker based on KLT [31] and a tracker based on Surf [5].

The proposed control method has three predefined parameters. The distance threshold Th
                        1 to decide whether two contexts are close enough (Sections 3.2.1 and 4.2) is set to 0.5. The minimum number of frames l of a context segment (Sections 3.2.1 and 4.1) is set to 50 frames. The diameter threshold d in QT clustering algorithm (Section 3.4.1) is set to 0.3. All these values are unchanged for all experiments presented in this article.

A HOG-based algorithm is used for detecting people [13] in videos.

In this experimentation, we select the tracking evaluation metrics used in several publications [36,27,26]. Let GT be the number of trajectories in the ground-truth of the test video. These metrics are defined as follows:
                           
                              –
                              Mostly tracked trajectories (MT): The number of trajectories that are successfully tracked for more than 80% divided by GT.

Partially tracked trajectories (PT): The number of trajectories that are tracked between 20% and 80% divided by GT.

Mostly lost trajectories (ML): The number of trajectories that are tracked less than 20% divided by GT.

The appearance tracker [10] takes as input a video stream and a list of objects detected in a predefined temporal window. The similarity of a pair of detected objects is defined as a weighted combination of five descriptor similarities on 2D area, 2D shape ratio, RGB color histogram, color covariance and dominant color. An object pair with the highest similarity is considered as belonging to a same object trajectory.

For this tracker, six parameters are selected for testing the proposed control method. The first five parameters are the object descriptor weights w
                        
                           k
                         (k
                        =1..5). These parameters depend on the tracking context and have a significant effect on the tracking quality. For the dominant color descriptor described in Ref. [10], the number of dominant colors is required as an input parameter. This parameter is also influenced by the tracking context (for example the smaller object, the higher the number of dominant colors should be). Therefore we use these six parameters as control parameters so that hypothesis 1 (Section 1.1) is ensured. Clearly, the number of dominant colors is only controlled when the weight of dominant color descriptor is not null. As the performance of the controller depends on the object detection quality, the online control process is tested in two cases: with automatically detected objects and with manually annotated objects.

In the learning phase, we use 12 video sequences belonging to different context types (i.e. different levels of density and occlusion of mobile objects as well as of their contrast with regard to the surrounding background, their contrast variance, their 2D area and their 2D area variance). These videos belong to three public datasets (ETISEO,
                              2
                           
                           
                              2
                              
                                 http://www-sop.inria.fr/orion/ETISEO/.
                            Caviar
                              3
                           
                           
                              3
                              
                                 http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/.
                            and Gerhome
                              4
                           
                           
                              4
                              
                                 http://www-sop.inria.fr/members/Francois.Bremond/topicsText/gerhomeProject.html.
                           ), to the European Caretaker project,
                              5
                           
                           
                              5
                              
                                 http://cordis.europa.eu/ist/kct/caretaker_synopsis.htm.
                            and are recorded in different places (see examples in Fig. 7
                           ). The annotated data of object 2D bounding boxes in the videos from Caviar and ETISEO datasets are available on their websites.

Each training video is segmented automatically in a set of context segments (of different temporal lengths). The number of context segments depends on the contextual variation of the training video. Fig. 8
                            presents the context segmentation result of sequence ThreePastShop2cor belonging to the Caviar dataset. The values of object 2D area and 2D area variance are normalized for displaying. The context of this sequence is divided automatically into six context segments. For each context segment, satisfactory control parameter values are learned.

In the tracking parameter optimization process, we use firstly an AdaBoost algorithm to learn the object descriptor weights (e.g. dominant color weight) for each context segment because each object descriptor similarity can be considered as a weak classifier for linking two objects detected within a temporal window. Secondly, we search the best number of dominant colors (denoted ℭ) in context segments when the dominant color descriptor is used. We suppose that the value range of ℭ is from 2 to 7 colors. An exhaustive search is performed to find its best value. Table 3
                            presents the learned parameter values for the context segments.

The first context segment is from frame 1 to frame 300. The learned tracking parameters for this context are w
                           2
                           =0.21, w
                           3
                           =0.46, w
                           5
                           =0.33, w
                           1
                           =
                           w
                           4
                           =0 and ℭ=2. In this context segment, the object occlusion level is very low. The color histogram is selected as the most important object descriptor for tracking mobile objects. The object 2D area variance is quite high, it means that there exist at the same time objects of large and small areas. So the 2D area is also selected as an object descriptor for tracking. With the existence of objects whose 2D areas are high, the use of dominant color descriptor is reasonable because this descriptor discriminates well large mobile objects. In the second context segment (from frame 301 to 600), the density and the occlusion level of mobile objects increase. The dominant color descriptor weight is higher than the one in previous context segment because this descriptor integrated with the spatial pyramid kernel can manage the object occlusion cases (see Ref. [10] for more details). For context segments 3 and 4, the dominant color descriptor weight is still selected as the most important descriptor for object tracking. In context segment 4, the objects are smaller, so the number of dominant color descriptor increases from 2 to 3 to better discriminate the objects. In context segment 5, the value of object 2D area decreases significantly. While the dominant colors between small objects might be similar, the color histogram descriptor is reliable because this descriptor takes into account all pixels belonging to objects. Therefore, in this context segment, the weight of the dominant color descriptor decreases from 0.83 to 0.33, and the color histogram descriptor reliability increases from 0 to 0.34. The color covariance descriptor is also used for solving the occlusion cases which occur frequently in this context segment. In the last context segment, the object 2D area variance increases, therefore the object 2D area descriptor is selected again with the weight w
                           2
                           =0.2.

After segmenting the 12 training videos, we obtain 58 contexts. By applying the clustering process, 21 context clusters are created. Table 4
                            presents the learned control parameters for each cluster. The shape ratio descriptor is defined as the ratio between object 2D width and height. This descriptor is never selected in the context clusters because it cannot well discriminate the mobile objects in these training videos.

The cost of this training phase mainly depends on the tracking parameter optimization time. This phases requires about 8h for 60.24min of training videos corresponding to 18,071 frames and 165 mobile objects. This is done with an Intel(R) Xeon(R) CPU E5430 @ 2.66GHz (4 cores) and of 4GB RAM.

The processing Caviar videos have 26 sequences in which 6 sequences belong to our training video set. The other 20 sequences including 143 mobile objects are used for testing. The proposed controller is experimented in two cases to show its robustness. In the first case, only five object descriptor weights are considered for tuning; the number of dominant colors ℭ is set by default to 2. In the second case, all the six parameters are considered for tuning.


                                 Table 5
                                  presents the tracking results of the proposed approach and of some recent trackers from the state of the art. In the first case, the proposed controller increases significantly the performance of the appearance tracker. The MT value increases from 78.3% to 84.6% and the ML value decreases from 5.2% to 5.1%. In the second case, when the parameter ℭ is also tuned by the controller, the tracking performance continues to be improved. The MT value increases from 84.6% to 85.7% and the ML value decreases from 5.1% to 3.0%. We obtain the best MT value compared to the state of the art trackers.

In the rest of the article, we only present the results of the controller while tuning all the six parameters (i.e. w
                                 
                                    i
                                  with i
                                 =1..5 and ℭ).

The video of the second test belongs to the PETS dataset 2009. PETS videos are not used for learning. We select the sequence S2_L1, camera view 1, time 12.34 for testing because this sequence is experimented in several state of the art trackers. This sequence has 794 frames and contains 21 mobile objects and several occlusion cases (see Fig. 9a.).

In this test, we use the tracking evaluation metrics presented in Ref. [23] to compare with other tracking algorithms. The first metric is ATA (Average Tracking Accuracy) which computes the average accurate tracking time per object. The second metric is MOTP (Multiple Object Tracking Precision) which is calculated from the spatio-temporal overlap between the ground-truth trajectories and the algorithm's output trajectories. The third metric is MOTA (Multiple Object Tracking Accuracy) which penalizes the number of missed detection, false positives and switches in the output trajectory for a given reference ground-truth trajectory. All the three metrics are normalized in the interval [0, 1]. The higher these metrics, the better the tracking quality is.

For this sequence, the controller selects the parameters associated to context cluster 6 for tracking. The dominant color descriptor is selected as the most important descriptor for tracked objects because this descriptor can well handle the object occlusion cases. With the proposed controller, the tracking result increases significantly. Table 6
                                  presents the metric results of the proposed approach and of different trackers from the state of the art. The metric 
                                    
                                       M
                                       ¯
                                    
                                  represents the average value of the three metrics. With the proposed controller, we obtain the best values in metrics ATA, MOTP and 
                                    
                                       M
                                       ¯
                                    
                                 . The MOTA value of our approach (0.75) gets the second rank due to some missed detection.

The video of the third test belongs to the European Vanaheim project (see Fig. 9b). Vanaheim videos are not used for learning. The test sequence contains 36006 frames and lasts for 2h. Table 7
                                  presents the performance of the proposed approach and three recent trackers from the state of the art.

For this sequence, the proposed controller improves the performance of the tracker [10]. The MT value increases from 55.26% to 60.53%. The ML value decreases significantly from 13.16% to 2.63%. The tracking result with the proposed controller gets the best quality among the trackers presented in Table 7.

All the six context feature values depend on the object bounding boxes. The training phase is performed with annotated objects, so a low quality object detection in the online phase decreases the quality of the context detection. So, one drawback of the proposed controller is the dependence of its performance on the object detection quality. In this section, manually annotated objects are used for testing the controller. This experiment helps to better evaluate the proposed controller performance because the errors of the object detection task are eliminated. We test two video sequences. The first one is the OneStopMoveEnter2cor sequence belonging to the Caviar dataset. The second one is the Vanaheim video experimented previously.


                              Table 8
                               summarizes the obtained tracking results (without and with the controller) on these two sequences in two cases: using automatically detected objects and using manually annotated objects. For the OneStopMoveEnter2cor sequence, the controller increases the MT value by 18.18% (from 72.73% to 90.91%) in the second case and only by 9.09% (from 72.73% to 81.82%) in the first case. For the Vanaheim sequence, in the second case, the controller increases the MT value by 7.53% (from 92.47% to 100%) compared to 5.27% in the first case.

From this analysis, we conclude that the improvement of the tracking performance using controller is more significant on manually annotated objects than on automatic detected objects. It means that the controller performance is proportional to the object detection quality.

The KLT tracker relies on the tracking of Kanade–Lucas–Tomasi (KLT) features [31]. The KLT tracker takes detected objects as input. The object tracking relies on the number of matching KLT features over time between the detected objects. For the KLT tracker, we find two parameters depending on the tracking context: the minimum distance between KLT feature points m and the size of feature window W (see the definition of W at formula (3) of Ref. [31]). For example, in the case of object occlusion, the values of m should be low to detect a high enough number of KLT features for each object. When object 2D area is large, the values of m and W should be high to take into account whole object. Therefore these two parameters are selected for experimenting the proposed control approach so that hypothesis 1 (Section 1.1) is ensured. We train the controller for this tracker on the same 12 training video sequences presented in Section 6.3.1. The 20 Caviar videos (not belonging to the training sequences) are used for testing.

We suppose that the minimum distance m can get the values 3, 5, 7 and 9 pixels and the feature window size W can get the values 5, 10 and 15 pixels. In the tracking parameter optimization, due to the small space of control parameters, we use an enumerative search to learn satisfactory parameter values for each context. Fig. 10
                            presents the learned control parameter values for each context cluster.


                           Table 9
                            presents the tracking results for 20 test Caviar videos in both cases: without and with the proposed controller. In the first case, the values of m and W are set by default to 5. While using the proposed controller, the tracking performance is increased significantly. The MT value increases by 5.6% (from 74.4% to 80%) and the ML value decreases from 12.2% to 6.7%. Compared to the improvement of the MT value for the appearance tracker which is 7.4% (from 78.3% to 85.7%, see Table 5), the controller performance for the KLT tracker is less significant because fewer parameters are controlled and these parameters influence less the tracking quality. Also, they depend less on the tracking context.

We train the controller for this tracker on the same 12 training video sequences presented in Section 6.3.1. The two videos belonging to PETS dataset
                           6
                        
                        
                           6
                           
                              http://www.cvg.rdg.ac.uk/PETS2013/a.html.
                         and TUD dataset [2] are used for testing. These two datasets are not used in the training phase.

The Surf tracker relies on the tracking of Surf (Speeded Up Robust Features) [5]. Similar to the KLT tracker, the Surf tracker takes detected objects as input. The object tracking relies on the number of matching Surf features over time between the detected objects. For the Surf tracker, we consider two parameters:
                           
                              •
                              Hessian threshold h: This is a threshold for the key point detector. Only features whose Hessian is larger than Hessian threshold are retained by the detector. Therefore, the larger the value, the less key points are detected.

Number of octave layers n: The number of images within each octave of a Gaussian pyramid.

We suppose that the Hessian threshold h can get the values 100, 300 and 500 and the number of octave layers n can get the values 2, 4 and 6. In the tracking parameter optimization, due to the small space of control parameters, we use an enumerative search to learn satisfactory parameter values for each context.

Similar to the training phases of the previous trackers, 21 context clusters are created. We compute then satisfactory tracking parameters for each context cluster. Figs. 11 and 12
                           
                            present respectively the training results of the parameters of Hessian threshold and the number of octave layers for 21 context clusters. For each context cluster, satisfactory tracking parameters are defined as weighted combinations of the ones of contexts belonging to that cluster. Therefore the learned values of control parameters can be different from the values which are initially determined. For example, the learned Hessian threshold value of context cluster 3 is 200 and the learned number of octave layers of context cluster 6 is 5. From clusters 1 to 9, the learned parameter values are quite different from each other. This means that these two control parameters are influenced by the tracking context.

In the testing phase, when the controller is not used, the value of Hessian threshold is set to 100, and the value of number of octave layers is set to 2. These two values are selected because they are determined as the satisfactory values for many context clusters in the training phase.

This PETS video is also the one tested at Section 6.3.2. Illustration of this video is presented at Fig. 9a. Table 10
                               presents the metric results of the proposed approach and of different trackers from the state of the art. While using the proposed controller, the tracking result increases significantly. The value of MOTA increases from 0.80 to 0.86; the value of MOTP increases from 0.66 to 0.69; and the value of 
                                 
                                    M
                                    ¯
                                 
                               increases from 0.73 to 0.78. The obtained values are the best compared to the ones presented in the table.

For the TUD dataset, we select the TUD-Stadtmitte sequence for testing. This video contains only 179 frames and 10 objects but it is very challenging due to heavy and frequent object occlusions. For this sequence, the controller selects context cluster 13 in which parameters h
                              =281 and n
                              =2 are used for parameterizing the tracking process. With such high value of h, the number of detected Surf points is small. In this tracker, we take the detected objects as input and compute Surf points in corresponding 2D bounding boxes. In the case of high occlusion level as in this video, object bounding boxes may contain a part of other objects. A low number of detected Surf points helps to decrease the distribution of these points on different objects. The tracking quality is then better.


                              Figs. 13 to 16
                              
                              
                              
                               illustrate the tracking output in two cases: without controller (Figs. 13 and 14) and with the proposed controller (Figs. 15 and 16). While there is an ID switch between two persons (marked my arrows) in the first case, this error is solved in the second case. Table 11
                               presents the tracking results of the proposed approach and three recent trackers from the state of the art. While using the proposed controller, the MT value increases significantly from 50% to 70%. Also the obtained MT value is the best compared to these three trackers.

In all the testing video sequences and for three trackers, the online processing time increases only slightly (less than 10%) when the controller is used.

@&#CONCLUSION@&#

In this article, we have presented a new control approach for object tracking which is generic, flexible and intelligent. More precisely in order to cope with tracking context variations, this approach learns how to tune the parameters of tracking algorithms. The tracking context of a video sequence is defined as a set of six features: density of mobile objects, their occlusion level, their contrast with regard to the surrounding background, their contrast variance, their 2D area and their 2D area variance. In an offline phase, we learn satisfactory tracking parameters for context clusters. In the online control phase, once a context change is detected, the tracking parameters are tuned using the learned values. This method is able to control trackers belonging to two different categories (appearance tracking and point tracking). Moreover, other tracker category can still be controlled by adapting the context notion to the tracker principle (for example to control silhouette-based trackers, we can add the object rigidity feature to the context). The training and testing phases are not time consuming. The proposed approach has been experimented with three trackers on a long, complex video and on three public datasets (Caviar, PETS and TUD). The experimental results show a significant improvement of the performances while using the proposed controller.

In future work, we will extend the context notion which should be independent from the object detection quality. Also, the proposed control approach should be able to interact with the object detection task to improve the detection quality. An online mechanism for updating the learned database is also necessary to increase the performance of the proposed approach.

@&#REFERENCES@&#

