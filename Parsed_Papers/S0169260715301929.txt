@&#MAIN-TITLE@&#ANAlyte: A modular image analysis tool for ANA testing with indirect immunofluorescence

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We propose ANAlyte, an automated tool for HEp-2 image analysis.


                        
                        
                           
                           ANAlyte is a valid solution to the problem of ANA test automatization.


                        
                        
                           
                           We integrate modules for the full characterization of HEp-2 slides.


                        
                        
                           
                           We propose a new technique for HEp-2 intensity characterization.


                        
                        
                           
                           Our tool is validated on two different public benchmarks of HEp-2 images.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Automated characterization of HEp-2 slides

Indirect immunofluorescence

ANA testing

Microscopy image processing

@&#ABSTRACT@&#


               
               
                  Background and objectives
                  The automated analysis of indirect immunofluorescence images for Anti-Nuclear Autoantibody (ANA) testing is a fairly recent field that is receiving ever-growing interest from the research community. ANA testing leverages on the categorization of intensity level and fluorescent pattern of IIF images of HEp-2 cells to perform a differential diagnosis of important autoimmune diseases. Nevertheless, it suffers from tremendous lack of repeatability due to subjectivity in the visual interpretation of the images. The automatization of the analysis is seen as the only valid solution to this problem. Several works in literature address individual steps of the work-flow, nonetheless integrating such steps and assessing their effectiveness as a whole is still an open challenge.
               
               
                  Methods
                  We present a modular tool, ANAlyte, able to characterize a IIF image in terms of fluorescent intensity level and fluorescent pattern without any user-interactions. For this purpose, ANAlyte integrates the following: (i) Intensity Classifier module, that categorizes the intensity level of the input slide based on multi-scale contrast assessment; (ii) Cell Segmenter module, that splits the input slide into individual HEp-2 cells; (iii) Pattern Classifier module, that determines the fluorescent pattern of the slide based on the pattern of the individual cells.
               
               
                  Results
                  To demonstrate the accuracy and robustness of our tool, we experimentally validated ANAlyte on two different public benchmarks of IIF HEp-2 images with rigorous leave-one-out cross-validation strategy. We obtained overall accuracy of fluorescent intensity and pattern classification respectively around 85% and above 90%. We assessed all results by comparisons with some of the most representative state of the art works.
               
               
                  Conclusions
                  Unlike most of the other works in the recent literature, ANAlyte aims at the automatization of all the major steps of ANA image analysis. Results on public benchmarks demonstrate that the tool can characterize HEp-2 slides in terms of intensity and fluorescent pattern with accuracy better or comparable with the state of the art techniques, even when such techniques are run on manually segmented cells. Hence, ANAlyte can be proposed as a valid solution to the problem of ANA testing automatization.
               
            

@&#INTRODUCTION@&#

The analysis of indirect immunofluorescent (IIF) images is of paramount importance to a large number of clinical applications. For example, the Anti-Nuclear Autoantibody (ANA) test leverages on the analysis of IIF images to diagnose autoimmune disorders falling into the category of so-called connective tissue diseases (CTDs), which affect a remarkable percentage of the population. Among the others, rheumatoid arthritis, psoriatic arthritis and scleroderma.

CTDs are characterized by a spontaneous overactivity of the immune system, which will introduce extra autoantibodies into the circulatory system of the patient. Thus, the presence of such autoantibodies in the blood is a reliable indication of disease.

ANA testing is a widespread blood exam that uses a slide of cultured HEp-2 (human epithelial type 2) cells as a substrate to reveal the presence of said antibodies in the serum of a patient. The HEp-2 cells have antigens that specifically bind the targeted autoantibodies which, in turn, create a link with secondary antibodies conjugated with fluorophores (see Fig. 1
                     ). When visualized under the microscope, the antigen-antibody reaction will establish a particular fluorescent pattern on the HEp-2 cells, which is specific of the autoantibody type. Hence, identifying the pattern of the IIF image (see Fig. 2
                      for examples) allows a differential diagnosis of the CTDs.

The analysis is commonly performed as follows. First, the physicians identify the positive specimens. This requires classifying the IIF images into a number of categories, based on the intensity of the fluorescent signal. Although there is no general consensus among the laboratories about the specific number of categories that should be identified, recent studies show that three intensity levels allow to obtain the highest discrimination capability and at the same time to minimize variability between operators. Such levels are negative (i.e. no fluorescence at all), intermediate and positive [1]. Second, the physicians categorize the intermediate and positive specimens based on the fluorescent pattern revealed on the HEp-2 cells (a few examples are reported in Fig. 2). This ultimately allows to identify the type of CTD affecting the patient.

While ANA testing per se is widely spread and universally recognized as a valid diagnostic technique, the significance and reliability of its results are critically affected by the subjectivity of the human interpretation of the IIF slides [2,3]. On top of that, employing highly skilled and trained physicians for the analysis of massive amounts of images often translates into unsustainable costs for the health-care system.

In the attempt to seek valid solutions to this problem, the automatization of IIF image analysis for ANA testing is now receiving an ever-growing attention. In the following, we provide a short overview of latest research directions in this field and we introduce the main contributions of our work.

To be useful to a clinical setting, a computer-aided image analysis system for ANA testing should be able to automatically characterize a IIF specimen in terms of both fluorescent intensity and fluorescent pattern. Such characterization can be either performed on a per-image basis (leveraging on global features of the input image) or on a per-cell basis. In most of the available literature, fluorescent intensity categorization is obtained with a per-image approach, while fluorescent pattern classification is preferably performed with a per-cell approach. In the latter case, the system first applies cell segmentation techniques to split the IIF slide into individual HEp-2 cells, and then it determines the fluorescent pattern of each cell, which ultimately allows to establish the pattern of the whole slide. This is based on the assumption that the pattern of the specimen coincides with the pattern of the large majority of the HEp-2 cells in the slide, which is valid for most of the ANAs used in diagnosis.

In the last few years, literature has proposed several automated techniques tackling IIF analysis. Nevertheless, most of the publications focus on individual tasks of the ANA testing chain, overlooking the other steps of the analysis (e.g. HEp-2 cell segmentation [4–6], fluorescent intensity classification [1,7,8], HEp-2 pattern recognition [9–12]). Hence, even though the proposed techniques show promising results per se, there is still little evidence on how effective they would be when they are integrated into a complete analysis work-flow and applied to a real clinical setting. On the other hand, first attempts of commercial systems for computer-aided IIF analysis are validated on private databases of HEp-2 images, which prevents a direct comparison with other techniques [13–15].

With the aim of sorting out the ever-growing activity of the research community in this field, since 2012 international contests on HEp-2 pattern classification have been regularly hosted by some of the major conferences on pattern recognition and image processing (respectively ICPR 2012, ICIP 2013 and ICPR 2014 [16–18]), as well as by a special issue of the journal Pattern Recognition in 2014 [11]. While classification paradigms presented in such contests are often well-established machine learning approaches (e.g. Support Vector Machines, boosting algorithms, neural networks and random forest techniques), most of the original contributions are in the design of suitable image attributes to characterize the HEp-2 patterns. Proposed descriptors include global statistics of grey-level distributions (e.g. grey-level co-occurrence matrices [19]), morphological measures of shape or topology [20], or combination of the two [21,22], as well as various formulations of local binary patterns. In particular, a novel variant of local binary patterns, namely Rotation Invariant Co-occurrence among adjacent LBPs (RIC-LBPs), was proposed by the winner of ICPR 2012 contest [23]. Other top performing approaches of the same contest focused on classic textural descriptors incorporated with feature encoding and dictionary learning, either bag of words (BoW, earlier proposed for HEp-2 classification by [9]), or specifically designed sparse representations of the cell images [24]. Feature encoding approaches were very popular also in the next contest hosted by ICPR 2014 (I3A), but with a more sophisticated characterization of the HEp-2 patterns: for example, the biologically-inspired dense local descriptors proposed by [25], the combined textural and structural information extracted by [22], or the multi-scale descriptors with cell pyramids designed by the winner of the contest [26]. Conversely, another top-performing approach replaced the use of hand-crafted features with convolutional neural networks taking the HEp-2 cells directly as input [27].

Ultimately, the outcome of the IIF image analysis contests is two-fold. First, a very interesting survey and assessment of the recent research efforts towards automated ANA testing, albeit solely focused on the HEp-2 pattern classification task. Second, the release of public benchmarks of IIF images, on which the automated techniques can now be tested and compared.

In our previous works we addressed two major steps of automated IIF analysis independently from each other, and we used the public HEp-2 benchmarks to demonstrate the accuracy of our proposed techniques compared to the most representative works in recent literature. In [28,29] we tackled the problem of fluorescent pattern classification. We investigated several types and combinations of image attributes, showing that the integration of different types of descriptors (morphological, local texture and global texture features) is the most suited for this purpose. Furthermore, we proposed a classification scheme to cope with the high within-class variance typical of HEp-2 cell images. When tested on manually segmented cells, this approach was found to be among the top pattern classification techniques proposed by recent literature [11]. Then, in [30] we focused on HEp-2 cell segmentation, which is a challenging step that should ideally precede HEp-2 pattern classification. We presented a cell segmentation approach that automatically adapts to images with different intensity levels or fluorescent patterns and we experimentally validated our technique based on the concordance with manual segmentations performed by human experts.

Although our previous works obtained promising results, they do not say much on how the automatization and integration of the segmentation and classification steps would impact on the final ANA testing outcome.

In this paper, we improve and extend our work as follows. (i) We tackle the task of automated intensity level classification, which is the IIF image analysis step that we had not addressed yet. In order to handle intensity variations that are inherent of fluorescence imaging, we propose a technique that leverages on multi-scale image contrast assessment and machine learning. (ii) We integrate intensity level classification, cell segmentation and pattern classification into a single modular tool, ANAlyte. After being trained on a suitable set of pre-labelled images, ANAlyte is able to receive a IIF slide as input and provide as output its intensity level and fluorescent pattern without requiring user-interaction. (iii) We experimentally validate ANAlyte on two different public benchmarks of IIF HEp-2 images to demonstrate the performance and robustness of our tool in a realistic clinical scenario. Unlike our previous works, here we focus our experimental analysis not on the validation of the individual techniques (the interested reader can refer to the publications mentioned before), but on demonstrating the effectiveness of our tool in the context of ANA testing.

This paper is structured as follows. In Section 2 we introduce ANAlyte, our proposed tool, and we describe its main modules in detail. In Section 3 we characterize the datasets used in our experiments and we present and discuss the experimental results. Finally, Section 4 concludes the paper.

The architecture of ANAlyte is summarized in Fig. 3
                     , where its main modules are represented with their connections:


                     Intensity Classifier (IC) receives a IIF image as input and categorizes such image into one of the intensity levels represented by a pre-labelled Training Set.


                     Cell Segmenter (CS) extracts individual HEp-2 cells from the input IIF image.


                     Pattern Classifier (PR) receives the individual cells as obtained from CS and provides a categorization of the input IIF image into one of the fluorescent patterns represented by a pre-labelled Training Set.

ANAlyte provides as output the intensity level and the fluorescent pattern of the input slide. As anticipated in Section 1, such information is critical for the differential diagnosis of autoimmune diseases.

In the following, we describe the main modules of ANAlyte with more details.

In the clinical practice, IIF samples are categorized into a specific number of levels based on the visual assessment of their fluorescent intensity compared to a set of negative and positive controls. The aim of IC module is the automatization of such practice, as follows: first, a number of significant sub-regions are automatically selected from the image (Sub-regions selection, Section 2.1.1). Based on these sub-regions, a set of features are extracted at different scales and locations of the image (Multi-scale contrast assessment, Section 2.1.2). Finally, based on the computed features, the input image is automatically assigned to a specific intensity level by means of machine learning techniques (Automatic intensity classification, Section 2.1.3).

The intensity level of a IIF image is the perceivable strength of its fluorescent signal. On account of this, the categorization of IIF intensity levels can be interpreted as a problem of image contrast quantification, where contrast is traditionally defined as the difference of brightness between objects and background. Nevertheless, measuring the contrast of a IIF image is complicated by the extreme imbalance of foreground and background and by the presence of intensity variations across the image. In order to overcome these problems we perform contrast assessment on multiple sub-regions of the IIF image, taking advantage of multi-scale local information.

Given an input image I, sub-regions are chosen as follows:
                              
                                 1.
                                 we perform a rough binarization of I by means of Otsu thresholding;

we compute the connected regions of the foreground;

we calculate the centroids of the connected regions, spatially sorted from left to right:
                                       
                                          (1)
                                          
                                             
                                                c
                                                ¯
                                             
                                             =
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  c
                                                                  1
                                                               
                                                            
                                                            
                                                               
                                                                  c
                                                                  2
                                                               
                                                            
                                                            
                                                               ⋯
                                                            
                                                            
                                                               
                                                                  c
                                                                  
                                                                     N
                                                                     /
                                                                     2
                                                                  
                                                               
                                                            
                                                            
                                                               ⋯
                                                            
                                                            
                                                               
                                                                  c
                                                                  
                                                                     N
                                                                     −
                                                                     1
                                                                  
                                                               
                                                            
                                                            
                                                               
                                                                  c
                                                                  N
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             ,
                                          
                                       
                                    where N is the total number of connected regions (see example in Fig. 4
                                    (b));

we select n equally-spaced elements from vector 
                                       
                                          c
                                          ¯
                                       
                                     (details are provided later on this section), obtaining a vector 
                                       
                                          
                                             
                                                c
                                                s
                                             
                                          
                                          ¯
                                       
                                    :
                                       
                                          (2)
                                          
                                             
                                                
                                                   
                                                      c
                                                      s
                                                   
                                                
                                                ¯
                                             
                                             =
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  c
                                                                  
                                                                     s
                                                                     ,
                                                                     1
                                                                  
                                                               
                                                            
                                                            
                                                               
                                                                  c
                                                                  
                                                                     s
                                                                     ,
                                                                     2
                                                                  
                                                               
                                                            
                                                            
                                                               ⋯
                                                            
                                                            
                                                               
                                                                  c
                                                                  
                                                                     s
                                                                     ,
                                                                     n
                                                                  
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

we generate n
                                    +1 rectangular windows:
                                       
                                          (3)
                                          
                                             
                                                W
                                                ¯
                                             
                                             =
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  W
                                                                  0
                                                               
                                                            
                                                            
                                                               
                                                                  W
                                                                  1
                                                               
                                                            
                                                            
                                                               
                                                                  W
                                                                  2
                                                               
                                                            
                                                            
                                                               ⋯
                                                            
                                                            
                                                               
                                                                  W
                                                                  n
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             ,
                                          
                                       
                                    where W
                                    0 is a global window that includes the whole image I, while W
                                    
                                       i
                                    (i
                                    >0) are local windows centered on c
                                    
                                       s,i
                                     and with size gradually decreasing at the increase of i.

Vectors 
                              
                                 
                                    
                                       c
                                       s
                                    
                                 
                                 ¯
                              
                            and 
                              
                                 W
                                 ¯
                              
                            are generated by an iterative windowing process reported in Algorithm 1, with the following characteristics: (i) at each iteration, few elements of vector 
                              
                                 c
                                 ¯
                              
                            (Eq. (1)) are selected with a certain sampling interval, and the corresponding local windows are generated; (ii) the sampling interval of the centroids as well as the size of the generated windows are decreased at each iteration. (iii) the procedure terminates when the generated windows are too small to include an adequate proportion of foreground and background. (iv) the final 
                              
                                 
                                    
                                       c
                                       s
                                    
                                 
                                 ¯
                              
                            and 
                              
                                 W
                                 ¯
                              
                            vectors (Eqs. (2 and 3), respectively) are obtained by concatenating the output of each iteration.


                           
                              Algorithm 1
                              Iterative windowing. 
                                    
                                       
                                          
                                          
                                          
                                             
                                                1:
                                                
                                                   N
                                                   
                                                      ITER
                                                   
                                                   ←1
                                             
                                             
                                                2:
                                                [d
                                                   
                                                      W1, d
                                                   
                                                      W2]=
                                                   size(I) {I: input image}
                                             
                                             
                                                3:
                                                
                                                   S
                                                   
                                                      INT
                                                   
                                                   ←
                                                   N {N: number of connected regions}
                                             
                                             
                                                4:
                                                
                                                   while (d
                                                   
                                                      W1
                                                   ·
                                                   d
                                                   
                                                      W2
                                                   ≥
                                                   TH) do
                                                
                                             
                                             
                                                5:
                                                
                                                   
                                                   if 
                                                   N
                                                   
                                                      ITER
                                                   
                                                   =1 then
                                                
                                             
                                             
                                                6:
                                                
                                                     Generate a window of size [d
                                                   
                                                      W1, d
                                                   
                                                      W2] centered on a point with coordinates (d
                                                   
                                                      W1/2, d
                                                   
                                                      W2/2)
                                             
                                             
                                                7:
                                                
                                                   
                                                   else
                                                
                                             
                                             
                                                8:
                                                
                                                     Perform a selection of S
                                                   
                                                      INT
                                                   -spaced elements from 
                                                      
                                                         c
                                                         ¯
                                                      
                                                   , symmetrically with respect to its middle.
                                             
                                             
                                                9:
                                                
                                                     Center a [d
                                                   
                                                      W1, d
                                                   
                                                      W2]-sized window on each of the selected elements.
                                             
                                             
                                                10:
                                                
                                                   
                                                   end if
                                                
                                             
                                             
                                                11:
                                                
                                                   
                                                   N
                                                   
                                                      ITER
                                                   
                                                   ←
                                                   N
                                                   
                                                      ITER
                                                   
                                                   +1
                                             
                                             
                                                12:
                                                
                                                   
                                                   S
                                                   
                                                      INT
                                                   
                                                   ←
                                                   S
                                                   
                                                      INT
                                                   /2
                                             
                                             
                                                13:
                                                
                                                   [d
                                                   
                                                      W1, d
                                                   
                                                      W2]=
                                                   size(I)/2
                                                      N
                                                      
                                                         ITER
                                                      
                                                   
                                                
                                             
                                             
                                                14:
                                                
                                                   end while
                                                
                                             
                                          
                                       
                                    
                                 
                              

With reference to the pseudo-code in Algorithm 1 and to the example displayed in Fig. 4(c), the iterative windowing proceeds as follows:
                              
                                 
                                    NITER
                                       =1 generates one window W
                                    0 that coincides with the bounding box of the whole image (red-coloured)


                                    NITER
                                       =2 generates two windows W
                                    1 and W
                                    2 of size(I)/4 (green-coloured), whose centers are obtained selecting N/2-spaced symmetric elements from vector 
                                       
                                          c
                                          ¯
                                       
                                     (Eq. (1)):
                                       
                                          
                                             
                                                
                                                   
                                                
                                             
                                          
                                       
                                    
                                 


                                    NITER
                                       =3 generates four windows W
                                    3
                                    ⋯
                                    W
                                    6 of size(I)/8 (yellow-coloured), whose centers are obtained selecting N/4-spaced symmetric elements from vector 
                                       
                                          c
                                          ¯
                                       
                                    :
                                       
                                          
                                             
                                                
                                                   
                                                
                                             
                                          
                                       
                                    
                                 

As reported in line 4 of Algorithm 1, the iterations end when the area of the local window is lesser than a threshold TH. Such threshold is set during the training phase of the algorithm, so that the sub-regions always contain a reasonable amount of background.
                              1
                           
                           
                              1
                              During the training phase, the operator draws the bounding box of a random HEp-2 cell in a IIF image. Such procedure is repeated for a set of training images of different fluorescent pattern. Hence, TH is estimated as twice the maximum bounding boxes area.
                              
                                 N.B. The training set must include images acquired with same size, imaging conditions and magnification of the input image.
                           
                        

The ending result of the iterative windowing process is the vector 
                              
                                 W
                                 ¯
                              
                           , obtained by concatenating all the windows generated at each iteration of the procedure. Hence, the total number of local windows, n, depends on the number of iterations, and ultimately on the threshold TH.

By computing image contrast within each window in 
                              
                                 W
                                 ¯
                              
                           , we obtain a contrast features vector 
                              
                                 Ct
                                 ¯
                              
                           :
                              
                                 (4)
                                 
                                    
                                       Ct
                                       ¯
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         Ct
                                                         
                                                            ,
                                                            0
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         Ct
                                                         
                                                            ,
                                                            1
                                                         
                                                      
                                                   
                                                   
                                                      ⋯
                                                   
                                                   
                                                      
                                                         Ct
                                                         
                                                            ,
                                                            n
                                                            −
                                                            1
                                                         
                                                      
                                                   
                                                   
                                                      
                                                         Ct
                                                         
                                                            ,
                                                            n
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           where Ct
                           ,0 is a measure of the global contrast of the whole image and Ct
                           ,1
                           ⋯
                           Ct
                           ,n
                            are measures of local contrast within sub-images of gradually decreasing size.

Literature provides several descriptors, which are best suited for quantifying image contrast at different scales [31]. Thanks to its robustness to intensity variations across the image, RMS Contrast (C
                           
                              RMS
                           ), defined as the standard deviation of pixel intensities, is the most used metrics for the quantification of global contrast:
                              
                                 (5)
                                 
                                    
                                       C
                                       RMS
                                    
                                    =
                                    
                                       
                                          
                                             1
                                             
                                                
                                                   d
                                                   
                                                      W
                                                      1
                                                   
                                                
                                                ·
                                                
                                                   d
                                                   
                                                      W
                                                      2
                                                   
                                                
                                             
                                          
                                          
                                             ∑
                                             
                                                i
                                                =
                                                0
                                             
                                             
                                                
                                                   d
                                                   
                                                      W
                                                      1
                                                   
                                                
                                                −
                                                1
                                             
                                          
                                          
                                             ∑
                                             
                                                j
                                                =
                                                0
                                             
                                             
                                                
                                                   d
                                                   
                                                      W
                                                      2
                                                   
                                                
                                                −
                                                1
                                             
                                          
                                          
                                             
                                                (
                                                
                                                   I
                                                   ij
                                                
                                                −
                                                
                                                   I
                                                   avg
                                                
                                                )
                                             
                                             2
                                          
                                       
                                    
                                    ,
                                 
                              
                           where I
                           
                              ij
                            is the [0, 1] intensity of the (i, j)-th pixel of the image, (d
                           
                              W1, d
                           
                              W2) is the image size, and I
                           
                              avg
                            is the average image intensity.

Conversely, Michelson Contrast (C
                           
                              M
                           ) is defined as the ratio between the spread and the sum of maximum and minimum intensity values over the image window, as follows:
                              
                                 (6)
                                 
                                    
                                       C
                                       M
                                    
                                    =
                                    
                                       
                                          
                                             I
                                             max
                                          
                                          −
                                          
                                             I
                                             min
                                          
                                       
                                       
                                          
                                             I
                                             max
                                          
                                          +
                                          
                                             I
                                             min
                                          
                                       
                                    
                                 
                              
                           
                        


                           C
                           
                              M
                            is most significant when bright and dark portions take up similar fractions. Hence, in our context we use this metrics to compute contrast in the sub-images, where such pre-condition most likely applies.

Summarizing, with reference to Eq. (4), the contrast features vector is calculated as follows:
                              
                                 (7)
                                 
                                    
                                       Ct
                                       
                                          ,
                                          i
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         C
                                                         RMS
                                                      
                                                   
                                                   
                                                      if
                                                         
                                                      i
                                                      =
                                                      0
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         C
                                                         M
                                                      
                                                   
                                                   
                                                      if
                                                         
                                                      i
                                                      >
                                                      0
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The contrast features vector 
                              
                                 Ct
                                 ¯
                              
                            is fed into a k-NN classifier, equipped with a set of training examples, i.e. images with a-priori known intensity labels. As introduced in Section 1, the intensity labels will be in our case three (either positive, intermediate or negative). Nevertheless, the classification technique can be generalized to any number of intensity levels, provided that such labels are adequately represented in the training set. The input image and the training examples are mapped into a (n
                           +1) dimensional feature space, where the coordinates are represented by the n
                           +1 elements of 
                              
                                 Ct
                                 ¯
                              
                           . Hence, the input image is assigned to the most represented intensity level among its k nearest neighbours.
                              2
                           
                           
                              2
                              
                                 k
                                 =7, heuristically set.
                           
                        

CS module performs the automated segmentation of the individual HEp-2 cells. Such task is one of the most challenging of automated IIF analysis, because a single segmentation algorithm has to cope with a large heterogeneity of shapes and textures (see some examples in Fig. 2). In order to address this problem, CS is implemented as an adaptive marker-controlled watershed algorithm, where both the preprocessing and the marker selection strategy self-adapt to the peculiar characteristics of the input image. First, the Image Normalization step is performed in order to soften textural differences of the input images. Then, the Automatic Marker Selection extracts a set of internal and external markers defining the position of the foreground and background, respectively. Finally, such markers are used to drive a Marker-Controlled Watershed technique towards the boundaries of the individual cells.

IIF images with different textural characteristics demand specific segmentation strategies. Hence, to enforce the adaptiveness of the algorithm, the incoming image is automatically categorized into one of the following:


                        (i) Smooth textured images, with bright cell bodies and dark background (e.g. first image of Fig. 2).


                        (ii) Rough textured images, with cell bodies partly bright and partly dark (e.g. last three images of Fig. 2).

Such categorization is based on the average area of the connected regions returned by Otsu's thresholding algorithm, as proposed in [30] (largearea→smoothtextured, smallarea→roughtextured).

For both image categories, histogram equalization is performed in to enhance the cell bodies with respect to the background, followed by grey-scale morphological opening. As Rough textures are characterized by large dark areas within the cell body, this category of images are also preprocessed with a combination of Top-Hat filtering and morphological greyscale reconstruction [32].

The automatic marker selection consists of the following steps. First, an adaptive fuzzy c-means clustering roughly separates the foreground regions from the background. Then, cell clusters (i.e. the regions including two or more touching cells) are split into separate objects by Randomized Hough Transform. At the end of this procedure, a collection of internal and external markers is obtained, defining the position of the individual cells and of the background, respectively.

As observed by [6], pixel intensities in IIF images can be roughly grouped into three bands: low, medium and high intensity. Pixels in the low and high intensity bands belong to, respectively, background and foreground. The content of the medium intensity band, on the other hand, strictly relates to the textural characteristics of the image, and hence to the image category: in the smooth textured images it contains background pixels only, while in rough textured images it contains both background and foreground pixels.

Upon such observation, the adaptive identification of foreground regions is implemented as follows. A three-classes fuzzy c-means clustering (FCM [33]) is applied to separate the image pixels into the three intensity bands (low, medium and high-intensity). FCM assigns each pixel value a [0,1] degree of membership to each of the three clusters, summarized by the membership functions M
                           
                              low
                           , M
                           
                              med
                            and M
                           
                              high
                            reported in Fig. 5
                           . Then, the ultimate separation of foreground and background is obtained by image thresholding, where the threshold I
                           
                              TH
                            is computed as follows:
                              
                                 (8)
                                 
                                    
                                       I
                                       TH
                                    
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         intersection
                                                         (
                                                      
                                                      
                                                         M
                                                         med
                                                      
                                                      ,
                                                      
                                                         M
                                                         high
                                                      
                                                      
                                                      )
                                                   
                                                   
                                                      if
                                                         
                                                      Smooth
                                                   
                                                
                                                
                                                   
                                                      
                                                         barycenter
                                                         (
                                                      
                                                      
                                                         M
                                                         med
                                                      
                                                      )
                                                   
                                                   
                                                      if
                                                         
                                                      Rough
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

To cope with uneven illumination problems typical of fluorescence microscopy, the clustering algorithm is implemented with a sliding window approach. Hence, each pixel is assigned a local I
                           
                              TH
                            value, based on the intensities of its neighbours.

The foreground objects identified with FCM may either contain (i) one individual cell or (ii) multiple touching cells (i.e. cell clusters), which need to be separated further into individual objects. These two categories are identified by imposing a threshold I
                           
                              E
                            on the ellipticity value of each object:
                              
                                 (9)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   ellipticity
                                                   ≥
                                                   
                                                      I
                                                      E
                                                   
                                                   ⇒
                                                   
                                                      individual
                                                      
                                                      cell
                                                   
                                                
                                             
                                             
                                                
                                                   ellipticity
                                                   <
                                                   
                                                      I
                                                      E
                                                   
                                                   ⇒
                                                   
                                                      cell
                                                      
                                                      cluster
                                                   
                                                   ,
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where ellipticity is the ratio between the area of a connected component and the area of its best-fitted ellipse, and I
                           
                              E
                            is the average ellipticity of all the objects in the image. The rationale of this approach is that individual cells have a more regular shape than cell clusters, and hence higher ellipticity.

The identified cell clusters are then split into elliptical sub-regions by means of a Randomized Hough Transform, RHT [34], a probabilistic variant of classical Hough Transform. The resulting ellipses are a very rough approximation of the cell boundaries. However, their centers indicate the most likely position of the individual cells within the cell clusters.

Ultimately, we obtain a set of internal markers defining the position of the individual cells in the image and a set of external markers defining the background. Internal markers include: (i) the pixels of the individual cells as obtained from Eq. (9) (ii) the centers of the ellipses returned by the RHT-based decomposition of the cell clusters. Conversely, the external markers include the edge points of a Voronoi diagram built using the internal markers as seeds.

Cell segmentation is obtained with a marker-controlled watershed algorithm [35]. In our implementation, watershed technique is applied to the gradient of the image as obtained after Image Normalization.

As touching cells as a rule do not have clear edges between each other, a certain degree of imprecision in detecting the exact boundaries can still be expected. Nevertheless, as it will be shown in Section 3, this does not have critical impact on pattern classification accuracy.

The segmented cells returned by CS are fed into PC module, which performs the automatic categorization of the fluorescent pattern.

Prior to the actual pattern classification algorithm, the cell images undergo a preprocessing stage. More specifically: (i) image normalization is applied in order to reduce variabilities of cell size and intensity. As for the size, all the bounding boxes of the input cells are resized to a standard 128x128 pixels dimension. As for the intensity, pixel values are linearly remapped so that the bottom 1% and the top 1% of the intensities are saturated. Thanks to intensity normalization, the same classifier can be applied irrespective of the intensity level of the input image. Hence, the PC module is completely independent from the IC module, in that it does not need to receive intensity as an input parameter. (ii) mitotic cell detection as reported in [36] is applied in order to identify and remove the mitotic cells (i.e. cells undergoing cellular division). Mitotic cells are in minority in a HEp-2 slide, and are characterized by a very distinct morphology compared to the other cells. Hence, selectively removing such cells facilitates the pattern recognition process.

Then, the following steps are performed: (i) feature extraction computes a feature vector representing relevant textural and morphological attributes of the cells (ii) a Subclass Discriminant Analysis (SDA) approach remaps the cells’ representation into a novel feature space providing a better class separation. (iii) Based on the remapped features of the individual cells, Image Classification provides as ultimate output the fluorescent pattern of the IIF image.

Three types of image descriptors are extracted:
                              
                                 1.
                                 
                                    morphological features, focusing on shape attributes of the fluorescent signal;


                                    global texture descriptors, summarizing the overall distribution of the grey-levels in the cell;


                                    local texture descriptors, representing isolated contribution of small regions or pixel neighbourhoods.

As for morphological features, the computed descriptors are the circularity of the cell (defined as Area/Perimeter2), as well as a set of shape attributes extracted from the binarization of the cell image at seven increasing intensity thresholds, as defined in [29].

As for global texture descriptors, a set of first and second order statistical measures are derived from the grey-level co-occurrence matrix (GLCM), representing the distribution of co-occurring values between neighbouring pixels according to different distances and directions.


                           Local texture descriptors, on the other hand, are based on Local Binary Patterns (LBPs), i.e. binary patterns representing the intensity relations between a pixel and its neighbours. More specifically, features are extracted applying a modified formulation of LBP which provides information about the co-occurrence of multiple adjacent LBPs (Co-occurrence of Adjacent LBPs, CoALBP).

Morphological, GLCM and CoALBP features are computed and concatenated into a single feature vector. Then, in order to extract the subset of features that is most relevant for the pattern classification task, a robust feature selection method based on minimum-Redundancy-Maximum-Relevance (mRMR) algorithm is applied. mRMR sorts the features that are most relevant for the characterization of the classification variable, pointing at the minimization of their mutual similarity and maximization of their correlation with the classification label, as provided by the training set. The ultimate size of the selected features vector is hence 150, as defined in [29].

After feature selection, fluorescent pattern classification is obtained with a machine learning technique based on Subclass Discriminant Analysis (SDA) that has been already introduced in [29].

To cope with the inherent non-linearity and within class variability of IIF patterns, SDA approximates the underlying distribution of each class by a mixture of Gaussians. Thereby, the classes are divided into a set of subclasses, working towards the maximization of the distance among means of classes and, at the same time, of the distance among means of subclasses within each class. Hence, the ultimate problem to be solved is finding the optimal number of subclasses for each class. In our work, the optimal subdivision is the one able to maximize the recognition rate of a leave-one-out process (details in Section 3) in a dataset independent from the one used for validation purposes.

Our implementation has two major differences compared to original SDA formulation. First, while classical SDA partitions the samples into a set of equal-sized subclasses sorted with a Nearest-Neighbour algorithm, we apply a K-means clustering algorithm. This allows to model more effectively the non-linearity in the data. Second, instead of increasing at each iteration the number of subclasses of all groups of a same amount, as in [37], we apply a heuristic approach. We first set a maximal number of subclasses for each class, based on the number of per-class samples. Then, all possible permutations of class subdivisions are tested, halting the subdivision of a specific class when the minimal number of samples in any sub-cluster is reached.

Once the feature vectors have been projected on the sub-space defined by SDA, the classification is performed with k-NN algorithm.
                              3
                           
                           
                              3
                              
                                 k
                                 =8, heuristically set.
                            More specifically, k-NN is applied to categorize each individual cell into one of the fluorescent patterns represented by the training set, treating the HEp-2 cells as independent samples. Ultimately, the class of the input IIF image is obtained based on the majority class of its individual cells.

All the experiments in this work are based on a leave-one-out cross-validation (LOOCV) protocol. At each iteration (one per each image of the validation dataset), all the IIF slides except one are used for training the tool, using the left out slide for testing purposes. This strategy ensures a complete separability and independence of training and testing, and maximizes the number of independent samples (i.e. of specimens belonging to different patients) used for training, which strengthens the learning process of the classification algorithm. Furthermore, the execution of several training/test runs allows to assess the generalization capabilities of the tool.

The two outputs of ANAlyte (i.e. fluorescent intensity level and fluorescent pattern) were evaluated independently, in terms of rate of correct classifications. In order to provide a better insight into the tool's behaviour, fluorescent pattern accuracy was evaluated not only image-wise (i.e. in terms of IIF slides correctly classified, which is the information that is relevant for clinical purposes) but also cell-wise, in terms of individual cells correctly classified.

In all cases the percentage accuracy value was obtained by adding the classification counts of each iteration and then calculating a percentage over all runs, as follows:
                           
                              (10)
                              
                                 LOOCVacc
                                 (
                                 %
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          
                                             
                                                n
                                                i
                                             
                                          
                                       
                                       
                                          #
                                          correct
                                          classifications
                                       
                                    
                                    N
                                 
                                 ×
                                 100
                                 ,
                              
                           
                        where n
                        
                           i
                         is the number of LOOCV iterations (which coincides with the number of IIF slides of the testing dataset) and N is the total number of classification counts over all iterations.

In our experiments we used two different datasets, here referenced as MIVIA and I3Asel. The full characterization of these datasets is reported in Table 1
                        .


                        MIVIA dataset is an annotated database of indirect immunofluorescence (IIF) HEp-2 images publicly available at [38]. The database was firstly described in [39] and later used for the HEp-2 Cells Classification Contest hosted by the conference ICPR 2012 and for a Special Issue of the journal Pattern Recognition on the Analysis and Recognition of IIF Images 
                        [40]. Ever since it has been largely adopted as a benchmark for IIF image analysis tools. It contains 1388×1038 sized 24 bit colour depth images of 28 different patients (one image per patient). HEp-2 substrates were prepared with fixed dilution of 1:80, as per recommendations of the guidelines, and images were acquired with a fluorescence microscope (40-fold magnification) coupled with a 50W mercury vapor lamp and a digital CCD camera with 6.45μm size square pixels. The dataset provides manual segmentations and annotations of all the HEp-2 cells, which are 1582 in total. Physicians specialized in immunology reviewed all the segmentations and reported on the fluorescence intensity (either intermediate or positive level), mitotic phase and fluorescent pattern of each cell (respectively, one among six: homogeneous, fine speckled, coarse speckled, nucleolar, centromere and cytoplasmic). The images are quite evenly distributed among different intensity levels and fluorescent patterns (details in Table 1).


                        I3Asel dataset originates from the First Workshop on Pattern Recognition Techniques for IIF images (I3A) of the conference ICPR 2014 [18]. 1001 sera of patients positive to ANA testing were diluted to 1:80 and HEp-2 specimens were photographed using a monochrome camera fitted on the fluorescent microscope, obtaining 1388×1040 sized IIF images with two different intensity levels (again, intermediate and positive) and seven different pattern classes (homogeneous, speckled, nucleolar, centromere, nuclear membrane, golgi and mitotic spindle), with large majority of images belonging to the first four patterns. Unlike MIVIA dataset, neither manual segmentation nor ground truth about the mitotic phase of the cells were provided, but only their fluorescent intensity level and pattern. 252 out of 1001 specimens were made publicly available, while the rest was used to test the participants’ results and remained private.


                        I3Asel dataset contains 71 images belonging to the public portion of the dataset,
                           4
                        
                        
                           4
                           As anticipated in Section 3, LOOCV strategy is best suited to evaluate the robustness and generalization capabilities of a classifier. Nevertheless, as SDA training is based on repeating a cross-validation per each permutation of the sub-classes, in our case LOOCV imposes a cross-validation within the cross-validation, which is computationally very intensive. Hence, in our experiments we run LOOCV on a sub-set of the I3A images, randomly selected. This is different from the strategy used by I3A contest, where tests were run on a private set of images with same characteristics of the training set.
                         chosen as follows: first, we picked all the specimens of the least represented pattern (i.e. golgi). Then, we randomly selected the specimens of the other classes ensuring (i) the independence of the images (i.e. each image derives from a different specimen) and (ii) a good balance of the different patterns and fluorescent levels (details in Table 1), except for mitotic spindle patterns, that were excluded from the study. Indeed, unlike regular ANAs, mitotic spindle autoantibodies (MSAs) can be observed on only a small portion of the specimen cells. As the per-cell classification approach followed by ANAlyte assumes that the pattern of the specimen coincides with the pattern of the majority of the cells in the image, the identification of MSAs is out of our scope.

@&#RESULTS AND DISCUSSION@&#


                        Table 2
                         reports the LOOCV accuracy of fluorescent intensity and fluorescent pattern classification (for the latter, both image-wise and cell-wise).

The first row of the table shows values obtained with a semi-automated version of ANAlyte, where the pattern classification is performed on manually segmented cells, using the cell segmentation ground truth provided along with the benchmark (the experiments were run only on MIVIA dataset in this case, because I3Asel does not provide such ground truth). The second row of the table shows results obtained with the fully-automated version of ANAlyte (i.e. including the automated segmentation of the HEp-2 cells, as depicted in Fig. 3), on both MIVIA and I3Asel datasets.

In the following, we analyze in depth the results on intensity level and fluorescent pattern classification.

The accuracy of intensity level classification was above 80% in all our experiments. More specifically, as shown in the third column of Table 2, the rate of IIF slides correctly classified was respectively 85.71% for MIVIA and 84.51% for I3Asel dataset. As the Intensity Classifier module works on the full slide and not on the individual HEp-2 cells (see Fig. 3), the image classification accuracy does not depend on cell segmentation. Hence, values reported in the second and third row of the table are the same.

For a deeper insight into the obtained results, in Table 3
                            we report the confusion matrices of both datasets, where columns represent the instances (i.e. the number of IIF slides) in a predicted intensity level, and rows the instances in the actual intensity level.

As can be seen from the table, the classification accuracy for intermediate and positive images was about 85% and 87% in MIVIA dataset, 92% and 77% in I3Asel dataset.

Ultimately, we report the accuracy of intensity classification obtained in both benchmarks, grouped by fluorescent pattern. As it is visible from Table 4
                           , the only patterns that obtained accuracy below 75% were the cytoplasmic in MIVIA dataset and golgi in I3Asel dataset (respectively, 50% and 63.64%).

The fully-automated version of ANAlyte generally obtained a good fluorescent pattern classification accuracy: the rate of images correctly classified was 92.86% for MIVIA dataset and of 90.14% for I3Asel dataset, respectively.

By comparing the results of the two experiments run on MIVIA dataset, we can also quantify the impact of automated cell segmentation on the final outcome of the tool. When considering cell-wise classification results (last column of Table 2), the accuracy drop from the semi-automated to the fully-automated version of ANAlyte was quite significant: from 89.55 to 73.44%, respectively. This is, indeed, not surprising, if we consider that the textural and morphological features used for pattern classification are highly dependent on the region of interest they are extracted from (see Section 2.3.1). As a consequence, small errors in the identification of the cellular borders can easily mislead textural analysis and hence translate into a wrong categorization of the cellular pattern. Nevertheless, in order to assess the real significance of these numbers in a realistic clinical context, we need to compare them with the level of accuracy of a human specialist working in the same conditions as the automated tool. As reported by [40], a physician was asked to manually classify each cell of MIVIA dataset, obtaining a cell classification accuracy of 73.3%. This value is indeed comparable to the 73.44% obtained by ANAlyte. Hence, we can conclude that the cell segmentation error does not affect pattern classification in a way that is critical for its clinical application.

On the other hand, if we analyze results on image-wise classification, we can observe that the accuracy drop due to cell segmentation errors was much smaller. As shown in Table 2, the accuracy was 96.43% with the semi-automated version of ANAlyte and 92.86% with the fully-automated version, while the accuracy reported for a human specialist in the same dataset is 87% [40]. If we consider that the fluorescent pattern of the IIF slide (and not of the individual cells) is the output that is relevant for diagnostic purposes, this result further supports the convenience of using ANAlyte in a clinical context.

Ultimately, in order to assess the behaviour of the tool in the different pattern classes, we report the confusion matrices of all performed experiments (respectively, in terms of image-wise classification in Table 5
                            and cell-wise classification in Table 6
                           ). Again, the columns represent the predicted classes and the rows the actual classes as reported by the corresponding benchmarks.

From the analysis of the confusion matrices, we can draw the following considerations:
                              
                                 (i)
                                 in general, image-wise (Table 5) and cell-wise (Table 6) accuracies do not have a direct correspondence, even when considered within the same experiment. In other words, the patterns with the lowest (or highest) cell-wise accuracy do not necessarily maintain the same trait in terms of image-wise accuracy. For example, nucleolar and speckled patterns in MIVIA dataset had relatively low cell-wise accuracy values (lower than 70%), but the corresponding image-wise accuracy was 100%. Likewise, golgi pattern in I3Asel dataset (around 61% cell-wise accuracy, against 92% rate of IIF slides correctly classified). To this end, we have to consider the different type of information carried by these two metrics. Image-wise accuracy (as already explained before in this section) is the most significant descriptor for assessing the overall performance of a pattern classification tool in the context of ANA testing. On the other hand, the rate of cells correctly classified supplies additional information on the robustness of classification, but only provided that it is considered jointly with the former metrics.

by comparing the results of the fully-automated ANAlyte with the ones on manually segmented cells (see Table 5(a) and (b)), we can observe that the only pattern which was somehow affected by cell segmentation errors is the cytoplasmic. Indeed, the cytoplasmic images are peculiar compared to all the others (i.e. the fluorescence arises from the cytoplasm rather than the nucleus of the cell), which translates into a much higher probability of cell segmentation errors. Nevertheless, the impact on pattern classification accuracy was reasonably low (one misclassified slide in Table 5(b) against none in Table 5(a)). All other patterns registered a drop of cell-wise classification accuracy, but their image-wise accuracy was left unchanged.

When considering the results on I3Asel datasets shown in Table 5(c), ANAlyte's performance was very good (higher than 90%) for all patterns, except for speckled (around 67%). Furthermore, the four (out of a total of twelve) misclassified speckled images were not categorized into one specific pattern class, but respectively as centromere, homogeneous, nucleolar and nuclear membrane. To clarify this behaviour, it is worth noting that the I3Asel benchmark is characterized by very high within-class variability of the speckled images, which makes the interpretation of this pattern particularly ambiguous compared to the others. For example, with reference to Fig. 6
                                    , sample (b), speckled, has textural characteristics that are in between those of (a), speckled, and (c), centromere.

This consideration does not apply to MIVIA dataset, where the speckled pattern is split into two distinct categories (coarse and fine, respectively). Indeed, the classification accuracy of the speckled patterns in MIVIA dataset was 100% (see Table 5(b)).

In order to assess the results obtained in our experiments, it is useful to compare them with the published results of recent pattern classification approaches. This assessment is summarized in Fig. 7
                           , where we show:
                              
                                 (i)
                                 on the left, a boxplot of the results of the works published in the Special Issue on Analysis and Recognition of Indirect Immunofluorescence Images of the journal Pattern Recognition [11]. Such numbers were obtained on MIVIA dataset, using the manually segmented cells as input, and computing fluorescent pattern accuracy in terms of rate of images correctly classified in a LOOCV protocol, as explained in Section 3.1. The magenta and green dots represent, respectively, the accuracy of the semi-automated version of ANAlyte (i.e. run on manually segmented cells, same as for the boxplot), and the accuracy of the fully-automated version.

on the right, a boxplot of the results obtained by the participants of I3A contest [18] (specimen classification task), with a green dot representing ANAlyte's accuracy on the I3Asel dataset. As anticipated in Section 3.2, the test database of I3A contest is not publicly available. Hence, a direct comparison between ANAlyte and the other methods is not possible in this case. However, in order to increase the consistency between I3A contestants and ANAlyte, both sets of results are shown for the same six patterns (i.e. with exclusion of MSAs). Accuracy is computed in terms of average per-class correct rate, same as it was done by the contest's organizers.

Ultimately, in Fig. 8
                            we show the LOOCV accuracy of ANAlyte (fully-automated version) and of the best performing techniques of Fig. 7. More specifically, the top and bottom parts of the figure show:
                              
                                 (i)
                                 the top two methods from the special issue Pattern Recognition 47 (7) 2014, respectively by Nosaka et al. and Xiangfei et al. In [23], Nosaka et al. applied a framework combining a novel type of LBP descriptor, namely Rotation Invariant Co-Occurrence among adjacent LBPs (RIC-LBP), with a traditional multi-class SVM classifier. In [24], Xiangfei et al. proposed a novel dictionary learning strategy for the sparse representation of the cell images, leveraging on textural and gradient features.

the top two methods from the proceedings of I3A Workshop 2014,
                                       5
                                    
                                    
                                       5
                                       As for the second boxplot of Fig. 7, it should be taken into consideration that ANAlyte was assessed on a subset of the I3A images.
                                     respectively by Manivannan et al. and Gragnaniello et al. In [26], Manivannan et al. applied sparse coding with max-pooling to aggregate texture features with cell pyramids at two different sets of scales. In [25], Gragnaniello et al. proposed a Bag of Words representation of a novel dense local descriptor combining log-polar transformation with multi scale Gaussian smoothing applied to the gradient images.

From the analysis of Figs. 7 and 8, we can observe that ANAlyte obtained the best results among all the other methods of Pattern Recognition 47 (7) 2014 and it was in line with the top-quartile performance methods of I3A workshop, albeit outperformed by the top ranking techniques proposed by Manivannan and Gragnaniello et al. (respectively, by 6% and 2.25%).

When interpreting such results, it must be taken into account that most of the other methods were assessed on pre-segmented cells, while ANAlyte addresses the automatization of the whole analysis, including cell segmentation. Sure indeed, in spite of the accuracy drop due to automated cell segmentation, we can conclude that ANAlyte is comparable in terms of fluorescent pattern classification with the top approaches proposed by recent literature.

@&#CONCLUSIONS@&#

The automatization of the analysis of IIF HEp-2 specimens is widely recognized as the only valid solution to avoid the subjectivity of the human interpretation of the images and to make the results of ANA testing completely repeatable. For this purpose, in this paper we proposed a fully-automated tool, ANAlyte, that provides the information required by the immunologists (i.e. the intensity level and fluorescent pattern of the IIF image) without needing user interaction.

To this effect, ANAlyte combines the following modules:
                        
                           (i)
                           the Intensity Classifier (IC) module performs a categorization of the fluorescent intensity into a given number of levels, based on a training set of labelled images. IC extracts a set of features aimed at quantifying image contrast at different scales and in several locations of the input slide, in order to handle intensity variations that are inherent of fluorescence imaging. Finally, this set of contrast features is fed into a machine learning algorithm that provides the intensity level of the input image

the Cell Segmenter (CS) module splits the input image into individual HEp-2 cells without any a-priori knowledge about its intensity level or pattern. Since cells with different patterns require different processing steps, CS automatically adapts the segmentation procedure based on a pre-classification step. Then, an adaptive marker-controlled watershed approach is applied to obtain the segmentation of the individual HEp-cells.

the Pattern Classifier (PR) module receives the individual HEp-2 cells from the CS module and categorizes them into a set of fluorescent patterns represented in its training set. The intra-class variability that is inherent of this type of images is softened by Subclass Discriminant Analysis, that splits each pattern into an optimal number of sub-classes. Finally, a voting on the fluorescent pattern of the individual cells allows to determine the fluorescent pattern of the input image.

Extensive experimental results run on two public benchmarks of HEp-2 images allow to:
                        
                           (i)
                           show that the integration of Cell Segmenter and Pattern Classifier modules does not impact in a critical way on the effectiveness of the tool when compared with human evaluation. Hence, a full automatization of pattern classification is feasible and can be applied to a clinical setting.

demonstrate the good level of accuracy and robustness of ANAlyte in analyzing the fluorescent intensity and pattern of IIF images without user-interaction.

In the future, we plan to make our tool available for use in a clinical laboratory and run more tests on real patient data.

@&#REFERENCES@&#

