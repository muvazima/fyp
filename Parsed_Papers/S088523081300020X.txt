@&#MAIN-TITLE@&#Comparative experiments using supervised learning and machine translation for multilingual sentiment analysis

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We study the possibility to employ Machine Translation (MT) systems and supervised methods for multilingual sentiment analysis.


                        
                        
                           
                           Experiments are done for English, German, Spanish and French.


                        
                        
                           
                           We use three MT systems – Google, Bing and Moses –, different supervised learning algorithms and various types of features.


                        
                        
                           
                           We show how meta-classifiers can be employed to mitigate the noise introduced by translation.


                        
                        
                           
                           Our extensive evaluations show that MT systems can be used for multilingual sentiment analysis.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multilingual sentiment analysis

Opinion mining

Machine translation

Supervised learning

@&#ABSTRACT@&#


               
               
                  Sentiment analysis is the natural language processing task dealing with sentiment detection and classification from texts. In recent years, due to the growth in the quantity and fast spreading of user-generated contents online and the impact such information has on events, people and companies worldwide, this task has been approached in an important body of research in the field. Despite different methods having been proposed for distinct types of text, the research community has concentrated less on developing methods for languages other than English. In the above-mentioned context, the present work studies the possibility to employ machine translation systems and supervised methods to build models able to detect and classify sentiment in languages for which less/no resources are available for this task when compared to English, stressing upon the impact of translation quality on the sentiment classification performance. Our extensive evaluation scenarios show that machine translation systems are approaching a good level of maturity and that they can, in combination to appropriate machine learning algorithms and carefully chosen features, be used to build sentiment analysis systems that can obtain comparable performances to the one obtained for English.
               
            

@&#INTRODUCTION@&#

Together with the increase in the access to technology and the Internet, the recent years have shown a steady growth of the volume of user-generated contents on the Web. The diversity of topics covered by this data (also containing expressions of subjectivity) in the new textual types such as blogs, fora, microblogs, has been proven to be of tremendous value to a whole range of applications, in Economics, Social Science, Political Science, Marketing, to mention just a few. Notwithstanding these proven advantages, the high quantity of user-generated contents makes this information hard to access and employ without the use of automatic mechanisms. This issue motivated the rapid and steady growth in interest from the natural language processing (NLP) community to develop computational methods to analyze subjectivity and sentiment in text. Additionally, apart from the research on sentiment analysis in the context of user-generated contents, studies have also focused on developing methods for sentiment analysis in newspaper articles. This task is especially relevant to the online reputation management of public figures and organization and to monitoring the reaction to the events described in mainstream media. As such, different methods have been proposed to deal with these phenomena for the distinct types of text and domains, reaching satisfactory levels of performance for English. Nevertheless, for certain applications, such as news monitoring, the information in languages other than English is also highly relevant and cannot be disregarded. Additionally, systems dealing with sentiment analysis in the context of monitoring must be reliable and perform at similar levels as the ones implemented for English.

Although the most direct solution to these issues of multilingual sentiment analysis would be the use of machine translation systems, researchers in sentiment analysis have been reluctant to using such technologies due to the low performance they used to have. However, in the past years, the performance of machine translation systems has steadily improved. Public or open access solutions (e.g. Google Translate,
                        2
                     
                     
                        2
                        
                           http://translate.google.it/.
                      Bing Translator
                        3
                     
                     
                        3
                        
                           http://www.microsofttranslator.com/.
                     ) offer more and more accurate translations for frequently used languages.

Bearing these thoughts in mind, in this article we study the manner in which sentiment analysis can be done for languages other than English, using machine translation. In particular, we will study this issue in three languages – French, German and Spanish – using three different machine translation systems – Google Translate, Bing Translator and Moses (Koehn et al., 2007) and different machine learning models.

We employ these systems to obtain training and test data for these three languages and subsequently extract different features that we employ to build different machine learning models using Support Vector Machines Sequential Minimal Optimization – SVM SMO – (Platt, 1999). We additionally employ meta-classifiers to test the possibility to minimize the impact of noise (incorrect translations) in the obtained data. To have a more precise measure of the impact of quality translation on this task, we create Gold Standard sets for each of the three languages, by translating the data with the Yahoo translation system
                        4
                     
                     
                        4
                        
                           http://www.babelfish.com/.
                      and subsequently manually correcting the output.

Our experiments show that machine translation systems are reaching a reasonable level of maturity so as to be employed for multilingual sentiment analysis and that for some languages (for which the translation quality is high enough) the performance that can be attained is similar to that of systems implemented for English, in terms of weighted F-measure.

@&#RELATED WORK@&#

The work presented herein is related to two different directions of research in NLP: multilingual sentiment analysis and the use of machine translation for multi- and cross-lingual tasks in NLP. The contributions in these two research directions that are relevant to the present research are presented in the following subsections.

Most of the research in subjectivity and sentiment analysis was done for English. However, there were some authors who developed methods for the mapping of subjectivity lexicons to other languages. To this aim, Kim and Hovy (2006) use a machine translation system and subsequently employ a subjectivity analysis system that was developed for English to create subjectivity analysis resources in other languages. Ahmad et al. (2007) use the topical distributions in different languages to detect important sentiment phrases in a multilingual setting, starting from the idea that words with a lower frequency are more representative of the topic and searching for sentiment-related terms around those. Inui and Yamamoto (2011) employ machine translation and, subsequently, sentence filtering to eliminate the noise obtained in the translation process, based on the idea that sentences that are translations of each other should contain sentiment-bearing words that have the same polarity. Mihalcea et al. (2007) propose a method to learn multilingual subjective language via cross-language projections. They use the Opinion Finder lexicon (Wilson et al., 2005) and use two bilingual English-Romanian dictionaries to translate the words in the lexicon. Another approach was proposed by Banea et al. (2008b). To this aim, the authors perform three different experiments – translating the annotations of the MPQA corpus, using the automatically translated entries in the Opinion Finder lexicon and the third, validating the data by reversing the direction of translation. In a further approach, Banea et al. (2008a) apply bootstrapping to build a subjectivity lexicon for Romanian, starting with a set of 60 words which they translate and subsequently filter using a measure of similarity to the original words, based on latent semantic analysis (LSA) (Deerwester et al., 1990) scores. Yet another approach to mapping subjectivity lexica to other languages is proposed by Xiaojun (2009), who uses co-training to classify un-annotated Chinese reviews using a corpus of annotated English reviews. He first translates the English reviews into Chinese and subsequently back to English. He then performs co-training using all generated corpora. Kim et al. (2010) create a number of systems consisting of different subsystems, each classifying the subjectivity of texts in a different language. They translate a corpus annotated for subjectivity analysis (MPQA), the subjectivity clues (Opinion Finder) lexicon and re-train a Naive Bayes classifier that is implemented in the Opinion Finder system using the newly generated resources for all the languages considered. Banea et al. (2010) translate the MPQA corpus into five other languages (some with a similar etymology, others with a very different structure). Subsequently, they expand the feature space used in a Naive Bayes classifier using the same data translated to 2 or 3 other languages. Another type of approach was proposed by Bader et al. (2011), who use latent semantic indexing as a manner to bridge between the concepts in different languages. Finally, Steinberger et al. (2011a,b) create sentiment dictionaries in other languages using a method called “triangulation”. They translate the data, in parallel, from English and Spanish to other languages and obtain dictionaries from the intersection of these two translations.

Attempts to use machine translation in different natural language processing tasks have not been widely used due to poor quality of translated texts, but recent advances in machine translation have motivated such attempts. In Information Retrieval, Savoy and Dolamic (2009) proposed a comparison between Web searches using monolingual and translated queries. On average, the results show a drop in performance when translated queries are used, around 15%. For some language pairs, the average result obtained is around 10% lower than that of a monolingual search while for other pairs, the retrieval performance is clearly lower. In cross-language document summarization, Wan et al. (2010) and Boudin et al. (2010) combined the MT quality score with the informativeness score of each sentence in a set of documents to automatically produce summaries in a target language using source language texts. In the work by Steinberger and Turchi (2012), different ways of using translated data for multi-document summary evaluation were tested: firstly, summaries were translated into the target language and compared against human-produced summaries; secondly, original documents were translated and then summarized; finally, human-produced summaries were translated and used to evaluate summaries in the target language. Their results show that the use of translated summaries or models does not alter much the overall system ranking, but a drop in the ROUGE score (Lin, 2004)
                           5
                        
                        
                           5
                           The ROUGE score is the measure which is currently employed to evaluate the performance of summarization systems.
                         is evident, and it strongly depends on the translation performance. In the work by Wan et al. (2010), each sentence of the source document is ranked according both the scores, the summary is extracted and then the selected sentences translated to the target language. Differently, in the work by Boudin et al. (2010), sentences are first translated, then ranked and selected. Both approaches enhance the readability of the generated summaries without degrading their content. In the context of ranking translated documents, in the research by Turchi et al. (2012c), sentences within a document were ranked according to their informativeness and translation quality and this ranking used to assign a global score to each document for the ranking of groups of documents. This required different evaluation strategies from those used in the text summarization field. Finally, systems employing MT for multi- and cross-lingual tasks were developed in the context of evaluations conducted within the Cross-Language Evaluation Forum (CLEF),
                           6
                        
                        
                           6
                           
                              http://clef.isti.cnr.it/.
                         the Text REtrieval Conference (TREC)
                           7
                        
                        
                           7
                           
                              http://trec.nist.gov/.
                         and the NII Test Collection for IR Systems Project (NTCIR)
                           8
                        
                        
                           8
                           
                              http://research.nii.ac.jp/ntcir/index-en.html.
                         competitions. The results obtained by the systems in these evaluations have shown that the use of MT for multi- and cross-lingual tasks is a promising direction for research.

In a formal manner, we can define the sentiment classification performance, scp, as a function of four factors: the feature set, fs, the feature representation, fr, the learning algorithm, l, and the experimental design, ed (e.g. data split): scp
                     =
                     fn(fs, fr, l, ed). On the one hand, by choosing the optimal parameters for each of the factors, in the case of correct training data from a language, we can obtain the maximum performance for sentiment classification on that specific data, for that language. We will denote this maximum performance by scp
                     
                        max
                     . On the other hand, when machine translated data is used for training and a human-produced translation of the Gold Standard for testing, the sentiment classification performance, scp
                     
                        tG
                     , is negatively affected by the translation error ϵ
                     
                        tr
                      such that scp
                     
                        tG
                     
                     =
                     scp
                     
                        max
                     
                     −
                     ϵ
                     
                        tr
                     . In case of perfect translations of the training data (i.e. human-produced translations), ϵ
                     
                        tr
                     
                     ⟶0 and scp
                     
                        tG
                     
                     ⟶
                     scp
                     
                        max
                     .

Our main contribution in this article is the comparative study of multilingual sentiment analysis performance in different target languages, using (a) distinct feature sets produced by employing different machine translation systems; (b) feature representations; (c) learning algorithms; and (d) noise removal mechanisms (in this case, meta-classifiers). The objective is to better understand the impact of the translation error on the multilingual sentiment classification performance.

Although, as we have seen in Section 2, a few attempts were made to build systems that deal with sentiment analysis in a multilingual setting they mostly employed bilingual dictionaries and used lexicon-based approaches. The very few that employed supervised learning using translated data have concentrated only on the issue of sentiment classification and have disregarded the impact of the translation quality and the difference that the use of distinct translation systems can make under these settings. Additionally, research in this area has concentrated only on the application of one set of features, one representation and one or two classifiers, whereas the distinct characteristics of translated data (when compared to the original data) may imply that other features could be more appropriate.

Another characteristic of previous approaches to multiligual sentiment analysis is that such approaches have usually employed only simple machine learning algorithms. No attempt has been made to study the possibility to enhance the performance of the classification through the removal of noise in the data. To this aim, in the present research, we study the impact of using meta-classifiers for noise removal.

We employ three different systems – Bing Translator, Google Translate and Moses to translate data from English to three languages – French, German and Spanish. We manually create a Gold Standard test set for all the languages used, on the one hand, to measure the translation quality and to test the performance of sentiment classification on translated (noisy) versus correct data. These correct translated test sets allow us to have a more precise measure of the impact of translation quality on the sentiment classification task.

The lack of manually translated training data for each of the target languages and the large cost of manually producing it do not allow us to compute the maximum sentiment classification performance, scp
                     
                        max
                     , in all the desired languages using training and testing Gold Standard data. In fact, most of the related approaches mentioned in Section 2 only employ corrected test data to measure their performance. Supported by the results proposed by Banea et al. (2008a,b, 2010) and Mihalcea et al. (2007), where the sentiment classification performance in English is generally better than the performance in the other languages, we use it as reference performance.

Another contribution this article brings is the study of different types of features that can be employed to build machine learning models for the sentiment task. Further on, apart from studying different features that can be used to represent the training data, we also study the use of meta-classifiers to minimize the effect of noise in the data.

We employ Yahoo Translate to translate the test data to the same three languages and manually correct the output, thus obtaining a Gold Standard, used, on the one hand, to measure the translation quality and to test the performance of sentiment classification on translated (noisy) versus correct data. Using these correct translations, we can have a more precise measure of the impact of translation quality on the sentiment classification task.
                        9
                     
                     
                        9
                        Yahoo translate was used because at a first inspection, it contained the least “correct” translations. Using it, although having to perform many manual corrections in the data, we can impede translation bias – i.e. the use of specific words by a human, if they were to translate the texts manually from scratch.
                     
                  

Our comparative results show, on the one hand, that machine translation can be reliably used for multilingual sentiment analysis and, on the other hand, which are the main characteristics of the data for such approaches to be successfully employed.

For our experiments, we employed the data provided for English in the NTCIR 8 Multilingual Opinion Analysis Task (MOAT).
                        10
                     
                     
                        10
                        
                           http://research.nii.ac.jp/ntcir/ntcir-ws8/permission/ntcir8xinhua-nyt-moat.html.
                      In this task, the organizers provided the participants with a set of 20 topics (questions) and a set of documents in which sentences relevant to these questions could be found, taken from the New York Times Text (2002–2005) corpus. The documents were given in two different forms, which had to be used correspondingly, depending on the task to which they participated. The first variant contained the documents split into sentences (6165 in total) and had to be used for the task of opinionatedness, relevance and answerness. In the second form, the sentences were also split into opinion units (6223 in total) for the opinion polarity and the opinion holder and target tasks. For each of the sentences, the participants had to provide judgements on the opinionatedness (whether they contained opinions), relevance (whether they are relevant to the topic). For the task of polarity classification, the participants had to employ the dataset containing the sentences that were also split into opinion units (i.e. one sentences could contain two/more opinions, on two/more different targets or from two/more different opinion holders).

For our experiments, we employed the latter representation. From this set, we randomly chose 600 opinion units, to serve as test set. The rest of opinion units will be employed as training set. Subsequently, we employed the Google Translate, Bing Translator and Moses systems to translate, on the one hand, the training set and on the other hand the test set, to French, German and Spanish. Additionally, we employed the Yahoo system (whose performance was the lowest in our initial experiments) to translate only the test set into these three languages. Further on, this translation has been corrected manually by a person, for all the languages. This corrected data serves as Gold Standard.
                        11
                     
                     
                        11
                        Please note that each sentence may contain more than one opinion unit. In order to ensure a contextual translation, we translated the whole sentences, not the opinion units separately. In the end, we eliminate duplicates of sentences (due to the fact that they contained multiple opinion units), resulting in around 400 sentences in the test and Gold Standard sets and 5700 sentences in the training set.
                      Most of these sentences, however, contained no opinion (were neutral). Due to the fact that the neutral examples are majority and can produce a large bias when classifying the polarity of the sentences, we eliminated these examples and employed only the positive and negative sentences in both the training, as well as the test sets. After this elimination, the training set contains 943 examples (333 positive and 610 negative) and the test set and Gold Standard contain 357 examples (107 positive and 250 negative). Although the upper bound for each of the systems would be possible to estimate using Gold Standard for each of the training sets, as well, at this point we considered the scenario that is closer to real situations, in which the issue is related to the inexistence of training data for a specific language. The process is illustrated in Fig. 1
                     .

During the 1990s the research community on machine translation proposed a new approach that made use of statistical tools based on a noisy channel model originally developed for speech recognition (Brown et al., 1994). In the simplest form, statistical machine translation (SMT) can be formulated as follows. Given a source sentence written in a foreign language f, the Bayes rule is applied to reformulate the probability of translating f into a sentence e written in a target language:


                     
                        
                           
                              
                                 e
                                 best
                              
                              =
                              arg
                              
                                 max
                                 e
                              
                              p
                              (
                              e
                              |
                              f
                              )
                              =
                              arg
                              
                                 max
                                 e
                              
                              p
                              (
                              f
                              |
                              e
                              )
                              
                                 p
                                 LM
                              
                              (
                              e
                              )
                           
                        
                     where p(f|e) is the probability of translating e to f and p
                     
                        LM
                     (e) is the probability of producing a fluent sentence e. For a full description of the model see Koehn et al. (2003).

The noisy channel model was extended in different directions. In this work, we analyse the most popular class of SMT systems: phrase-based statistical machine translation (PBSMT). It is an extension of the noisy channel model using phrases rather than words. A source sentence f is segmented into a sequence of I phrases f
                     
                        I
                     
                     ={f
                     1, f
                     2, …, f
                     
                        I
                     } and the same is done for the target sentence e, where the notion of phrase is not related to any grammatical assumption; a phrase is an n-gram. The best translation e
                     
                        best
                      of f is obtained by:


                     
                        
                           
                              
                                 
                                    e
                                    
                                       b
                                       e
                                       s
                                       t
                                    
                                 
                                 =
                                 arg
                                  
                                 
                                    
                                       max
                                    
                                    e
                                 
                                 p
                                 (
                                 e
                                 |
                                 f
                                 )
                                 =
                                 arg
                                  
                                 
                                    
                                       max
                                    
                                    e
                                 
                                 p
                                 (
                                 f
                                 |
                                 e
                                 )
                                 
                                    p
                                    
                                       L
                                       M
                                    
                                 
                                 (
                                 e
                                 )
                                 =
                                 arg
                                  
                                 
                                    
                                       max
                                    
                                    e
                                 
                                 
                                    
                                       ∏
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    I
                                 
                                 φ
                                 
                                    
                                       (
                                       
                                          f
                                          i
                                       
                                       |
                                       
                                          e
                                          i
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          φ
                                       
                                    
                                 
                                 d
                                 
                                    
                                       (
                                       
                                          a
                                          i
                                       
                                       −
                                       
                                          b
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          d
                                       
                                    
                                 
                                 
                                    
                                       ∏
                                    
                                    
                                       i
                                       =
                                       1
                                    
                                    
                                       |
                                       e
                                       |
                                    
                                 
                                 
                                    p
                                    
                                       L
                                       M
                                    
                                 
                                 
                                    
                                       (
                                       
                                          e
                                          i
                                       
                                       |
                                       
                                          e
                                          1
                                       
                                       …
                                       
                                          e
                                          
                                             i
                                             −
                                             1
                                          
                                       
                                       )
                                    
                                    
                                       
                                          λ
                                          
                                             L
                                             M
                                          
                                       
                                    
                                 
                              
                           
                        
                     where ϕ(f
                     
                        i
                     |e
                     
                        i
                     ) is the probability of translating a phrase e
                     
                        i
                      into a phrase f
                     
                        i
                     . d(a
                     
                        i
                     
                     −
                     b
                     
                        i−1) is the distance-based reordering model that drives the system to penalise significant reorderings of words during translation, while allowing some flexibility. In the reordering model, a
                     
                        i
                      denotes the start position of the source phrase that is translated into the ith target phrase, and b
                     
                        i−1 denotes the end position of the source phrase translated into the (i
                     −1)th target phrase. p
                     
                        LM
                     (e
                     
                        i
                     |e
                     1
                     …
                     e
                     
                        i−1) is the language model probability that is based on the Markov's chain assumption. It assigns a higher probability to fluent/grammatical sentences. λ
                     
                        ϕ
                     , λ
                     
                        LM
                      and λ
                     
                        d
                      are used to give a different weight to each element. For more details see Koehn et al. (2003).

Three different SMT systems were used to translate the human annotated sentences: two existing online services such as Google Translate and Bing Translator
                        12
                     
                     
                        12
                        
                           http://translate.google.com/ and http://www.microsofttranslator.com/.
                      and an instance of the open source phrase-based statistical machine translation toolkit Moses, Koehn et al. (2007).

To train our models based on Moses we used the freely available corpora: Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), Opus (Tiedemann, 2009), News Corpus (Callison-Burch et al., 2009). This results in 2.7 million sentence pairs for English–French, 3.8 for German and 4.1 for Spanish. All the modes are optimized running the MERT algorithm (Och, 2003) on the development part of the News Corpus. The translated sentences are recased and detokonized (for more details on the system, please see Turchi et al. (2012a).

The performance of an SMT system is automatically evaluated comparing the output of the system against human-produced translations (reference). The BLEU score (Papineni et al., 2001) is based on n-gram precision that is the fraction of n-grams of the target sentences that occur in references. This quantity is affected by the fact that the same part of the reference sentences can be matched more than one time with a n-gram in the target sentence. This implies that n-gram precision can produce misleading results. To avoid this situation, the BLEU score uses a modified n-gram precision that does not allow the same part of the reference sentence to be used twice. Modified n-gram precision is also used to penalise target sentences that are longer than their references, but it is not enough to enforce the proper length of the translation. To solve this a brevity penalty factor has been introduced to give better score to those target sentences that reflect the reference sentence length. BLEU score is the product of the geometric average of the modified n-gram precision with n-gram up to N and the brevity penalty and it ranges between 0 and 1, and larger value identifies better translation. The BLEU score strongly correlates with other automatic metrics used in machine translation (Turchi et al., 2012), and, although other measures correlate better with the human-judgements (e.g. AMBER; Chen et al., 2012), nowadays it is the most used score in the evaluation of translated texts.

In the field of sentiment analysis, most work has concentrated on creating and evaluating methods, tools and resources to discover whether a specific “target” or “object” (person, product, organization, event, etc.) is “regarded” in a positive or negative manner by a specific “holder” or “source” (i.e. a person, an organization, a community, people in general, etc.). This task has been given many names, from opinion mining, to sentiment analysis, review mining, attitude analysis, appraisal extraction and many others.

The issue of extracting and classifying sentiment in text has been approached using different methods, depending on the type of text, the domain and the language considered. Broadly speaking, the methods employed can be classified into unsupervised (knowledge-based), supervised and semi-supervised methods. The first usually employ lexica or dictionaries of words with associated polarities (and values – e.g. 1, −1) and a set of rules to compute the final result. The second category of approaches employ statistical methods to learn classification models from training data, based on which the test data is then classified. Finally, semi-supervised methods employ knowledge-based approaches to classify an initial set of examples, after which they use different machine learning methods to bootstrap new training examples, which they subsequently use with supervised methods.

The main issue with the first approach is that obtaining large-enough lexica to deal with the variability of language is very expensive (if it is done manually) and generally not reliable (if it is done automatically). Additionally, the main problem of such approaches is that words outside contexts are highly ambiguous. Semi-supervised approaches, on the other hand, highly depend on the performance of the initial set of examples that is classified. If we are to employ machine translation, the errors in translating this small initial set would have a high negative impact on the subsequently learned examples. The challenge of using statistical methods is that they require training data (e.g. annotated corpora) and that this data must be reliable (i.e. not contain mistakes or “noise”). However, the larger this dataset is, the less influence the translation errors have.

Since we want to study whether machine translation can be employed to perform sentiment analysis for different languages, we employed statistical methods in our experiments. More specifically, we used Support Vector Machines Sequential Minimal Optimization (SVM SMO) since the literature in the field has confirmed it as the most appropriate machine learning algorithm for this task (Pang and Lee, 2008).

In the case of statistical methods, the most important aspect to take into consideration is the manner in which texts are represented – i.e. the features that are extracted from it. For our experiments, we represented the sentences based on the unigrams and the bigrams that were found in the training data. Although there is an ongoing debate on whether bigrams are useful in the context of sentiment classification, we considered that the quality of the translation can also be best quantified in the process by using these features (because they give us a measure of the translation correctness, both regarding words, as well as word order). Higher level n-grams, on the other hand, would only produce more sparse feature vectors, due to the high language variability and the mistakes in the translation.

@&#EXPERIMENTS@&#

In order to test the performance of sentiment classification when using translated data, we employed supervised learning using different features:
                        
                           •
                           In the first approach, we represented, for each of the languages and translation systems, the sentences as vectors, whose features marked the presence/absence (boolean) of the unigrams contained in the corresponding training set (e.g. we obtained the unigrams in all the sentences in the training set obtained by translating the English training data to Spanish using Google and subsequently represented each sentence in this training set, as well as the test set obtained by translating the test data in English to Spanish using Google marking the presence of the unigram features).

In the second approach, we represented the training and test sets in the same manner as described above, with the difference that the features were computed not as the presence of the unigrams, but the tf-idf score of that unigram.

In the third approach, we represented, for each of the languages and translation systems, the sentences as vectors, whose features marked the presence/absence of the unigrams and bigrams contained in the corresponding training set.

In the fourth approach, we represented the training and test sets as in the previous point, with the difference that the features were computed not as the presence of the unigrams and bigrams, but the tf-idf score of the unigrams and bigrams, respectively.

In our experiments, we also studied the possibility to employ sentiment-bearing words in the sentences to be classified as features for the machine learning algorithm. In order to do this, we employed the SentiWordNet (Esuli and Sebastiani, 2006), General Inquirer (Stone et al., 1966) and WordNet Affect (Strapparava and Valitutti, 2004) dictionaries for English and the multilingual dictionaries created by Steinberger et al. (2012a). The main problem of this approach was, however, that very few features were found, for a small number of the sentences to be classified, on the one hand because affect is not expressed in these sentences using lexical clues and, on the other hand, because the dictionaries we had at our disposal for languages other than English were not very large (around 1500 words). For this reason, we will not report these results.


                     Table 1
                      presents the number of unigram and bigram features employed for each of the languages, per translation system (N.B. the features are extracted from the training data).

Subsequently, we performed two sets of experiments:
                        
                           •
                           In the first set of experiments, we trained an SVM SMO classifier on the training data obtained for each language, with each of the three machine translations, separately (i.e. we generated a model for each of the languages considered, for each of the machine translation systems employed), using the four types of aforementioned features. Subsequently, we tested the models thus obtained on the corresponding test set (e.g. training on the Spanish training set obtained using Google Translate and testing on the Spanish test set obtained using Google Translate) and on the Gold Standard for the corresponding language (e.g. training on the Spanish training set obtained using Google Translate and testing on the Spanish Gold Standard). Additionally, in order to study the manner in which the noise in the training data can be removed, we employed two meta-classifiers – Bagging (Breiman, 1996) (with varying sizes of the bag and SVM SMO as classifier) and AdaBoost (Freund and Schapire, 1995), but the best results were obtained using Bagging.

In the second set of experiments, we combined the translated data from all three machine translation systems for the same language and created separate models based on the four types of features we extracted from this data (e.g. we created a Spanish training model using the unigrams and bigrams present in the training sets generated by the translation of the training set to Spanish by Google Translate, Bing Translator and Moses). We subsequently tested the performance of the sentiment classification using the Gold Standard for the respective language, represented using the features of the corresponding model built on the training data.

All the learning algorithms have been run using the default settings proposed in the machine learning library Weka.
                        13
                     
                     
                        13
                        
                           http://www.cs.waikato.ac.nz/ml/weka/.
                      While this choice may affect the overall performance on the data in the given settings, preventing the classifier from reaching the best F-score, it allows to perform a fair comparison across different algorithms, feature representations and languages. For instance, given two settings with two different feature representations and the parameters optimized, e.g. via cross-validation on the training set or feature selection, it would subsequently be difficult to understand if a large difference in performance between the two settings is due to better features or simply better parameters for the classifier. Using the same parameters guarantees that a feature representation is more discriminative than another.

The results of the experiments (in terms of weighted F-score, per language) are presented in Tables 2, 4–6
                     
                     
                     
                     
                     , and for the second set of experiments are presented in Table 7
                     .

@&#RESULTS AND DISCUSSION@&#

Generally speaking, from our experiments using SVM, we could see that incorrect translations imply an increment of the features, sparseness and more difficulties in identifying a hyperplane which separates the positive and negative examples in the training phase. Therefore, a low quality of the translation leads to a drop in performance, as the features extracted are not informative enough to allow for the classifier to learn. We can consider that the results obtained on the test sets are representative for the translation quality of the training sets.

From Tables 2, 4–6, we can see that there is a small difference between performances of the sentiment analysis system using the English and translated data, respectively. In the worst case, there is a maximum drop of 11.8% using SMO, 11.5% using AdaBoost and 8% using Bagging. Ideally, to better measure this drop we would have had to use Gold Standard training data for each language. As mentioned in Section 4, the creation of the Gold Standard is a very difficult and time consuming task. We are considering the manual translation of the training data into French, German and Spanish for the future work. Nonetheless, the scenario considered was aimed at studying the use of MT for SA in the real-life scenario, in which there is little or no annotated data for the language on which SA is done. As expected, the performance of the classification is much higher for data obtained using the same translator than on the Gold Standard. This is true, as the same incorrect translations are repeated in both sets and therefore the learning is not influenced by these mistakes.

In the following part of this section, we discuss the results from three points of view: the feature representation, the learning algorithm and the languages and translation systems.

The noise in the data appears from two sources – namely the incorrect translations or the features that are not appropriate. In our experiments, we want to understand which feature representation is more robust to the noise, gives the best performance and under which conditions. We summarizes the results in Table 3. For the SVM algorithm and for each language, we check how many times the use of a certain representation leads to a higher F-score than another testing the Gold Standard for the translation system. To guarantee a reasonable difference between the two strategies, if the absolute value of the F-score difference between two values is smaller than 0.005, we consider the two representations equal. For each comparison, x
                           >
                           y, the count can range from 0 to 3: 0 means that y is always better than x, while 3 means that x always performs better than y. e.g. for the uni+bigrams > unigram comparison for the German language in Table 4 we check the following values: 0.641<0.655 and |(0.641−0.655)|>0.005, we do not increase the count; 0.646>0.64 and |(0.646−0.64)|>0.005, we increase the count by 1.

In the first row of Table 3, it is evident how the uni+bigrams representation needs better translations to provide some benefits in sentiment classification compared to the unigram representation. In presence of bad translations, the use of bigram has a multiplicative effect of the noise: one untranslated or misplaced word in the target text affects two bigrams (features). Furthermore, if this effect is systematically present in all the training data, the bigram representation will generate a lot of features which are not discriminative and even harmful.

The comparison between the frequency and the presence/absence representations (uni tf-idf >uni and uni+bigrams tf-idf>uni+bigrams) shows that the frequency approaches are less suitable for the noisy data. This can be explained taking into account the nature of the data that we are using. The MOAT data contains questions coming from twenty different topics. From a manual evaluation of the data, we noticed that wrong translations are consistent inside each topic which means in a subset of the questions. This is a type of situation where representing a feature using tf-idf makes a higher difference than representing it in a boolean manner. In such a case, the meaning of the tf-idf score corresponds to the fact that the feature (in our case, n-gram) does not appear in all the documents, and where it appears it is frequent. For this reason, the tf-idf representation gives importance to features which are not present in the Gold Standard. This phenomenon is leveraged by increasing the quality of the translated data.

As mentioned in the survey by Pang and Lee (2008), there is no clear evidence about the benefits of using unigrams or unigrams plus bigrams with the frequency representation.

Summarizing, the most reliable representation in the case of low translation performance data is the unigram boolean one. This type of representation is able to better deal with mistranslated words. However, when the quality of the translation is higher, the frequency-based feature representation outperforms the boolean one.

Bagging, by reducing the variance in the estimated models, produces a positive effect on the performance increasing the F-score, as compared to the learning process and features without Bagging. These improvements are larger using the German data (bigger than one F-score percent when testing on the Gold Standard), because the poor quality of the translations increases the variance in the data. For the same reason, Bagging is quite effective when unigrams and bigrams (frequency and presence/absence approach) are used to represent the data.

AdaBoost confirms its sensibility with noisy data producing in general worse performance than the other two algorithms. Despite it has substantial increments on the English data only for the frequency representation, on the translated data, except for few cases, it does not highlight real benefits. In this work we pair Bagging with SMO and AdaBoost, but we are interested in running experiments using classifiers such as Naive Bayes or neural networks.

Comparing the performance language by language on the Gold Standard, the best results are obtained for the Spanish language, for which in most of the cases the best F-score reaches 0.66% with a maximum of 0.684. For the other two languages there is more variation in the performance and it is difficult to distinguish for which language there are the best results.

For the same language and feature representation, despite the gap between the best and the worst translation systems is quite large in terms of BLEU scores, it is interesting to note that there is no correlation between the BLEU score values and the classification performance at system level. The BLEU score is computed taking into account the co-occurrences of phrases ranging from one to four words between the translated and the Gold Standard data. In presence of changes in the translations from a system to another, they can produce a reasonable variation in the BLEU score modifying the count of the large phrases. Vice versa, the use of the vector space model representation in the classification problem, which is based on the assumption of independence of the terms, makes the impact of translation changes less critical compared to the BLEU score computation.

When translated data is used for training and testing, in most of the cases the sentiment classification performance obtained using the Moses data results in the best performance, while it has the smallest translation performance according to the BLEU score. It is evident that in this case inaccurate translations which appears systematically in the training and test data creates a positive effect in the classification task.

The amount of parallel data used to train Moses clearly may affect the classification performance. Turchi et al. (2012) showed that doubling the MT training size there is a constant increment in translation performance, this means that having more data it is possible to reach better translation performance and indirectly we believe better classification accuracy. In case of less resourced languages (<1/1.5million sentence pairs), the commercial translation engines can be used in support of the SA, but, even in this case, it is not guarantee an acceptable level of translation. We are also interested in better understanding the relation between the BLEU score and the classification performance, and for this purpose we will run experiments using Moses trained on different sizes of the training data.

Looking at the results in Table 7, we can see that adding all the translated training data together makes the features in the representation more sparse and increases the noisy level in the training data, creating harmful effects in terms of classification performance: the classifier can lose its discriminative capability. This is not the case when using tf-idf on unigrams and unigrams plus bigrams, in which case the combination of the data improves the classification, as this type of features deter sparsity in data.

At language level, clearly the results depend on the translation performance. Only for Spanish (for which we have the highest BLEU score), each classifier is able to properly learn from the training data and try to properly assign the test samples. For the other languages, translated data are so noisy that: (a) either the classifier is not able to properly learn the correct information for the positive and the negative classes, and this results in the assignment of most of the test points to one class and zero to the other, or (b) there is significant drop in performance, e.g. for the French language, but the classifier is still able to assign the test points to both the classes. This differs from the results in the previous experiments, where it is not so evident the relation between the translation and classification performance. When all the translated data coming from different SMT systems are merged together, the level of noise in all the training data is larger than when only the translations from one SMT are used. This means that only when the general quality of the translated texts is acceptable, the learning algorithm can properly learn from the data. While in the experiment in the previous Section, the learning algorithm is able to cope with a smaller noisy level coming from the translations of a single SMT system. This is also evident in the results in Table 7 where the differences in performance across languages are larger than the experiments in Section 8.1.

The results confirm the capability of Bagging to reduce the model variance and increase the performance in classification, in particular for those feature representations which include the tf-idf term weight or for the Spanish language. In both the cases, performances are comparable and for some configurations even better than what we obtained using each dataset independently.

In the case of Spanish, the combined F-score is slightly better than the English result. This shows that different systems can translate differently the same input sentence catching more linguistic variations in the target language. For example, for the English sentence: “They were old enough to remember how badly things can go when frenzy is the order of the day and laws are put aside by feelings.” The three different translation systems proposed three quite different translation, both in the quality of the translated words, as well as in the manner in which syntax rules are obeyed and the logic and meaning of the sentence is still maintained. The respective translations of each of the systems is given below. “Sie waren alt genug, um Denken Sie daran, wie schlecht Dinge gehen knnen, wenn Rausch an der Tagesordnung ist, und Gesetze sind durch Gefühle beiseite gefegt.” (translation to German by Bing) “Sie waren alt genug, sich daran zu erinnern, wie schlimm es gehen kann, wenn der Rausch der Reihenfolge des Tages, und Gesetze sind auer von Gefühlen mitgerissen.” (translation to German by Google) “Sie waren alt genug, sich daran zu erinnern, wie dringend es gehen kann, wenn eine Intoxikation ist an der Tagesordnung, und die Gesetze von Gefühlen.” (translation to German by Moses). Overall, we can see that by adding together the features that we can extract from the combination of these three translations, we can obtain a better approximation for the representation of the correct translation than when using each of the translation systems separately. Words that are mistaken by one are correctly translated by the other two and, at the same time, the correct translation is reinforced.

@&#CONCLUSIONS AND FUTURE WORK@&#

In this work we propose an extensive evaluation of the use of translated data in the context of sentiment analysis. Our findings show that SMT systems have reached a reasonable level of maturity to produce sufficiently reliable training data for languages other than English. The gap in classification performance between systems trained on English and translated data is minimal, with a maximum of 12% in favor of source language data.

Working with translated data implies an increment number of features, sparseness and noise in the data points in the classification task. To limit these problems, we test three different classification approaches, using different types of features and classifiers, showing that using unigrams or tf-idf on unigrams as features, and/or Bagging as a meta-classifier, has a positive impact in the results. Furthermore, in case of good translation quality, we noticed that the union of the same training data translated with various systems can help the classifiers to learn different linguistic aspects from the same data.

The proposed approach clearly depends on the availability of the translation engines for the required languages. Although, commercial engines are able to translate from and into a large number of languages, they cannot be used to freely translate large amounts of data (usually not more than a certain number of characters). On the other hand, the parallel corpora needed for training the open source SMT systems cover only the most used languages, and their sizes are not comparable to the dataset used to train commercial engines. These aspects may limit the use of MT in support of other natural language processing sectors, in particular if focused on less resourced languages.

In future work, we plan to investigate different document representations, in particular we believe that the projection of our documents in space where the features belong to a sentiment lexica (in conjunction to the types of features we have already employed) and include syntax information can reduce the impact of the translation errors.

@&#ACKNOWLEDGMENTS@&#

The authors would like to thank Ivano Azzini, from the BriLeMa Artificial Intelligence Studies, for the advice and support on using meta-classifiers. We would also like to thank the reviewers for their useful comments and suggestions, which helped to improve on the clarity and completeness of the article.

In the previous sections, all the results are expressed in terms of F-measure to evaluate the accuracy of our experiments. For completeness and for better evaluating the findings of this work we present the classification performance using the weighted precision (Tables A.1, A.3, A.5, A.7 and A.9
                     
                     
                     
                     
                     
                     
                     
                     
                     ) and recall (Tables A.2, A.4, A.6, A.8 and A.10
                     ). In all the experiments, precision and recall are well balanced showing the capability of the classification algorithm in identifying more relevant sentiment label than irrelevant and in returning most of the relevant labels.

In general, all the outcomes of the papers are validated also analyzing the performance in terms of precision and recall. An exception is given by the results in Table A.9, where the highest score for the translation into French is given by the uni+bigrams representation instead of the uni+bigrams tf-idf as shown in Table 7. Cross-checking the precision and recall performance, it is evident that the uni+bigrams tf-idf has a very high precision, but a very small recall. This confirms that the uni+bigrams tf-idf representation produces the best results and that precision and recall alone are not sufficed for a complete analysis.

@&#REFERENCES@&#

