@&#MAIN-TITLE@&#An improved electromagnetism-like mechanism algorithm and its application to the prediction of diabetes mellitus

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The utilization of ROST technique on EM algorithm with 1NN shows its superiority.


                        
                        
                           
                           Statistical analysis reveals that our method outperforms all compared methods.


                        
                        
                           
                           We have identified four risk factors for this disease.


                        
                        
                           
                           Our research can help the diagnosis and prognosis of Type 2 DM.


                        
                        
                           
                           The findings can help to reduce the morbidity and mortality rate caused by DM.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Electromagnetism-like mechanism algorithm

Nearest-neighbor heuristic

Opposite sign test

Feature selection

Diabetes mellitus

@&#ABSTRACT@&#


               
               
                  Recently, the use of artificial intelligence based data mining techniques for massive medical data classification and diagnosis has gained its popularity, whereas the effectiveness and efficiency by feature selection is worthy to further investigate. In this paper, we presents a novel method for feature selection with the use of opposite sign test (OST) as a local search for the electromagnetism-like mechanism (EM) algorithm, denoted as improved electromagnetism-like mechanism (IEM) algorithm. Nearest neighbor algorithm is served as a classifier for the wrapper method. The proposed IEM algorithm is compared with nine popular feature selection and classification methods. Forty-six datasets from the UCI repository and eight gene expression microarray datasets are collected for comprehensive evaluation. Non-parametric statistical tests are conducted to justify the performance of the methods in terms of classification accuracy and Kappa index. The results confirm that the proposed IEM method is superior to the common state-of-art methods. Furthermore, we apply IEM to predict the occurrence of Type 2 diabetes mellitus (DM) after a gestational DM. Our research helps identify the risk factors for this disease; accordingly accurate diagnosis and prognosis can be achieved to reduce the morbidity and mortality rate caused by DM.
               
            

@&#INTRODUCTION@&#

The overwhelming amount of data makes most of the existing data mining algorithms inapplicable and inefficient to many real world problems, particularly those data with high dimensionality and features [14]. Feature selection depends on the determination of an optimal feature subset among all of the full features, and is constantly studied to improve computational efficiency and effectiveness by optimizing criteria, such as accuracy and total feature subset costs. However, this possesses another problem called exponential complexity [43] due to the exponential number of candidate subsets. Researchers addressed this problem through exhaustive search [31,1]. Yet, exhaustive search is computationally impractical, except if the total number of features is quite small.

The use of metaheuristic algorithms as a solution to feature selection problems has gained significant attention lately. Among them, EM is a new popular optimization algorithm that has been known for its efficiency. EM is a heuristic approach proposed by Birbil and Fang [5] that mimics the attraction–repulsion mechanism of electrically charged particles in the magnetic field. EM is capable of optimizing continuous and discreet problems.

Chen et al. [9] first introduced opposite sign test (OST). It is a new local search technique that have the capability to handle the local optima trap problem. In their study, they demonstrated the abilities of OST to increase population diversity in the Particle Swarm Optimization (PSO) algorithm and to escape from local optimal trapped by refining the jump ability of flying particles. OST is implemented by carrying out simple test on the particular states of particle. The objective is to obtain a better solution for each particle. There are two kinds of OST: forward opposite sign test (FOST) and random opposite sign test (ROST). The FOST algorithm works by changing the particular states of particles in a forward manner, while the ROST algorithm randomly changes the particular states of particles.

In this study we combine EM algorithm with nearest neighbor classifier (1NN) as the wrapper method. OST method is utilized as local search procedure to help EM algorithm explore for its best.

This novel feature selection approach (denoted as IEM) can achieve high classification accuracy and Kappa index. A thorough investigation is conducted to evaluate the performance of the proposed IEM. Initially, we apply IEM on 54 public datasets that consist of 46 datasets from the UCI Repository [2] and 8 public microarray dataset. We conducted this preliminary test to justify our method can be applied to any kind of data size and dimensions. Subsequently, the results are compared with nine other popular feature selection techniques using a non-parametric statistical test.

Diabetes mellitus (DM) is one of the disease that contributes to the high mortality rate in the world and also in Taiwan [13,35]. Gestational diabetes is one type of DM that occurs during or after pregnancy and are recognized as one of the most common pregnancy’s complication. Therefore, this study aims at applying IEM on a real clinical dataset to predict the occurrence of Type 2 DM from a gestational diabetes mellitus (GDM) that occurs during or after pregnancy in Taiwan.

The remainder of the paper is organized as follows: In Section 2, we review the related literature. Section 3 presents the proposed IEM algorithm. The experimental results and discussion are presented in Section 4. Section 5 concludes the paper.

Selecting an optimized subset from original features is the major task in the feature selection problem. In feature selection one needs to remove irrelevant or redundant features from our dataset. This process influences on the performance of the classification algorithm, particularly in improving accuracy and efficiency [37]. For a specific a dataset, feature selection helps analysts in understanding which features are important and how they are related [36]. As the dimensionality of a domain increases, the number of features N also increases. Finding an optimal feature subset is usually is not an easy task [21]. Many problems related to feature selection have been shown to be NP-hard [6]. This phenomenon is also known as the curse of dimensionality or the Hughes phenomenon.

Several researchers have utilized feature selection based methods to predict the occurrence of a disease. Ban et al. [3] used SVM to predict the Type 2 DM in Korean population. Chandra and Gupta [8] used a features weight with Naïve Bayes and SVM classifier to identify the most relevant genes that associate with diseases like leukemia, colon tumor, lung cancer, diffuse large B-cell lymphoma (DLBCL), and prostate cancer. Peng et al. [25] integrated filter and wrapper methods into a sequential search procedure to improve the classification performance of the selected features by using breast cancer and heart disease dataset from UCI repository. Cho et al. [10] proposed a linear SVM combined with wrapper or embedded feature selection methods and applied it to predict the onset of diabetic nephropathy. Huang et al. [18] proposed a feature selection method via supervised model construction for features ranking to predict the occurrence of Type 2 diabetes.

The EM algorithm was first introduced as a stochastic global optimization technique on Birbil’s PhD dissertation in 2002. Since then it has gained popularity due to its simplicity and efficiency in solving many engineering problems.

There are four main phases exist on the EM algorithm: initialization, local search, force calculation, and particle movement. In the initialization phase, m particle points are randomly selected from the feasible region, which is an n dimensional hyper-solid features. Each feature of a point is assumed to be scattered between the corresponding upper bound and lower bound. The next step is local search. Hence the OST technique was used as the local search for EM algorithm. In the third phase, the total force exerted on each point is calculated based on the principle of electromagnetism theory. Each particle will calculate the charged value by using Eq. (1). For instance, F
                        1 is the force exerted by q
                        3 on q
                        1 (Fig. 1
                        ); that is, q
                        1 is repulsed by q
                        3 if the value of the objective function of q
                        1 is better than that of q
                        3. F
                        2 is the force exerted by q
                        2 on q
                        1; that is, q
                        1 is attracted by q
                        2 if the objective function value of q
                        1 is worse than that of q
                        2. Thus we can count the eventual force exerted on q
                        1 as F
                        1
                        =
                        F
                        21
                        +
                        F
                        31. Each point is viewed as a magnetic particle and contains a force of attraction or repulsion with respect to other particles. If the objective function value of one point is better than that of the other points, then it is attracted to those points.

On the other hand, if the objective function value of one point is worse than that of the other particles, it is repulsed by them. The total force exerted on every point is calculated after all of the forces of attraction and repulsion are determined. The total force is calculated using Eq.
                        (2):
                           
                              (1)
                              
                                 
                                    
                                       q
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 exp
                                 
                                    
                                       
                                          -
                                          n
                                          ×
                                          
                                             
                                                f
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                )
                                                -
                                                f
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      best
                                                   
                                                
                                                )
                                             
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   
                                                      m
                                                   
                                                
                                                (
                                                f
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      k
                                                   
                                                
                                                )
                                                -
                                                f
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      best
                                                   
                                                
                                                )
                                                )
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 ∀
                                 i
                              
                           
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          ≠
                                          i
                                       
                                       
                                          m
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               j
                                                            
                                                         
                                                         -
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                      
                                                         |
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               j
                                                            
                                                         
                                                         -
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         
                                                            
                                                               |
                                                            
                                                            
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                   
                                                   if
                                                   
                                                   f
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   )
                                                   <
                                                   f
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   )
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   
                                                      
                                                         q
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         -
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               j
                                                            
                                                         
                                                      
                                                      
                                                         |
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               j
                                                            
                                                         
                                                         -
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                         
                                                            
                                                               |
                                                            
                                                            
                                                               2
                                                            
                                                         
                                                      
                                                   
                                                   
                                                   if
                                                   
                                                   f
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         j
                                                      
                                                   
                                                   )
                                                   ⩾
                                                   f
                                                   (
                                                   
                                                      
                                                         x
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 ∀
                                 i
                              
                           
                        
                     

The last phase implicates the movement according to the direction of the force. The total force of each point is calculated point, then this point moves at random step length in the direction of the force and causes the particles to move into any unvisited area in the search space. These procedure is repeated until a termination criterion is reached. The termination criterion can be set as the maximum number of iterations [5].

Despite the popularity of EM algorithm in solving engineering optimization problems, the utilization of EM algorithm in data mining is still few. Su and Lin [34] are the first to apply EM for feature selection. They use the EM algorithm to solve binary problems. In their proposal, EM is combined with the nearest-neighbor heuristics as the wrapper method. Experimental results on 13 public datasets indicate that their method outperforms other well-known algorithms in classification accuracy and feature selection efficiency for balanced data. Lin and Su [22] propose an EM algorithm combined with robust Bayes classifier, and apply it for feature selection with incomplete data. They conduct experiments on 11 public datasets, and show that the results from their proposed method are better than those from other methods, including GA. In our study, the proposed IEM algorithm is embedded with the OST techniques to help the algorithm explore for the global optima solution and escape from local optima trapped.

The nearest neighbor algorithm (specifically, 1NN) [12] is one of the successful techniques used in classification task in data mining area [32] and has been widely applied to solve various classification problems. It becomes a popular classifier due to the simplicity and high convergence speed. The 1NN using Euclidean distance to calculate the difference between attributes for continuous data [19]. The algorithm classifies the object by simply assigning it to the class of the single nearest neighbor. Despite its benefit, 1NN can cause the effects of curse of dimensionality which is one of the major issues in biomedical data analysis and mining [25]. In order to avoid that effect, 1NN is usually combined with some optimization algorithm, such as genetic algorithm (GA), PSO and other kind of search methods to improve classification performances. In this paper, we propose the EM algorithm to combine with 1NN as the classifier.

The OST technique can help avoid local optima by improving the jump ability of flying particles. This technique has been utilized to exploit the region for a probable global optimization. Some recent literatures indicate the promising results of using OST techniques [38]. The OST technique entails the testing of the current state and the subsequent change of a feature (0→1 or 1→0). For particle 10100, for example, the first bit 1 was changed to 0 in the first state. Subsequently, the second bit 0 was changed to 1 in the second stage. This process continued until all bits are changed sequentially.

The OST procedure is described as follows:
                           
                              
                                 Step 1: Set d: 1.


                                 Step 2: Set 
                                    
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                          
                                             new
                                          
                                       
                                       :
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                       
                                    
                                 .


                                 Step 3: 
                                 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             id
                                          
                                       
                                       :
                                       1
                                    
                                  then 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             id
                                          
                                          
                                             new
                                          
                                       
                                       :
                                       0
                                    
                                 . If 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             id
                                          
                                       
                                       :
                                       0
                                    
                                  then 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             id
                                          
                                          
                                             new
                                          
                                       
                                       :
                                       1
                                    
                                 .


                                 Step 4: If the fitness value of 
                                    
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                       
                                       <
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                          
                                             new
                                          
                                       
                                    
                                  then 
                                    
                                       
                                          
                                             x
                                          
                                          
                                             id
                                          
                                       
                                       :
                                       
                                          
                                             x
                                          
                                          
                                             id
                                          
                                          
                                             new
                                          
                                       
                                    
                                  and 
                                    
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                       
                                       :
                                       fitness
                                       
                                       value
                                       
                                       of
                                       
                                       
                                          
                                             X
                                          
                                          
                                             i
                                          
                                          
                                             new
                                          
                                       
                                    
                                 .


                                 Step 5: d: d
                                 +1.


                                 Step 6: If d
                                 <
                                 D, proceed to step 2; otherwise, stop.

There are two OST techniques namely, FOST and ROST. The FOST algorithm is a simple test that aims to acquire better optimization for each particle. The algorithm sequentially changes the particular states of particle in a forward manner. In particle 10,100, for example, the first bit changed from 1 to 0 in the first step. If the fitness value of the old particle (Xi
                        ) is less than that of the new particle (
                           
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                                 
                                    new
                                 
                              
                           
                        ), we accept the alteration of the new particle. Otherwise, we retain the alteration of the old particle. The second bit was then changed from 0 to 1 in the second step, and its fitness value is tested. This process continues until all bits were tested sequentially. Meanwhile, the ROST algorithm randomly changes particular particle states. Fig. 2
                         shows the illustration of ROST technique. In particle 10,100, as an example, we first generate a random key such as ‘35,421’. Following the random key order, the third bit changes first from 1 to 0. If the fitness value of the old particle (Xi
                        ) is less than that of the new particle (
                           
                              
                                 
                                    X
                                 
                                 
                                    i
                                 
                                 
                                    new
                                 
                              
                           
                        ), we accept the alteration of the new particle. If the results illustrate otherwise, we accept the alteration of the old particle. Subsequently, the fifth bit changed from 0 to 1. We test for the fitness value of the fifth bit. We continues this process until all bits are changed.

By performing the OST technique, our proposed algorithm is able to find better particle before the particle movement. The aim of using it is to promote the correct direction. The OST technique has increased the population diversity in metaheuristics such as the PSO [9]. It is helpful for the EM algorithm to avoid local optima traps as well.

Chen et al. [9] utilized both FOST and ROST in their proposed approach and exposed that ROST outperforms FOST. Therefore, this study will utilize the ROST technique to embed in our proposed method.

To justify the performance of our proposed IEM method, we compared it with different types of feature selection and classification methods. There are nine popular methods that are used as comparisons, i.e. improved genetic algorithm (IGA), standard version of EM with 1NN classifier, support vector machine (SVM), back propagation neural network (BPNN), logistic regression (LR), C4.5, radial basis function (RBF), self-organizing map (SOM), and learning vector quantitation (LVQ).

The IGA is an improved version of genetic algorithm that utilized ROST techniques. The reason to make such comparison is to see the performance difference of GA and EM algorithms when the ROST is adopted. The comparison to a standard EM algorithm is to investigate the effect of ROST on a naïve EM and the improved version. The SVM algorithm is a powerful machine learning tool using kernel. The SVM maps nonlinear inputs to a high-dimension feature space where a linear classification surface is constructed [11]. The BPNN is a common method of training artificial neural networks (ANNs). It was first introduced by Paul Werbos in the 1970s. The BPNN trains multi-layered neural networks such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output [28]. The LR is a type of probabilistic statistical classification model proposed in the 1970s as an alternative technique to overcome the limitations of ordinary least squares regression. It has been used extensively in numerous disciplines, including the medical and social science fields [30,16,41]. The C4.5 is a decision tree based classifier developed by Quinlan [27] and an extension of Quinlan’s earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, therefore it is often referred to as a statistical classifier. The RBF appears as a variant of neural network in the late 1980s. The RBF is embedded in a two-layer neural network, where each hidden unit implements a radial activated function. The input into an RBF network is a nonlinear while the output is linear. Its capabilities has been studied [24,26]. The self-organizing feature map (SOFM) or known as self-organizing map (SOM) is a type of ANNs that is trained using unsupervised learning to produce a low-dimensional, discretized representation of the input space of the training samples, called a map. The SOM is different from other ANNs since it uses a neighborhood function to preserve the topological properties of the input space. The model was first described as an ANN by the Teuvo Kohonen, and is sometimes called a Kohonen map or network [20]. SOM may be considered a nonlinear generalization of principal components analysis (PCA). It has been shown, using both artificial and real geophysical data, that SOM has many advantages over the conventional feature extraction methods [29,42]. LVQ is a prototype-based supervised classification algorithm that considered as a special case of an ANN. LVQ applies a winner-take-all Hebbian learning-based approach. It is a precursor to self-organizing maps (SOM). The LVQ was developed by Teuvo Kohonen. It can be applied to multi-class classification problems and has been widely used in a various practical applications.

We use binary digits for feature representation in applying IEM to the feature selection problem. The bits consist of 0 and 1 digit, corresponding to non-selected and selected features respectively. Each particle is coded as a binary alphabetical string. For instance, particle 10,100 contains five features where only the first and the third features are selected.

Accuracy is the most commonly used parameter in classifier performance assessment [40]. In the present study, the classification accuracy is determined through 1NN classification rate. Accuracy is used to evaluate classifier performance by defining the total number of good classifications over the total number of available examples. The classified test points can be divided into the following four categories, which are usually represented in the well-known confusion matrix [33]: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). The column of the matrix denotes the instances in a predicted class, while the row denotes the instances in an actual class. Given the four categories of the confusion matrix, accuracy is defined as the fitness function in 1NN. We calculate the fitness function using Eq. (3):
                           
                              (3)
                              
                                 Accuracy
                                 =
                                 
                                    
                                       TP
                                       +
                                       TN
                                    
                                    
                                       TP
                                       +
                                       TN
                                       +
                                       FP
                                       +
                                       FN
                                    
                                 
                              
                           
                        
                     


                        Fig. 3
                         shows the pseudo-code of the proposed IEM algorithm in solving feature selection problem and Fig. 4
                         shows the flowchart of IEM.

@&#EXPERIMENTS AND RESULTS@&#

To evaluate the effectiveness of the proposed IEM algorithm, we apply it on 46 datasets from the UCI Repository [2] and 8 public microarray datasets [7]. Table 1
                               summarizes the dataset characteristics. The datasets have various data sizes ranging from hundreds to thousands. All of the datasets have no missing values, and all of the features are numeric.

Diabetes mellitus (DM) is a type of metabolic diseases where a person has high level of blood sugar. In 2012, World Health Organization (WHO) reveal that there are 347million people in the world suffering from DM and the mortality rate will be double from 2005 to 2030 [13]. In Taiwan, DM has known as one of the five leading cause of death and second leading cause for middle age woman [35]. There are three major types of diabetes mellitus, i.e. Type 1, Type 2 and gestational DM (GDM). As the third form of DM, gestational diabetes occurs on woman during pregnancy and are recognized as one of the most common pregnancy’s complication. GDM is a condition where a pregnant woman without a previous diagnosis of diabetes develops a high blood glucose level. A woman with GDM still has a high chance of developing DM after childbirth even though the glucose metabolism and the insulin resistance will return to normal in most cases [23]. Due to the high morbidity and mortality and rate, many researchers focus on the diagnosis and prediction of DM by using data mining techniques.

The data used in this study was collected from a general hospital located in Taipei, Taiwan. The hospital routinely performs 50-g oral glucose challenge test during the 24th to 28th week of gestation, followed by a 3-h 100-g oral glucose tolerance test if the 1-h 50-g glucose value was ⩾140mg/dL. According to the National Diabetes Data Group criterion, more than one of the following venous plasma glucose concentrations must be met for gestational DM diagnosis: (1) fasting value ⩾105mg/dL, (2) 1-h⩾190mg/dL, (3) 2-h⩾165mg/dL, and (4) 3-h⩾145mg/dL. The data were collected from 1998 to 2002 and registered into the gestational DM registry. Table 2
                               shows the factors that were collected on the GDM women, and were used as the predictors of developing Type 2 DM after a gestational DM pregnancy. To investigate the Type 2 DM status after gestational DM, all pregnant women who showed gestational DM complications were advised to return for metabolic testing. A 2-h 75-g oral glucose tolerance test was conducted to examine Type 2 DM from 3 to 6months postpartum. Among the 558 gestational DM registries, there were 152 subjects who were available and who successfully completed the follow-up examination for Type 2 DM. The Type 2 DM examining results are grouped into three categories according to the criterion set by Department of Health, Taiwan: first, normal (fasting value<110mg/dL and 2-h 75-g oral glucose tolerance test <140mg/dL), second, pre-DM state (126mg/dL>fasting value110mg/dL or 200mg/dL>2-h 75-g oral glucose tolerance test>140mg/dL), and third, DM (fasting value126mg/dL or 2-h 75-g oral glucose tolerance test200mg/dL). In this dataset, there are a total of 152 sample data that are consisted of 10 DM, 32 pre-DM, and 110 normal.

We use ten-fold cross-validations for all of the datasets to ensure fair distribution among the data and to prevent bias results. The IEM parameter setting for these experiments are as follows: number of particles, 50; MAXITER, 300; and LISTER, 100. Nearest neighbor parameter, k
                           =1. Our proposed IEM method is coded in C++ and run on a Core 2 Duo P8800 (2.2GHz) PC equipped with 4GB of RAM with Windows 7 operating system.

To evaluate the effectiveness of the proposed IEM, we compare our result with those of nine popular feature selection methods, including several metaheuristic techniques. These methods are EM-1NN, IGA, SVM, BPNN, LR, C4.5, RBF, SOM, and LVQ. We obtain results for SVM, BPNN, LR, C4.5, RBF, SOM, and LVQ with the use of the WEKA software [17,39], while the EM-1NN and IGA are coded in C++ programming language and running on the same computing environment as IEM.

Since the data did not follow perfect normality distribution, we perform a non-parametric statistical test to examine the effectiveness of all of the methods. Kruskal–Wallis test is used to detect the mean difference among the methods. The mean difference is then detected through a p value 0.001<
                           α: 0.05. We then use the Wilcoxon test to conduct pair-wise comparisons among all of the considered methods. In these experiments, the α-level is set to 0.05.

@&#RESULTS@&#

In this study there are two criterions for method performance evaluation, i.e. classification accuracy and the Cohen Kappa. Accuracy had been known as the most commonly used metric in classifier performance assessment [40]. The Cohen Kappa is known in classifications because of its simplicity and usefulness for multi-class problems and its ability to compensate random hits, which are similar to the AUC measure [4].

The kappa coefficient is denoted as K, where pr
                        
                        (
                        
                           a
                        
                        ) is the relative observed agreement among raters, and pr
                        
                        (
                        
                           e
                        
                        ) is the hypothetical probability of chance agreement.
                           
                              (4)
                              
                                 Kappa
                                 
                                 coefficient
                                 
                                 (
                                 κ
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             p
                                          
                                          
                                             r
                                             (
                                             a
                                             )
                                          
                                       
                                       -
                                       
                                          
                                             p
                                          
                                          
                                             r
                                             (
                                             e
                                             )
                                          
                                       
                                    
                                    
                                       1
                                       -
                                       
                                          
                                             p
                                          
                                          
                                             r
                                             (
                                             e
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The Wilcoxon statistical test [15] is used to identify which method is significantly different. This test is conducted in lieu of the 54 pair-wise combinations in this case. Table 3
                         shows the classification accuracy of IEM, EM-1NN, IGA, SVM, BPNN, LR, C4.5, RBF, SOM, and LVQ on the 54 datasets. IEM obtained the highest average accuracy 90.73 as well as the Kappa coefficient, 0.7903. The p-value of Wilcoxon pair test in Table 4
                         reveals that IEM performances are superior over the other methods on the 54 datasets, denoted by the p-value=0.000. Based on these superior results, we apply IEM to predict the occurrence of Type 2 DM after a GDM pregnancy. Table 5
                         shows that IEM obtains the highest accuracy and for the GDM dataset. Table 6
                         reveals that F
                        4, F
                        2, F
                        3, F
                        9, are the most frequent features that are essentially contribute to the occurrence of Type to DM from a GDM during pregnancy. These features F
                        4, F
                        2, F
                        3, F
                        9 are 1-h 100-g oral glucose tolerance test value, 50-g oral glucose challenge test value, fasting glucose value, and weight gain during pregnancy respectively. These results can lead us to make a better diagnosis and treatment plans for the patients.

@&#DISCUSSION@&#

In this study, 54 UCI datasets are used to evaluate the performance of various classification algorithms. These datasets are characterized according to data sizes, features, and classes. Several points ware drawn based on the results and analyses. (1) The IEM obtains the highest average classification accuracy and Kappa index for the 54 public datasets and the GDM dataset, this is due to the combination of EM algorithm with the OST technique that helps to explore for a global optimum solution. (2) The performance of IEM compared to IGA is not significantly different due to the use of ROST technique that embedded inside the algorithms. However, in terms of average accuracy and Kappa index, IEM obtains higher results than IGA. (3) The IEM shows better results compared to the standard version of EM algorithm. This result again confirmed the superior effect of ROST technique. (4) Among all methods we can rank the first three best methods as IEM, IGA and EM algorithm. (5) IEM shows a consistent performance for both 54 datasets and GDM dataset on both performance measurements. These results confirm that IEM is a reliable method. (6) The top four important risk factors for a Type 2 DM from a GDM are found in the study, including the value of 1-h 100-g oral glucose tolerance test, the value of 50-g oral glucose challenge test, the value of fasting glucose, and the weight gained during pregnancy.

The first three factors are related to the screening test/diagnosis for GDM. In order to decrease the risk of GDM, a woman should have these following result for the GDM screening: (i) 1-h 100-g oral glucose tolerance test value should be below 190mg/dL, (ii) 50-g oral glucose challenge test value should be below 140mg/dL, (iii) fasting glucose value should be below 105mg/dL. If a woman are being overweight with a body mass index (BMI) of 30 or higher, then she is likely to developed GDM during or after her pregnancy. The first 3 factors can help doctors design a more friendly examination glucose test for the pregnant woman and prevent unnecessary screening test. These factors allow us to reduce the number of glucose tests to the patients and comfort the pregnant patients. Another interesting finding is the weight gained during pregnancy is one of the risk factors for this disease. The doctors and the nutrients advisor can design a meal plan for the pregnant woman to prevent them from a Type 2 DM. (7) IEM only require few parameters to adjust, therefore make it easy for the implementation compares to other features selection methods. Moreover the OST techniques makes IEM more effective for searching the best solution.

@&#CONCLUSION@&#

In this study we proposed a novel method for feature selection by combining EM algorithm with the nearest neighbor classifier and OST as the local search. We applied the proposed IEM on 46 datasets collected from the UCI repository and 8 public microarray dataset to verify its performance on datasets with various characteristics, sizes, and features. Furthermore, we compared the IEM performance with those of other nine common feature selection methods. We performed a non-parametric statistical comparison using the Kruskal–Wallis and Wilcoxon tests. The results show that the average accuracy and Kappa index of the proposed IEM is outperformed than those of the other methods. We also confirmed that the IEM mean accuracy and Kappa index are significantly different from those of the other methods, shows by the p-value=0.000. These results confirmed that IEM can be applied as an effective diagnosis tool for predict the Type 2 DM after a GDM pregnancy in Taiwan woman. This method can function as a support tool that can help doctors to diagnose the Type 2 DM for during or after pregnancy. Therefore, the effective treatment or prevention plan can be implemented early. Moreover, the important factors that contribute to the disease have been identified. These results will be useful to design a more friendly prevention or treatment plans for the patients.

This paper highlights five contributions. First, we propose a novel and effective feature selection method by combining ROST technique on the EM algorithm with 1NN classifier. Second, we perform a thorough investigation of the IEM performance on data level. The IEM is applied to datasets with different characteristics, sizes, and features, the results shows that our method are generally applicable to various kind of data and effective for diseases’ prediction. Third, we conduct exhaustive comparisons with various common feature selection methods. Fourth, we perform a non-parametric statistical test using the Kruskal–Wallis and Wilcoxon tests to evaluate the IEM performance. Fifth, the utilization of ROST technique on the EM algorithm with 1NN classifier shows remarkable performance compared with the standard EM-1NN. Sixth, we have successfully applied IEM to predict the Type 2 DM from a gestational DM. Seventh, we have identified the risk factors for this disease, so that the prevention or treatment plan can be carried out early and effectively, not just for prolonging the patients’ survival but also for improving patients’ quality of life.

Future study on the parameter setting with a comprehensive experimental design is recommended to conduct for the proposed IEM algorithm so as to optimize feature selection and parameters setting simultaneously. To investigate more advanced k-NN and other promising classifiers are important issue that requires further research. A trade-off between information burden due to the use of the number of features and the classification quality can be further investigated. Another issue worthy to investigate is the capability of our proposed method to deal with the imbalanced data problem.

@&#ACKNOWLEDGMENTS@&#

This work is partially supported by the National Science Council Taiwan, ROC.

@&#REFERENCES@&#

