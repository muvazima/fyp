@&#MAIN-TITLE@&#Protein–protein interaction identification using a hybrid model

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           A PPI identification approach using relational similarity framework is proposed.


                        
                        
                           
                           We construct word similarity matrices that are sensitive to the PPI identification task.


                        
                        
                           
                           We develop a hybrid model to integrate word similarity with the basic RS model.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Relational similarity model

Word similarity model

Biomedical text mining

Protein–protein interaction

@&#ABSTRACT@&#


               
               
                  Background
                  Most existing systems that identify protein–protein interaction (PPI) in literature make decisions solely on evidence within a single sentence and ignore the rich context of PPI descriptions in large corpora. Moreover, they often suffer from the heavy burden of manual annotation.
               
               
                  Methods
                  To address these problems, a new relational-similarity (RS)-based approach exploiting context in large-scale text is proposed. A basic RS model is first established to make initial predictions. Then word similarity matrices that are sensitive to the PPI identification task are constructed using a corpus-based approach. Finally, a hybrid model is developed to integrate the word similarity model with the basic RS model.
               
               
                  Results
                  The experimental results show that the basic RS model achieves F-scores much higher than a baseline of random guessing on interactions (from 50.6% to 75.0%) and non-interactions (from 49.4% to 74.2%). The hybrid model further improves F-score by about 2% on interactions and 3% on non-interactions.
               
               
                  Conclusion
                  The experimental evaluations conducted with PPIs in well-known databases showed the effectiveness of our approach that explores context information in PPI identification. This investigation confirmed that within the framework of relational similarity, the word similarity model relieves the data sparseness problem in similarity calculation.
               
            

@&#INTRODUCTION@&#

Information on protein–protein interactions (PPIs) is crucial for understanding the functional role of individual proteins as well as the entire biological process. Although numerous PPIs have been manually curated into database such as BioGRID [1], BIND [2],DIP [3], HPRD [4], IntAct [5] and MINT [6] by experts, information about many PPIs is still only available through the PubMed database. However, the amount of biomedical literature in PubMed grows rapidly and it is not practical to get complete coverage by manual curation. Therefore, mining PPIs from literature has become increasingly important and has attracted a lot of research interests. The well-known BioCreAtIvE (Critical Assessment of Information Extraction Systems in Biology) challenge includes a PPI detection task in two evaluations [7,8]. The primary goal of the task is to determine whether two target proteins interact.

Approaches for mining PPIs from biomedical text range from co-occurrence analysis to more sophisticated natural language processing systems. Co-occurrence analysis is the most straightforward approach and generally results in high recall but low precision [9,10]. Some other approaches construct patterns specifying how an interaction is described in literature and use them as rules to find PPIs [11–16]. Rule or pattern-based approaches can increase precision but significantly lower recall. In addition, these rule sets are derived from training data and are therefore not always applicable to other data they are not developed for [17,18]. In recent years, more and more approaches explore natural language processing technologies with a favor on machine learning (ML) methods. Some approaches focus on identifying features that are helpful in PPI identification, including lexical features, syntactic features, and semantic features [19–24]. Some approaches investigate various strategies of measuring the distance of two data points and explore it in kernel functions [25–31]. These ML approaches do not require manual construction of rules or patterns and often achieve better accuracy. However, they are experiencing some difficulties.

Given two target proteins, these ML approaches determine whether they interact based on evidence within a rather small text span, typically a sentence in which the proteins co-occur. Similar to other information extraction tasks, for PPI identification, the task is defined as determining whether there is an interaction relation between any two proteins mention in a sentence, as in the following example.
                        
                           The screen identified interactions involving c-Cbl and two 14-3-3 isoforms, cytokeratin 18, human unconventional myosin IC, and a recently identified SH3 domain containing protein, SH3 P17.
                        
                      In this sentence, three proteins are mentioned (marked as bold). The task is to determine whether there is an interaction between any two of them, i.e., which ones of the three pairs (c-Cbl, cytokeratin 18), (c-Cbl, SH3 P17), (cytokeratin 18, SH3 P17) are interactions. The decision is made solely on evidence within this sentence.

These single-sentence-based approaches have some disadvantages. Firstly, complex syntactic structures of sentences often make the predictions very difficult. PPIs are complex biological processes and it is often the case that multiple proteins playing various roles are mentioned in the same sentence. Actually, in the AImed dataset [11] of PubMed abstracts annotated by human experts with protein interactions, over 40% of the sentences have more than three protein mentions. In order to depict these roles, complex syntactic structures are often used in a sentence. As a result, the connections of two protein mentions are often implicit, which makes it difficult to determine their relationship. As in the above sentence, there is a long distance (in terms of words) between c-Cbl and SH3 P17 and it would be difficult to derive a direct relation between them even through a deep syntactic parsing of the sentence. Secondly, context of interactions is ignored in these approaches. Actually, information in nearby sentences often provide the context of the interactions, thus could be very helpful in identifying the target interactions. However, this context is ignored in single-sentence-based approaches. In addition, an interaction may be reported and described by different pieces of research work hence appears in various papers. All these descriptions provide valuable evidence in recognizing the target PPI. Yet this information is not fully explored in the single-sentence-based approaches. Thirdly, these ML approaches suffer from small training datasets. In a single-sentence-based approach, in order to build the training data, every protein pair appearing in a sentence has to be manually annotated as positive (interactions) or negative (non-interactions). This is very intensive labeling work. As a result, these machine learning algorithms are usually trained on small datasets. This will inevitably affect the accuracy and portability of the models.

To address these issues, we propose a novel approach exploring corpus-based strategy to identify PPIs. Although there have been attempts to explore corpus-wide properties, they mostly explore frequency of interesting patterns [32,33]. Different from them, in the present work, relations between proteins are analyzed within the framework of relational similarity (RS) in natural language processing. In addition, a word similarity model that is derived from a large corpus is introduced to further improve the accuracy of the similarity calculation. Our method takes known PPIs in existing PPI databases (e.g., HPRD) as training data and no extra annotation is required. The experimental results show that this approach achieves high accuracy and well-balanced precision and recall.

The rest of this paper is organized as follows: Section 2 introduces the relational similarity framework. The process of PPI identification using the basic RS model and the results are discussed in Section 3. In Section 4, we introduce the word similarity model to further improve the accuracy of PPI identification and analyze the results of the hybrid model in detail. Section 5 concludes all our work.

Research on relational similarity (RS) in the field of natural language processing provides a unified framework for accurately recognizing relations in text. Medin, Goldstone, and Gentner [34] describe relations as follows: relations are predicates taking two or more arguments (e.g., X collides Y, X is larger than Y), which are used to express abstract connection between objects. Most work on RS analysis tries to identify relations implied by word pairs, through comparing the similarity of the target relation with some known relations [35–38]. Usually, distributional properties of relations are first extracted from large-scale text. These properties characterize the connections between the two involved words. Then, some similarity measures are applied to calculate the similarity between the target relation and the known relations. The most similar one would be used to label the relation between the two target words.

Our decision to perform PPI recognition within the RS framework is based on two evaluations. First of all, interactions between proteins are typical semantic relations that match Medin's definition. More important, as discussed in the previous section, context information in a large corpus is crucial in determining whether two proteins interact. Within the RS framework, relations are indeed characterized by properties presented in large-scale text. This matches well with our intension to incorporate context in PPI recognition. Therefore, in the presented work, we analyze PPIs from the viewpoint of relational similarity. In the proposed method, the prediction is made upon the rich context information in a large corpus.

The RS framework contains three modules: collecting relation descriptions, relation representation, and similarity calculation. The first module is to get the collection of text that is likely to describe the relation between the two arguments from a large corpus. These descriptions can be phrases, sentences or paragraphs, etc. For example, Turney [35] selected 128 groups of phrases (e.g., X of Y, Y for X, X to Y) that contain the arguments (X, Y), while Nakov [36] used the set of sentences containing the two arguments. In the module of relation representation, vector space models are often used. Dimensions of the vectors correspond to properties characterizing the target relation. In the third module, appropriate similarity measures need to be designed and applied to calculate the distance between the target relation and the known relations. Finally, the target relation is labeled with the most similar known relation.

In the presented PPI recognition system, if two proteins interact, they form a positive pair. Otherwise, it is a negative pair. In order to determine whether two proteins interact, we calculate the similarity between the target pair and the known positive pairs, and the similarity between the target pair and the known negative pairs, respectively. The target pair gets a positive label if it is more similar to the positive pairs and a negative label otherwise.


                        Fig. 1
                         shows the architecture of the PPI identification system. The basic RS model is presented in the solid-line frame. As in the RS framework, our system of PPI recognition contains three modules, marked by the dashed-line frames in Fig. 1. They are described in the following subsections. The word similarity model is in the wavy-line frame and will be discussed in Section 4.

The whole PubMed is used as the corpus from which descriptions of protein pairs are extracted. For a protein pair (p1, p2), we extract from PubMed all the sentences in which p1 and p2 co-occur, as they are likely to describe the relationship between p1 and p2. This set of sentences is regarded as the signature of (p1, p2). The signature is obtained by two steps.
                           
                              (1)
                              Collect abstracts in which p1 and p2 co-occur by querying PubMed using the two proteins. This is done by using the E-utilities (esearch and efetch) through the API of PubMed [39]. We use the names of the target proteins as the keywords without trying to expand the query by including synonyms of them.

Search for sentences that contain both p1 and p2 throughout these abstracts. Each abstract is processed using a sentence segmentation tool [40]. Then sentences that contain the target protein pair are retrieved.

We use vector space model to represent the relation between p1 and p2. Dimensions of a vector are features that characterize the relation and the features are extracted from the signature of a pair. Since words are in fact crucial in expressing the relation, we use unigrams (single word) in the signature files (stop words, single-character words and numbers are removed) as features. The weight of a feature is assigned using two measurements: binary values (0/1) and term frequency-inverse document frequency (tf.idf) values. Using binary values, the weight of a feature word is 1 if it appears in the signature file and 0 otherwise. In the second measurement, a feature word is weighted according to the importance of the word in the signature file, which is captured by the tf.idf value as shown in Eq. (1).
                           
                              (1)
                              
                                 
                                    w
                                    i
                                 
                                 =
                                 
                                    tf
                                    i
                                 
                                 ×
                                 log
                                 
                                    
                                       
                                          
                                             N
                                             
                                                
                                                   df
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where tf
                        
                           i
                         is the frequency of the ith feature in a signature file (the term frequency), df
                        
                           i
                         is the number of signature files in which the ith feature word appears (the document frequency), N is the total number of signatures.

In this module, the vector of the target protein pair which represents the relation between the two involved proteins is compared to the vectors of known protein pairs. The most similar vector would be found and the label of that relation (positive or negative) would be assigned to the target pair. We use cosine similarity metrics to measure the distance of two relation vectors 
                           
                              
                                 
                                    r
                                    1
                                 
                              
                              →
                           
                         and 
                           
                              
                                 
                                    r
                                    2
                                 
                              
                              →
                           
                        . Here 
                           
                              
                                 
                                    r
                                    1
                                 
                              
                              →
                           
                         represents the relation between two target proteins, and 
                           
                              
                                 
                                    r
                                    2
                                 
                              
                              →
                           
                         represents the relation denoted by a known protein pair, as shown in Eq. (2).
                           
                              
                                 
                                    
                                       
                                          r
                                          1
                                       
                                    
                                    →
                                 
                                 =
                                 〈
                                 
                                    r
                                    
                                       1
                                       ,
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    r
                                    
                                       1
                                       ,
                                       n
                                    
                                 
                                 〉
                                 ,
                                 
                                 
                                    
                                       
                                          r
                                          2
                                       
                                    
                                    →
                                 
                                 =
                                 〈
                                 
                                    r
                                    
                                       2
                                       ,
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    r
                                    
                                       2
                                       ,
                                       n
                                    
                                 
                                 〉
                              
                           
                        
                        
                           
                              (2)
                              
                                 cos
                                 
                                 θ
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          n
                                       
                                       
                                          
                                             r
                                             
                                                1
                                                ,
                                                i
                                             
                                          
                                       
                                       ·
                                       
                                          r
                                          
                                             2
                                             ,
                                             i
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                             
                                             
                                                
                                                   
                                                      (
                                                      
                                                         r
                                                         
                                                            1
                                                            ,
                                                            i
                                                         
                                                      
                                                      )
                                                   
                                                   2
                                                
                                             
                                             ·
                                             
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                             
                                             
                                                
                                                   
                                                      (
                                                      
                                                         r
                                                         
                                                            2
                                                            ,
                                                            i
                                                         
                                                      
                                                      )
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                r
                                                1
                                             
                                          
                                          →
                                       
                                       ·
                                       
                                          
                                             
                                                r
                                                2
                                             
                                          
                                          →
                                       
                                    
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      r
                                                      1
                                                   
                                                
                                                →
                                             
                                             ·
                                             
                                                
                                                   
                                                      r
                                                      1
                                                   
                                                
                                                →
                                             
                                          
                                       
                                       ·
                                       
                                          
                                             
                                                
                                                   
                                                      r
                                                      2
                                                   
                                                
                                                →
                                             
                                             ·
                                             
                                                
                                                   
                                                      r
                                                      2
                                                   
                                                
                                                →
                                             
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                r
                                                1
                                             
                                          
                                          →
                                       
                                       ·
                                       
                                          
                                             
                                                r
                                                2
                                             
                                          
                                          →
                                       
                                    
                                    
                                       ∥
                                       
                                          
                                             
                                                r
                                                1
                                             
                                          
                                          →
                                       
                                       ∥
                                       ·
                                       ∥
                                       
                                          
                                             
                                                r
                                                2
                                             
                                          
                                          →
                                       
                                       ∥
                                    
                                 
                              
                           
                        
                     

In the proposed approach, we do not need to annotate the relation of two proteins suggested by a specific sentence. Instead, the training data are acquired from PPI databases such as HPRD. In our experiment, to get the positive pairs, we first extracted all the protein pairs from HPRD. Then, these pairs are searched in PubMed and those that appear in more than one abstract are kept, which leads to a positive set of 1420 pairs. To build a negative dataset, we took an approach commonly used in Bioinformatics research. Firstly, we randomly paired proteins appearing in HPRD to form an initial set. Then, the pairs in the initial set that happen to be PPIs in HPRD were removed. Finally, every pair left was searched throughout PubMed. Those that the two involved proteins co-occur in more than one sentence were added to the negative set. The result was a set of 1353 negative pairs. Therefore, there are 2773 pairs in total in the dataset.

In the experiments, following Turney [35] and Nakov [36], we performed leave-one-out cross validation to test the basic RS model. One protein pair in the dataset was taken as the test pair and the others were used as training data. This process was repeated 2773 times to allow each protein pair being tested once. Using the cosine similarity measure, we built a 1-nearest neighbor (1NN) classifier, which tags the test pair using the label (positive or negative) of the protein pair in the training set that is most similar to it. When there were more than one protein pair having the maximum similarity value, we counted the number of the positive pairs (C
                        
                           pos
                        ) and the negative pairs (C
                        
                           neg
                        ) respectively and checked the ratio r
                        =
                        C
                        
                           pos
                        /C
                        
                           neg
                        . If r
                        >1 the test pair got a positive label. If r
                        <1 it got a negative label. Otherwise, it got the label of the second nearest pair.


                        Table 3 shows the results of the basic relational similarity model in detecting positive and negative protein pairs. In the following experiments, random labeling is taken as a baseline. Performance of the two feature weighting schemes is compared in the table. Since we make predictions based on signatures of protein pairs instead of single sentences, it is difficult to make a fair comparison with existing single-sentence-based approaches. Nonetheless, we report the results of a single-sentence-based classifier on our dataset.

In order to apply a single-sentence-based classifier on our dataset, we performed the two steps. Firstly, we train a single-sentence-based classifier that determines whether two proteins in a sentence interact solely based on this sentence, as it is in related work. Then, for a target protein pair in our dataset, we apply this classifier to every sentence in its signature. If the target pair is classified to be positive in at least one sentence, we label the pair as positive. Otherwise, we label the pair as negative. The experiment is described as follows.

In the relational-similarity-based approach presented in the paper, protein pairs are represented by unigram features in the signatures. Similarly, we use the neighbor words of the target protein pair in a sentence as features to build the single-sentence-based classifier. These features have been shown effective in this task. The features we use are detailed in Table 1
                        .

We built the single-sentence-based classifier on AImed corpus [11] using support vector machine (SVM). In this corpus, interactions in a sentence are manually labeled. AImed has been used to evaluate many PPI identification approaches. The five-fold cross-validation results on AImed using libsvm [41] with default parameters are shown in Table 2
                        .

Then we used the whole AImed corpus as training data to train the single-sentence-based classifier. The classifier was then applied to our dataset, as discussed before. The results are presented in Table 3
                        .

As shown in the table, using binary weights, the basic RS model achieves much higher F-score (about 25% improvements) than the random baseline in detecting both interactions and non-interactions. Furthermore, precision and recall are well-balanced. Compared to the single-sentence-based approach, the basic RS model gets much higher recall (+14.8%) in detecting positive pairs while precision is almost the same, i.e., the basic RS model is able to pick up more PPIs. On negative pairs, the basic RS model also achieves higher F-score.

The results clearly show that descriptions about PPIs extracted from the large corpus actually present some common patterns and these patterns usually are not observed in the negative pairs. This commonality can be effectively captured by the proposed similarity-based model. Therefore, identifying PPIs by measuring similarity of descriptions extracted from large corpus is a direction worth further investigation. The tf.idf weighting scheme tends to label more pairs as interactions, thus achieves better recall in the positive class. In contrast, using binary weights gets a less biased model in detecting interactions and non-interactions. It gets better F-score in both the positive and the negative classes. Binary weights are used in all the following experiments.

Although every protein pair in the dataset has at least one nearest neighbor, the actual similarity value could be low. Table 4
                         shows the number of correctly classified positive and negative pairs whose similarity with the nearest neighbor is greater than a threshold α.

As shown in the table, at every similarity level (indicated by α), the percentage of positive pairs is larger than that of the negative pairs, i.e., more positive pairs can find a neighbor that is actually similar to it. This result again reveals that descriptions about interactions are more similar while descriptions about non-interactions are more diverse.

We also evaluate the KNN classifier with different K. The results are shown in Fig. 2
                         (on the positive pairs) and Fig. 3
                         (on the negative pairs). For the positive pairs, as K increases precision first goes up then slowly decreases. The highest precision is achieved when K is 5. In contrast, recall first decreases and then slowly increases. For the negative pairs, recall first increases and then decreases slowly. The highest recall is achieved when K is 5. There is not much change in precision for the negative pairs. This indicates that when K gets bigger some negative pairs that happen to have a positive nearest neighbor are correctly labeled. At the same time, some true positive pairs are misclassified. When K gets even bigger, precision and recall are getting close to those values when K is 1. As a result, F-score is also getting close to the value of the 1NN classifier. This is true for both positive and negative pairs. Generally, F-score of the positive pairs does not change much as K increases while that of the negative pairs is higher.

In the next section, we attempt to improve the basic RS model by incorporating a word similarity model, which is also derived from PubMed.

The performance of similarity-based approaches may be degraded by the sparsity of lexical features. In our basic RS model, even though more than 5000 features are extracted from the signature files of protein pairs, only a small number of them are relevant to a specific pair. For example, the words molecule and protein are features in the vector space model of protein pairs. The two words may appear separately in the signatures of two positive pairs A and B. Hence, in the feature vector of each pair, only one of the two corresponding dimensions has a non-zero value. Therefore, although these two words are clearly semantically related, connections between them would be missed in the similarity calculation. As a result, similarity of the two pairs could be decreased. If the similarity value of its nearest neighbor is low, the confidence of a pair getting a label would be low. Actually, this issue is indeed observed with the basic RS model. We found that about 64.6% protein pairs that are incorrectly classified have a low nearest similarity value (less than 0.35).

This motivates our work to explore word similarity to enhance our basic RS model. More specifically, a word similarity model is built and applied to reveal the connections between semantically related features in the basic RS model. Thus, the absence of lexical features can be compensated by taking these connections into account when calculating similarity between protein pairs.

A straightforward way to find similar words is to search existing resources such as dictionaries or thesaurus. However, the disadvantages are also obvious. Firstly, words in these resources are usually general, hence not sensitive to a specific domain or task. For example, in the PPI identification task, words like inhibit, stimulate, induce are similar in the sense that these words are often used to indicate the type of protein interactions. Yet this connection is unlikely to be found in available dictionaries. Secondly, these resources are limited in terms of coverage. This problem becomes more serious when we are dealing with rapidly growing corpus such as PubMed, in which new words and new expressions appear every day.

Therefore, we explore a corpus-based strategy to identify similar words. The hypothesis is that two words are similar if they occur in similar contexts. In this approach, a word is represented by its distributional profile [42,43], which is formed by the context of the word in a large-scale corpus. The context is defined as words within a fixed-size window around the target word. Then the distributional profiles of words are compared to each other to find those that are similar. In our experiment, PubMed is used to get the distributional profiles of words appearing in the signatures of the protein pairs.

The purpose of incorporating the word similarity model is to build connections between the feature words in the vectors of protein pairs, thus to relieve the problem of data sparseness in calculating similarity of these pairs. Therefore, we took all the words in the signatures (from which features are extracted) of the protein pairs in the dataset as the initial target words. Then, these words were grouped by their part-of-speech (POS) tags. Since nouns, verbs, adjectives, and adverbs are content words that actually convey meanings, they were kept as the target words and the other groups were removed. To get POS tags of words, we conducted shallow parsing on sentences in the signature files using Apache OpenNLP syntactic analysis tool [44]. A word may belong to more than one group.

We calculated similarities of any two target words having the same POS tag. Four similarity matrices corresponding to nouns, verbs, adjectives, and adverbs were derived by the following steps. Take the verb group GV as an example.
                              
                                 S1:
                                 
                                    For each target word in 
                                    GV
                                    , construct its distributional profile.
                                 

We extracted 1G-sized abstracts from PubMed randomly and used them as the corpus to extract the context of a target word. The context is defined as words within a 5-word window around each occurrence of the target word in the corpus (stop words, single-character words and numbers were removed). All the context words constitute the distributional profile of the target word.


                                    Representing the distributional profile using the vector space model.
                                 

The distributional profiles of all target words in the same POS group formed a profile set. All words in this set were used as features to build a co-occurrence vector that represents a target word. The weight of a feature was determined by the conditional probability 
                                       P
                                       (
                                       w
                                       |
                                       w
                                       1
                                       )
                                    , where 
                                       w
                                       1
                                     is the target word and 
                                       w
                                     is the feature word. It was estimated by the frequency that 
                                       w
                                     and 
                                       w
                                       1
                                     co-occur divided by the frequency that 
                                       w
                                       1
                                     appear in the 1G-sized PubMed corpus. After that, a co-occurrence matrix A was established, in which each row was the co-occurrence vector of a target word in GV.


                                    Calculate the similarity of any two words within the same POS group.
                                 

Since each target word is represented by its co-occurrence vector, the similarity between two target words can be measured by the cosine similarity metrics (Eq. (2)). The result is a word similarity matrix B, in which rows and columns correspond to the targets words in GV. The element B
                                    
                                       ij
                                     is the similarity value between the word at row i and the word at column j. To get matrix B, we first normalized matrix A to unit length. Then B can be calculated as A
                                    ·
                                    A
                                    
                                       T
                                    . Therefore, B is a |GV|×|GV| triangular matrix with |GV|(|GV|−1)/2 similarity values.

Some results of word similarity calculation are shown in Table 5
                           . In the table, words that are most similar to the target word as well as the similarity values are listed in descending order.

We can see that first of all, the similarity between inflections of a word (e.g., bind, binds, bound) is high, which provides clear evidence that comparing distributional profiles of words is effective in gathering similar words. Secondly, as expected, the distributional profiles collected from PubMed are domain-specific. They characterize words through expressing the roles they play in various biological processes, thus are very helpful in revealing semantic connections between words that are interesting to specific tasks. As Table 5 shows, our similarity matrix indicates that bind, interact, and activate are very similar, because they are used in similar context describing similar biological processes, such as PPI. Inhibit, stimulate, enhance, suppress, and induce are regarded as similar words for the same reason, although some of them are antonyms in general dictionaries. Nonetheless, their similarity revealed by our matrix is clearly reasonable and crucial in PPI identification.

In this section, the four POS-grouped word similarity matrices are incorporated into the basic RS model, as shown in the wavy-line area in Fig. 1. In the new hybrid model, a voting scheme is first used to make an initial prediction on the label of a protein pair. The votes are given by a 1NN classifier and a KNN (K
                        >1) classifier separately, using the basic RS model. If the two votes are the same, they are taken as the final decision. Otherwise, the weights of features in the vectors of protein pairs will be adjusted according to the word similarity matrices. Then the 1NN classifier is applied again to get the final prediction. Fig. 4
                         gives the detailed algorithm.

Feature words of protein pairs were extracted from the signature files of all protein pairs in the dataset. They were grouped by their POS tags. In the similarity matrix of a POS group, each element is the similarity value of two feature words with the same POS tag. θ is a threshold determining whether a similarity value should be used to update the weight of a feature word. Various θ values are evaluated in the experiments in the next section.

@&#RESULTS AND ANALYSIS@&#

In this section, the hybrid model is evaluated and the results are compared to the basic RS model. In addition, another approach of using the centroid of the training data to relieve the data sparsity problem is also evaluated. In this approach, a centroid vector is calculated for positive pairs and negative pairs respectively by averaging with the vectors of pairs in each class. Then the label of the target protein pair is assigned to be the class of the nearest centroid. Table 6
                         shows the results of the nearest centroid method as well as the hybrid model on positive pairs when using different K in the KNN classifier at different similarity threshold θ. Table 7
                         presents the results on negative pairs. The highest precision, recall, and F-score in every K-group are marked as bold.

As shown in the tables, the nearest centroid method tends to label a protein pair as negative thus recall of the positive class is pretty low. The feature weights in the negative centroid vector are generally low as there are less common features between negative pairs compared to those of the positive pairs. On the other hand, the vector representing a target pair is sparse. Hence, it is more likely that the similarity between the target pair and the negative centroid is higher. This is probably the reason of the high recall of the negative class. F-Score of this method is worse than the basic RS model in both positive and negative classes.


                        F-Score achieved by the hybrid model (K
                        =9, θ
                        =0.8) is significantly higher than the basic RS model (paired t-test, p
                        <0.05). Furthermore, the hybrid model achieves better F-score in all the cases on both the positive and the negative pairs. It clearly shows that incorporating the word similarity matrix leads to a better model of relational similarity calculation. On the positive pairs (Table 6), compared to the precision achieved by the basic RS model (75.6%), the best result achieved by the hybrid model is 79.1%. In three of the four K-groups (K
                        =3, 5, 7), the best precision is achieved when θ is 0.7. It suggests that 0.7 is a better choice if precision has higher priority in a PPI recognition task. The best recall achieved by the hybrid model is 75.5%, about 1 percentage point higher than that of the basic RS model. In all the four K-groups, the best recall is achieved when θ is 0.9, which suggests that 0.9 is a better choice if recall is valued more. The best F-score is achieved when K
                        =9 and θ is 0.8, which is about 2.1 points higher than the baseline. As shown in Table 7, on negative pairs, the hybrid model outperforms the basic RS model in all the cases. The highest precision is 75.2%, which is about 1.6 points higher than the basic model. The highest recall is 79.5%, which is about 4.8 points higher than the basic model. The largest improvement in F-score is about 2.9 points. As shown in Tables 6 and 7, when θ is 0.8, the hybrid model gets the best F-score in most K-groups.

In the step of weight adjustment in the PPI recognition algorithm (Fig. 4), any feature word whose weight is zero would be considered for adjustment. In this process, we notice that some feature words are seldom involved in portraying the relations. For example, words like describe, explain, and depict often appear in abstracts of journal articles just to introduce the structures of the papers. Thus, adjusting their weights according to the word similarity matrix could be misleading. On the other hand, some words are more likely to play a role in building the relationships. For example, the neighbor words of protein mentions in a sentence are often part of the relation expressions. Adjusting weights of these words is more likely to be beneficial in the hybrid model. Based on this observation, in the following experiment, we select a subset of features as candidates of weight adjustment. From the signature files of the target protein pairs we extracted words that co-occur with the target proteins within a five-word window and took them as the candidates of weight adjustment. Tables 8 and 9
                        
                         are the results of exploiting this subset (strategy 2). As a comparison, the results obtained by the algorithm in Fig. 4 are also shown (strategy 1).

As shown in the tables, on positive pairs, strategy 2 improves recall in all the different K-groups without lowering precision. As a result, F-score in all the groups is also improved. On negative pairs, strategy 2 improves precision without lowering recall and achieves better F-score. Since candidates in strategy 2 (2632 words) is a subset of that in strategy 1 (4867 words), strategy 2 is more efficient in similarity calculation. These results suggest that identifying a proper subset of the feature words as candidates for weight adjustment is a direction worth further exploring.

@&#CONCLUSIONS@&#

This paper contributes to the research on mining PPIs from literature with a focus on addressing the difficulties of current single-sentence-based approaches. Specifically, we (1) propose a novel approach that recognizes PPIs under the framework of relational similarity, which collects evidence from context of protein pairs in a large corpus, (2) construct word similarity matrices that are sensitive to the PPI identification task using a corpus-based approach, (3) develop a hybrid model to integrate the word similarity model with the basic RS model, which further improves the performance. Experimental results show the effectiveness of the hybrid model. The results of the system can be added to the PPI network directly. Moreover, the proposed approach takes known PPIs in existing PPI databases (e.g., HPRD) as the training data and no extra annotation is required.

We believe that adopting relational similarity framework in PPI identification has great potentials. Firstly, it would be natural to explore semi-supervised algorithms within the framework of relational similarity calculation. Semi-supervised learning algorithms have been successfully applied in biomedicine [45–47]. Many semi-supervised algorithms leverage similarity between labeled data and large amount of unlabeled data to improve accuracy of classification [48,49]. For example, Zhu et al. [48] develop an algorithm that propagates labels from labeled data points to their neighbors in the feature space under the hypothesis that similar data points tend to have the same label. Our hybrid model that measures similarity between protein pairs matches well with this hypothesis. In addition, our approach provides similarity measurements that are essential in this kind of algorithms. In PPI research, the number of known PPIs is small. In contrast, the amount of unlabeled protein pairs is huge. Therefore, it is a promising direction to explore semi-supervised algorithms that make use of the small amount of labeled PPIs as well as the huge amount of unlabeled protein pairs in PPI identification. Secondly, the RS-based approach could be extended to other biological relation detection tasks, e.g., gene-disease association detection, and even to tasks with many target classes such as gene-function correlation identification. Our attempts in PPI detection serve as an example that is beneficial to similar tasks.

There are several avenues that might be explored in the future work. In the proposed approach, feature weights are adjusted to release problems caused by sparseness of features. In the next step, we attempt to investigate other strategies on this issue. As shown in our experiment, identifying appropriate subset of features for weight adjustment could lead to a more efficient system of higher accuracy. We believe this is a direction worth further exploring.

@&#ACKNOWLEDGMENTS@&#

This study is funded by the National Natural Science Foundation of China, grant #61202132.

We gratefully acknowledge access to the U.S. National Library of Medicine (NLM) MEDLINE/PubMed database through the NLM Data Distribution Program, using the license code 1463NLM154.

@&#REFERENCES@&#

