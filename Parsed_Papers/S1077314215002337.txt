@&#MAIN-TITLE@&#Activity detection using Sequential Statistical Boundary Detection (SSBD)

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a novel activity detection scheme tailored for home environment scenes.


                        
                        
                           
                           We introduce three new action datasets for action detection evaluation.


                        
                        
                           
                           Fast spatio-temporal action localization with the use of statistical tools.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Activity localization

Activity detection

Activity recognition

Motion segmentation

@&#ABSTRACT@&#


               
               
                  The spiralling increase of video data has rendered the automated localization and recognition of activities an essential step for video content understanding. In this work, we introduce novel algorithms for detecting human activities in the spatial domain via a binary activity detection mask, the Motion Boundary Activity Area (MBAA), and in the time domain by a new approach, Statistical Sequential Boundary Detection (SSBD). MBAAs are estimated by analyzing the motion vectors using the Kurtosis metric, while dense trajectories are extracted and described using a low level HOGHOF descriptor and high level Fisher representation scheme, modeling a Support Vector Data Description (SVDD) hypersphere. SSBD is then realized by applying Sequential Change Detection with the Cumulative Sum (CUSUM) algorithm on the distances between Fisher data descriptors and the corresponding reference SVDD hyperspheres for rapid detection of changes in the activity pattern. Activities in the resulting video subsequences are then classified using an multi-class SVM model, leading to state of the art results. Our experiments with benchmark and real world data demonstrate that our technique is successful in reducing the computational cost and also in improving activity detection rates.
               
            

@&#INTRODUCTION@&#

The automated analysis of real world videos is a challenging problem, as it requires the reliable discovery of unknown activities both in space and time, that may occur at any moment, followed by their accurate classification, which is particularly demanding due to the various realizations of activities that may occur. In this work, we address both problems, that of activity discovery and that of recognition, by introducing novel, theoretically well-founded methods that are shown to lead to accurate detection and recognition rates, at a reduced computational cost.

The spatio-temporal localization of activities is a necessary precursor to activity recognition in video sequences of long duration. In the literature, it is common to localize activities in space and time by deploying spatio-temporal sliding windows to detect activity subsequences, and then recognize the activities in them using a high level classifier. This is usually performed by applying two classifiers: a human detector for spatial localization and an activity classifier, usually a multi-class model, to recognize the activity that takes place inside each spatial window (i.e. human location). The spatio-temporal sliding window framework entails the sequential process of the video frames with an overlap and multiple spatial and temporal windows in order to deal with scale variance, which significantly increases the computational cost and renders these techniques inappropriate for real-life scenarios, such as the localization and recognition of activities of daily living (ADLs) in videos of long duration.

In this work only regions of interest are examined in each frame, namely pixels where varying motion occurs, to reduce computational cost and improve the subsequent temporal localization of activities and recognition rates. These regions correspond to a spatial binary mask, the Motion Boundary Activity Area (MBAA), that contains the regions in each video frame where varying motion occurs. We leverage the MBAAs for a first stage, coarse localization of activities in time, which leads to a reduction in the duration of the video analysis to almost half of real time. In particular, we consider that the frames where MBAA trajectories are present correspond to an activity subsequence, and we only analyze the data in them. To achieve this, interest points are detected in the MBAAs and are tracked, as detailed in Section 4. The video subsequences are considered to end when the trajectories in the MBAAs end. The resulting subsequence may still contain multiple activities in it, however it requires less processing than the entire video, and will result in fewer false alarms due to the lack of uninformative frames with constant motion (or no motion).

In addition to performing coarse temporal localization, MBAAs are used for spatial activity localization, in order to extract human trajectories from the scene and describe the activities that occur inside them. This technique cannot separate trajectories that originate from different individuals, so it cannot be used for the recognition of multiple activities in a scene, which forms part of our future research. This work focuses on the detection of one dominant activity per scene, which may involve one individual or an interaction between two or more people (e.g. handshake, talk), for which it introduces novel methods that reduce the high computational cost of the current sliding window based state of the art techniques.

For precise temporal segmentation, we introduce a novel method, Sequential Statistical Boundary Detection (SSBD), which leads to quicker and more reliable recognition outcomes at a much lower computational cost. This algorithm processes video frames sequentially using a non-overlapping sliding window, and extracts activity boundaries quickly via Cumulative Sum (CUSUM) sequential statistical change detection applied to the outcomes of a discriminative SVDD classifier. We first create a one-class classifier for normal activity patterns using a Support Vector Data Description (SVDD), and then calculate the instantaneous log-likelihood ratio (llr) between reference and current activity patterns. The llr is used in conjunction with the CUSUM termination test statistic for fast but robust activity localization, resulting in accurate activity classification.

Fig. 1 shows the main differences between the sliding window and SSBD approaches. Sliding window methods search through the entire video over multiple spatial and temporal scales in order to discover desired activities, while SSBD uses only a small number of frames (the reference data) to train its model. Initially, MBAAs spatially localize the activity in the video frame, eliminating the need for a costly thorough spatial search with multiscale windows. MBAAs are also used for initial coarse temporal segmentation, by terminating video subsequences when the MBAA pixels are all equal to zero, as this indicates that motion in them does not change any more. SSBD then uses the current (most recent) frames to estimate the log-likelihood ratio and incorporate it in the CUSUM test to find if a change from the normal (reference) pattern occurs, and detect the precise temporal boundaries of the activity to be classified.

This work is organized as follows: Section 2 presents some of the key work in the literature, Sections 3 and 4 describe the spatial localization algorithm and the activity representation adopted in this work, respectively. Section 5 presents our approach for the detection and classification of ADLs in videos. Experiments on datasets from real scenarios are provided in Section 6, in order to evaluate the overall system.

@&#RELATED WORK@&#

Activity detection is an open research topic on computer vision that localizes activities in videos in space and time. Most of the previous works solve this problem by deploying a spatio-temporal window combined with machine learning algorithms.

Early works [14,29] focused on detecting specific action-poses in movies (e.g. drinking, smoking) by using a spatio-temporal video block classifier or by training specialized space-time cubes cast by localization hypotheses. In [8], temporal sliding windows are used to detect human activities in videos, where the extraction of spatio-temporal interest points is followed by a bag-of-visual-words framework. However, this approach did not improve detection rates compared to earlier works. In [4], activity detection takes place by accumulating space-time volumes from each video frame. The selection of each cuboid is then performed by a weighted spatio-temporal graph, leading to a higher computational cost than when using a simple spatio-temporal sliding cuboid, as the computation of such a matrix requires the square of the time needed to detect spatio-temporal cuboids.

Spatial localization is taken under consideration in more recent works [12], where a human detector and tracker using a HOG descriptor is built to follow human subjects throughout the video and improve detection accuracy. However, a classifier needs to be computed beforehand to recognize upper bodies, where many false positives may arise. Furthermore, such a procedure may increase the computational cost, which is undesirable in real world scenarios. More focus on spatial localization is found in [13], where the classifier for person detection was treated as a latent variable and  [27] where structured output learning is applied. More recently in [15], hierarchical space-time segments are introduced and used to represent human bodies and detect/recognize activities afterwards. Tubelets were introduced in [10], where supervoxels are exploited to solve the localization problem.

For temporal localization of activities, a fast technique was proposed in [30], based on Naive-Bayes Mutual Information Maximization (NBMIM) for multi-class action categorization. Inspired by this technique, we also base the detection of motion boundaries (the temporal segmentation of actions) on the instantaneous log-likelihood ratio. However, in our case the computation is data driven and does not require prior knowledge/description of the activities to be detected.

Recently, an alternative sliding window technique [9], where weights denote the start, middle and end keyframes of the windowed frames under examination, has led to State of the Art spatiotemporal activity detection and recognition accuracy. In that work, the authors introduced the idea of decomposing actions into atomic action units (actoms) and developed the Actom Sequence Model (ASM) to detect activities. A more recent method [18] shows the improvements that sophisticated high level representation schemes, such as Fisher encoding, can add to this. Similarly to this approach, we adapt high level representation schemes to improve detection accuracies.

Other techniques that have performed action detection were introduced in [6,23]. Both of them rely on a global feature template to detect and localize actions concurrently, while a different recognition approach is proposed. A more recent technique that was proposed in [25] also adopt a holistic approach and uses a two phase decomposition of a tensor for representation purposes and a boosting strategy to solve activity detection problem.

It is worthwhile to note that, although the techniques presented here achieve very accurate detection rates, their computational cost is too high for real-life scenarios. In this work, spatiotemporal activity detection and classification takes place at a much lower computational cost, allowing for deployment in real world applications.

In the literature, the detection of activities of interest most often occurs using overlapping temporal sliding windows, but without refined spatial localization, as they use multi-scale bounding boxes to denote the region of interest. We overcome this limitation by introducing the Motion Boundary Activity Areas (MBAA) Section 3.1, which result in regions where pixels undergo a varying motion indicating that an activity of interest is occurring in that location. Once activities are localized in space, their appearance and motion features are derived in an activity representation framework to be used for their recognition. In this work, activity representation involves the construction of a trajectory structure over MBAAs by tracking densely sampled interest points and computing spatio-temporal cuboids around them to describe their motion and appearance over time. Hybrid cuboids are then constructed by concatenating the HOGHOF and trajectory descriptors. The overall procedure for the construction of our action descriptor is depicted in Fig. 2.

In [1] Motion Boundary Activity Areas (MBAAs) were used to isolate pixels with varying motion and sample interest points only around them, to reduce the processing time compared to dense sampling, which is used in the SoA to represent human activities [28]. In this work, MBAAs are used for spatial localization and also for initial coarse temporal localization of activities, to reduce processing time. This twofold use of MBAAs can significantly reduce the computational time of activity detection compared to sliding window methods, while also improving detection accuracy.

For MBAA extraction, once the OF is estimated, its gradients are computed over successive frames by applying the horizontal and vertical Sobel operators and calculating their root sum square (RSS). Gradients of OF are indicative of motion edges, producing higher values on motion boundaries, and lower values in smoother motion regions, and are a strong indicator of regions that are informative about human activities.

In practice high motion gradients can also be induced by background clutter or illumination variations, introducing errors in the binary MBAA mask. In order to separate true motion boundaries from noise-induced ones, we make the assumption that noisy OF gradients can be approximated by a Gaussian distribution (hypothesis H
                        0), so that true changes in OF induce a deviation from Gaussianity (hypothesis H
                        1). Eq. (1) denotes the estimated OF gradient
                        
                         for each hypothesis Hi
                         at pixel r and frame k:

                           
                              (1)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                H
                                                0
                                             
                                             :
                                             
                                                u
                                                
                                                   k
                                                
                                                0
                                             
                                             =
                                             
                                                z
                                                k
                                             
                                             
                                                (
                                                r
                                                )
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             
                                                H
                                                1
                                             
                                             :
                                             
                                                u
                                                
                                                   k
                                                
                                                1
                                             
                                             =
                                             
                                                u
                                                k
                                             
                                             
                                                (
                                                r
                                                )
                                             
                                             +
                                             
                                                z
                                                k
                                             
                                             
                                                (
                                                r
                                                )
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 u
                                 k
                                 i
                              
                              
                                 (
                                 r
                                 )
                              
                              ,
                              i
                              =
                              
                                 {
                                 0
                                 ,
                                 1
                                 }
                              
                           
                         is the true OF gradient for each hypothesis and zk
                        (r) is additive noise-induced. The assumption of Gaussianity for noise-induced OF gradients can be considered valid, as OF estimates originate from sums of many i.i.d. random variables, which ultimately converge towards the same Gaussian distribution according to the Central Limit Theorem [21]. This does not apply to motion-induced OF gradients, as they change depending on the video and the activities in it, unlike noise-induced OF, which contains far fewer outliers.

To verify the Gaussianity assumption for the gradients of constant OF, we apply the Kolmogorov–Smirnov (K–S) test [16] and use the Kurtosis test to compute the MBAA mask that will separate gradients of constant OF from gradients of varying OF.

The K–S test checks if an empirical data distribution matches a reference distribution, in this case the Gaussian fit to our data. We apply the K–S test on OF gradients from 10 videos randomly sampled from the URADL dataset [17] that are divided in two groups: flow gradients inside manually extracted ground truth MBAAs and flow gradients outside of these MBAAs. Fig. 3 shows the empirical cumulative distribution function (cdf) of these flow gradients and the cdf of their Gaussian approximation for both cases: it is evident that flow gradients outside MBAAs are indeed adequately approximated by a Gaussian distribution, while flow gradients in MBAAs that are time-varying do not match the Gaussian model. The Root Mean Squared Deviation (RMSD) for data inside an MBAA, averaged over the 10 URADL videos, is 0.3847, while outside MBAAs it is 0.1975.

The Kurtosis is a classical test of deviations from Gaussianity [5], whose value becomes equal to zero for Gaussian data. Based on this property, we build a binary mask that separates time-varying OF values from those that remain constant in the time interval being examined. In order to accurately estimate the empirical value of the kurtosis, we apply the unbiased Kurtosis estimator of [2] to the OF gradients uk
                           (r) at pixel r and in frames k within a temporal window W:

                              
                                 (2)
                                 
                                    
                                       
                                          G
                                          2
                                       
                                       
                                          [
                                          
                                             u
                                             k
                                          
                                          
                                             (
                                             r
                                             )
                                          
                                          ]
                                       
                                       =
                                       
                                          3
                                          
                                             W
                                             (
                                             W
                                             −
                                             1
                                             )
                                          
                                       
                                       
                                          ∑
                                          
                                             k
                                             =
                                             1
                                          
                                          W
                                       
                                       
                                          
                                             u
                                             k
                                          
                                          
                                             
                                                (
                                                r
                                                )
                                             
                                             4
                                          
                                       
                                       −
                                       
                                          
                                             W
                                             +
                                             2
                                          
                                          
                                             W
                                             (
                                             W
                                             −
                                             1
                                             )
                                          
                                       
                                       
                                          
                                             (
                                             
                                                ∑
                                                
                                                   k
                                                   =
                                                   1
                                                
                                                W
                                             
                                             
                                                u
                                                k
                                             
                                             
                                                
                                                   (
                                                   r
                                                   )
                                                
                                                2
                                             
                                             )
                                          
                                          2
                                       
                                       .
                                    
                                 
                              
                           
                        

To further verify that the kurtosis reliably separates gradients of true OF from gradients of noise-induced OF, we estimate the kurtosis for 10 videos chosen randomly from the URADL dataset [17], as before, where we manually extracted ground truth MBAAs. The kurtosis values of the OF gradients inside and outside the MBAAs are shown in Fig. 4, where their difference is very clear: in Fig. 4(right) the vertical axis, showing the kurtosis values, has values up to 0.1, with most kurtosis values below 0.02, while in Fig. 4(left), for data in the MBAA, kurtosis values are much higher, surpassing 300 in some cases. More detailed quantitative results in Table 1 also show that the kurtosis reliably separates pixels with changing OF from the rest. Similar empirical results with many other ADL videos led us to the following rule for binarizing the kurtosis of flow gradients:

                              
                                 
                                    
                                       M
                                       B
                                       A
                                       A
                                       
                                          (
                                          x
                                          ,
                                          y
                                          )
                                       
                                       =
                                       
                                          {
                                          
                                             
                                                
                                                   0
                                                
                                                
                                                   
                                                      
                                                      if
                                                      
                                                      
                                                         G
                                                         2
                                                      
                                                      
                                                         [
                                                         x
                                                         ,
                                                         y
                                                         ]
                                                      
                                                      <
                                                      2
                                                      ·
                                                      10
                                                      
                                                         e
                                                         
                                                            −
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   1
                                                
                                                
                                                   
                                                      
                                                      else
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

Spatial localization in MBAAs is followed by the extraction of interest points, for which hybrid spatio-temporal descriptors are constructed as described in the section below.

Once MBAAs are found, a spatial grid is formed in each video frame, creating a number of blocks at different scales 
                           
                              s
                              =
                              {
                              1
                              ,
                              2
                              ,
                              .
                              .
                              .
                              N
                              }
                           
                        . The central pixel of each block is represented as Ps
                        (i) at the ith grid point, and its corresponding location is given by 
                           
                              
                                 P
                                 s
                              
                              
                                 (
                                 i
                                 )
                              
                              =
                              
                                 (
                                 x
                                 ,
                                 y
                                 )
                              
                           
                        . The accumulation of all grid points for each scale s at each frame forms the vector 
                           
                              P
                              
                                 a
                                 l
                                 l
                              
                              s
                           
                        . These points are considered as candidate interest points 
                           
                              
                                 P
                                 c
                                 s
                              
                              ⊆
                              
                                 P
                                 
                                    a
                                    l
                                    l
                                 
                                 s
                              
                           
                         when more than 50% of the block belongs to an MBAA. The resulting sampled candidate points 
                           
                              P
                              c
                              s
                           
                         are accumulated in a 2D-point set 
                           
                              S
                              c
                              s
                           
                         for each spatial scale s: 
                           
                              
                                 S
                                 c
                                 s
                              
                              =
                              
                                 {
                                 
                                    P
                                    c
                                    s
                                 
                                 
                                    (
                                    1
                                    )
                                 
                                 ,
                                 
                                    P
                                    c
                                    s
                                 
                                 
                                    (
                                    2
                                    )
                                 
                                 ,
                                 …
                                 }
                              
                              ,
                           
                         so that the final set comprises of several candidate sets on multiple spatial scales 
                           
                              
                                 S
                                 c
                              
                              =
                              
                                 {
                                 
                                    S
                                    c
                                    1
                                 
                                 ⋃
                                 
                                    S
                                    c
                                    2
                                 
                                 ⋃
                                 ⋯
                                 ⋃
                                 
                                    S
                                    c
                                    N
                                 
                                 }
                              
                           
                        . These points are then tracked using a boosted KLT tracker, which uses RANSAC for homography computation to remove bad correspondences 
                           
                              
                                 P
                                 b
                                 s
                              
                              
                                 (
                                 i
                                 )
                              
                           
                         at each scale (this does not increase the computational cost as the ratio between inlier and outlier points is less than 10%), denoted by 
                           
                              
                                 S
                                 b
                              
                              =
                              
                                 {
                                 
                                    S
                                    b
                                    1
                                 
                                 ⋃
                                 
                                    S
                                    b
                                    2
                                 
                                 ⋃
                                 ⋯
                                 ⋃
                                 
                                    S
                                    b
                                    N
                                 
                                 }
                              
                              ,
                           
                         where 
                           
                              
                                 S
                                 b
                                 s
                              
                              =
                              
                                 {
                                 
                                    P
                                    b
                                    s
                                 
                                 
                                    (
                                    1
                                    )
                                 
                                 ,
                                 
                                    P
                                    b
                                    s
                                 
                                 
                                    (
                                    2
                                    )
                                 
                                 ,
                                 …
                                 }
                              
                           
                        .

Final validated interest points are then extracted from the candidate interest points and bad correspondences, resulting in the set S of candidate interest points Ps
                         at scale s, with only the good correspondences 
                           
                              S
                              =
                              {
                              
                                 S
                                 c
                              
                              −
                              
                                 S
                                 b
                              
                           
                        }. The validated points 
                           
                              
                                 P
                                 s
                              
                              ⊆
                              
                                 P
                                 c
                                 s
                              
                           
                         are tracked over time until all trajectories become motionless and are then concatenated according to their temporal order to form the final trajectory descriptor 
                           
                              T
                              r
                              a
                              
                                 j
                                 s
                              
                              =
                              
                                 {
                                 
                                    P
                                    s
                                 
                                 
                                    (
                                    k
                                    −
                                    
                                       W
                                       t
                                    
                                    +
                                    1
                                    )
                                 
                                 ,
                                 
                                    P
                                    s
                                 
                                 
                                    (
                                    k
                                    −
                                    
                                       W
                                       t
                                    
                                    )
                                 
                                 ,
                                 …
                                 ,
                                 
                                    P
                                    s
                                 
                                 
                                    (
                                    k
                                    )
                                 
                                 }
                              
                              ,
                           
                         where k is the current frame and Wt
                         the trajectory length. This leads to initial, coarse temporal localization, where the video subsequence is considered to end when no more trajectories exist in the scene. Fig. 5 depicts an example where activity detection locates the start and end frame, and also shows an in-between frame of a video segment.

Once activities are localized in space by the MBAA, the hybrid spatio-temporal descriptor of [1] is extracted for activity representation, as it was shown to achieve SoA recognition rates. Trajectories are extracted in MBAAs as described above, and HOG and HOF descriptors are computed around trajectory points Trajs
                         in an area equal to the grid block (8s, 8s), to include both appearance and motion information in the action descriptor. The computed HOG and HOF descriptors of the same trajectory are then concatenated sequentially and subdivided into a nx
                         × ny
                         × nt
                         (
                           
                              
                                 n
                                 x
                              
                              =
                              
                                 n
                                 y
                              
                              =
                              2
                              ,
                              
                                 n
                                 t
                              
                              =
                              3
                           
                        ) grid of cuboids. For each cuboid, histograms are averaged over time and normalized by the L
                        2 norm to form the final local HOGHOF descriptor that will represent the ADLs performed around trajectory points. The trajectory coordinates are also added to this local HOGHOF descriptor, to include spatial localization information, leading to a hybrid, global/local HOGHOF+Traj descriptor, which is shown in the experiments of Section 6 to lead to higher recognition rates than local HOGHOF. GMM clustering is then applied for vocabulary construction, while the Fisher framework is used to encode the descriptors in each video subsequence under examination.

The hybrid descriptors described in Section 4 and in more detail in [1] are extracted for all training video samples, which have previously been manually segmented to contain a specific activity, and are imported into a clustering algorithm, either K-Means or GMM in this work, to build a visual vocabulary for the ADLs. VLAD and Fisher encoding are compared on sample data and depending on their classification accuracy and computational cost, the most appropriate one is applied to the video segments’ hybrid descriptors and visual vocabulary cluster centers to transform them into fixed size feature vectors.

Test video samples, on the other hand use MBAAs for initial coarse temporal segmentation, by starting a video sequence when MBAA detects a trajectory point inside the video frame and terminating video sequences when the MBAA pixels are all equal to zero. After this coarse segmentation two different activity detection approaches are presented and compared: (i) the commonly used sliding window approach deploys a sliding window classifier to recognize the ADLs in a video segment (Section 5.1). (ii) A new approach, based on sequential statistical change detection using CUSUM (Section 5.2), is applied on non-overlapping temporal windows to detect a video segment’s activity boundaries, and is shown to achieve quicker and more accurate results. The methodology followed in this work is shown in Fig. 6
                     .

Temporal sliding windowing is a widely used technique for detecting and recognizing activities in an unsegmented video, i.e. a video that may contain more than one ADLs. Before the application of the sliding window, a global representation needs to be constructed for each training video to build the corresponding classifiers. A clustering algorithm (i.e. K-Means, GMM) is deployed to partition the ADL feature space and acquire the visual vocabulary corresponding to different activities. Afterwards, appropriate encoding (e.g. VLAD [11], Fisher [20]) takes place to characterize each video segment by a fixed size feature vector. The feature vectors obtained from training videos are used to train 
                           
                              C
                              (
                              C
                              −
                              1
                              )
                              /
                              2
                           
                         SVM linear classifiers, where C is the number of ADL classes in them.

Test videos, on the other hand, are segmented using a temporal sliding window applied to subsequences containing active MBAA pixels (Section 3.1). These video segments are then classified by a temporal sliding window of size W
                        0, with a sampling step J
                        0. For J
                        0 < W
                        0, there is temporal overlap among the predictions for better localization of activity boundaries (i.e. the start and end of an activity). One-against-one SVM classification is used to predict each windowed video segment’s class, and a voting schema is applied, to declare the prominent ADL recognized by the classifiers. The class with the highest number of votes always wins, while in the case of a draw a second round of voting is performed among the 1st and 2nd classifiers, and the one that wins among these two is the detected activity. In case of triple or higher draws, we announce ambiguity between the classes (a scenario that usually occurs during a transition between clearly detected activities) and no recognition data is stored. Results are accumulated in a classification index, which is used to find the number of activities in the video segment, their activity boundaries and ambiguous regions between successive detected activities. Fig. 7
                         visualizes ambiguous activity states, as well as the location of the relevant features (in the green boxes) inside MBAAs and their trajectories (in yellow/orange/red).

This work presents a more principled and computationally efficient method than the sliding window technique for the detection of activity boundaries, based on CUSUM statistical sequential change detection, which gives more accurate results, more quickly. After creating the visual vocabulary from training data, we proceed with the analysis of the unsegmented test video segments derived as before from the video subsequences that result when all trajectories are terminated (Section 4.1). Action descriptors are then encoded into VLAD or Fisher vectors at each frame and Statistical Sequential Boundary Detection (i.e. SSBD) is applied to them.


                        Fig. 8
                        
                         shows the overall SSBD procedure, which is based on detecting changes in the distance of a frame’s descriptor from the reference (training) data model, described in detail below. Reference data is chosen from the first W
                        0 frames and tested to see if any changes take place in those frames, as described below. If no change is detected, these W
                        0 frames form a reference SVDD model, while test data in a window of length W
                        1 is sequentially compared to the reference model to determine whether a change occurs at each frame k. The algorithm is repeated until a change occurs, or W
                        1 exceeds W
                        0, or k reaches the last frame of the video N. Classification of the reference data follows whenever a change is detected and the algorithm is reset, setting the current frames as reference data.

As can be seen in Fig. 8, the first W
                        0 frames make up the reference (baseline) data, with W
                        0 equal to the number of frames recorded per second (fps), which is a time interval where activities are most likely to occur, regardless of the video’s recording frequency. The effect of changing the size of W
                        0 was examined experimentally via cross-validation of various window sizes: as Fig. 10
                         shows, the best recognition rates were indeed achieved for windows of length equal to fps. This results in initial reference data 
                           
                              
                                 X
                                 0
                              
                              =
                              
                                 {
                                 
                                    x
                                    
                                       i
                                       +
                                       1
                                    
                                 
                                 ,
                                 
                                    x
                                    
                                       i
                                       +
                                       2
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    x
                                    
                                       i
                                       +
                                       
                                          W
                                          0
                                       
                                    
                                 
                                 }
                              
                              ,
                           
                         where 
                           
                              
                                 x
                                 j
                              
                              ,
                              ∀
                              j
                              ∈
                              
                                 [
                                 i
                                 +
                                 1
                                 ,
                                 …
                                 ,
                                 i
                                 +
                                 
                                    W
                                    0
                                 
                                 ]
                              
                           
                         is the respective VLAD/Fisher descriptor at each frame and 
                           
                              
                                 W
                                 0
                              
                              =
                              fps
                           
                        . The samples X
                        0 then train a Support Vector Data Description (SVDD) model [26], which will create a hypersphere around them, enclosing similar ADLs and excluding outliers. Thus, given reference data X
                        0, we solve the optimization problem:

                           
                              (3)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                min
                                                
                                                   R
                                                   ,
                                                   a
                                                   ,
                                                   ξ
                                                
                                             
                                             
                                                R
                                                2
                                             
                                             +
                                             C
                                             
                                                ∑
                                                
                                                   j
                                                   =
                                                   i
                                                   +
                                                   1
                                                
                                                
                                                   i
                                                   +
                                                   
                                                      W
                                                      0
                                                   
                                                
                                             
                                             
                                                ξ
                                                j
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             
                                                s
                                                u
                                                b
                                                j
                                                e
                                                c
                                                t
                                                
                                                t
                                                o
                                                
                                                ∥
                                                ϕ
                                             
                                             
                                                (
                                                
                                                   x
                                                   j
                                                
                                                )
                                             
                                             
                                                
                                                   −
                                                   a
                                                   ∥
                                                
                                                2
                                             
                                             ≤
                                             
                                                R
                                                2
                                             
                                             +
                                             
                                                ξ
                                                j
                                             
                                             ,
                                             ∀
                                             j
                                             ∈
                                             
                                                [
                                                i
                                                +
                                                1
                                                ,
                                                i
                                                +
                                                
                                                   W
                                                   0
                                                
                                                ]
                                             
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        to obtain the radius of the hypersphere R, the Lagrange multipliers a and the slack variables ξ that will define the reference SVDD model, where ϕ() is a function mapping data to a higher dimensional space, and C > 0 is a user specified parameter. After Eq. (3) is solved and a SVDD reference model is determined, we compute all reference distances from the hypersphere:

                           
                              (4)
                              
                                 
                                    
                                       D
                                       0
                                    
                                    
                                       =
                                       {
                                    
                                    
                                       d
                                       j
                                    
                                    
                                       =
                                       ∥
                                       ϕ
                                    
                                    
                                       (
                                       
                                          x
                                          j
                                       
                                       )
                                    
                                    −
                                    a
                                    
                                       
                                          ∥
                                          2
                                       
                                       :
                                       ∀
                                       j
                                       ∈
                                       
                                          [
                                          i
                                          +
                                          1
                                          ,
                                          …
                                          ,
                                          i
                                          +
                                          
                                             W
                                             0
                                          
                                          ]
                                       
                                       }
                                    
                                    .
                                 
                              
                           
                        
                     

When the number of distances enclosed in the hypersphere (dj
                         ≤ R
                        2) is bigger than W
                        0/2 (i.e. more than the half of the reference data is in the hypersphere), we approximate their probability density function (pdf) f(D
                        0, θ
                        0) by its deterministic parameters θ
                        0 given by the mean 
                           
                              μ
                              
                                 D
                                 0
                              
                           
                         and variance 
                           
                              σ
                              
                                 
                                    D
                                    0
                                 
                              
                              2
                           
                         of D
                        0 for a Gaussian pdf. Otherwise, the first W
                        0 reference frames are considered to contain too many outliers, denoted as ambiguous and are therefore insufficient for approximating the reference pdf. In that case, in order to obtain reference data, the window for reference sampling slides by J
                        0, so that a new set of data is extracted starting at 
                           
                              i
                              +
                              
                                 J
                                 0
                              
                           
                         to exclude the initial noisy data, and is processed in the same manner. This procedure can be repeated until the reference window exceeds the end frame of the video segment.

Once the SVDD model is built for the first W
                        0 reference frames (frames i to 
                           
                              i
                              +
                              
                                 W
                                 0
                              
                           
                         in Fig. 8), we examine the next W
                        1 frames to detect changes in them, in the test window from frame 
                           
                              i
                              +
                              
                                 W
                                 0
                              
                           
                         to 
                           
                              i
                              +
                              
                                 W
                                 0
                              
                              +
                              
                                 W
                                 1
                              
                              −
                              1
                           
                        . We thus initially extract the test data 
                           
                              
                                 X
                                 1
                              
                              =
                              
                                 {
                                 
                                    x
                                    
                                       i
                                       +
                                       
                                          W
                                          0
                                       
                                       +
                                       1
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    x
                                    
                                       i
                                       +
                                       
                                          W
                                          0
                                       
                                       +
                                       
                                          W
                                          1
                                       
                                    
                                 
                                 }
                              
                              ,
                           
                         also expressed as 
                           
                              
                                 X
                                 1
                              
                              =
                              
                                 {
                                 
                                    x
                                    
                                       k
                                       −
                                       
                                          W
                                          1
                                       
                                    
                                 
                                 ,
                                 …
                                 ,
                                 
                                    x
                                    
                                       k
                                       −
                                       1
                                    
                                 
                                 }
                              
                           
                         for 
                           
                              k
                              =
                              i
                              +
                              
                                 W
                                 0
                              
                              +
                              
                                 W
                                 1
                              
                              +
                              1
                              ,
                           
                         and use these first W
                        1 frames to compute the test data’s SVDD. We then calculate its distance from the Fisher encoded descriptor xk
                         at frame 
                           
                              k
                              =
                              i
                              +
                              
                                 W
                                 0
                              
                              +
                              
                                 W
                                 1
                              
                              +
                              1
                           
                         and apply CUSUM, as explained below, to detect a change. If no change is detected, the test window W
                        1 is increased by 1 to include a new frame, and CUSUM is applied to the augmented test data. This procedure continues until a change is detected, or W
                        1 > W
                        0, i.e. W
                        1 becomes higher than the reference data W
                        0. If W
                        1 ≥ W
                        0 and no change has been found, the reference data are updated by including test data, so 
                           
                              
                                 X
                                 0
                                 ′
                              
                              =
                              
                                 X
                                 0
                              
                              ⋃
                              
                                 X
                                 1
                              
                              ,
                           
                         and the procedure restarts until a change is detected or all N frames have been tested. The reference SVDD hypersphere is created as soon as we have test data, and the corresponding test distances are computed as follows:

                           
                              (5)
                              
                                 
                                    
                                       D
                                       1
                                    
                                    
                                       =
                                       {
                                    
                                    
                                       d
                                       j
                                    
                                    
                                       =
                                       ∥
                                       ϕ
                                    
                                    
                                       (
                                       
                                          x
                                          j
                                       
                                       )
                                    
                                    −
                                    a
                                    
                                       
                                          ∥
                                          2
                                       
                                       :
                                       ∀
                                       j
                                       ∈
                                       
                                          [
                                          k
                                          −
                                          
                                             W
                                             1
                                          
                                          ,
                                          …
                                          ,
                                          k
                                          −
                                          1
                                          ]
                                       
                                       }
                                    
                                    .
                                 
                              
                           
                        
                     

The first W
                        1 distances dj
                         until the current frame k are used to compute the parameters θ
                        1 of f(D
                        1, θ
                        1), while the distance of the last frame’s Fisher encoded descriptor from the test SVDD, i.e. dk
                        , which is used to determine if a change occurs in the ADL patterns.

Once the reference and test data distributions have been approximated by computing θ
                           0 and θ
                           1, we apply CUSUM for quickest online change detection [7]. This leads to the rapid and accurate temporal localization of activity boundaries when the distance of the current data descriptor from the corresponding test SVDD changes significantly compared to the distance of the reference data descriptor from its SVDD.

In practice, the change may either be an abrupt increase or decrease between θ
                           1 and θ
                           0, because a change can be reflected either by an increase from the hypersphere’s center R or by an abrupt decrease from it. For this reason, we deploy two-sided CUSUM testing, proposed by Page in [19]:

                              
                                 (6)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   s
                                                   k
                                                   i
                                                
                                                =
                                                l
                                                n
                                                
                                                   (
                                                   
                                                      
                                                         f
                                                         (
                                                         
                                                            d
                                                            k
                                                         
                                                         ,
                                                         
                                                            θ
                                                            1
                                                         
                                                         )
                                                      
                                                      
                                                         f
                                                         (
                                                         
                                                            d
                                                            k
                                                         
                                                         ,
                                                         
                                                            θ
                                                            0
                                                         
                                                         )
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   s
                                                   k
                                                   d
                                                
                                                =
                                                −
                                                l
                                                n
                                                
                                                   (
                                                   
                                                      
                                                         f
                                                         (
                                                         
                                                            d
                                                            k
                                                         
                                                         ,
                                                         
                                                            θ
                                                            1
                                                         
                                                         )
                                                      
                                                      
                                                         f
                                                         (
                                                         
                                                            d
                                                            k
                                                         
                                                         ,
                                                         
                                                            θ
                                                            0
                                                         
                                                         )
                                                      
                                                   
                                                   )
                                                
                                                .
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

The Instantaneous and Cumulative test statistics, defined as 
                              
                                 
                                    S
                                    k
                                    i
                                 
                                 ,
                                 
                                    S
                                    k
                                    d
                                 
                              
                            and 
                              
                                 
                                    G
                                    k
                                    i
                                 
                                 ,
                                 
                                    G
                                    k
                                    d
                                 
                              
                            respectively (i.e. 
                              
                                 
                                    S
                                    k
                                    i
                                 
                                 ,
                                 
                                    G
                                    k
                                    i
                                 
                              
                            for testing an increase and 
                              
                                 
                                    S
                                    k
                                    d
                                 
                                 ,
                                 
                                    G
                                    k
                                    d
                                 
                              
                            for a decrease) are used in the iterative formula for CUSUM sequential change detection:

                              
                                 (7)
                                 
                                    
                                       
                                          
                                             
                                                S
                                                k
                                                i
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                
                                                   s
                                                   k
                                                   i
                                                
                                                +
                                                
                                                   S
                                                   
                                                      k
                                                      −
                                                      1
                                                   
                                                   i
                                                
                                                ,
                                                
                                                
                                                   S
                                                   k
                                                   d
                                                
                                                =
                                                
                                                   s
                                                   k
                                                   d
                                                
                                                +
                                                
                                                   S
                                                   
                                                      k
                                                      −
                                                      1
                                                   
                                                   d
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                G
                                                k
                                                i
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                max
                                                
                                                   (
                                                   0
                                                   ,
                                                   
                                                      s
                                                      
                                                         k
                                                      
                                                      i
                                                   
                                                   +
                                                   
                                                      G
                                                      
                                                         k
                                                         −
                                                         1
                                                      
                                                      i
                                                   
                                                   )
                                                
                                                ,
                                                
                                                
                                                   G
                                                   k
                                                   d
                                                
                                                =
                                                max
                                                
                                                   (
                                                   0
                                                   ,
                                                   
                                                      s
                                                      
                                                         k
                                                      
                                                      d
                                                   
                                                   +
                                                   
                                                      G
                                                      
                                                         k
                                                         −
                                                         1
                                                      
                                                      d
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           with initial values equal to zero 
                              
                                 
                                    S
                                    k
                                    i
                                 
                                 =
                                 0
                                 ,
                                 
                                    S
                                    k
                                    d
                                 
                                 =
                                 0
                                 ,
                                 
                                    G
                                    k
                                    i
                                 
                                 =
                                 0
                                 ,
                                 
                                    G
                                    k
                                    d
                                 
                                 =
                                 0
                              
                           . For two-sided online change detection, whenever 
                              
                                 G
                                 k
                                 i
                              
                            or 
                              
                                 G
                                 k
                                 i
                              
                            exceeds a threshold h > 0. This threshold is set separately for each dataset via a cross validation procedure (Table 3), to ensure quickest and accurate detection. The detection algorithm then searches for the minimum of 
                              
                                 S
                                 k
                                 i
                              
                            or 
                              
                                 
                                    S
                                    k
                                    d
                                 
                                 ,
                              
                            and determines a change point respectively as:

                              
                                 (8)
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      n
                                                      c
                                                   
                                                   ^
                                                
                                                =
                                                arg
                                                
                                                   min
                                                   
                                                      k
                                                      −
                                                      
                                                         W
                                                         1
                                                      
                                                      ≤
                                                      
                                                         n
                                                         c
                                                      
                                                      ≤
                                                      k
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   (
                                                   
                                                      S
                                                      
                                                         
                                                            n
                                                            c
                                                         
                                                      
                                                      i
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      n
                                                      c
                                                   
                                                   ^
                                                
                                                =
                                                arg
                                                
                                                   min
                                                   
                                                      k
                                                      −
                                                      
                                                         W
                                                         1
                                                      
                                                      ≤
                                                      
                                                         n
                                                         c
                                                      
                                                      ≤
                                                      k
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   (
                                                   
                                                      S
                                                      
                                                         
                                                            n
                                                            c
                                                         
                                                      
                                                      d
                                                   
                                                   )
                                                
                                                .
                                             
                                          
                                       
                                    
                                 
                              
                           
                        

If a change
                            is detected at 
                              
                                 
                                    
                                       n
                                       ^
                                    
                                    c
                                 
                                 ,
                              
                            
                           X
                           0 is updated by including all data before that changepoint, so that 
                              
                                 
                                    X
                                    0
                                    ′
                                 
                                 =
                                 
                                    X
                                    0
                                 
                                 ⋃
                                 
                                    {
                                    
                                       x
                                       j
                                    
                                    :
                                    j
                                    ∈
                                    
                                       [
                                       i
                                       +
                                       1
                                       ,
                                       …
                                       ,
                                       i
                                       +
                                       
                                          
                                             n
                                             c
                                          
                                          ^
                                       
                                       ]
                                    
                                    }
                                 
                              
                            and an SVM classifier is used to classify the ADL in these frames. The reference window then slides by 
                              
                                 
                                    n
                                    c
                                 
                                 ^
                              
                            (i.e. 
                              
                                 i
                                 =
                                 i
                                 +
                                 
                                    
                                       n
                                       ^
                                    
                                    c
                                 
                              
                           ) and the procedure is reset for the remaining data. If 
                              
                                 G
                                 k
                                 d
                              
                            or 
                              
                                 G
                                 k
                                 i
                              
                            do not exceed h > 0, no change occurs, and the current frame’s dk
                            is included in D
                           1, updating θ
                           1 accordingly and the change detection procedure restarts until W
                           1 > W
                           0. When W
                           1 surpasses W
                           0 and no change is detected in the test samples, the X
                           1 data are concatenated with the reference, giving 
                              
                                 
                                    X
                                    0
                                    ′
                                 
                                 =
                                 
                                    X
                                    0
                                 
                                 ⋃
                                 
                                    X
                                    1
                                 
                                 ,
                              
                            and updating the SVDD model. The algorithm is then reset, this time using 
                              
                                 X
                                 0
                                 ′
                              
                            as baseline (reference) data.

In order to apply CUSUM, we need to determine the pdf of the data before and after a change. As the distributions are unknown, we determine them empirically by fitting appropriate model parameters to our data. However, this implies that the family of pdfs of the data still needs to be known a priori. In this work, we make the assumption that the distances between Fisher encoded descriptors and reference SVDDs follow a Gaussian distribution, which we verify empirically by applying the Kolmogorov–Smirnov(K–S) test. Indeed, K–S tests applied to the data showed that it can be satisfactorily approximated by a Gaussian pdf given by:

                              
                                 (9)
                                 
                                    
                                       f
                                       
                                          (
                                          
                                             D
                                             
                                                0
                                                |
                                                1
                                             
                                          
                                          )
                                       
                                       =
                                       
                                          1
                                          
                                             
                                                σ
                                                
                                                   D
                                                   
                                                      0
                                                      |
                                                      1
                                                   
                                                
                                             
                                             
                                                
                                                   2
                                                   π
                                                
                                             
                                          
                                       
                                       exp
                                       
                                          (
                                          −
                                          
                                             
                                                
                                                   (
                                                   
                                                      d
                                                      k
                                                   
                                                   −
                                                   
                                                      μ
                                                      
                                                         D
                                                         
                                                            0
                                                            |
                                                            1
                                                         
                                                      
                                                   
                                                   )
                                                
                                                2
                                             
                                             
                                                2
                                                
                                                   σ
                                                   
                                                      
                                                         D
                                                         
                                                            0
                                                            |
                                                            1
                                                         
                                                      
                                                   
                                                   2
                                                
                                             
                                          
                                          )
                                       
                                    
                                 
                              
                           
                        

Substituting Eq. (9) into Eq. (6) gives instantaneous log-likelihood ratios:

                              
                                 (10)
                                 
                                    
                                       
                                          
                                             
                                                s
                                                k
                                                i
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                ln
                                                
                                                   
                                                      σ
                                                      
                                                         D
                                                         0
                                                      
                                                   
                                                   
                                                      σ
                                                      
                                                         D
                                                         1
                                                      
                                                   
                                                
                                                +
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            d
                                                            k
                                                         
                                                         −
                                                         
                                                            μ
                                                            
                                                               D
                                                               0
                                                            
                                                         
                                                         )
                                                      
                                                      2
                                                   
                                                   
                                                      2
                                                      
                                                         σ
                                                         
                                                            D
                                                            0
                                                         
                                                         2
                                                      
                                                   
                                                
                                                −
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            d
                                                            k
                                                         
                                                         −
                                                         
                                                            μ
                                                            
                                                               D
                                                               1
                                                            
                                                         
                                                         )
                                                      
                                                      2
                                                   
                                                   
                                                      2
                                                      
                                                         σ
                                                         
                                                            D
                                                            1
                                                         
                                                         2
                                                      
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                s
                                                k
                                                d
                                             
                                          
                                          
                                             =
                                          
                                          
                                             
                                                −
                                                ln
                                                
                                                   
                                                      σ
                                                      
                                                         D
                                                         0
                                                      
                                                   
                                                   
                                                      σ
                                                      
                                                         D
                                                         1
                                                      
                                                   
                                                
                                                −
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            d
                                                            k
                                                         
                                                         −
                                                         
                                                            μ
                                                            
                                                               D
                                                               0
                                                            
                                                         
                                                         )
                                                      
                                                      2
                                                   
                                                   
                                                      2
                                                      
                                                         σ
                                                         
                                                            D
                                                            0
                                                         
                                                         2
                                                      
                                                   
                                                
                                                +
                                                
                                                   
                                                      
                                                         (
                                                         
                                                            d
                                                            k
                                                         
                                                         −
                                                         
                                                            μ
                                                            
                                                               D
                                                               1
                                                            
                                                         
                                                         )
                                                      
                                                      2
                                                   
                                                   
                                                      2
                                                      
                                                         σ
                                                         
                                                            D
                                                            1
                                                         
                                                         2
                                                      
                                                   
                                                
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           where dk
                            is the current frame’s descriptor, and 
                              
                                 
                                    μ
                                    
                                       D
                                       0
                                    
                                 
                                 ,
                                 
                                    μ
                                    
                                       D
                                       1
                                    
                                 
                                 ,
                                 
                                    σ
                                    
                                       D
                                       0
                                    
                                 
                                 ,
                                 
                                    σ
                                    
                                       D
                                       1
                                    
                                 
                              
                            are empirically determined parameters of the reference and test distributions. They are approximated using the VLAD/Fisher descriptors at each frame based on the initial W
                           0 frames and the most recent W
                           1 test samples, respectively.

@&#EXPERIMENTAL RESULTS@&#

In this section, we present experimental results for spatial localization of activities, as well as temporal localization and subsequent recognition. The experimental results take place on benchmark datasets to compare with the SoA, as well as on realistic datasets recorded in lab and home environments.

We evaluate the performance of spatial localization on the benchmark UCF sports dataset, which comprises of videos of pre-segmented sports activities, and also provides groundtruth for spatial location of activities. Temporal localization is tested on two long duration, realistic datasets recorded for the EU project Dem@Care in home-like environments, in the Centre Hospitalier Universitaire de Nice (CHUN) in Nice, France (CHUN dataset), and the day center of the Greek Association for Alzheimer’s Disease and Related Disorders (GAADRD) in Thessaloniki, Greece (Dem@Care dataset). The spatiotemporally localized activities in these two datasets are also recognized by calculating average accuracy over all classes for standard vocabulary sizes ranging from 32 to 256 for VLAD and Fisher. Additionally, we provide classification accuracy rates for one public dataset, namely MSR action dataset-I proposed in [30]. The maximum vocabulary size was set based on the results of previous works on activity recognition [3,11,28], where these vocabulary sizes led to optimal recognition rates. Indeed, we observed experimentally that outside this vocabulary range, improvements or degradations are only in the range of 1%. Experiments were performed on a i5-3570K CPU, running at 3.4 GHz, while no GPU was used to accelerate the processing time of our algorithm.

To evaluate
                     
                      spatial localization accuracy, we consider the intersection of the results with the ground truth, instead of their union, as this metric is most commonly used in the literature  [13,15,27]. We consider the OV20 evaluation criterion [9], which requires the Jaccard coefficient (intersection over union) to be over 20% between groundtruth and detected activities in the time domain, in order to consider a detected activity to be a true positive.

In this section we provide information about the datasets that we used for evaluation purposes. UCF-sports action dataset [22] was used to evaluate the spatial localization accuracy of the MBAA mask. The dataset consist of 150 videos and depict 11 kinds of actions recorded under a moving camera and high intra-class variation. The action include: swinging on the pommel horse, on the floor and at the high bar, golf swinging, diving, kicking, weight-lifting, horse-riding, running, skateboarding and walking. The results of spatial localization is presented in Section 6.3.

Temporal localization is tested on three long duration, realistic datasets recorded for the EU project Dem@Care in home-like environments, in the Centre Hospitalier Universitaire de Nice (CHUN) in Nice, France (CHUN dataset), and the day center of the Greek Association for Alzheimer’s Disease and Related Disorders (GAADRD) in Thessaloniki, Greece (Dem@Care dataset, Dem@Care3 dataset).

The DemCare1 action dataset (top row on Fig. 9) consists of 1 h and 52 min recordings of 32 people with dementia that perform ADLs in a home-like environment. The camera viewpoint is in front of the person while they perform directed ADLs (i.e. activities dictated by a clinician). The ADLs observed include: Cleaning Up (CU), Drink Beverage (DB), End Phonecall (EP), Enter Room (ER), Eat Snack (ES), Hand-Shake (HS), Prepare Snack (PS), Read Paper (RP), Serve Beverage (SB), Start Phonecall (SP) and Talk to Visitor (TV). They included large anthropometric differences and activity performance styles, while the ADLs took place continuously, introducing additional difficulty and increasing the computational cost needed for activity detection.

The DemCare3 action dataset (Middle row on Fig. 9) consists of 25 people with dementia and mild cognitive impairment that perform ADLs in a nurse-like environment. The camera monitors a whole room where a person performs directed ADLs. The ADLs observed include: Answer the Phone (AP), Establish Account Balance (EAB), Prepare Drink (PD), Prepare Drug Box (PDB), Water Plant (WP), Read Article (RA), Turn On Radio (TOR).

The CHUN dataset consists of 15 h and 10 min recordings of 64 PwD that perform ADLs in a Lab environment. The camera viewpoint is such that it monitors the whole room where the PwD is performing semi-directed ADLs (i.e. they followed instructions for performing a set of ADLs listed on a paper). The recording frequency is at 8 fps and our algorithm performance achieved 3.55 fps without SSBD and 4.24 fps with SSBD for video analysis, which is a near real time process achievement considering that for 2 video frames of the actual video, one is processed by our activity detection algorithm. The ADLs observed are: (AP) answering phone and (DP) dialing phone, (LoM) look on map, (PB) pay bill, (PD) prepare drugs, (PT) prepare tea, (RP) read paper, (WP) water plant and (WtV) watch TV. They included large anthropometric variations and activity performance styles, while severe occlusions introduced great difficulty in discriminating
                        
                         actions. Bottom row on Fig. 9 depicts some activities with their trajectory and HOGHOF rectangles.

MSR action dataset is also used for temporal evaluation of our algorithm. The dataset was proposed in [30] and is among the most common datasets that are used for activity detection evaluation. This dataset uses KTH action dataset [24] video samples to train the multi-class SVM model and detect box, handclap and handwave in 16 long duration, outdoors recorded video samples. The real challenges of this dataset is the large illumination changes and that some activities are performed simultaneously.

In this section, we present the experiments that we made so that we optimize the performance of our detection algorithm. Firstly, we present the parameters’ selection for representation and encoding and then the performance of the detection algorithm using varying temporal window and SSBD threshold.


                        Table 2 aggregates the average accuracy rates over all ADL classes when our representation scheme was used in a one-subject-against-all scenario for DemCare1, DemCare3, KTH and CHUN dataset. For DemCare1, 64 means combined with HOGHOF+Traj are enough to create a discriminative Fisher encoding scheme. This scheme surpasses the VLAD recognition rates, so we choose this encoding to proceed with activity detection. CHUN and DemCare3, on the other hand performs better when HOGHOF+Traj is encoded with a VLAD encoding scheme. KTH action dataset achieves the highest recognition rates when HOGHOF+Traj is used and 128 means are selected for visual vocabulary.

The effect of the window size W
                        0 is determined empirically, by performing experiments with varying W
                        0 on DemCare1, DemCare3 and MSR datasets. From Fig. 10 it can be seen that when W
                        0 is equal to the frequency of the camera recording rate (
                           
                              F
                              P
                              S
                              =
                              8
                           
                        ), the average accuracy is increased significantly. Lower 
                           
                              
                                 W
                                 0
                              
                              <
                              F
                              P
                              S
                              =
                              8
                           
                         provides the worst results, as the number of samples in SSBD is not sufficient to compute the pdfs, while a larger window 
                           
                              
                                 W
                                 0
                              
                              >
                              F
                              P
                              S
                              =
                              8
                           
                         worsens the average accuracy rates, as the boundaries are coarser and fast changes are missed.

Several experiments were also carried out with SSBD’s sensitivity threshold h over DemCare1, DemCare3 and MSR action datasets (Table 3) in order to evaluate the detection performance of SSBD algorithm. Activity classification after detection over all classes is measured with OV20 (i.e. 20% overlap between the groundtruth and detected intervals), and we report the number of detections and computational time over FPS. Generally we observed a slight decrease in accuracy when h increased, while no variations were observed when h changed from 20 to 50 and then to 100. This is attributed to the fact that a small h can detect more changes, as it increases sensitivity, while large h only finds fast, abrupt ADL changes, so some smoother ADL changes may be missed or detected with delay. This can be also observed in Table 3, where a low h, with 
                           
                              h
                              =
                              10
                              ,
                           
                         leads to nearly 200 detections less than h = 100(i.e. 5% more). This phenomenon does not have any effect on the computational cost of the algorithm for small thresholds (h<100), however, speedup may increase abruptly if no changes are detected for long time, as it is observed in MSR dataset, so we choose to have a low threshold (
                           
                              h
                              =
                              10
                           
                        ) in our experiments instead.

This section provides the results that MBAA achieved as a spatial localization module. Frame per second processing time is also provided to evaluate the method’s computational cost. We achieved a processing time of 0.8732 fps, because the image frames have high resolution. Table 4 shows the accuracy rates when spatial localization was performed on the subset of UCF videos proposed by Tran and Yuan [27] and also used in  [13,15]. As we can see, our algorithm outperforms the literature for almost all activities, except for ride and run which are localized more accurately by Tran and Yuan [27].

Similar results were obtained
                        
                         when all videos in the UCF dataset were taken into account. Our algorithm again outperforms most spatial localization results from the literature, except for the ride and run activities, which require a very robust pretrained detector, as in [27], in order to deal with severe background clutter and camera motion. We thus conclude that our spatial localization algorithm is very accurate, with a very reasonable computational cost, outperforming more sophisticated and cumbersome spatial localization techniques on most activities of the benchmark UCF dataset.

The method proposed here for temporal localization was applied to three datasets of long duration that were recorded during the Dem@Care project at CHUN and GAADRD and on one public, namely MSR action dataset. The SSBD parameters (W
                        0, h) are determined empirically, while vocabulary sizes and encoding schemes are also computed separately for each dataset, so as to acquire optimal results for each of them as described in Section 6.2. Comparisons between sliding window and SSBD algorithm are presented below.

In this section we provide the results that our algorithm achieved when tested on Dem@Care1 dataset.

The recording frequency is 8 fps and our algorithm performance achieved 3.34 and 4.54 fps for simple sliding window and SSBD respectively, which is a near real time achievement considering that for 1.5–2 video frames of the actual video, one frame is processed by our action detection algorithm.


                           Fig. 11 shows the performance of the HOGHOF without SSBD and HOGHOF+Traj with and without SSBD for different Jaccard coefficient values. It can be seen that the inclusion of trajectory information leads to better classification rates than when using simple HOGHOF. The combined use of HOGHOF+Traj with SSBD improves recognition rates by almost 4%, at a lower computational cost, as it achieved 4.5 fps against the 3.34 fps of HOGHOF+Traj without SSBD (Table 5).


                           Fig. 12 shows classification by detection over all classes for the HOGHOF and HOGHOF+Traj descriptors applied to the DemCare action dataset. The OV20 Jaccard coefficient was used as a comparison metric and a Fisher descriptor with 64 cluster centers was picked for the vocabulary size. The results are quite similar for distinct classes (see ER/DB/ES/CU), while great differences are noted in the recognition of static ADLs, where trajectory coordinates make a big difference (see TV/RP/HS), with HOGHOF+Traj outperforming HOGHOF. Other activities (classes), such as SB/PS and SP/EP, located in the same region and including very similar actions, were usually confused and performed quite poorly with both descriptors. SSBD improved accuracy rates in almost all ADL categories, achieving the best recognition rate in our experiments.

Precision–recall curves are also provided for SSBD detection algorithm in Fig. 13, where we can see how our detection algorithm performs over all activities. Some activities are detected with very high average precision score, such as clean up, eat snack, enter room and talk to visitor, while others contain a lot false alarms in their predictions, such as prepare snack, serve beverage and start phonecall leading to a relatively small average precision. In the figure, we also include a precision–recall curve for all detections, where we can see the overall evaluation of our
                            detection algorithm.

In this section we provide the results that our algorithm achieved when tested on Dem@Care1 dataset.


                           Fig. 14 shows the performance of the HOGHOF+Traj with and without SSBD for different Jaccard coefficient values. While SSBD outperforms sliding window for small Jaccard coefficients, a larger overlap between groundtruth and detected intervals (i.e. larger OV number) lead to smaller detection accuracy rates and HOGHOF+Traj without SSBD surpass outperform the proposed approach. This occurs because the recording frame per second is low and SSBD cannot adapt so fast.


                           Fig. 15 shows classification by detection over all classes for the HOGHOF and HOGHOF+Traj descriptors applied to the DemCare3 action dataset. The OV20 Jaccard coefficient was used as a comparison metric and a VLAD descriptor with 128 cluster centers was picked for the vocabulary size. The results are quite similar almost for all classes, with a small favor for SSBD detection algorithm. The only difference is detected on Establish Account Balance (EAB) where sliding window technique outperforms SSBD, as soon as the action is quite static and SSBD cannot find changes for this activity. On the other hand, SSBD outperforms sliding window in short duration activities with large motion variation, such as Turn on Radio (ToR) and Water Plant (WP), leading eventually to higher average accuracy rates.

In this section we provide the results that our algorithm achieved when tested on CHUN action dataset (Fig. 16).


                           Fig. 17 shows the performance of the HOGHOF without SSBD and HOGHOF+Traj with and without SSBD when varying the Jaccard coefficient. For all percentages of overlap, we have a consistent improvement in accuracy with the inclusion of trajectory coordinates in the HOGHOF descriptor, while the use of SSBD leads to better results with both HOGHOF and HOGHOF+Traj, than without SSBD, for all overlap ratios. Considering that SSBD achieves better recognition rates with lower computational cost than the former, we can safely say that it is a better solution. As we can see from Table 6, HOGHOF+Traj SSBD surpasses HOGHOF+Traj without SSBD by almost 2.5% and is almost 6 h faster, analyzing on average almost 4 fps (the half of the camera recording rate, which is 8 fps).

Precision–recall curves are also provided for SSBD detection algorithm in Fig. 18, where we can see how our detection algorithm performs over all activities. In this case, some activities that dominate over others, such as Call Phone does to Answer Phone, leading to a very low Answer Phone score and a relatively high Call Phone. Furthermore, activities that are performed close to the camera like Prepare Drugs and Pay Bill have the advantage of a more accurate appearance and motion descriptor (i.e. HOGHOF) and thus achieve an almost 100% average precision score, while others, which are far away from the camera cannot be distinguished clearly and contain a lot false positives, such as Look on Map. In the figure, we also include a precision–recall curve for all detections, where we can see the overall evaluation of our detection algorithm.


                           Table 7 aggregates the classification accuracy rates when OV20 Jaccard coefficient is used and 
                              
                                 W
                                 =
                                 F
                                 P
                                 S
                              
                            is selected as a temporal window. We can see that SSBD outperforms sliding window not only on classification accuracy rate, but also at a lower computational cost. Boxing is detected at 92.00% accuracy rate, while handclap at 78.57%, outperforming even the state-of-the-art [25]. This is attributed to the fact that this classes dominate in the video sequences (i.e. performed closer to the camera) and their detection is easier than wave.

@&#CONCLUSION@&#

In this work, a novel method for accurate activity detection and recognition is proposed, at a reduced computational cost. Regions of interest, the Motion Boundary Activity Areas (MBAAs), are located by separating pixels undergoing a changing motion from pixels moving constantly, and dense, multi-scale sampling takes place in MBAAs to extract interest points. HOGHOF descriptors characterize the interest points, which are tracked over time using a KLT tracker supplemented by a RANSAC homography outlier estimator. The resulting trajectories are used for an initial, coarse temporal segmentation of activities in long videos. A theoretically founded approach is then applied for detecting changes in motion through Sequential Statistical Boundary Detection (SSBD), which finds changes in activity patterns between current and reference data in an online manner. This leads to refined temporal segmentation of the video sequence, localizing activities in time. SoA encoding techniques are then used in combination with a BoVW framework to recognize the activities taking place, and also introduce the characterization of “ambiguity intervals”, located between recognized activities. In order to evaluate the spatial segmentation accuracy of the MBAAs, comparisons take place with the annotated benchmark data of the UCF dataset, where it is shown that MBAAs accurately localize regions of interest. In order to evaluate the accuracy of the temporal segmentation and recognition, experiments take place with videos recorded in more challenging, real life datasets, recorded in home-like environments from CHUN and DemCare. In these experiments, it can be seen that the proposed method leads to very accurate activity detection and recognition rates at a significantly lowered computational cost.

@&#ACKNOWLEDGMENT@&#

This work was funded by the European Commission under the Seventh Framework Program (FP7 2007–2013), Grant agreement 288199 Dem@Care.

@&#REFERENCES@&#

