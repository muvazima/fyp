@&#MAIN-TITLE@&#Semantic labeling for prosthetic vision

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Semantic labeling is applied to improving navigation for prosthetic vision users.


                        
                        
                           
                           A new semantic labeling dataset using head-mounted camera images is introduced.


                        
                        
                           
                           An improved sparse approach increases accuracy for real-time semantic labeling.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Prosthetic vision

Egocentic vision

Semantic labeling

@&#ABSTRACT@&#


               
               
                  Current and near-term implantable prosthetic vision systems offer the potential to restore some visual function, but suffer from limited resolution and dynamic range of induced visual percepts. This can make navigating complex environments difficult for users. We introduce semantic labeling as a technique to improve navigation outcomes for prosthetic vision users. We produce a novel egocentric vision dataset to demonstrate how semantic labeling can be applied to this problem. We also improve the speed of semantic labeling with sparse computation of unary potentials, enabling its use in real-time wearable assistive devices. We use simulated prosthetic vision to demonstrate the results of our technique. Our approach allows a prosthetic vision system to selectively highlight specific classes of objects in the user’s field of view, improving the user’s situational awareness.
               
            

@&#INTRODUCTION@&#

Low or impaired vision is a common cause of disability, with prevalence rates estimated between 2.7% and 5.8% [1–3]. This represents a global health burden of nearly US$3 trillion [4]. Those with low vision report reduced independence and social function, resulting in lower overall quality of life [5]. Prosthetic vision systems are a type of therapeutic device which aim to enable or improve key functional abilities in users with low vision, such as face recognition, reading and orientation and mobility [6,7]. In general, these devices attempt to replace the function of parts of the human visual system, which may be affected by disease or injury, by providing artificial stimuli to the user based on artificial sensory input [8].

There have been two large scale multi-centre chronic human trials of implantable visual prosthetic devices: The 60 electrode Argus II device [9], and the approximately 1500 electrode alpha-IMS device consisting of microphotodiodes implanted on the retina [10]. In addition, trials are underway by Pixium Vision SA (ClinicalTrials.gov identifier: NCT01864486) and a two year trial has recently been completed by Bionic Vision Australia [11].

The current state-of-the-art in prosthetic vision is limited in terms of functional outcomes for users. For example, the best visual acuity reported from the Argus II retinal prosthesis system is 20/1260 with a 20 degree field of view [9]. This is still well below the 20/200 threshold at which a person is considered legally blind in the United States
                        1
                     
                     
                        1
                        42 U.S.C. §416(i)(1)(B) (Supp. IV 1986)
                     . The number of discrete levels of stimulation intensity that a user can perceive is also limited. In retinal implants, up to ten levels may be discernable [12]. This limits users’ ability to perceive contrast, and in systems where image processing is minimal, functional tests are usually performed in high contrast environments [13,14].

With these limitations, users often cannot interpret complex, uncontrolled scenes. However, computer vision and image processing techniques can be used in some systems to improve functional outcomes [11]. In this paper, we use semantic labeling techniques, which label each pixel in an image with a semantic category, to produce high-contrast stimuli from natural images. Stimuli based on semantic content can allow a user to distinguish objects in their field of view. This has the potential to improve functional outcomes for prosthetic vision users, by enabling navigation in complex environments. We use simulated prosthetic vision techniques, as in [15], to evaluate the resulting stimuli. An example is shown in Fig. 1
                     .

We make three significant contributions in this paper:

                        
                           •
                           We apply semantic labeling to the problem of navigation using prosthetic vision. To our knowledge, this is the first application of semantic labeling in prosthetic vision. We show how semantic pixel labels can be converted to stimulation values so that important semantic distinctions can be made by the user. We demonstrate this in a range of navigation scenarios using simulated prosthetic vision.

We introduce a new dataset for semantic labeling in egocentric vision. We provide semantic labels for 34 frames of the First-Person Social Interactions Dataset [16]. To our knowledge, no existing public egocentric vision datasets include pixel-wise semantic ground truth, and this represents a new and challenging application area for semantic labeling.

We present a fast semantic labeling system which allows near real-time performance (faster than 1 Hz) for use in embedded devices. We improve upon our approach in [17] with a more accurate edge detector for predicting class boundaries, and a new cross-validation method to find selection parameters. We show improved and more detailed results on the CamVid [18,19], KITTI [20–22] and Stanford Background [23] semantic labeling benchmarks.

The paper is structured as follows. In Section 2, we give a review of previous work in vision processing for prosthetic vision, and semantic labeling. In Section 3, we present our framework for the application of semantic labeling in prosthetic vision. We highlight the need for improved vision processing techniques, and explain our model for incorporating semantic labeling. In Section 4, we present our fast semantic labeling system based on sparsely computed unary potentials. This enables the use of semantic labeling in a real-time embedded system, such as a prosthetic vision system.

In Section 5, we introduce our new dataset of manually labelled egocentric video. This allows us to qualitatively evaluate our proposed system in a realistic application scenario, using only images from a head-mounted camera in a dynamic environment. In Section 6, we measure the performance of our fast semantic labeling system on publicly available benchmark datasets. Then, we qualitatively demonstrate our proposed system, incorporating egocentric video, semantic labeling, and simulated prosthetic vision output. We conclude in Section 7.

@&#RELATED WORK@&#

Most current prosthetic vision systems use electrical stimulation to induce phosphenes, which are a sensation of light produced by stimulation other than light [24]. We focus on our work as part of an Australian government funded consortium, Bionic Vision Australia, which has recently conducted human trials showing that suprachoroidal implants are a viable technology for producing visual percepts [11]. Since this and similar systems use an external computer to process images from a camera to produce a stimulation pattern which is then fed to the implant, it is feasible to incorporate more complex computer vision techniques [25].

The potential for computer vision systems to improve functional outcomes in prosthetic vision users is often explored in the literature using simulated prosthetic vision. Simulation has also been used to predict functional outcomes in future devices [26,27]. Simulation allows a wide range of computer vision systems to be developed and tested without requiring implanted devices [6].

The use of abstract stimuli, that is, phosphene patterns not directly representing a downsampled version of a camera image, has been tested in the literature. Some approaches modify the field of view to aid the identification of objects. [28] uses zooming the field of view which is then communicated to the implant to aid in facial recognition, and [29] uses segmentation and zooming to aid in reading signs. For navigation, typically a wide field of view is used, and image processing compensates for poor acuity in this case. Visual saliency [30,31] can be used to highlight potential obstacles or objects of interest. Depth information, from stereo or RGBD cameras, can be used to detect obstacles [32]. [33] uses edges in depth images as well as face detection to produce a simplified high-contrast representation of the environment. Depth-based approaches have been tested using simulation [15,34].

Many state of the art approaches to semantic labeling formulate the problem as inference in a conditional random field (CRF) [18,23,35,36]. From this, much progress has been made towards fast CRF inference [37,38], since the inference stage has traditionally been the most expensive operation in this approach. In particular, work towards fast inference in fully-connected CRFs [39] allows both improved inference speed, and improved modelling of long-range connections, potentially improving labeling quality.

Much recent work in scene parsing focuses on producing high-accuracy labelings using large databases of training data [40–42]. Nearest-neighbor methods may be used for accurate classification of specific objects or matching similar image regions [43]. While using graphical models for patch correspondences [44,45] yields high accuracy, computational costs are still too high. For example, [41] reports an average of 27.5 s per image for their region-based parsing on a six-core 3.4 GHz Intel Xeon processor with 48 GB of RAM.

In this paper, we model the scene parsing problem as a maximum a posteriori (MAP) inference in a conditional random field (CRF) defined over image pixels [46–48]. CRF potentials incorporate local information about a pixel or patch as unary potentials, and smoothness terms to maximize label agreement between similar pixels or patches as pairwise potentials.

Recent work investigating approximate inference in fully-connected CRFs is promising for embedded applications, such as wearable assistive devices. These have been used with some success [49–52] but are limited by the vast complexity of the problem when posed as a fully-connected CRF. This complexity has been reduced recently with work in efficient approximate inference on fully-connected CRFs [39], which can produce a pixel-wise labeling for an image, given precomputed unary potentials, in 0.2 s.

However, with fast inference, calculating unary potentials becomes the dominant time cost in producing a semantic labeling of an image. This has motivated recent work towards reducing the execution time required to calculate unary potentials. A common approach consists of reducing the number of nodes in the graph (thus the number of unary potentials to compute) by using superpixels instead of pixels, at the expense of reducing per-pixel accuracy [53]. With the increasing availability of high-end computing resources, other approaches have focused on optimizing code for specific architectures such as GPUs. For example, in [54] the computation time is reduced with a GPU optimized implementation for computing local descriptors.

In this paper, we further develop our approach from [55] and [17]. In [55], we speed up the segmentation process by reducing the number of unary potentials to be computed based on fixed grid sampling. Although ad-hoc, this naive approach shows competitive performance with a promising reduction in the computational cost. In [56], Roig et al. reduce the computational cost of unary estimation by introducing the Expected Label Change (ELC) ranking. Instead of computing all unary potentials, a subset are selected and computed on the fly, with MAP estimation performed using only this partial information. However, this approach must run full optimization multiple times. In addition, their experiments show reduction of computational cost at the expense of a significant reduction in accuracy.

Our approach in [17] takes advantage of the typical formulation of the Potts model to predict locations where computing unary potentials will most likely improve labeling accuracy. This allows us to make better use of a limited computational budget for each image. Since these locations can be found quickly, our method avoids the increased computational cost of [56] while improving labeling quality over the naive approach of [55]. In this paper, we further develop this approach by incorporating the edge detector of [57] to predict class boundaries, which results in improved placement of computed unary potentials.

We consider a modal prosthetic vision system which can (either manually, or by detecting the scene) be put into an outdoor navigation mode. Confidently navigating a real-world environment unaided is important for quality of life in those with low vision [5,15]. The VF-14 questionnaire [5] in particular identifies “Seeing steps, stairs, or curbs” as a functional vision task which is important for quality of life. We consider producing a stimulus that would be perceived as showing obstacles and possible landmarks to support orientation and mobility in an outdoor environment.

Image processing systems in prosthetic vision take an image as input, typically from a camera placed near the user’s eyes, and must produce a stimulation pattern as output. The nature of this stimulation pattern depends on the specific prosthetic device used, but in general will be a two-dimensional pattern of phosphenes, each fixed at a known location in the visual field of the user, and with an intensity which may be varied.

The stimulation pattern can be thought of as a low resolution, low dynamic range, distorted image, and the phosphene intensities are computed by downsampling a high-resolution input image [11]. Semantic labeling provides a classification of each pixel in an image into a semantic class. This allows us to assign a discrete value to each pixel according to its semantic class.

Current prosthetic vision systems have a low number of discrete levels of stimulation intensity. Up to ten discrete levels of intensity have been shown in current devices [12–14]. However, since this is only reported for some individuals, we take a conservative approach and use only two or three discrete levels.

Many semantic classes may map to the same stimulation intensity, however, the mapping used may be changed according to the task being performed, or the context of the task (such as indoor vs. outdoor). In Section 6.2 we demonstrate mappings. Specifically, we map classes representing obstacles to bright phosphenes, enabling obstacle avoidance. Highlighting detected obstacles based on depth images has been demonstrated [15,58] to assist mobility in simulation of [11], and patients implanted with [59] Bionic Vision Australia’s first retinal prosthesis.

For each pixel in the input image, we map its semantic class to one of a number of intensity values. The resulting intensity map is the same size as the image, but with only a small number of distinct values - we refer to this intensity map as a low dynamic range version of the image. Downsampling these low dynamic range values, instead of pixel intensities, allows us to generate a phosphene pattern based on semantic information. Thus the user can differentiate objects in their field of view semantically, as shown in Fig. 2
                     .

In this paper, we use the simulated prosthetic vision system of [15,32,60] to render example stimulation patterns. This allows us to test what may be perceived by a user of our system without implantation of devices. We present our simulated results using a regular grid as in previous work, however in an actual device, the grid may be perceived as distorted due to variation in the interaction of the device with different patients [11]. We use a 44-channel hexagonal grid and a 256-channel square grid as phosphene arrangements in our examples. The 44-channel hexagonal grid arrangement is designed to simulate a retinal implant device designed by Bionic Vision Australia, which will be used in clinical trials in 2016.

We compute intensity for each phosphene using a two-dimensional Nyquist band-limited Lanczos2 filter over the reduced dynamic range image, centered around the location of that phosphene in the visual field. The Lanczos2 filter has been shown to be beneficial for spatial localization of visual stimuli in prosthetic vision patients [61]. Our sampling method, and the simulated prosthetic vision system we use to render examples, is similar to that used in [15,32,60].

We identify obstacle avoidance and object localization as two distinct but related tasks that semantic labeling can assist with. For obstacle avoidance, we activate phosphenes corresponding to locations of potential obstacles in the visual field, where obstacles are considered a set of semantic classes. A user can thus follow “gaps” between phosphenes to navigate through a crowded environment without using other senses. This obstacle avoidance method alone is potentially a useful enhancement to existing prosthetic vision systems.

For object localization we select a single class to use as a landmark object for maintaining orientation. We can then highlight this in the output phosphenes. Navigating towards a target in a complex environment requires awareness of the position of the target, and any obstacles that may be between the user and the target. A major advantage of our system is that it can detect both the target and surrounding obstacles in the visual field simultaneously. Using only binary phosphenes, we can communicate this information to the user with a bimodal approach [58]. The user can again follow “gaps” in the bottom rows of phosphenes to navigate through a crowded environment to the target, depicted by phosphenes in the upper rows.

We formulate the labeling of pixels as an inference problem in a pairwise conditional random field (CRF). To allow for long-range connections between distant image pixels, we consider the case of a fully-connected pairwise CRF [38,39]. Here, we make use of the approach of [39], which combines a mean field approximation with an efficient Gaussian filtering technique.

More specifically, let 
                           
                              χ
                              =
                              {
                              
                                 x
                                 1
                              
                              ,
                              …
                              
                              ,
                              
                                 x
                                 N
                              
                              }
                           
                         be the set of pixels associated with image 
                           I
                        . Each pixel xi
                         can be assigned a label in the set 
                           
                              L
                              =
                              {
                              1
                              ,
                              …
                              
                              ,
                              L
                              }
                           
                        . The maximum a posteriori (MAP) labeling can be found by computing the assignment that maximizes the distribution 
                           
                              P
                              
                                 (
                                 χ
                                 |
                                 I
                                 )
                              
                              =
                              
                                 1
                                 Z
                              
                              exp
                              
                                 (
                                 −
                                 E
                                 
                                    (
                                    χ
                                    |
                                    I
                                    )
                                 
                                 )
                              
                              ,
                           
                         or equivalently minimizes the energy

                           
                              (1)
                              
                                 
                                    E
                                    
                                       (
                                       χ
                                       |
                                       I
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          i
                                          ∈
                                          N
                                       
                                    
                                    
                                       ψ
                                       u
                                    
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       )
                                    
                                    +
                                    
                                       ∑
                                       
                                          i
                                          ,
                                          j
                                          ∈
                                          N
                                       
                                    
                                    
                                       ψ
                                       p
                                    
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       ,
                                       
                                          x
                                          j
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        
                     

where 
                           
                              N
                              =
                              {
                              1
                              ,
                              …
                              
                              ,
                              N
                              }
                              ,
                           
                         and ψu
                        (·) and ψp
                        (·, ·) denote the unary and pairwise potentials, respectively. The unary potential encodes the cost of assigning a specific label to a pixel, and is computed independently for a subset of pixels by a classifier that produces a distribution over the label assignment of xi
                         given local image features.

To be able to perform efficient inference, the pairwise potential function is constrained to a sum of M weighted Gaussian kernels,

                           
                              (2)
                              
                                 
                                    
                                       ψ
                                       p
                                    
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       ,
                                       
                                          x
                                          j
                                       
                                       )
                                    
                                    =
                                    μ
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       ,
                                       
                                          x
                                          j
                                       
                                       )
                                    
                                    
                                       ∑
                                       
                                          m
                                          =
                                          1
                                       
                                       M
                                    
                                    
                                       w
                                       m
                                    
                                    
                                       k
                                       m
                                    
                                    
                                       (
                                       
                                          v
                                          i
                                       
                                       ,
                                       
                                          v
                                          j
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where μ(·, ·) is a label compatibility function, which, in practice, encodes a Potts model, i.e., 
                           
                              μ
                              
                                 (
                                 
                                    x
                                    i
                                 
                                 ,
                                 
                                    x
                                    j
                                 
                                 )
                              
                              =
                              
                                 1
                                 
                                    [
                                    
                                       x
                                       i
                                    
                                    ≠
                                    
                                       x
                                       j
                                    
                                    ]
                                 
                              
                           
                        . The kernel km
                        (·, ·) is a Gaussian kernel computed over the feature vector v
                        
                           i
                         that describes pixel i, with a scalar weight wm
                        . In particular, we utilize spatial smoothness and appearance similarity kernels

                           
                              (3)
                              
                                 
                                    
                                       k
                                       1
                                    
                                    
                                       (
                                       
                                          v
                                          i
                                       
                                       ,
                                       
                                          v
                                          j
                                       
                                       )
                                    
                                    =
                                    exp
                                    
                                       (
                                       −
                                       
                                          
                                             
                                                ∥
                                             
                                             
                                                p
                                                i
                                             
                                             −
                                             
                                                p
                                                j
                                             
                                             
                                                
                                                   ∥
                                                
                                                2
                                             
                                          
                                          
                                             σ
                                             p
                                             2
                                          
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    
                                       k
                                       2
                                    
                                    
                                       (
                                       
                                          v
                                          i
                                       
                                       ,
                                       
                                          v
                                          j
                                       
                                       )
                                    
                                    =
                                    exp
                                    
                                       (
                                       −
                                       
                                          
                                             
                                                ∥
                                             
                                             
                                                p
                                                i
                                             
                                             −
                                             
                                                p
                                                j
                                             
                                             
                                                
                                                   ∥
                                                
                                                2
                                             
                                          
                                          
                                             σ
                                             l
                                             2
                                          
                                       
                                       −
                                       
                                          
                                             
                                                ∥
                                             
                                             
                                                I
                                                i
                                             
                                             −
                                             
                                                I
                                                j
                                             
                                             
                                                
                                                   ∥
                                                
                                                2
                                             
                                          
                                          
                                             σ
                                             c
                                             2
                                          
                                       
                                       )
                                    
                                    ,
                                 
                              
                           
                        where v
                        
                           i
                         is the concatenation of pixel location vector p
                        
                           i
                         and color intensity vector I
                        
                           i
                        .

The efficient approximate inference approach we use requires unary potentials ψu
                        (xi
                        ) for each pixel xi
                        . In [36] and [35], these potentials are computed using a classifier for each pixel xi
                        , however this computation can be expensive, taking one or more seconds per image. Instead we compute unary potentials sparsely, and produce an approximation of the unary potentials, 
                           
                              
                                 
                                    ψ
                                    ^
                                 
                                 u
                              
                              
                                 (
                                 
                                    x
                                    i
                                 
                                 )
                              
                           
                        .

We perform a nearest-neighbor interpolation over unary potentials. That is, for a set of computed unary potential locations 
                           
                              U
                              ,
                           
                         we set 
                           
                              
                                 
                                    ψ
                                    ^
                                 
                                 u
                              
                              
                                 (
                                 
                                    x
                                    i
                                 
                                 )
                              
                           
                         according to

                           
                              
                                 
                                    
                                       
                                          ψ
                                          ^
                                       
                                       u
                                    
                                    
                                       (
                                       
                                          x
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       {
                                       
                                          
                                             
                                                
                                                   
                                                      ψ
                                                      u
                                                   
                                                   
                                                      (
                                                      
                                                         x
                                                         i
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                
                                                   if
                                                   
                                                   i
                                                   ∈
                                                   U
                                                
                                             
                                          
                                          
                                             
                                                
                                                   
                                                      ψ
                                                      u
                                                   
                                                   
                                                      (
                                                      
                                                         x
                                                         
                                                            i
                                                            ′
                                                         
                                                      
                                                      )
                                                   
                                                   
                                                   where
                                                   
                                                   
                                                      i
                                                      ′
                                                   
                                                   =
                                                   
                                                      
                                                         arg
                                                         min
                                                      
                                                      
                                                         j
                                                         ∈
                                                         U
                                                      
                                                   
                                                   Δ
                                                   
                                                      (
                                                      i
                                                      ,
                                                      j
                                                      )
                                                   
                                                
                                             
                                             
                                                
                                                   if
                                                   
                                                   i
                                                   ∉
                                                   U
                                                
                                             
                                          
                                       
                                    
                                    ,
                                 
                              
                           
                        where Δ(i, j) is the Manhattan distance between pixels i and j in the two-dimensional pixel coordinate space. In the case where there is more than one solution to 
                           
                              
                                 i
                                 ′
                              
                              =
                              
                                 
                                    arg
                                    min
                                 
                                 
                                    j
                                    ∈
                                    U
                                 
                              
                              Δ
                              
                                 (
                                 i
                                 ,
                                 j
                                 )
                              
                              ,
                           
                         i.e. pixel i is equidistant to the closest pixels 
                           
                              j
                              ∈
                              U
                              ,
                           
                         we select one at random. For this application, we find directly computing the interpolation in this manner is sufficiently fast for a small number of computed unary potential locations (
                           
                              |
                              U
                              |
                              ≤
                              1000
                           
                        ).

We can see that Eq. (4) leads to large energy whenever two nearby points have a different label and are not separated by an intensity boundary. This captures the basic assumption that class boundaries typically occur at intensity boundaries. Thus, in the usual case where unary potentials are available densely, one expects most unary points would be labeled with their correct class on either side of the boundary, and hence the CRF would preserve these correct labels and smooth the occasional incorrect label.

Accordingly, in our case, we should find a set of points 
                           U
                         that emphasize accurate interpolated unary potentials 
                           
                              
                                 
                                    ψ
                                    ^
                                 
                                 u
                              
                              
                                 (
                                 
                                    x
                                    i
                                 
                                 )
                              
                           
                         on either side of intensity boundaries, to allow the CRF to smooth the labeling according to image content. In the case of [55] the unary potentials are sampled on a grid, and interpolated, with no regard for intensity boundaries. This interpolation will lead to any pixel’s label being most influenced by the nearest point at which a unary term is computed. Clearly it would be ideal if, for every term where k
                        2(v
                        
                           i
                        , v
                        
                           j
                        ) is large, we have unary terms to compensate for the small and unreliable influence of the pairwise terms. This could be achieved by sampling along intensity boundaries with unary terms, and relying on interpolation to provide unary potentials for the remainder of the image.

However, unary potentials are typically predicted poorly near intensity boundaries, not only because features in these regions can be ambiguous, but also because the precise location of the true class boundary may be ambiguous in manually labelled ground truth images. In the usual case of densely computed unary potentials, this is a minor problem, since the majority of pixels in a region will typically be assigned correct unary potentials, and the CRF inference can effectively use these correct classifications to resolve ambiguities within regions.

In our case, ideally, we would like to evaluate a unary term at a pixel where k
                        2 can be large. That means that, for some pair of pixels i and j in the same image, 
                           
                              
                                 ∥
                              
                              
                                 p
                                 i
                              
                              −
                              
                                 p
                                 j
                              
                              
                                 ∥
                              
                           
                         and 
                           
                              
                                 ∥
                              
                              
                                 I
                                 i
                              
                              −
                              
                                 I
                                 j
                              
                              
                                 ∥
                              
                           
                         are small relative to their respective σ. Here, if their labels are different, their energy should be large. Using a regular grid to compute unary potentials [55], most large regions where there are no intensity boundaries will be well-sampled, so classes that are large in the image such as road and sky will generally be handled well. However, near edge boundaries, such points may have been interpolated to the same unary potential value. Hence, we should sample within any image region where k
                        2 is large to avoid incorrect unary labeling.

Since we expect this to be the case near intensity boundaries, we can find these regions efficiently by extracting intensity edges from the image as pre-processing, and sampling around them. Where best to sample depends on the reliability of the unary classifier around intensity boundaries.

Since the final output of our system is a small number of phosphene intensities, it may seem intuitive to compute unary potentials at image locations corresponding to positions of phosphenes in the visual field. However, phosphene locations do not, in general, coincide with accurate unary potentials. Rather, to maintain accurate and consistent phosphene intensities, we compute unary potentials where the classifier is most likely to give an accurate result. The application of Lanczos2 filtering during downsampling of the low dynamic range image ensures each phosphene’s intensity value is drawn from a large region of spatial support about the phosphenes location. Thus, we do not consider the arrangement of phosphene locations when computing unary potentials. Future work may consider using the phosphene arrangement to inform classifier locations, ideally using data from existing prosthetic devices and patient trials.

We use an empirical approach to find the optimal placement of unary potential sampling points. We parameterize the problem as a balance between placement along the edge itself, the distance from the edge at which unary terms become reliable, and how far from the edge we should keep evaluating these. We also allow unary terms to be allocated sparsely in regions far from intensity edges, by a ratio between the number of points allocated to near-edge and far-from-edge regions. Our approach will attempt to optimize performance (measured by mean per-class accuracy) by varying the placement pattern relative to edges, and ratio of sampling location count between regions.

We group image pixels into three regions; for dense (
                           
                              R
                              
                                 D
                                 E
                                 N
                                 S
                                 E
                              
                           
                        ), sparse (
                           
                              R
                              
                                 S
                                 P
                                 A
                                 R
                                 S
                                 E
                              
                           
                        ), and no sampling (
                           
                              R
                              
                                 N
                                 O
                              
                           
                        ); based on the pixel’s distance from the nearest image edge. To detect edges, we use the structured forest edge detector of [62]. We then compute a Euclidean distance map as described in [63]. We use D(i) to denote this distance map - this is the minimum Euclidean distance from a pixel i to an edge pixel. Our method relies in part on using the distance map D(i) to predict local unary classifier performance. Fig. 3
                         shows accuracy of the unary classifier as a function of D(i), as computed using different edge detectors including ground truth class boundaries.

Thus the regions are defined by thresholding this distance map with two threshold values d
                        
                           t1, d
                        
                           t2 ≥ 0:

                           
                              
                                 
                                    
                                       
                                          
                                             No
                                             
                                             sampling
                                          
                                       
                                       
                                          
                                             
                                                R
                                                
                                                   N
                                                   O
                                                
                                             
                                             =
                                             
                                                {
                                                
                                                   i
                                                   |
                                                   D
                                                   
                                                      (
                                                      i
                                                      )
                                                   
                                                   <
                                                
                                                
                                                   d
                                                   
                                                      t
                                                      1
                                                   
                                                
                                                }
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             Dense
                                             
                                             sampling
                                          
                                       
                                       
                                          
                                             
                                                R
                                                
                                                   D
                                                   E
                                                   N
                                                   S
                                                   E
                                                
                                             
                                             =
                                             
                                                {
                                                
                                                   i
                                                   |
                                                
                                                
                                                   d
                                                   
                                                      t
                                                      1
                                                   
                                                
                                                ≤
                                                D
                                                
                                                   (
                                                   i
                                                   )
                                                
                                                <
                                                
                                                   d
                                                   
                                                      t
                                                      2
                                                   
                                                
                                                }
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             Sparse
                                             
                                             sampling
                                          
                                       
                                       
                                          
                                             
                                                R
                                                
                                                   S
                                                   P
                                                   A
                                                   R
                                                   S
                                                   E
                                                
                                             
                                             =
                                             
                                                {
                                                
                                                   i
                                                   |
                                                
                                                
                                                   d
                                                   
                                                      t
                                                      2
                                                   
                                                
                                                ≤
                                                D
                                                
                                                   (
                                                   i
                                                   )
                                                
                                                }
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

If real-time labeling is desired, then we have a known, fixed maximum amount of time to produce a labeling for each image. We expect inference time to be fixed in the case where all images have the same number of pixels N. Thus we can subtract this time from the total per-image time budget to give an upper bound limit for the time available to compute unary potentials (making the assumption that other operations required for labeling require relatively negligible time). From this we can determine a fixed number of unary potentials which can be computed per frame. We allocate n unary potentials to the union of 
                           
                              R
                              
                                 D
                                 E
                                 N
                                 S
                                 E
                              
                           
                         and 
                           
                              R
                              
                                 S
                                 P
                                 A
                                 R
                                 S
                                 E
                              
                           
                        . The direct speed advantage that this unary potential budget approach yields is also described in [55].

We use a ratio parameter, r, to determine how the budget, n, is to be divided between 
                           
                              R
                              
                                 D
                                 E
                                 N
                                 S
                                 E
                              
                           
                         and 
                           
                              R
                              
                                 S
                                 P
                                 A
                                 R
                                 S
                                 E
                              
                           
                        . We select up to 
                           
                              n
                              (
                              1
                              −
                              r
                              )
                           
                         pixels from 
                           
                              R
                              
                                 D
                                 E
                                 N
                                 S
                                 E
                              
                           
                         at regular intervals - if there are fewer than 
                           
                              n
                              (
                              1
                              −
                              r
                              )
                           
                         pixels in the dense region, we simply select all of 
                           
                              R
                              
                                 D
                                 E
                                 N
                                 S
                                 E
                              
                           
                        . Then we select the remainder of the budget at regular intervals from 
                           
                              R
                              
                                 S
                                 P
                                 A
                                 R
                                 S
                                 E
                              
                           
                        . Selecting pixel indices at regular intervals encourages a uniform spatial distribution of sample points within regions. This sampling is illustrated in Fig. 4
                        
                        .

We can now set the parameters d
                        
                           t1, d
                        
                           t2, and r to tune the performance of our unary potential selection method. With fast inference and pre-computed unary potentials, it is feasible to test a large number of parameter combinations rapidly and find the optimal parameters for a given validation set. We can also separately optimize unary potential selection for different outcomes, such as improving accuracy over certain classes.

We use a leave-one-out cross-validation approach to find the optimal parameters for each image in the parameter training set independently. For each image, we find the parameters which maximize accuracy over the remainder of the parameter training set. We can then select parameters to use over the test set as the mode of parameters selected over the validation.

Prosthetic vision systems which use an external camera typically place the camera near the user’s eyes. The camera is thus subject to natural movement of the user’s head, and captures a variety of environments that the user passes through, which may be indoor or outdoor, artificial or natural.

Current semantic labeling benchmarks do not adequately capture typical egocentric vision (i.e. head-mounted camera) scenarios. Vehicle-mounted cameras [18,21] do not exhibit the range of motion that a head-mounted camera typically experiences (in particular, pointing up and down), and typically move along roads free of static obstacles. Curated image databases {CITEGouldBplusal:ICCV09,Shotton:2009:TextonBoost typically use still photographs, often from photo sharing websites such as Flickr or Picasa. These show a diverse range of environments and points of view but are typically centred on a subject, and rarely show cluttered environments which present a navigational challenge. These datasets have been shown to not generalize well to egocentric vision [64], and thus are of limited use for benchmarking in a prosthetic vision context.

Recent work in egocentric vision has focused on tasks such as action recognition [65], video summarization [64], and social interaction recognition [16]. However, to our knowledge, no public datasets with pixel-wise semantic labels on egocentric video exist. We manually produce our own semantic labels on an existing egocentric video dataset in order to demonstrate our system in a context applicable to prosthetic vision.

We use video sequences from the first-person social interactions dataset of [16]. The dataset contains more than 42 h of video recorded by 8 subjects. The video is captured at a resolution of 1280 × 720, with 15 frames per second extracted. The dataset is manually annotated with time intervals labelled with categories of social interaction. The video was recorded at theme parks, presenting a challenging real-world environment.

From this, we take only subsequences which are annotated as “walking”, in order to capture the process of visually navigating complex environments. We selected two subsequences to label manually. We label one in every 15 frames, corresponding to one frame per second. A typical manually labelled frame is shown in Fig. 5.

The set of semantic classes was designed according to the content of the sequences, and relevance for our applications of obstacle avoidance and object localization. We select classes which are likely to present obstacles (“fence”, “wall”), indicate a safe region to navigate through (“Path”), or are likely to be useful as landmarks (“Garbage can”, “Door”). The classes, and how they may be considered relevant to navigation, are listed in Table 1
                     . Note that while we list typical roles for these classes, this is dependent on the situation. For instance, an individual person may be a desired navigation or localization target, but a dense crowd of people may often be considered an obstacle.

@&#RESULTS@&#

We evaluate the quantitative performance of our method against the CamVid [18,19], KITTI [20–22] and Stanford Background [23] publicly-available semantic labeling benchmarks in Section 6.1. We also validate our method by performing the same analysis on our dataset from Section 5, but in this case, we reuse training images to test, due to the small number of labelled ground truth images available.

Since we are introducing the idea of improving performance within pixel-wise sparse unary potentials through point selection, there is little work in the literature to directly compare with. We primarily use the sparse grid approach of [55] as a baseline for comparison, since this is directly comparable in terms of computational cost. We also compare with dense unary potentials - which is equivalent to the method of [39] - to illustrate the tradeoff of accuracy vs. computation time that our method represents. Other methods based on sparsely computed unary potentials, such as [56], typically use inference over superpixels rather than individual pixels. Finding a sufficiently accurate oversegmentation method, and computing unary potentials for superpixels rather than pixels, each represent distinct accuracy vs. computation tradeoffs. Thus, we consider a fair comparison with superpixel methods outside the scope of this paper, but an interesting topic for further work.

We present results from our phosphenization methods on our egocentric dataset, as well as images from CamVid and KITTI, demonstrating a range of navigation scenarios, in Section 6.2.

Quantitative evaluation is performed using pixelwise comparisons of the obtained segmentations with ground-truth. For each dataset, we employed the training set to learn pixel-level classifiers. These classifiers were used to generate unary potentials on the remaining validation and test images.

In order to test our approach, we must select parameters (d
                        
                           t1, d
                        
                           t2, r), and report performance over a test set of images. Parameters are selected by testing overall performance on a set of labelled images (which we refer to as the parameter training set). We describe how we split the datasets for these tasks in Table 2
                        . We selected the CamVid and KITTI datasets to demonstrate our method on existing real-world video benchmarks. The images in these datasets are captured from vehicle-mounted cameras. We also selected the Stanford Background dataset as a diverse set of natural images. This dataset uses images from a range of existing sources (LabelMe, MSRC, PASCAL VOC, Geometric Context datasets), where all images are of outdoor scenes.

We want to avoid the case where the boundaries of small objects (which may represent trip hazards) are not captured accurately. Our sampling and interpolation may cause pixels near object boundaries to be labeled incorrectly. For an individual class which typically manifests as small image regions, the number of pixels which should be positively classified is a small proportion of the total number of image pixels. Thus, if we use a traditional formulation [66,67] of per-class accuracy, i.e. the proportion of correctly labelled pixels (
                           
                              
                                 TP
                                 +
                                 TN
                              
                              N
                           
                        ), we risk biasing our method away from finding accurate boundaries for small objects.

We follow the method suggested in [67] for per-class measures. We compute both the intersection-over-union [68], and F
                        1-measure, for each class, for each image. Reported per-class accuracy values are these values, averaged over all images in the test set.

Intersection-over-union (or Jaccard Index) is computed as:

                           
                              
                                 
                                    Class
                                    
                                    accuracy
                                    =
                                    
                                       
                                          True
                                          
                                          pos.
                                       
                                       
                                          True
                                          
                                          pos.
                                          +
                                          False
                                          
                                          pos.
                                          +
                                          False
                                          
                                          neg.
                                       
                                    
                                 
                              
                           
                        
                     

While F
                        1-measure is computed as:

                           
                              
                                 
                                    
                                       F
                                       1
                                    
                                    =
                                    
                                       
                                          2
                                          ×
                                          Precision
                                          ×
                                          Recall
                                       
                                       
                                          Precision
                                          +
                                          Recall
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Note that the labeling of each class, on each image, is considered separately as a binary classification problem, then results are averaged over classes and images. We use the intersection-over-union accuracy as our main metric of comparison, and select parameters to maximize this when training our method.

In our experiments, we compare the accuracy of our semantic labeling approach with the results obtained by using all computed unary potentials and a fully-connected CRF over each individual image [39]. For this baseline, and for our method, the parameters of the CRF were fixed to the values reported in [39]. We also compare with the method of [55], which also uses sparse unary potentials. When comparing sparse methods, we allocate the same budget of unary potentials (and thus computation time) to each.

We use the Darwin software library [69] to train and generate unary potentials, as described in [70]. A one-versus-all boosted decision tree classifier is learned for each class from the unary training images. The input to the boosted classifiers are 669-element per-pixel feature vectors comprised of 17-dimensional filter bank responses, dense HOG descriptors, RGB color, and the normalized x and y coordinates of the pixel. The unary potentials for a pixel are computed as the negative log probability of each label for that pixel, according to the classifier results.

However, our method is independent of the specific pixel classifier, and any method to assign label probabilities to pixels could be used here. Given a fixed time per frame, a more computationally expensive classifier could be used, with a corresponding reduction in the number of unary potentials that can be directly computed.

We show the pixel classifier performance as a function of the distance from edge, for both CamVid and KITTI datasets, in Fig. 6
                           . We also show values of parameters d
                           
                              t1 and d
                           
                              t2 selected by our method. We see that for both datasets, classifier performance is poor when d < d
                           
                              t1, improves rapidly between d
                           
                              t1 and d
                           
                              t2, and remains high when d > d
                           
                              t2. This confirms our hypothesis that selecting locations to compute unary potentials in this way may enable improved performance with a limited unary budget, since we can consistently avoid regions where the classifier performance is poor.

We vary the parameters d
                           
                              t1, d
                           
                              t2, and r, over the parameter training images. The parameters are bounded: d
                           
                              t1 > 0; d
                           
                              t2 ≥ d
                           
                              t1; 0 ≤ r ≤ 1. In order to maintain d
                           
                              t2 ≥ d
                           
                              t1, we vary the difference 
                              
                                 
                                    d
                                    
                                       t
                                       2
                                    
                                 
                                 −
                                 
                                    d
                                    
                                       t
                                       1
                                    
                                 
                                 ≥
                                 0
                              
                            - this difference corresponds to the width of the dense sampling region. We select a range of 6 values for d
                           
                              t1, and 9 values each for 
                              
                                 
                                    d
                                    
                                       t
                                       2
                                    
                                 
                                 −
                                 
                                    d
                                    
                                       t
                                       1
                                    
                                 
                              
                            and r, giving a total of 486 test configurations. We can test all configurations, over 15 images, in less than 2.5 h (including precomputing unary potentials) on a single workstation, without significant optimization.

For each dataset and unary budget, we select parameters to test with by running leave-one-out cross-validation over the parameter training set. For each frame in the parameter training set, we generate a new split by removing that frame. We find parameters which maximize intersection-over-union accuracy for each split, and consider this a vote for the parameters to use over the test set.

We show parameter selection results in Table 3
                           , over three datasets and four values of n. For each dataset, for each value of n, we show all parameter combinations which were voted for by at least one split, and the resulting per-class accuracy over the test set. We also compare with results of grid-based selection [55].

Note that in all but one configuration (Stanford at 
                              
                                 n
                                 =
                                 200
                              
                           ), more than half of the splits vote for the same parameter combination. This suggests that we can calibrate our approach to be effective over a range of images. It is interesting to note that in some configurations (KITTI and Stanford at 
                              
                                 n
                                 =
                                 50
                              
                           ) this method is better than the grid method in terms of F
                           1 accuracy, but not intersection-over-union. This suggests that in these cases of very few computed unary potentials per image, our method finds solutions which better take contours into account [67]. For CamVid and Stanford at 
                              
                                 n
                                 =
                                 50
                                 ,
                              
                            the best result with our method is not found by the parameter combination with the most votes. However, in these cases, both proposed parameter combinations are better than the grid method by F
                           1 accuracy (in CamVid’s case, by both accuracy measures). Our method does perform relatively poorly on KITTI where n < 500, which may be due to the small number of labeled images used for testing (only 31, compared with 128 for Stanford and 156 for CamVid) - since we average results over images, a poor result on a single image will have a larger effect in this case.

For our dataset, introduced in Section 5, we perform similar cross-validation. However, due to the small number of labeled images in this dataset, we do not further split this to separately train a pixel classifier. Instead, we produce unary potentials directly from ground truth, in order to test our selection method independently of the accuracy of the pixel classifier. We train a pixel classifier and test on unseen images in Section 6.2.

We present accuracy over this cross-validation, using best-case unary potentials, in Table 4
                           . Note that our edge-based method relies in part on the accuracy of the pixel classifier varying over the image, which is not the case here. As such, in this setup, our edge-based method is only better than grid selection for n ≤ 100.

We explore performance for individual classes in Table 5
                           . Here we can see which classes the edge-based method performs better on. CamVid’s “pedestrian” and “sidewalk”, KITTI’s “pavement”, and Stanford’s “tree”, “grass” and “mountain” classes all show improvement compared with the grid method, with both accuracy measures, for all values of n. For other classes, accuracy changes as n and parameters change. CamVid’s “car” and “tree”, KITTI’s “fence”, and Stanford’s “water” classes all show improved accuracy with our edge-based method for smaller values of n, but grid sampling gives better results above a certain value of n.

Accuracy is lower overall with the edge-based method for CamVid’s “building” and “sky”, and KITTI’s “vegetation” and “road” classes. In the context of these datasets, these classes typically take up large image regions, and so our method may sacrifice some accuracy for these classes in order to gain more on small objects. Since these larger classes comprise a larger proportion of pixels, this can result in lower accuracy reported per-pixel overall. We use per-class accuracy since correct classification of obstacles and other hazards is more important in a prosthetic vision context.

Performance on narrow objects (“column_pole” in CamVid and “pole” in KITTI) is generally poor with sparse methods. This can also be seen in Fig. 7
                           . The setting of d
                           
                              t1 will impact performance on such objects. By excluding pixels close to edges from direct classification, we may completely exclude objects narrower than 2d
                           
                              t1 pixels. However, performance is also poor when unary potentials are densely computed, which suggests the pixel classifier performs poorly on these objects, so the overall impact on accuracy by excluding these objects is minimal.

We also compute results by considering all pixels without averaging over images in Table 6
                           . Here we compute accuracy as in [66], reporting the total proportion of correctly classified pixels, as well as the proportion of correctly classified pixels per class, averaged over all classes in the dataset. We use the same parameters for our edge-based method here as in Table 5, so we can compare these results with the per-class statistics directly.

Note that while the overall per-pixel and per-class accuracy suggests the grid method is superior in most cases, we have shown that our edge-based method improves results particularly for classes which typically manifest as small image regions.

Since this overall per-class accuracy is computed as:

                              
                                 
                                    
                                       Class
                                       
                                       accuracy
                                       =
                                       
                                          
                                             True
                                             
                                             pos.
                                             +
                                             True
                                             
                                             neg.
                                          
                                          
                                             Total
                                             
                                             pixels
                                          
                                       
                                       ,
                                    
                                 
                              
                           a class with only a small number of positive samples (i.e. manifesting as small regions in images) will rarely have a large impact on the overall per-class accuracy, since even if the class is not detected at all, the large number of true negative samples means the reported class accuracy is still high.

Overall, these results suggest that the best sampling method to use depends on the application. Edge-based sampling improves performance on small or narrow objects compared with grid sampling, particularly with low budget n. However, if the computation budget allows a larger number of unary potentials to be computed, or if the application requires accurate segmentation of large, distinct image regions, then grid sampling may be superior. We use edge-based sampling for the prosthetic vision application for the improved performance on small objects, and because we expect strict computation limits in a wearable device.

We show a qualitative comparison of our method with grid sampling, and dense unary potentials, in Fig. 7. Note here that our method finds the contour of the sidewalk in CamVid more accurately (similarly to the dense unary result) than grid sampling.

We report measured execution time for CamVid frames (11 classes, 320 × 240 pixels) in Table 7
                           . “Features” refers to time to compute features over the entire image, such as HOG, which does not scale with number of unary potentials computed. Times were measured and averaged over multiple trials with all tasks running in a single thread on a standard Intel ®Core ™2 Quad Q9400 CPU 2.66GHz desktop. Performance could be further improved by taking advantage of GPU technology as in [54]. We plan to explore this in future work by implementing our pipeline on an embedded GPU system.

We produce visual percepts from semantic labels produced by our method. In this paper, we use the simulated prosthetic vision system of [15,32,60] to render example stimulation patterns. Here we demonstrate different modes of operation and show that in these scenarios, semantic labeling may improve a user’s situational awareness.

We use the optimal sampling parameters shown in Table 3 for our method on the CamVid and KITTI datasets. For our egocentric dataset, we use 
                           
                              
                                 d
                                 
                                    t
                                    1
                                 
                              
                              =
                              4
                              ,
                           
                        
                        
                           
                              
                                 d
                                 
                                    t
                                    2
                                 
                              
                              =
                              12
                           
                         and 
                           
                              r
                              =
                              0.4
                              ,
                           
                         which was found to give qualitatively good results.

We detect obstacles by selecting a subset of semantic classes which do not correspond to freely navigable space, and activating phosphenes which correspond to occurrences of these classes. We list semantic classes we consider obstacles in Table 8
                           . Reducing the set of all semantic classes into a binary indicator of navigability in this way allows a user to navigate their environment even in the case where only binary phosphenes are available.

Our obstacle detection and phosphenization is demonstrated in Fig. 8
                           . A user would be able to use these stimuli to identify safe areas to walk, by following gaps between activated phosphenes.

In Fig. 9
                            we combine our object detection with localization using a bimodal representation - highlighting nearby obstacles in the bottom 3 rows of phosphenes, and the location of the garbage can in the remainder of the visual field.

In Fig. 10
                            we demonstrate both obstacles and the localization target using multiple levels of phosphene intensity. We also demonstrate another advantage of the semantic representation - we can manipulate the appearance of classified objects to improve visibility. The garbage can only occupies a small area in the visual field, and directly binarizing its outline in this case is not sufficient to activate a phosphene. We can dilate the outline of the object to increase its apparent size to ensure its visibility in the resulting phosphene pattern after downsampling. This can be performed dynamically, so even smaller objects in the visual field could be dilated more to ensure that at least one phosphene is activated. Thus the ability of the user to localize a distant object is limited by the camera resolution and semantic labeling performance, rather than the phosphene density or field of view.

Detecting boundaries between semantic classes can also be important for confident visual navigation. Curbs, which we can detect as a boundary between “road” and “sidewalk” classes, can be a trip hazard. These trip hazards can be detected using a depth approach as in [15], but there are many similar cases where a user may need to be aware of class boundaries, for example, following marked paths such as pedestrian crossings, or following a garden path without stepping on grass.

We show an example of curb detection in Fig. 11
                           . We detect the curb by dilating the outline of the “sidewalk” class, then finding the intersection of this with the “road” class. We show this, along with obstacles, with high intensity phosphenes, and the “road” class with low intensity phosphenes. This way, the user can be aware of the location of the curb, and which side of the curb is the road.

@&#CONCLUSION@&#

We have demonstrated, to our knowledge, the first application of semantic labeling to prosthetic vision. Using semantic labeling, we can produce high-contrast stimuli which give a much clearer signal for navigation than the traditional intensity downsampling approach. Further, semantic labeling allows identification and localization of specific categories of objects, which depth-based approaches cannot provide. We have demonstrated this using our semantically labelled egocentric vision dataset.

Real time performance is achieved by sparsely computing unary potentials. Furthermore, accuracy is improved by selecting locations for computing unary potentials, and we can train our method using a validation set of images to yield improved accuracy for a specific environment or task. We have demonstrated that it is feasible to improve accuracy this way with negligible increase in computation time. However, improvements in accuracy are not consistent over our tested datasets, and more work towards selecting locations to compute unary potentials may yield further improved results.

Further work must be done to develop our work in this paper into a viable assistive device. Accuracy of semantic labeling, even in offline systems where computation is less restricted, is still not sufficient for a device which may be relied upon for safety. A multimodal approach, incorporating other sensors such as depth cameras, stereo cameras, or inertial measurement for pose estimation, may enable improved performance. An RGBD or stereo system would allow a semantic labeling system to be used in tandem with a depth-based approach such as [15], improving likelihood of obstacle detection.

Our semantic labeling method relies on an accurate pixel classifier for unary potentials. Our sparse approach should allow the exploration of more computationally expensive pixel classifiers in real time systems to improve accuracy. Furthermore, our simple interpolation method only respects image boundaries insofar as the selected sample points do, and future work may explore the application of higher dimensional interpolation (using, for example, FLANN [43]). It may also be possible to improve upon interpolation by replacing or combining it with a faster, but less accurate classifier for the majority of pixels.

@&#ACKNOWLEDGMENTS@&#

NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program. This research was supported by the ARC through its Special Research Initiative (SRI) in Bionic Vision Science and Technology grant to Bionic Vision Australia (BVA).

@&#REFERENCES@&#

