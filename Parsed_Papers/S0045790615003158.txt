@&#MAIN-TITLE@&#A virtualization approach to develop middleware for ubiquitous high performance computing

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Uware is a virtualization-based approach to effectively improve the UHPC application design.


                        
                        
                           
                           Uware is an alternative approach to minimize the overhead and to develop a middleware.


                        
                        
                           
                           The approach relies on contention management and zero copy buffer mechanism to from isolation model.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Middleware

Ubiquitous high performance computing

Virtualization

Concurrency control

@&#ABSTRACT@&#


               
               
                  In ubiquitous high-performance computing (UHPC), performing concurrent services is an important task of middleware. Because a major development effort is not easily achieved, isolating a virtual machine (VM) may be a helpful solution but will likely suffer from additional overhead costs. This challenge can also be resolved by reusing the device driver. However, these solutions are difficult to implement without the VM technique. In this study, we present an alternative approach to minimizing overhead and develop a middleware called userspace virtualized middleware (Uware). Instead of bypassing the VM, the proposed approach depends on userspace transparency and contention management to shift the VM concept into middleware. We introduce two strategies to enhance the system adaptively: comprehensive restructuring by simplifying the VM memory mechanism and implementing zero-copy buffers to reuse devices. The result demonstrates that Uware is feasible and could be applied in a broad variety of UHPC.
               
            

@&#INTRODUCTION@&#

Ubiquitous high-performance computing (UHPC) is an I/O-intensive application that contains various activities to support intelligent services. Currently, various services and devices are required to perform service consolidation; this causes an integration issue for data communication [1]. This issue becomes a challenge to ease platform integration with various heterogeneous devices. Many studies have been proposed to solve this challenge because they always result in their own weaknesses. For example, middleware can provide central control on data distribution services. These elastic middleware methods have been shown to improve throughput, including component-based, content-aware, and qos-aware software [2]. Though they are effective at dispatching services, they regularly need to be re-written to resolve conflicts and are dependent on operating system (OS) services. A customized OS [3] could provide an adaptive environment, though customized benefits typically have a large implementation overhead. Two major implications of this challenge must be solved. First, full reuse of a driver is difficult to achieve. In addition, undocumented or, in the worst-case scenario, incorrectly documented OS behavior makes driver reuse with a fully emulated execution environment questionable. The second implication of the large fraction of driver code in a mature OS is the extent of driver crashing events. Conversely, this challenge could be addressed by using a virtual machine (VM), which can improve concurrency and allow high-performance computing. However, if middleware closely couples an application and its virtualization, a dominant factor to decrease performance is I/O activity [4]. This factor occurs and may be over-provisioned in both memory and computation resources. Although high-speed interconnects, such as InfiniBand, bring high-performance I/O into the realm of virtualization, more adaptive infrastructures are still needed to support UHPC; thus, the observed experimental results in a wide spectrum of current studies have proposed to improve performance. For example, Spear et al. [5], used a comprehensive strategy for contention management to provide fairness and good throughput in transactional memory. Liao et al. [6], proposed a new server I/O architecture using extended on-chip direct memory access (DMA) descriptors. As reported in these studies, we discuss several intuitive solutions and find that the following issues are required for UHPC: 
                        
                           1.
                           
                              Resource modeling and dispatching to single services. A user may require one service to minimize execution duration while satisfying certain constraints in terms of reputation. Concurrently, another user may give more importance to a job than another previous request. An isolation-oriented approach is thus required.


                              Portable and lightweight service execution in a dynamic environment. By exploiting the intelligence found in modern hardware, the aforementioned approach can significantly improve I/O performance. However, maintaining the performance between the OS and UHPC will be critical. Therefore, thin-layer virtualization yields an opportunity to achieve lightweight and portable solutions.

Therefore, if middleware has strong isolation, which is similar to VM, and removes memory mapping overhead, middleware may be feasible to support UHPC. The observed experimental results show that memory protection produces unnecessary overhead in I/O operations. Instead of depending on a VM, we leave all of the drivers and services in their original and fully compatible execution environment per the original OS. To support two issues with UHPC, we propose a pragmatic approach to fully reuse and provide strong isolation of legacy device drivers. In the first method, we design a new memory mapping mechanism in the userspace to achieve userspace transparency. This mechanism is motivated using VM bypassing and direct I/O and will significantly simplify the protection mechanism. We then shift the VM concept into middleware to achieve strong isolation. These two strategies also reduce overhead. The proposed strategies are now presented to resolve the overhead of massive context switching and data copy. The first strategy uses contention management, which uses an I/O ring for its basic construction; the second strategy is a zero-copy I/O architecture. Ultimately, VM concept shifting is performed by the proposed approach and is optimized by two strategies to build a userspace glue prototype called userspace virtualized middleware (Uware). We can nearly guarantee that the self-adaptive properties of the system are preserved and that incompatibilities are limited to the performance overhead introduced by VM multiplexing. Compared with previous studies, the proposed system has the following advantages. First, it does not require hardware support and thus reduces costs. Second, unlike VM and OS bypass approaches, the proposed system is relatively simple because it uses buffering control and then performs path optimization to adapt with respect to different OSs. Third, the proposed guest I/O path uses hardware to achieve isolation from its special architecture. The remainder of this paper is organized as follows. Section 2 discusses the related work. We describe the motivation of Uware and introduce their challenge in Section 3. Section 4 presents the proposed virtualization approach. Section 5 presents the strategies proposed to enhance system adaptively. We then describe certain implementation details in Section 6 and discuss related issues in Section 7. The experimental environment and results are shown in Section 8. Finally, we conclude this study in Section 9.

@&#RELATED WORK@&#

@&#BACKGROUND@&#

Because no single task can expose all system weaknesses, different structures are used at different steps in the execution process for both an application and its platform. Recent strategies typically use a simplified system design such as direct I/O, and bypass I/O because these designs integrate the service function into the OS. One strong solution has developed in a similar way to a microkernel-like approach. For example, it is possible that a second connection may be required to send the same package before it has finished under high UHPC workloads. Thus, more than one copy of each package must be prepared to allow for two packages containing the same data but different destinations to be in the DMA ring at a time. To meet this challenge, Bruijn De et al. [7], presented an application-tailed I/O to automate the optimization of I/O bottlenecks. Alternatively, the server code was built into a kernel, producing a fast and flexible kernel-space server. Thus, the strong isolation and fully reused driver are the major features that are used to construct a robust system and improve performance. Virtualized I/O was another solution but also introduced many new challenges. We now present a virtualized I/O system, which involves completing an I/O request that cooperates with different services of each layer. For detailed I/O operation, each I/O access logs information to let the Isolation Device Driver (IDD) execute a Page Table (PT) switch and then reference the Grant Table (GT). A hypervisor needs to protect memory space and designs a ring flow controlled by IDD for isolation; then, the system waits for the shared memory section. In virtualization, a page-sharing grant mechanism in the I/O ring controls all of the I/O requests; page sharing gives the IDD have safer procedures. We measure performance by porting the page-sharing mechanism into the middleware application; this experiment shows similar results with previous studies. The overhead comes from the I/O subsystem, as shown in Fig. 1
                        . These VM mechanisms take 40% of the systems performance to complete their work, as many related studies have also reported. Santos et al. [9], and Apparao et al. [10], analyzed the virtualization I/O system. Committed to an I/O request, it needs a use performance tradeoff into the right place from isolation. The performance tradeoff comes from memory mapping and a large preventative I/O. Conversely, the proposed approach presents a comprehensive solution, which includes shifting the VM to the userspace to reduce overhead.

Using isolation awareness to construct a UHPC is difficult and complex. Although a modern VM has multiple approaches, including kernel-based VMs and full-system simulation, these methods are typically considered for running systems. For example, Li et al. [11], and Deshane et al. [12], reported that Xen spends a large percentage of its performance on full isolation behaviors, including grant mechanisms and I/O rings. Most previous studies have focused on I/O optimization to accelerate these systems. Ongaro et al. [13], proved that if VMs issue significantly more I/O requests simultaneously, unreasonable I/O throughput can be used by guest virtual machines (GVMs), despite the CPU utilization that is distributed fairly among GVMs. Cherkasova et al. [14],and Pu et al. [15], indicated that the usage of different I/O sizes is one issue that must be solved to achieve system optimization. Some studies examined I/O procedures using priorities. Kim et al. [16], extended an I/O contention manager for the user to define priority-based configuration. Menon et al. [17], applied additional mechanisms to make improvements. Gupta et al. [18], modified the I/O procedure to reduce memory latency to improve throughput. Numerous studies proposed direct I/O to achieve better I/O performance. Willmann et al. [19], modified the network interface controller (NIC) to allow traffic multiplexing and reallocated the I/O procedure to achieve near direct I/O access. Willmann et al. [20], also presented two technologies to approximate direct I/O access: the hard-based strategy and the soft-based strategy. Consequently, Kloster et al. [8], changed that configuration and allocation in the memory tables to decrease overhead. Other approaches are used for applied-hardware issue. For example, Sugerman et al. [21], used a cache to reduce the interrupt overhead of high-ratio context switching. However, certain authors modified the grant mechanism and demonstrated that their work could be improved by 52% via I/O throughput. Given these results, the proposed I/O mechanism was used to pass through a virtualized I/O.

Many applications may run faster in specialized environments. Alternative solutions can enhance a system by constructing the operating system adaptively and improving the performance of applications. However, in UHPC, a more amenable solution is necessary because applying traditional approaches in specific environments is difficult. For example, a Library Operating System (LOS) provided strong isolation at a lower overhead than a VM [22]. and provided applications with fine-grained, customized designs to use as hardware resources. An LOS can also communicate with the underlying OS through an application binary interface (ABI). Higher-level abstraction made it easier to share the resources of the underlying host OS. However, LOS must modify the lower-level kernel to support the systems mechanisms. Traditional methods of transforming existing systems into specialized environments are difficult to implement. For instance, the Linux VServer [23] and containers [24] add additional mechanisms when isolating kernel objects. Although UHPC is a heterogeneous and I/O-intensive environment, it was shown to degrade system performance. Ammons et al. [25] proposed an execution environment that was specially made for IBMs J9 JVM. Libra did not provide specialized functionality into the kernel but rather implemented the services of a traditional operating system directly in application-level libraries and relied on an instance of Linux in another hypervisor partition to provide services. Significant overhead in a remote I/O model was thus produced by multiple unnecessary data copies. Consequently, unlike these systems, the goals of Uware were to improve I/O performance and provide strong isolation to adaptively support UHPC.

@&#MOTIVATION@&#


                        Fig. 2
                         a shows the memory mapping of the VM approach. The I/O mechanisms of the VM approach were called the original system, which consists of many protection mechanisms that produced overhead. If we could shift the VM concept into the middleware, the loading of the VM would decrease due to the userpace transparency; thus, both contention management and the zero -copy strategy would hopefully be attainable, as shown in Fig. 2b. As a result, protection overhead would become easier, and the challenges of strong isolation would be resolved. To achieve these objectives, we present two strategies to resolve the issue of overhead by constructing a middleware prototype. This prototype is called the userspace virtualized middleware (Uware).

Preemption gives time-critical applications more opportunities to achieve faster responses. Asynchronous I/O (AIO) combined with an event-based service is typically used to design a fast-response system. Notice that AIO does not provide a way to open files without blocking disk I/O operations. However, every client in UHPC is of the same importance, and thus, their I/O operations are not of only a single type; UHPC may contain two services: one that is small, and the other that is large and I/O-bound. The first is suitable for event-based service design, and the second is suitable for thread-based service design. To address most situations, the performance of UHPC applications may benefit from keeping data in a type of memory such as cache memory. However, frequent buffer flushes can adversely affect performance. Logging the access situation to prevent data conflicts is required for UHPC. These considerations make Uware a virtualized approach. Therefore, we propose a memory and I/O control method that is adaptive, as shown Table 1
                        . The proposed approach is a simple glue software to insert into the OS; this brings another benefit in which a real-time OS, a new I/O system design and thus the proposed middleware exhibit improved performance and portability. Because Uware is fully isolated in the memory model, it will be suitable for parallel I/O systems.

The aforementioned VM approaches have been proposed to improve system performance. Three mechanisms are required to achieve this: the I/O ring, the grant mechanism, and the virtualized driver. A schematic of VM concept shifting is shown in Fig. 3
                     . To shift the VM concept into userspace, the operating visibility for three mechanisms must be defined. The operating visible of each computing service (CS) will inform each service of the others operations. The proposed approach combines a shared table to record memory items for operating visibility. Second, isolation must be achieved. The proposed approach uses a memory placement mechanism such as a grant mechanism because a shared table allows information from each operation to be recorded. Finally, a virtualized driver must be used. The details are described as follows. In the first mechanism, the shared tables information regarding each request is shown in Table 2
                     , where CS Id indicates that one driver is executed by a given service and cannot be changed. For example, one service using a one-driver buffer can be protected by the proposed approach and may record the control level of control; thus, Uware must manage the synchronization. Node Id indicates that several services will be dispatched to a fixed platform node. Here, interactive services are placed on a nearby node. Because services have their own features, the services that are not required will be passed to another node; this will produce context-switching overhead. The memory address and size indicate that the drivers memory address will be dispatched with the other driver. For example, the drivers memory address is constructed on the kernel space; thus, we use mmap() mapping to the userspace to transform the new address. Memory dispatch is more complex than the previous cases because it considers sync challenges (e.g., read after write). Thus, this table provides a chance to reach userspace transparency, take control to certain operations, and allow efficient development.

The second issue arises when multiple CSs have access to multiple drivers and are all in the lock-acquisition phase. While the proposed approach for userspace transparency will naturally perform in visible operations, we observe that a hypervisor is required in practice. The intuition is that by design, either α or β holds D1 for a brief period of time. Conflict occurs when the lock-holding CS causes data conflicts, which can be prevented when the hypervisor-like mechanism can identify these conflicts. Thus, we can still isolate each drivers reuse and safety. The final issue arises when a driver reuses a large buffer. To allow data inter-connection will reduce the amount of data copying, but conflicts cannot protect the driver buffer. A hypervisor-like mechanism can only discover a conflict not to be in isolation. For example, CS α begins at the buffer B1. Unfortunately, once CS β becomes irregular to access buffer B1 (e.g., CS α and β start to data inter-connection); buffer reuse is prone to unsafe operation. The proposed system uses a global table, allowing a CS to identify when none have committed during its execution. Thus, we have found that contention management and a zero-copy buffer are required when using VM concept shifting. A potentially more troubling challenge appears in these mechanisms. Each operation suffers overhead from protection. Thus, we use two strategies to resolve these challenges.

In this section, we describe two strategies to achieve the proposed approach as follows. In Fig. 3(a), we analyze several intuitive solutions of an application and attempt to find the easiest way to reduce overhead. Uware has become operationally visible based on a shared table. This design can cooperate with the I/O ring to achieve synchronization. Thus, this contention should be managed by the proposed strategy. Indeed, the proposed first strategy is contention management, which controls the access sequence similar to that of the I/O ring. For example, service α and β of an application can be used by the buffer (e.g., B1 to B8) of the drivers (e.g., D1 to D3), as shown in Fig. 3(b). If several buffers are present, then the concurrent control buffer will be selected based on priority. However, constant data copying still exists, which will require difficult debugging procedures. We define many buffers for each driver, such as a circular buffer, to provide mapping to the kernel space memory. The second strategy is the zero-copy mechanism. For example, CS α sends a request to read D1 data;

Then, CS β also reads D1. The data from D1 will then be placed on B8. The memory address of D1 is constructed on the kernel space, and we then use mmap() mapping to the userspace B8 buffer. In a multiple-access scenario, the thin layer handles the consistency through the proposed table items. Driver consistency is not yet managed by native Linux, which reduces the required memory mapping. Thus, the first strategy is used to identify the buffer to the next CS without copying. Thus, the zero-copy mechanism is the proposed second strategy. This strategy avoids the common system and I/O bottlenecks by reconfiguring a data path during the design of the application to match the workload and special-purpose hardware. Each buffer connects to the I/O for top-down buffer transactions; this is similar to I/O virtualization. In general, these two extremes in managing the kernel and user space with heterogeneous hardware and software configurations are either (a) not addressed (i.e., bypassed), or (b) fully restructured in the operating system for special optimization. The proposed strategy implements a midway point between the customized OS and bypass because both are not comprehensive; instead, we try to build a glued thin layer to perform portable and lightweight executions. This method avoids bottlenecks by optimizing the mapping of application buffering. A zero-copy strategy selects the set of CS that (1) satisfies the application, (2) supports the portable structure, and (3) minimizes data movement. Thus, these two strategies are used to benefit from userspace transparency. The proposed approach consists of three components to construct the proposed prototype: a thin layer, the adaptive glue engine (AGE), and an absolute isolation model.

@&#OVERVIEW@&#

In this section, an architectural overview is shown in Fig. 4
                        , where two strategies from the OS to Uware are used for the in-memory caching mechanism. Uware enables the easy extension of management with an operational visibility for data incurring overhead. The I/O path is composed of three major components: a thin layer, AGE, and a uniform memory model. We now present the block I/O procedure in Uware, as shown in Fig. 4. Uware reserves the I/O ring and grant mechanism in the original and modifies the principle of the virtual I/O path. First, the physical device driver is moved into the thin layer, and the backend driver only performs I/O ring control. This setup is different from the original VM tool, where the physical device driver is placed in the original driver domain. In the original I/O, IDD transfers an I/O request to the VM after the driver domain receives an I/O request. After the VM validates and allocates memory pages, it returns a machine address to the driver domain. Finally, the driver domain calls the physical device driver to execute an I/O access. Consequently, multiple CSs must compete with each other when they simultaneously issue many I/O requests. Compared with the original, the frontend driver in Uware executes the physical device driver through Uware after AGE returns the I/O response to the frontend driver. Therefore, Uware produces significantly better I/O performance. The proposed designs are describe in the following subsections.

To shift the VM concept, we need to build the proposed hypervisor called a thin layer, which must have low overhead and efficient usage. The most important aspect of the thin layer is that it is placed in the userspace; thus, combining it with path optimization and concurrent services is important. Concurrent path optimization requires a simple bypass layer, for which Uware reuses the VM concept. Uware also refines the implementation to avoid unnecessary costs from system and copying. Unlike a VM tool, Uware uses an spt table, p2m tables, and page tables to support a separate driver architecture to design the frontend driver of the CS, the physical driver, and the Adaptive Glue Engine (AGE), as shown in Table 3
                        . The original record table is reduced by the proposed design as motivation. Second, when the CS requests an I/O event, the thin layer provides a backend driver in AGE, which is treated as an IDD, which handles the I/O scheduler.

The backend driver cooperates with the I/O ring, which makes it possible to enable contention management. Both the backend driver and the thin layer attempt to not use the original OS design to make a more portable system to follow different OS designs for high-speed I/O execution. Thus, we isolate the memory instead of translating an interrupt or system event. Third, each CS has its own fronted driver that is mapped to a protected memory section. When the physical driver receives external data, the thin layer places the data into each protected memory location of the frontend driver until the I/O ring allows the CS move the data. In this design, the proposed Uware can provide a content-aware approach to perform adaptive I/O path optimization. Finally, using VM shifting, Uware can support an isolation-oriented design to acquire UHPCs features. When we can perform visible operations, all we need is to control the I/O behavior via the AGE

In Uware, each I/O operation is a buffer copy. Because the influence of data racing on overall performance continues to grows, buffer management is a critical factor in I/O application performance. This layer presents a view of Uware, and hidden data are moved via a shared memory model to minimize copying. The I/O procedure of Uware has several advantages before by considering previous studies and the concept of the thin layer. First, for fast switching and reducing memory usage, we let AGE only control the ring queue and communications. Thus, the control flow is simple and clear, and the most important part of this design is that it is similar to glue software. Second, user-level virtualization allows the system to become more portable. We thus build direct mapping of the original device driver. The bottom half in the kernel level records every request because the original OS driver was not modified; the top half is constructed using a memory allocator for memory usage. If the OS has been changed, then we only need to replace the bottom half. Third, the CS is simple in that we can dispatch resources based on the hardware feature. We also prevent page flushing via the system paging mechanism, and with this mechanism, we can record a benchmark to test the I/O performance to prove a different I/O flow. We found that the proposed design requires less than 5% CPU utilization, which proves that Uware can be an optimized middleware design.

A uniform memory model comprises many of the buffer elements; this is the basis for isolation middleware: optimization of the I/O path during application design by guaranteeing atomic access that maximizes throughput. For this model, the proposed approach contains the I/O, modified driver, and memory allocator. The memory allocator considers the CPU architecture to be like a cache (i.e., containing core information to dispatch the CS). It is a resource dispatcher to interleave the allocated memory. Thus, Uware must exist on an m2p table, p2m tables, and page tables, which are responsible for recording the memory mapping. Therefore, the I/O system will generate significant overhead. However, as described in the previous section, the proposed thin layer is a glue software; we did not transfer the interrupt. Otherwise, because the thin layer refers to the control rights of each CS, we only protected the memory and scheduled I/O requests to prevent I/O competition. Such a design can be consistent with future micro-kernel designs.

In the proposed design, when a CS requests an I/O, it does not require page mapping to occur twice; each frontend driver will map to the proposed thin layer. Thus, the proposed AGE only handles the I/O schedule. An AGE is driven by repeated calls to the function in which the CS is running. Each CS request in AGE was handled by a ring. It is a stackless design, similar to coroutine, to achieve an event-driver design with a fast dispatch. The proposed AGE majority of the state machines are complete removed and are corrected and simple to use. Then, memory protection still requires memory mapping if the performance remains limited by the thin layer. We use the memorys parallel access to improve the performance and to overlap the overhead of the memory mapping. Thus, each CS has a set of pseudo-physical addresses. Then, we add a memory allocator between the physical address and the memory to achieve absolute isolation. The memory allocator has the responsibility of assigning memory pages for the CS. In addition, we fix page mappings from the pseudo-address to the machine address to decrease the complexity of the memory allocation. Combining these designs, Uware can achieve better I/O performance. In Uware, we use the memory allocator to fix page mapping; thus, we can decrease the overhead of Uware allocating and recycling memory mapping and decrease the cost required for the m2p table update. To fix the page mapping from the pseudo-address to the machine address in the middleware, Uware adds a memory allocator to the CS.

@&#DISCUSSIONS@&#

In this section, we discuss issues related to Uware, such as performance bottlenecks in implementing glue software with the I/O mechanism. Virtualization is expensive but enables the specialized operation in the UHPC to match the special requirement for a mixed common workload. In virtualized middleware implementation, the resource cost is higher due to virtualization. However, virtualization will guarantee resource allocations to a computing service (CS). It not only can be adapted for a specific hypervisor at load time but also remains compatible with real hardware. For example, to specify memory address translations that are functionally equivalent but incur fewer performance costs to implement than virtualizing the analogous hardware, the page tables should be consulted. To understand the issue of performance, we implement a virtualization mechanism for evaluation. This simulation has a similar performance situation to other related studies. Fig. 5
                     a shows the performance usage of the original protection mechanism, and Fig. 5b shows the performance usage of reduced mechanism. The proposed simulation indicates that the overhead of the protection mechanism is nearly 20% higher/lower than ours, and Uware in only a single guest VM. The difference shows the tradeoff between performance and mechanism.

We also examine the memory situation, which highlights a clear tradeoff due to the mechanism limits of the I/O request. Fig. 6
                     a shows the memory usage of the original system. In this experiment, we notice that the original mechanism requires nearly 40% of the systems performance to maintain isolation, which caused throughput to reduce due to the overhead burden. This experiment also identified the memory overhead caused by the mechanism. Fig. 6b shows the results of the reduced mechanism, where the memory usage is marginally less than 50% of the original.

In addition, we examine two guest VMs in the original system and Uware to evaluate Uwares performance. Fig. 7
                      shows the memory usage of the original and of the proposed system. The results show that Uware has more resources to process the services. In Fig. 7, a double arrow identifies performance shifting. The system has move the data using the reduced mechanism. Thus, Uware has fewer procedures to keep the system safe and can thus make a guest VM use more resources.

However, UHPC requires a significant interconnection with each CS, where improving performance and performing middleware functionality are critical. The proposed key idea is for high-level virtualization that yields an opportunity if we can prevent context switch and to reduce the number of I/O procedures. Thus, we use a minimum number of abstractions to achieve a minimum performance cost, depending on the userspace transparency. Uware serves as a hardware abstraction layer with platform independent abstractions and services that encapsulate platform dependencies.

In virtualized middleware, overhead occurs at only two points. First, a CS must make its operation visible in the services connection because it incurs buffer-copy overhead. Second, when the thin layer takes control of the operations, each operation must pass through the thin layer, increasing the overhead. Traditionally, one of the notable features of an I/O bound design is VM bypass to support the userspace communication; it is only for the CS interconnection, not for improving the driver throughput. In UHPC, most operations contain driver throughput. In addition, using a bypass I/O cannot reuse drivers. Another feature of the I/O bound design is a zero-copy buffer, which cannot include a total as defined by the previous feature. Thus, we use two strategies to perform a concurrent CS operation without overhead. The proposed glue thin layer has certain special-purpose aspects to its architecture to improve its performance, including zero-copy, micro-kernel service, and being multicore-friendly.

The UHPC is designed to deploy on ubiquitous computing platforms. Such explosive growth in concurrency creates daunting challenges for I/O systems. Several experiments have been conducted to evaluate the performance of the proposed prototype. These experiments use an Intel(R) Xeon(R) CPU L5640 2.27GHz with 8GB of RAM memory as the platform to evaluate the original (Ori), and Uware (Uwa) designs. UHPC applications support different types of services including multimedia and I/O transmission. Most services are I/O intensive for information exchange. To evaluate I/O, Uware is evaluated by PostMark and Bonnie++ in the experiments. In addition, Uware can run many concurrent benchmarks to perform an isolation experiment. PostMark was designed to measure the transaction rates for a workload approximating a large electronic mail server, and Bonnie++ was designed to evaluate the data processing of multimedia.

Conversely, concurrent access to a single file using parallel I/O APIs, such as message passing interface I/O (MPI- I/O), is slowly emerging. UHPC may transfer large files; therefore, we examine large block-size data transmission via the IOR benchmark, which focuses on measuring the sequential read/write performance with different file sizes, I/O transaction sizes, and concurrencies. IOR can be used to set expectations regarding when the I/O subsystem achieves saturation. By examining parallel I/O operations between the benchmarks, we can analyze the advantages and disadvantages of Uware in more detail. We also compare the proposed results with those of LXC containers, which are system- dependent, light-weighting virtualizations. In the proposed experiments, we also compare Uware with the original mechanism and event-based designs. By comparing these services and middleware, the experiments indicate that Uware is a feasible middleware.

I/O operations can basically be divided into two types. The first type include frequent I/O access of small-size data. These systems require I/O processing performance to be as high as possible and are thus sensitive to I/O access latency. PostMark is a benchmark that is designed around this type of access and helps Uware to evaluate small-size data transfers. We examine 512 bytes of data in an I/O operation in this experiment. The experimental results show the I/O throughput and CPU utilization.

We first evaluate the results by reading the data. Fig. 8
                         shows the I/O throughput for multiple CSs reading data simultaneously using the original and Uware designs. In Fig. 8, the performance of the original design is limited to reading data concurrently (i.e., from two CSs up to four CSs). When eight CSs are reading data, the I/O throughput of the original design is approximately 60MB per second. Compared to the original design, Uware begins to be limited after six CSs. The I/O throughput of Uware is 110MB per second with eight CSs. Therefore, we find that the gap in performance between the original and Uware designs is large when more CSs are reading data simultaneously. From one CS to eight CSs, Uware can improve throughput from 12% to 84% when reading data concurrently. The experimental result also show that the competition among CSs will lead to an I/O performance decrease in virtualization. However, Uware has a modified I/O procedure and memory allocation system; therefore, we can increase I/O performance by decreasing the latency of the I/O response. Fig. 9
                         shows the CPU utilization for multiple CSs reading data simultaneously using the original and Uware designs. In this figure, we examine the original mechanism running one CS to eight CSs, which are denoted ori-1CS to ori-8CS, and Uware running one CS to eight CS, which are denoted Uwa-1CS to Uwa-8CS. The maximum CPU utilization is 300% in the y-axis when three CPUs are used. In this figure, Uware has a higher CPU utilization than the original design, and each CS shows an increased and more distributed CPU usage; thus, the experimental results show that Uware can also increase CPU utilization by decreasing the latency of the I/O response. Uware can thus improve CPU utilization.

Next, we evaluate the process of appending data. Fig. 10
                         shows the I/O throughput for multiple CSs appending data simultaneously using the original and Uware designs. Similar to the above experiment of reading data, the performance of the original is limited when appending data concurrently above four CSs. With eight CSs appending data, the I/O throughput of the original design is approximately 55000 KB per second. Compared to the original design, Uware begins to be limited above six CSs. The I/O throughput of Uware is 90MB per second with eight CSs appending data. These results are found because I/O write requests cause more data conflicts. Therefore, Fig. 10 shows that Uware improves throughput from 7% to 61% when appending data simultaneously from one CS to eight CSs. The reason for this result is the same as mentioned previously: Uware improves the I/O performance by decreasing competition among CSs and by decreasing the latency of the I/O response.


                        Fig. 11
                         shows the CPU utilization for multiple CSs appending data simultaneously using the original and Uware designs. As in the previous experiment of reading data, the maximum CPU utilization is 300% in the y-axis. The figure shows that the CPU utilization of the original is approximately 220% when four and eight CSs are used. Compared with the original design, the CPU utilization of Uware is 260% higher when four CSs are used and 280% higher when eight CSs are used. Therefore, Uware improves CPU utilization from 17% to 27% when either four or eight CSs are used. This experimental result again shows again that Uware can increase CPU utilization in a virtualization system, as mentioned previously.

The second important I/O operation is sequential I/O access in large-size data. Applications of this type of operation including data processing of multimedia applications. Their systems primarily process data sequentially for large-size data.

Bonnie++ is a benchmark that is typically used to determine the performances of hard drive and file system, and evaluates sequential read and write speeds in a file system. In this experiment, we used a 6480-MB file, and a 8192-byte file in I/O operations. The experiment results also describe the I/O throughput and CPU utilization as being equal to those found in the above experiments.

We first evaluated the sequential read speed, which is different from the findings produced by PostMark above. The I/O throughput is shown in Fig. 12
                         for multiple CSs reading data simultaneously using the original and Uware designs. The competition overhead is indicated for each experiment by an increase in the number of VMs. Thus, resource competition is not avoided in a system, and the associated performance loss is more significant due to the isolation and safe mechanisms used. In the original design, eight CSs reading data concurrently decreased the I/O performance by more than half compared to a single CS; thus, the original design shows serious performance losses in its I/O system. Therefore, the results show that Uware exhibits less performance lost caused by isolation and safe mechanism.


                        Fig. 13
                         shows the CPU utilization for multiple CSs executing sequential read simultaneously using the original and Uware designs. The figure shows that all of the systems do not exhibit high CPU utilization for accessing large-size data. The reason this is likely the block size of an I/O operation. Block size may affect the speed of the I/O request, thereby affecting CPU utilization. In the original design of Fig. 13, the CPU utilization gradually decreases when more CSs sequentially read simultaneously. The CPU utilization of the original design is 36% with one CS operating, but the CPU utilization reduces to 17% when eight CSs read simultaneously. Compared to the original design, Uware can maintain a given CPU utilization even if multiple CSs are used. The CPU utilization of Uware is approximately 40% with eight CSs. Therefore, Uware improves CPU utilization by up to 125% compared to the original design when eight CSs are used to read data simultaneously.

Using Bonnie++, we now evaluate the sequential-write operation. The experiment is also compared to the original mechanism. Fig. 14
                         shows the I/O throughput for multiple CSs writing data simultaneously using the original and Uware designs. The result of the original in the figure shows that eight threads writing large-size data concurrently would decrease the I/O performance by 25% compared to a single CS. This condition is similar to reading large-size data, as shown in the above experiment. This result identifies the competition for resources in the systems investigated in this study. Therefore, the limit of a hardware resource is larger than the limit of a system when CSs perform large-size sequential writing operations. For this reason, hardware resource limits make the relationship between safety mechanisms and I/O performance difficult to define.


                        Fig. 15
                         shows the CPU utilization for multiple CSs that are executing sequential write operations simultaneously using the original and Uware designs. As previously mentioned, the throughput experiment proved that the limit of a hardware resource is larger than the limit of a system when CSs perform large-size writing operations. Therefore, CPU utilization is similar in the original and Uware designs. In addition, Fig. 15 shows that Uware has a marginally higher CPU utilization than the original, even if their I/O throughput is similar; this is because the I/O type resulted in a different situation.

To examine the read/write performance for larger block size, we used the IOR benchmark to evaluate performance. In IOR, we set the POSIX I/O to access 640-MB files and set the block size to 64MB during I/O operations. This experiment compares the original and Uware designs. Fig. 16
                        a shows that the proposed Uware design performs better than the original design during read operations. As the number of concurrent services increase, the memory cache cannot hold all of the required data; thus, the read operation must obtain data from the disk. The large block size will reduce the memory bandwidth if too many data are waiting for transfer. However, the performance of the write operations in the Uware design is better, as shown in Fig. 16b. Writing large-size files during an I/O operation, Uware is limited by hardware resources, which indicates that Uware reduces resource competition more than the original design in a competitive situation. Therefore, the proposed Uware design can provide memory protection, and its I/O system is feasible.

Uware only depends on the services of a system because UHPC is a heterogeneous environment; we have thus attempted to keep Uware simple to modify for different systems. For porting and performance, we constructed the services of Uware at the user-level and maintain only a few services in the kernel space. To analyze the benefits of Uware, we compare Uware with LXC to show that the proposed virtualization approach is a viable alternative. Fig. 17
                        
                        a shows the read throughput. Uware exhibits better performance because LXC executes Ubuntu for running applications and supports the userspace virtualized approach to reduce overhead. Fig. 17b shows similar results for LXC and Uware. However, LXC performs worse than Uware when multiple VMs are used because it uses Ubuntu for running applications.

Path optimization is an option to mitigate performance lost. Event-driven design must prevent the blocking from I/O and resource allocation. In the proposed experiments, we use Bonnie++ to implement an event-driven version. In this study, we compare Uware with an event-based design in terms of application throughput. Event-based design is a standard approach that has many drawbacks, including a complex design and too many services. Thus, Uware combines path optimization with the virtualization concept to amortize costs at high levels of service cooperation. Its approach is unique in that both system issues and cost values can be set individually for each buffer. Therefore, we built a prototype to achieve a QOS and to avoid blocking. Fig. 18 shows eight threads reading large-size data simultaneously between event-based software and Uware. Uware has a similar performance with the event-based design and is also therefore an all-purpose, asynchronous I/O framework.

@&#CONCLUSION@&#

We have presented Uware as a prototype design that fully uses the virtualization concept inside a middleware form of the OS on which it is run. The proposed design is a novel concept for presenting alternative in middleware development. The shifting-virtualization concept supports the use of glued software for driver reuse and crash prevention. This approach has reduced the overhead required and makes software portable so that Uware can be used as a normal UHPC application. Unlike previous methods of enhancing performance in a commodity system, Uware relies on a simpler hypervisor to achieve portability and adaptability. Uware can thus be used in an enhanced OS or embedded system and has better CPU utilization than the original design. Uware also shares its resources more fairly, as indicated from the experiment results. Compared to event-driven middleware, Uware has a similar throughput and better performance isolation, especially when compare with container-based virtualization, due to its relative simplicity. Therefore, we propose a virtualization-based approach to effectively improve the UHPC application design.

@&#REFERENCES@&#

