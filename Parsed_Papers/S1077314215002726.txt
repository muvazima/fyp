@&#MAIN-TITLE@&#Simple change detection from mobile light field cameras

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present a framework for solving moving-camera problems with still-camera solutions.


                        
                        
                           
                           Geometry captured by light field cameras is used directly without a 3D scene model.


                        
                        
                           
                           The framework yields a simple, efficient, closed-form solution for change detection.


                        
                        
                           
                           The solution outperforms structure-from-motion methods for commonly occurring scenes.


                        
                        
                           
                           The framework can be generalized to a broad class of computer vision problems.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Change detection

Light field filtering

Plenoptic flow

Light field rendering

@&#ABSTRACT@&#


               
               
                  Vision tasks are complicated by the nonuniform apparent motion associated with dynamic cameras in complex 3D environments. We present a framework for light field cameras that simplifies dynamic-camera problems, allowing stationary-camera approaches to be applied. No depth estimation or scene modelling is required – apparent motion is disregarded by exploiting the scene geometry implicitly encoded by the light field. We demonstrate the strength of this framework by applying it to change detection from a moving camera, arriving at the surprising and useful result that change detection can be carried out with a closed-form solution. Its constant runtime, low computational requirements, predictable behaviour, and ease of parallel implementation in hardware including FPGA and GPU make this solution desirable in embedded application, e.g. robotics. We show qualitative and quantitative results for imagery captured using two generations of Lytro camera, with the proposed method generally outperforming both naive pixel-based methods and, for a commonly-occurring class of scene, state-of-the-art structure from motion methods. We quantify the tradeoffs between tolerance to camera motion and sensitivity to change, and the impact of coherent, widespread scene changes. Finally, we discuss generalization of the proposed framework beyond change detection, allowing classically still-camera-only methods to be applied in moving-camera scenarios.
               
            

@&#INTRODUCTION@&#

Having a static camera simplifies a wide range of important computer vision problems: change/motion detection, object tracking, segmentation, isolation and removal, and a range of spatio-temporal filtering techniques including denoising and velocity filtering [1–4]. If the camera is mobile, however, nonuniform apparent motion complicates these techniques, generally requiring structure from motion approaches which generate explicit 3D models of the scene. These methods are conceptually, computationally and behaviourally much more complex than their still-camera counterparts.

We show that light field cameras [5,6] offer a simplification by allowing a virtual, stationary view to be rendered from a dynamic light field sequence. The process, depicted in Fig. 1
                     , does not form an explicit 3D model of the scene. Rather the geometry implicitly encoded in the light field is directly exploited to produce a virtual, stationary camera, effectively reducing moving-camera problems to stationary-camera problems.

The proposed framework can simplify a wide range of problems. In this work we demonstrate the simplification of change detection, arriving at the surprising, important and novel result that change detection can be carried out with a single closed-form expression. To our knowledge this is the first published closed-form solution to change detection from a moving camera in a 3D environment.

The proposed method outperforms competing single-camera structure from motion approaches for a commonly-occurring class of scene. Because structure from motion jointly estimates camera velocity and scene geometry, changes in the scene can be confused for apparent motion, leading to a significant underestimation of change.

In contrast to competing methods, our solution has constant runtime, low computational requirements, predictable behaviour, and is easily implemented in hardware including FPGA or GPU, making it desirable in a range of challenging application domains including robotics.

The remainder of this paper is organized as follows: we discuss related work in Section 2 and provide background on the closed-form method of camera motion estimation from plenoptic flow in Section 3. We then describe a linear, additive rendering method based on plenoptic flow in Section 4, and combine the methods to effect change detection in Section 5. Section 6 shows results for imagery captured using two generations of Lytro camera, giving quantitative and qualitative analyses of the method’s performance and limitations, including explorations of the interplay between sensitivity to change and tolerance to camera motion, and sensitivity to widespread scene changes. The paper concludes with discussion and directions for future work in Section 7, including generalization of the proposed framework over an important class of computer vision problems.

@&#RELATED WORK@&#

Change detection from mobile platforms is nontrivial due to the apparent motion of the environment in the captured imagery. This apparent motion is nonuniform in the case of non-planar 3D scene geometry, and so methods based on pixel-level statistics are insufficient for such applications. The key limitation of these techniques is in their direct use of 2D monocular imagery in what is fundamentally a higher-dimensional problem.

Several successful approaches to change detection have been demonstrated under a variety of scene and camera constraints. For sequences with a static camera, the projection of the background onto the image plane is also static, and so it is possible to utilize simple pixel-based statistics to accomplish segmentation [1–3]. This is appealing for several reasons: it is computationally efficient regardless of scene complexity, it is easily parallelized, and it does not rely on identifying and tracking features, which can be problematic in noisy or self-similar environments. Other more sophisticated linear methods are also possible in the case of a stationary camera. For example, the linear velocity filters for object detection proposed in [4]. The work we present is conceptually similar to these filters, but is also applicable when the camera is in motion.

Extension to rotating cameras exploits the lack of parallax in the motion of the background [7–9], and so methods similar to the static-camera case may be employed. Similarly, approximately planar scenes with camera motion parallel to the plane – such as in aerial surveillance – present little or no parallax, and so similar techniques may be employed once the images are registered [10].

In the case of a freely moving camera and nontrivial scene geometry, background elements display different projected velocities. Several approaches have been proposed for addressing this scenario, including the use of occlusion detection, and employing concepts from optical flow to perform iterative camera motion and motion boundary estimation [11,12].

Other interesting approaches exploit constraints on projected background motion in an orthographic camera, as in [13] which tracks features across the image sequence, modelling background motion as a sum of basis trajectories. Dense per-pixel labelling is then performed in a final optimization step. In [14] motion between pairs of images is considered, for which background elements are shown to lie on a 1D locus. This constraint is exploited to detect foreground elements, though only low-density results are demonstrated. Dey et al. [15] present a generalization of the epipolar constraint and propose a feature-based approach for exploiting it. Finally, a lightweight algorithm exploiting similar ideas has recently been demonstrated operating in realtime on mobile devices [16].

In a related light field processing paper, Smith et al. [17] render views from a virtual camera with a smoothed trajectory, to effect video stabilization. Our approach differs in rendering views from a stationary virtual camera, allowing change detection to operate simply on a per-pixel basis.

The proposed method requires no feature tracking, no explicit 3D scene model is formed, and no iterative optimization is required. This is behaviourally and computationally simpler than existing methods, and yields results in constant runtime.

This paper builds on the concept of plenoptic flow introduced in [18], introducing a framework for simplifying moving-camera problems, deriving closed-form rendering from plenoptic flow, and providing a simple closed-form expression for change detection. A more detailed treatment can be found in [19].

In this work we employ a relative two-plane parameterization of light rays in which an s, t plane defines ray position, and a u, v plane, closer to the scene at an arbitrary distance D, defines ray direction. In the relative parameterization, u and v are expressed relative to s and t [19]. We employ τ to denote time.

Plenoptic flow and its precursors were first introduced to estimate camera motion [18–20]. This operates much like motion estimation from 2D optical flow [21,22], but generalizing to six degree-of-freedom (DOF) motion. The equation of plenoptic flow expresses the temporal light field derivative Lτ
                      in terms of the spatial and angular derivatives Ls, Lt, Lu
                      and Lv
                     , and the camera’s translation (qx, qy, qz
                     ) and rotation (wx, wy, wz
                     ):

                        
                           (1)
                           
                              
                                 
                                    
                                       
                                          [
                                          
                                             
                                                
                                                   
                                                      L
                                                      s
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      L
                                                      t
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      −
                                                      
                                                         (
                                                         u
                                                         
                                                            L
                                                            s
                                                         
                                                         +
                                                         v
                                                         
                                                            L
                                                            t
                                                         
                                                         )
                                                         /
                                                         D
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      −
                                                      
                                                         (
                                                         t
                                                         u
                                                         
                                                            L
                                                            s
                                                         
                                                         +
                                                         t
                                                         v
                                                         
                                                            L
                                                            t
                                                         
                                                         +
                                                         u
                                                         v
                                                         
                                                            L
                                                            u
                                                         
                                                         +
                                                         
                                                            v
                                                            2
                                                         
                                                         
                                                            L
                                                            v
                                                         
                                                         )
                                                         /
                                                         D
                                                      
                                                      −
                                                      D
                                                      
                                                         L
                                                         v
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         (
                                                         s
                                                         u
                                                         
                                                            L
                                                            s
                                                         
                                                         +
                                                         s
                                                         v
                                                         
                                                            L
                                                            t
                                                         
                                                         +
                                                         
                                                            u
                                                            2
                                                         
                                                         
                                                            L
                                                            u
                                                         
                                                         +
                                                         u
                                                         v
                                                         
                                                            L
                                                            v
                                                         
                                                         )
                                                      
                                                      /
                                                      D
                                                      +
                                                      D
                                                      
                                                         L
                                                         u
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      s
                                                      
                                                         L
                                                         t
                                                      
                                                      −
                                                      t
                                                      
                                                         L
                                                         s
                                                      
                                                      +
                                                      u
                                                      
                                                         L
                                                         v
                                                      
                                                      −
                                                      v
                                                      
                                                         L
                                                         u
                                                      
                                                   
                                                
                                             
                                          
                                          ]
                                       
                                    
                                    
                                       T
                                    
                                 
                                 
                                    [
                                    
                                       
                                          
                                             
                                                q
                                                x
                                             
                                          
                                       
                                       
                                          
                                             
                                                q
                                                y
                                             
                                          
                                       
                                       
                                          
                                             
                                                q
                                                z
                                             
                                          
                                       
                                       
                                          
                                             
                                                w
                                                x
                                             
                                          
                                       
                                       
                                          
                                             
                                                w
                                                y
                                             
                                          
                                       
                                       
                                          
                                             
                                                w
                                                z
                                             
                                          
                                       
                                    
                                    ]
                                 
                                 =
                                 
                                    −
                                    
                                       L
                                       τ
                                    
                                 
                                 ,
                              
                           
                        
                     where L
                     * denotes the partial derivative ∂L/∂*. Partial derivatives are estimated using the first difference.

The equation of plenoptic flow (1) is a linear system, which we can write more compactly as

                        
                           (2)
                           
                              
                                 
                                    A
                                 
                                 v
                                 =
                                 
                                    L
                                    τ
                                 
                                 .
                              
                           
                        
                     A closed-form least-squares solution to this linear system yields an estimate of the camera’s motion 
                        
                           v
                           ˜
                        
                      [18,23] – note that we have absorbed the negation of the temporal derivatives into 
                        v
                      to directly yield camera motion. In the following sections we will use this motion estimate to render a novel view which aligns two input light fields.

Each of the columns of the matrix 
                        A
                      is shown in expanded form in (1). Note that the matrix is shown transposed so that each column is printed as a row, and each of these columns can be interpreted as the change in the light field in response to one of six separate motion components. We will refer to these components as Lx, Ly, Lz, Lωx, Lωy
                      and Lωz
                     , respectively. Though they are treated as vectors in solving for camera motion, each of the six components can also be interpreted as a 4D light field, taking on the same dimensions as the input. Taking this approach, we decomposed the light field depicted in Fig. 2 into its six motion components, depicted in Fig. 3
                      – negative values are shown as dark, positive as bright, and zero as grey. For these figures, the input was band-limited to a normalized bandwidth of 
                        
                           10
                           
                              −
                              
                                 0.5
                              
                           
                        
                      to increase the visibility of the derivatives for display.

One of the immediate applications of this decomposition is that novel views can now be synthesized via the weighted addition of these six motion components to the original light field, provided the desired camera motion is relatively small. This is difficult to demonstrate in print, given the need for relatively small camera motions, but the two frames in Fig. 2 display shifted camera perspectives. The camera has been moved forward in the frame on the right, causing the bird to appear larger, with little change to the more distant background elements. The effect is accomplished entirely through addition of motion components – in this case the displayed light field is the result of adding 8 × Lz
                      to the input light field.

Examining Fig. 3, notice that the vertical spatial derivative, Ly
                         and the rotational derivative Lωx
                         are visually similar, and likewise for Lx
                         and Lωy
                         – the negation of Lωx
                         is displayed to emphasize the structural similarity to Ly
                        . This similarity is even more pointed for scenes with less depth variation. In some circumstances, the spatial and rotational derivatives are sufficiently similar that the method of plenoptic flow is unable to distinguish them. This problem has been previously noted [24], and is generally worse in cameras with narrower fields of view, for which the ambiguity is stronger.

Fortunately, though this ambiguity can severely impede motion estimation, it does not significantly impact the rendering of views from a stationary virtual camera. This will be discussed in Section 5.1.

Given two frames, we begin by finding the least squares solution to the equation of plenoptic flow (2) to yield an estimate of the camera’s motion 
                           
                              v
                              ˜
                           
                        . Based on this motion estimate, we wish to render a novel view using the additive method described in Section 4. Again the equation of plenoptic flow gives us the tool to do this, by allowing us to derive the temporal derivative due to the estimated camera motion:

                           
                              (3)
                              
                                 
                                    
                                       
                                          L
                                          ˜
                                       
                                       τ
                                    
                                    =
                                    
                                       A
                                    
                                    
                                       v
                                       ˜
                                    
                                    .
                                 
                              
                           
                        
                     

Rendering the light field measured at time τ
                        0 as though viewed from the camera’s position at time τ
                        1 can be accomplished by adding

                           
                              (4)
                              
                                 
                                    
                                       L
                                       ˜
                                    
                                    
                                       (
                                       
                                          τ
                                          1
                                       
                                       )
                                    
                                    =
                                    L
                                    
                                       (
                                       
                                          τ
                                          0
                                       
                                       )
                                    
                                    +
                                    
                                       
                                          L
                                          ˜
                                       
                                       τ
                                    
                                    .
                                 
                              
                           
                        
                     

Finally, we effect change detection through pixel differencing, by taking the difference between the measured frame L(τ
                     1) and the estimated stationary frame 
                        
                           
                              L
                              ˜
                           
                           
                              (
                              
                                 τ
                                 1
                              
                              )
                           
                        
                     . By substituting (4) and from the definition of the temporal derivative, we find

                        
                           (5)
                           
                              
                                 R
                                 =
                                 L
                                 
                                    (
                                    
                                       τ
                                       1
                                    
                                    )
                                 
                                 −
                                 
                                    L
                                    ˜
                                 
                                 
                                    (
                                    
                                       τ
                                       1
                                    
                                    )
                                 
                                 =
                                 
                                    L
                                    τ
                                 
                                 −
                                 
                                    
                                       L
                                       ˜
                                    
                                    τ
                                 
                                 .
                              
                           
                        
                     
                  

In other words, the result of pixel differencing using this method simplifies to the residual error in the equation of plenoptic flow. This is a satisfying result, as dynamic objects will break the rules underlying plenoptic flow, appearing as areas of high error in the residual. This simple solution is featureless, linear and closed-form.

@&#LIMITATIONS@&#

Rendering views from a stationary virtual camera limits the range of camera motion so that the content of interest remains in-frame. Because we employ plenoptic flow, we introduce the further constraint that camera motion between frames must be small, as in conventional optical flow [21,22].

It is an elegant result that pixel-wise change detection simplifies to the residual error in plenoptic flow. However, this means that other forms of residual error will also appear as motion. These include occlusions and specular highlights, which break the assumptions underlying the equation of plenoptic flow. Because camera motion between frames is necessarily small, the impact of these effects should be limited. These sources of error should also be easy to detect and ignore – we leave this as future work.

Scenes dominated by dynamic elements can sometimes cause plenoptic flow to describe the dynamic elements’ motion rather than the camera’s motion, effectively breaking this solution. Changes in illumination will also, as in conventional pixel-wise change detection, cause false positives.

In Section 4.1 we described ambiguities between pairs of rotational and translational motion components within the equation of plenoptic flow. In the present application, we are interested only in identifying elements that break the rules of parallax motion. In this sense, we are not immediately concerned with the velocity estimate 
                           
                              
                                 v
                                 ˜
                              
                              ,
                           
                         but rather in the reconstructed temporal derivative estimate 
                           
                              
                                 L
                                 ˜
                              
                              τ
                           
                         that it yields. As such, ambiguity in the motion components is irrelevant to the task – these components are able to explain the temporal derivative, but not the dynamic scene elements, and so serve our purpose despite the ambiguity in the motion estimate.

@&#EXPERIMENTS@&#

We applied the method of plenoptic residuals to pairs of images captured using commercially available Lytro consumer-grade plenoptic cameras. The results in the present section were captured with a first-generation Lytro, while those in Section 6.1 onward were captured using a Lytro Illum second-generation camera. The cameras were calibrated and imagery rectified using the MATLAB Light Field Toolbox [25]. For the Illum, pixels near the edges of lenslets were discarded, as these did not conform well to the simple distortion model employed in [25].

In poorly-lit scenes a hyperfan volumetric focus filter [26] was applied to improve contrast and reject noise, while maintaining depth of field and 3D scene information. We applied a numerically stable form of plenoptic flow, including the method for directly estimating derivatives from rectified light field imagery described in Section 5.3.1 of [19]. Finally, we computed the plenoptic residual (5) to build a map highlighting dynamic scene elements.

The top row of Fig. 4
                      shows two input frames with a small inter-frame camera motion and a single dynamic scene element. The centre row shows the magnitude of the difference between frames Lτ
                     , as computed after band-limiting, for plenoptic flow (left), and the plenoptic residual 
                        R
                      (right). The bottom row highlights dynamic scene elements in red using Lτ
                      and 
                        R
                     . The results in Fig. 4(c) and (e), representative of naive pixel differencing, show significant sensitivity to apparent motion. Though imperfect, the plenoptic residual results in Fig. 4(d) and (f) show significant attenuation of apparent motion, while retaining genuine changes.

Additional results are shown in Fig. 5
                     . Each of the three tests captured both dynamic scene elements and nonuniform apparent motion due to a change in camera pose. The left column depicts the result of naive frame differencing, while the right shows the proposed method of plenoptic residuals. Notice the correctly identified shadow change in the first row, and that the two highlights in this row correspond to the original and destination locations of the toothpick in a relatively large translation. In the bottom row, the square object was removed between frames, while in the centre row it was rotated.


                     Table 1 summarizes the signal energy resulting from naive pixel differencing and the method of plenoptic residuals, and their ratio. Values are shown for eight pairs of images from the three test scenes depicted in Figs. 4 and 5. The tabulated values represent signal energy expressed in dB, for input light fields normalized to a peak value of one. The mean ratio of 4 dB establishes that the plenoptic residuals method is more than twice as selective as naive pixel differencing. Referring to Figs. 4 and 5, we confirm the method has selectively attenuated static scene elements while passing dynamic objects.

One of the limitations of the proposed method is that camera motion between frames must be small. There is an interplay between input bandwidth, sensitivity to change, and tolerance to camera motion. To demonstrate this we measured the performance of plenoptic residuals over a range of camera motion magnitudes for a range of input bandwidths. A first, fixed frame was compared with a series of frames showing increasingly more camera motion. The camera was translated along x from 1 to 10 mm in increments of 1 mm. The test was run on a static scene, shown in Fig. 6
                        (c), and on a dynamic scene, for which the second and subsequent frames had a change as seen in Fig. 6(d).

Experiments for both static and dynamic-scenes were repeated twice, with strong agreement between experiments. The average results are shown in Fig. 6. In the case of the static scene, performance was evaluated as the ratio of false positive change detected to the estimate yielded by naive pixel differencing. Because the scene was static, the ideal result is that no change be detected, and so lower values are better. Notice that for higher camera displacements the optimal input bandwidth, highlighted in red, is lower. This is because the coherence of the input must be increased to tolerate larger shifts.

For the dynamic scene, hand-labelled ground truth was used to find the ratio between false positive change detection and true positive change detection – again, lower values are better. The shape of the result, shown in Fig. 6, is similar to the static case, except for a more prominent decrease in performance for very small bandwidths. This is due to a decrease in sensitivity to change, which does not appear in the static experiment.

As discussed in Section 2, most competing change detection methods are either sparse or much more complex than the proposed method. Indeed, in single-camera scenarios change detection generally requires joint estimation of the scene geometry and the camera’s motion, which can only be accomplished using sophisticated, iterative optimization methods [27,28]. By estimating scene geometry and camera motion, two views of the scene can be aligned, and a difference computed to identify dynamic elements.

The depth information implicitly captured by the light field confers two advantages: (1) it allows simplification of change detection to a single-step, closed-form solution, with no explicit geometry estimation required, and (2) the resulting method is robust to an important failure mode common to most if not all competing single-camera techniques.

When elements of the scene move with a projected velocity consistent with apparent motion due to the camera’s velocity, they can appear as stationary objects at shifted depths. We demonstrate this effect in Fig. 7
                        , and quantify it in Fig. 8
                        .

Although the effect applies to dense structure from motion in general, we restrict our attention to the case of a camera moving with known velocity along x. Under these circumstances, dense structure from motion simplifies to dense stereo matching. We estimate disparity using a semi-global block matching approach [29], and use this disparity to reproject the first frame as though seen from the point of view of the second frame. Because this simplified approach is a subset of structure from motion with fewer sources of error, we employ it as an upper bound of performance over more general approaches.

The scene shown Fig. 7(a) includes an object moving horizontally along x. The temporal derivative for a horizontal shift of 4 mm, as seen from a stationary camera, is shown in (b). Fig. 7(c) shows the disparity estimate, in which the motion of the dynamic object has yielded an overestimation of disparity. The resulting change estimate, shown in (d), shows how the motion of the test pattern has been underestimated, as it has been misinterpreted as depth.

To better understand this failure mode, we tested a variety of motion types and compared the change estimates from naive pixel differencing, plenoptic residuals, and the stereo-based approach. An important parameter of the stereo approach is the maximum disparity, which we tested at 16 and 32 pixels, with the minimum fixed at 0.

Four experiments were run over two repetitions each, with results shown in Fig. 8. In each experiment a fixed frame was compared to a series of frames showing increasing motion. Because the camera was fixed, the naive temporal derivative acts as ground truth. All traces are normalized to the maximum temporal derivative, and the heavy lines indicate the mean over the two repetitions shown as dashed lines.

The scene for Fig. 8(a) showed only vertical motion and acts as a control, verifying that the stereo-based change detection method operates well for projected changes orthogonal to apparent motion. The scene for (b) showed translation of the test pattern to the right along x, while (c) showed rotation about y, i.e. with projected motion to the right, and (d) showed diagonal motion to the right and away from the camera at about a 45 degree angle in x, z.

Plenoptic residuals performed well across all four motion types, while the stereo method showed poor performance for examples with horizontal projected motion. Note that fixing a maximum stereo disparity of 16 yielded improved results where the projected motion exceeded 16 pixels, as can be seen in Fig. 8(b) and (d), but not in (c), for which projected motion did not exceed 16 pixels.

When motion dominates a scene it can be confused for apparent motion, causing dynamic elements to be misinterpreted as being static. To demonstrate this we constructed a scene out of 12 movable tiles, and compared the change estimates for naive pixel differencing, plenoptic residuals, and the stereo-based method described above. A first, fixed frame was compared to a series of frames in which tiles were displaced one at a time. Again the camera was fixed, so that naive pixel differencing represents the ground truth.

For the first two experiments, shown in Fig. 9
                        (a) and (b), tiles were translated one at a time, 1 mm to the right, whereas in Fig. 9(c) and (d) they were rotated randomly by a few degrees. For the random rotations plenoptic residuals performed well, while for the translations it did not, showing decreasing estimates as more of the scene moved. The coherent translation of the tiles was consistent with apparent motion in that it could have been caused by camera displacement, while the random rotations were not, and so only the former caused a loss in performance.

When scene dynamics are consistent with apparent motion it should be possible to fool plenoptic flow into believing there is no scene change whatsoever. To prove this we performed a simulation, depicted in Fig. 9(e), in which an increasing percentage of the scene was artificially shifted. The experiment was repeated over a range of shift magnitudes between 1 and 10 pixels. For small pixel shifts of the whole image, all motion was interpreted as camera motion, yielding a plenoptic residual near zero. Larger shifts showed less dramatic results, however. This is consistent with the results shown in Fig. 6: large motions beyond the coherence of the scene break plenoptic flow, and so rather than being misinterpreted as camera motion, these were are at least partially interpreted as scene motion.

Note that throughout these experiments, the stereo-based method significantly underestimated change due to the presence of horizontal motion, even in the case of random tile rotations as depicted in Fig. 9(c) and (d).

We presented a general approach for converting moving-camera problems into stationary-camera problems. No depth estimation or other complex scene modelling is required – apparent motion is disregarded by directly exploiting the geometric information implicitly encoded by the light field.

Using this approach, we derived a method for closed-form change detection from moving platforms. By effecting both camera motion estimation and rendering using closed-form plenoptic flow, we showed that pixel-wise change detection from a virtual still camera can be found from the residual error in plenoptic flow. This is an important, surprising and useful result: Its constant runtime, low computational requirements, predictable behaviour, and ease of parallel implementation in hardware including FPGA and GPU make it desirable for deployment in demanding embedded applications including robotics.

We evaluated the method of plenoptic residuals using first- and second-generation Lytro cameras. We showed the method to outperform naive 2D per-pixel methods, which are sensitive to nonuniform apparent motion of the scene, and sophisticated structure from motion approaches, for the important case of projected motion parallel with apparent motion due to the camera’s velocity. We quantified the tradeoff between tolerance to camera motion and sensitivity to change, and the susceptibility of the proposed method to coherent, widespread scene movement.

As future work it should be possible to derive a more conventional approach that nevertheless exploits the depth information captured by light field cameras. For example, pairs of virtual views could be rendered for each camera pose and employed in stereo structure from motion. We expect such approaches to show less sensitivity to apparent motion than their monocular counterpart, but that plenoptic residuals will remain behaviourally and computationally simpler and therefore attractive for hardware implementation and embedded deployment.

The method of plenoptic residuals is susceptible to false positives where the assumptions underlying plenoptic flow are broken. These include occlusions, specular reflections, and changes in illumination. A method of detecting and explicitly ignoring these phenomena would be desirable, both in change detection and in improving the performance of plenoptic flow-based visual odometry.

Finally, this work started with a framework to efficiently and linearly co-register light field images to simplify a class of computer vision problems. We leave as future work demonstration on other problems in this class, including object tracking, segmentation, isolation and removal, and a range of spatio-temporal filtering techniques including denoising and velocity filtering [1–4].

@&#ACKNOWLEDGMENTS@&#

This work was supported by the Australian Centre for Field Robotics grant no. DP150104440 and the Australian Research Council Centre of Excellence for Robotic Vision (Project CE140100016). Thanks to the reviewers, Dr. Linda Miller and Dr. Jürgen Leitner for their helpful suggestions.

@&#REFERENCES@&#

