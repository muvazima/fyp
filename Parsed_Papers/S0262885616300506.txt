@&#MAIN-TITLE@&#Current research in eye movement biometrics: An analysis based on BioEye 2015 competition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present a review of the state-of-the-art in eye movement biometrics.


                        
                        
                           
                           We explain the general steps for the creation of a database of eye movement recordings.


                        
                        
                           
                           We describe basic eye movement features and methodologies with application in biometrics.


                        
                        
                           
                           We present extended analysis and results for the BioEye 2015 competition.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Eye movement biometrics

Biometric databases

Biometric competition

@&#ABSTRACT@&#


               
               
                  On the onset of the second decade of research in eye movement biometrics, the already demonstrated results strongly support the promising perspectives of the field. This paper presents a description of the research conducted in eye movement biometrics based on an extended analysis of the characteristics and results of the “BioEye 2015: Competition on Biometrics via Eye Movements.” This extended presentation can contribute to the understanding of the current level of research in eye movement biometrics, covering areas such as the previous work in the field, the procedures for the creation of a database of eye movement recordings, and the different approaches that can be used for the analysis of eye movements. Also, the presented results from BioEye 2015 competition can demonstrate the potential identification accuracy that can be achieved under easier and more difficult scenarios. Based on the provided presentation, we discuss topics related to the current status in eye movement biometrics and suggest possible directions for the future research in the field.
               
            

@&#INTRODUCTION@&#

The human oculomotor system is responsible for the coordination of the eye movements for filtering the inundating information of the visual environment. The functionality of the oculomotor system is supported by six extra-ocular muscles [1]. Four of them are dedicated to horizontal and vertical eye movements, namely, the lateral and medial recti (horizontal) and the superior and inferior recti (vertical). The other two extra-ocular muscles, the superior and the inferior oblique, co-ordinate the torsional rotations of the eye. The extra-ocular muscles are innervated by axons of the III, IV, and VI cranial nerves (known as the oculomotor, trochlear, and abducent nerves). Through the eye movements, the visual environment is projected on the retina, and the visual information is transferred via the lateral geniculate nucleus (LGN) to the visual processing centers of the brain. The complex mechanisms involved in the generation of the eye movements can open a window for the exploration of various human characteristics, both physiological (e.g., the structure of oculomotor system) and cognitive/behavioral related to the brain-driven neural circuitry and visual attention.

Some of the pioneering studies on eye movements investigated their functionality in the task of reading [2,3]. Subsequent researchers explored the interconnections between the eye movements and the cognitive processes even further, both for the case of reading and for picture viewing [4,5]. Furthermore, it was found that the visual scanning strategies can be affected by several factors, such as the task under consideration [6]. The second half of the 20th century brought important advances on the eye movement capturing technologies, promoting thus the research of eye movements even further. The oculomotor activity was explored in conjunction to visual perception and information processing [7–9], and also as the guiding mechanism for visual attention [10–12]. Apart from their role as a source of information, eye movements were also used for building practical applications for human computer interaction. Some examples involve the creation of smart input interfaces [13] and touch-less applications for people with disabilities [14]. They were also used as the means for improving the speed and accuracy of traditional pointing devices, e.g., the mouse [15].

Several studies on eye movements presented findings that actively support the existence of an idiosyncratic character in the oculomotor behavior. In the early 70s, Noton and Stark stated the so-called scanpath theory [16,17], supporting that the scanpaths formed by the eye movement activity can be modulated both by the content itself and by the idiosyncratic strategies of the observers. The research of Andrews and Coppola [18] suggested that certain characteristics of the eye movements, such as the fixation duration and saccadic size, can be influenced by endogenous factors. Their experiments explored the eye movements in different visual tasks such as natural scene viewing, simple pattern viewing, absence of visual stimulation (dark room), reading, and active visual search. The work of Schnitzer and Kowler [19], which focused specifically on the task of reading, reported individual differences in reading patterns both for single-text reading and for repeated-text reading. The experiments of Castelhano and Henderson [20] further supported the existence of individual saccadic eye movement consistencies during the inspection of images of various image formats, contents, and qualities. The study of Poynter et al. [21] supported the findings of previous research efforts regarding the saccadic characteristics, and additionally, they reported the observation of stable idiosyncrasies in fixational characteristics such as the fixation size, micro-saccadic rate, and micro-saccadic amplitude. Recently, Choi et al. [22] reported the existence of idiosyncrasies in saccadic vigor, which was additionally correlated with the anticipatory behavior of subjects in decision-making tasks.

The findings of the above studies can support the investigation of eye movement characteristics for the task of biometric recognition. Besides the pure scientific interest regarding the uniqueness of eye movement characteristics, the development of biometric recognition algorithms based on eye movements can contribute on addressing the requirements of modern biometric recognition systems. For example, soft biometric traits extracted by the eye movements can provide advanced anti-spoofing resistance [23,24] and facilitate tasks like continuous authentication [25]. Other advantages of eye movement biometrics are the provided ability for touch-less (remote) capturing and the potential employment of the extracted features for medical monitoring purposes [26,27].

In this section, we present a review of the most important methodologies in the field of eye movement biometrics up to the “BioEye 2015: Competition on Biometric via Eye Movements” [28]. We also provide a summarizing table (Table 1) for a quick reference to the basic information regarding each methodology.

The first use of eye movement features in biometrics was reported in the work of Kasprowski and Ober [29]. The authors recorded the eye movements from 9 subjects in an experiment with a “jumping”-point-of-light stimulus. They used spectrum analysis (Cepstrum transform) for the extraction of biometric features. These features were employed directly for classification by using different classifiers, such as the k-nearest neighbor (k-NN), the naïve Bayes, the C4.5 decision trees, and the support vector machines (SVM). The k-NN classifier (k
                     =3) achieved a false acceptance rate (FAR) of 1.48% for a false rejection rate (FRR) of 12.59%.

The work of Bednarik et al. [30] assessed static and dynamic features of the eyes such as the pupil diameter, pupil diameter variation, distance between the eye reflections, and gaze velocity. Their experimental database was formed by 12 subjects. The data were recorded using a “static cross” stimulus. They applied dimensionality reduction techniques on the time-tracked measurements (tested techniques: FFT, PCA, and combination of them) and then classified the features using a k-NN classifier. The reported identification rates for the dynamic features (pupil diameter variation) were 40% –50%, and for the static features (distance between reflections) as high as 90%.

Silver and Biggs [31] conducted a study combining keystroke and eye movement biometrics using a stimulus-task of “reading while typing.” The extracted eye movement features involved measures of fixation activity (number of fixations and fixation duration), saccadic activity (velocity and duration), and gaze position. During the classification process, they employed probabilistic neural networks (PNN). The data for their experiments were collected from 21 subjects, and the reported performances were an average true positive proportion (TPP) of 65.8% for a corresponding true negative proportion of 98.2% and accuracy of 96.6%.

The study presented by Kinnunen et al. [32] tackled the issue of task-independent verification based on eye movements. In this case, the stimulus consisted of a short introductory instruction text followed by a video presentation. The extracted features were the histograms of velocity directions for short-term windows of the recorded signal. The biometric templates were formed using Gaussian mixture models (GMM). The used experimental database consisted of 17 subjects, and for the task of verification, they reported an equal error rate (EER) of about 30%.

The research of Komogortsev et al. [33] proposed the oculomotor plant mathematical model (OPMM) and utilized it as the basis for a method of authentication via the oculomotor plant characteristics (OPC) [34]. The used stimulus was a “jumping”-point-of-light, inducing horizontal and vertical saccades. The features describing the oculomotor system were extracted via the comparison of the trajectories of mathematically simulated saccades with the real ones. The used database was formed by 59 subjects, and the reported half total error rate (HTER) was 19%.

The work of Holland and Komogortsev introduced [35] and extensively evaluated [36] the method of complex eye movement pattern (CEM-P) biometrics. Different kinds of stimuli were used, e.g., “jumping”-point-of-light, Rorschach inkblot images, cognitive dot-patterns, and text. They extracted a variety of features related to fixation number and duration, saccadic amplitude and velocity, and eye movement scanpaths. The feature vectors were compared via a Gaussian CDF modeling process. They used three data sets captured for different sampling rates from 22, 32, and 173 subjects, respectively. The best Rank-1 Identification Rate (IR) was 53%, and the EER was 28% (for the set of 32 subjects).

The paper of Cuong et al. [37] proposed the extraction of Mel-frequency Cepstral Coefficients from the eye position, eye difference, and eye velocity signals. The features were forwarded at different classifiers for evaluation (tested classifiers: decision trees, Bayesian network, and SVM). During the experiments, they utilized two of the data sets provided by the First Eye Movements Verification and Identification Competition (EMVIC 2012) [38], recorded with a “jumping”-point-of-light stimulus. The best identification rates were 93.6% and 91.1% for the data sets of 37 and 75 subjects, respectively.

The work of Rigas et al. [39] employed a graph-based technique for the comparison of the position distributions of eye movement samples on the 2-D plane. Their approach used the multivariate Wald–Wolfowitz runs test applied on the joint minimal spanning tree (MST) of the two distributions under comparison. The experimental stimulus consisted of face images, and the used database was formed by 15 subjects. The reported results were a Rank-1 IR of 70% and EER of 30%. A modification of the method was applied on velocity and acceleration features [40] and tested for two of the EMVIC 2012 data sets. The best rates were 91.5% (for the 37 subjects set) and 82% (for the 75 subjects set).

The study of Liang et al. [41] employed an attention-related stimulus set of video clips for testing a variety of features extracted from acceleration, geometric, and muscle properties of the eye movements. The video clips showed a white ball moving in various ways related to the extracted features under consideration. They applied feature selection based on the mutual information among features and employed back-propagation (BP) neural network and SVM classifiers. The evaluation was conducted on a small pool of 5 subjects, and the best reported identification accuracy was 82%.

Zhang and Yuhola [42] explored several properties of saccadic eye movements using a stimulus of a horizontal “jumping”-point-of-light. Their methodology involved the application of machine learning techniques on features of saccadic amplitude, accuracy, latency, velocity, and acceleration. The tested techniques included the multilayer perceptron networks, the radial basis function networks, the support vector machines, and the logistic discriminant analysis. The experimental database was formed from 132 subjects, and the reported best verification accuracy was 89%.

The work of Holland and Komogortsev [43] (named CEM-B) investigated the distributions of various primitive eye movement features using a text-reading stimulus. The extracted features of fixation duration and position and saccadic amplitude, duration, and velocity were statistically compared using various two-sample tests (t-test, Ansari–Bradley test, Mann–Whitney U-test, Kolmogorov–Smirnov test, and Cramér–von Mises test). They used two data sets (high and low sampling rates) formed by 32 and 173 subjects, respectively. The scores from the comparison of individual features were fused for achieving a top Rank-1 IR of 82.6% and EER of 16.5% (for the 32 subjects).

The method developed by Komogortsev and Holland in [44] explored the biometric potential of the complex oculomotor behavior (COB) using features related to the saccadic dysmetria, compound saccades, dynamic overshoot, and express saccades. The used stimulus consisted of a “jumping”-point-of-light inducing horizontal and vertical saccades, and the used database was formed by 32 subjects. The calculation of comparison scores was based on a Gaussian CDF modeling process, and the scores were fused for achieving a Rank-1 IR of 47% and EER of 25%.

In the work of Rigas and Komogortsev [45], the characteristics of fixational distributions where investigated using probabilistic maps of attention, named fixation density maps (FDM), and by employing a stimulus consisting of video sequences. The activations of the maps for sequential time intervals were compared via the following measures: similarity score [46], Kullback–Leibler divergence, Pearson's ratio, Earth Mover's distance. Then the comparison scores from the successive time intervals were fused. The evaluation database was formed by 200 subjects, and the reported Rank-1 IR was 51%, whereas the reported EER was 11%.

The study of Yoon et al. [47] investigated the properties of gaze velocity during a specialized experiment involving cognitive-dot stimuli related to the Gestalt grouping principles of similarity, continuation, proximity, and closure. The gaze velocities were analyzed using hidden Markov models (HMM), and then the log-likelihood was used to compare the velocities of the testing sequences with the constructed template models. The experiments were performed on a database of 12 subjects, and the best achieved classification accuracy reached 76%.

The paper of Cantoni et al. [48] presented an approach for the analysis of fixation points during gaze by using graph-based representations (GANT) and by employing visual stimulus of face images. Weights were assigned to the arcs of the constructed graphs based on the density and duration of fixations in the eye scanpaths. The adjacency matrices of the graphs for the respective features were compared via the Frobenious norm of their difference. The used database was formed by 112 subjects, and the reported best Rank-1 IR was 31%, whereas the best EER was 25%.

Lately, the efficiency of a multi-stimulus and multi-biometric fusion scheme for eye movement-based biometrics was demonstrated by Rigas et al. [49]. The performances of different methods for data recorded for diverse visual stimuli (“jumping”-point-of-light, text, and video) were efficiently combined via a weighted fusion scheme, for achieving a Rank-1 IR of 88.6% and EER of 5.8% for a large database of 320 subjects.

Several competitions on eye movement-based recognition were organized in the past. The First and the Second Eye Movements Verification and Identification Competitions (EMVIC 2012 [38], EMVIC 2014 [50]) introduced the field of eye movement biometrics into a broader audience and offered publicly available eye movement data sets—some of them were used by some of the methods described above. Recently, the BioEye 2015 competition [28] was organized with the aim to further advance the conducted research in eye movement biometrics, and to address the emerging challenges in the field.

During the implementation of an experiment for the collection of eye movement recordings, a number of well-defined steps should be performed in order to ensure the creation of a high-quality database with practical utility to the scientific community. Some of the main questions that need to be considered when organizing such an experiment are the following:
                        
                           •
                           
                              What types of visual stimuli should be used?


                              What are the setup details for the experimental procedure?


                              What measures should be used to assess data quality?


                              What pre-processing operations need to be applied on the raw data?

Following, we present a general overview of the steps performed during an experiment for the collection of eye movements, and we demonstrate how these steps were implemented for the creation of the eye movement data sets used in BioEye 2015 competition.

The eye movements can encapsulate both physiological and cognitive characteristics. Thus, the different eye movement biometric methods developed in the past employed different types of visual stimuli, every time related to the aspects of the oculomotor characteristics under consideration. The previously used types of visual stimuli can be generally classified into the following categories:
                           
                              •
                              “Jumping”-point-of-light (horizontal, vertical, and random)


                                 Cognitive-dot patterns
                              


                                 Text excerpts
                              


                                 Face images
                              


                                 Various-content (natural) images
                              


                                 Video sequences
                              

In Fig. 1
                        , we provide a general demonstration of some common types of visual stimuli that can be used in an eye movement experiment, along with examples of performed eye movement scanpaths. The visual stimuli for the data used in BioEye 2015 needed to conform to the following two criteria: (1) facilitate the processing of the eye movement signals by the participants and (2) allow for the extraction of various eye movement features (both physiological and cognitive). To fulfill these requirements, the adopted types of visual stimuli were a random “jumping”-point-of-light (RAN) and text excerpts (TEX).

In Fig. 2
                        , we present examples from the BioEye 2015 data sets showing the used stimuli and the performed eye movements. As we can observe, the random “jumping” point stimulus induces saccades of various amplitudes, which can be used for the inspection of the physiological characteristics of the oculomotor system. The horizontal and vertical components of movement can be processed separately, and also, the computation of velocities for the respective amplitudes allows for the examination of more complex saccadic characteristics, e.g., the main sequence [51]. For the text excerpts, the performed fixations and saccades are semi-guided by the stimulus, allowing thus for the inspection of personal eye movement patterns which can be co-modulated by cognitive and behavioral factors.

This section discusses several details regarding the capturing environment, the employed apparatus, and the general procedures followed during an eye movement recording experiment.

An eye movement recording experiment can be conducted either in a lab-controlled or in an open-free environment. The first case (lab-controlled environment) can be adopted when the highest possible recording quality is required. The second case (open-free environment) can simulate more accurately the real-world conditions of operation for an eye-tracking application. However, this setup can lead to the collection of signals of degraded quality, due to factors such as the variable lighting conditions, excessive head movements, and noise, which can affect subjects' behavior.

The recordings used in BioEye 2015 were captured in a lab-controlled environment using a chinrest with a head-bar to minimize the effects from small head movements and ensure maximum data quality.

The selection of a suitable eye-tracking device for an eye movement recording experiment is dictated by the exact goals of the conducted research. There are several characteristics that can differentiate eye-tracking devices, and below we provide a general categorization and examples of them:
                              
                                 •
                                 
                                    Recording technology: optical/video-oculography (e.g., [52–56]), electrooculography (e.g., [57,58]), eye-attached setup (e.g., [59,60]), direct infrared-oculography (e.g., [61,62])


                                    Device configuration: desk-mounted, remote, head-mounted/wearable


                                    Temporal resolution: typical values in range of 30Hz to 2000Hz


                                    Spatial accuracy: typical values in range of 0.5° to 1°


                                    Spatial resolution: typical values in range of 0.01° to 1°


                                    Cost: typical values in range of $300 to $50000

For the BioEye 2015 data sets, the used device was an EyeLink 1000 eye tracker [55]. The specific device has a vendor reported sampling rate of 1000Hz, a spatial accuracy of 0.5°, and a resolution of 0.01° RMS. During the eye movement recording experiments, the device operated in monocular mode capturing exclusively the left eye.

An important parameter during the construction of a database related to biometric research is the collection of data from a sufficiently large amount of subjects, so that the information regarding the inter-subject and intra-subject variability can be explored satisfactorily. Also, it is usually required to have a relatively balanced gender ratio to ensure a certain degree of generalization for the tested biometric methodologies. For the special case of an eye-related recording experiment, it is also valuable to record information for the subjects with or without corrected vision.

The data sets for the BioEye 2015 were assembled by using recordings captured from a large pool of 306 subjects. The gender ratio was 165 males/141 females and the ages of the participants were in range of 18 to 46years (M
                           =22, SD=4.3). The ratio of subjects with/without corrected vision was 154 corrected/152 uncorrected, and for the subjects with corrected vision the correction type was 63 glasses/91 contact lenses.

In a typical eye movement recording, the subject is positioned in front of a computer screen where the visual stimulus is presented while the eye tracker captures the performed eye movements. Some important setup parameters that need to be configured for ensuring a clear and comfortable visual field during the experiments are the stimulus screen dimensions and resolution and the distance of the subject from the stimulus screen. Other experimental parameters that need to be determined are the calibration setup, the duration of the experimental trials, and the time interval between the experimental trials.

An example of an eye movement recording setup is shown in Fig. 3(a), where we demonstrate the exact configuration used for recording the data of BioEye 2015. In this case, the selected distance in combination to the screen dimensions led to a total effective vertical visual field of 30.2° and horizontal visual field of 46.6°. This field of view covered sufficiently the requirements of the random “jumping” point stimulus (RAN), where the point position visual angles changed randomly in range of ±9° vertical and ±15° horizontal. Also, the adopted setup allowed a clear view of the text reading stimulus (TEX), where the presentation height for each text character in the screen was 0.98°.

Before the main recording experiment (stimulus presentation), a calibration procedure needs to be performed. During calibration, the participants are instructed to fixate on a number of points positioned at predefined locations on the screen. This process leads to the formation of the respective calibration map [63], which encodes the disparity between the actual calibration points and the subject's eye correspondences. The calibration information is used during the main experiment for accurately transforming the eye tracker measurements into the corresponding eye movement positions.

During the collection of the data used in BioEye 2015, a rectangle calibration pattern of 9 points was used, covering homogeneously the screen area. The calibration points appeared one after another in a random way. In Fig. 3(b), we demonstrate the employed calibration pattern (showing all points) along with some examples of “good” and “bad” calibration maps.

After the calibration procedure is completed, the main recording experiment (experimental trial) can be performed. The duration of each experimental trial can be selected based on several factors such as the nature of the stimulus (e.g., cognitive), the amount of required information for the task under consideration, and the prevention of eye fatigue due to prolonged eye movement activity.

The data used in BioEye 2015 had the following experimental trial durations: for the random “jumping”-point-of-light (RAN), the total duration of an experimental trial was 1min 40s, with the point changing its position every 1s. For the text reading stimulus (TEX), the total duration of an experimental trial (time given to subjects to read the text) was 1 minute.

For experiments that aim to evaluate the appearance of template aging effects on the biometric characteristics, it is important to select an appropriate time interval between sessions, i.e., temporal disparity between consecutive experimental trials for the same subject for each type of stimulus.

The recordings used in BioEye 2015 were performed in three sessions (one experimental trial per stimulus per session) separated by short-time and long-time intervals. The experiments of the short-time interval were conducted with the participation of all 306 subjects, whereas the experiments of the long-time interval were conducted for a subset of 74 subjects from the original subject pool. The short-time interval sessions (S01–S02) were separated by approximately 20min. Between the experimental trials for the RAN and TEX stimuli, the subjects conducted other eye movement tasks and had brief periods of rest to avoid eye fatigue. The long-time interval sessions (S01–S03) were separated by approximately 1year. Table 2
                            presents detailed information regarding the short-time and long-time intervals of the recordings used in BioEye 2015.

Two important measures that can be used to assess the overall quality of an eye movement recording are the following:
                           
                              •
                              
                                 Calibration accuracy: the quality of the performed calibration procedure directly affects the quality of the recorded eye movement signals. The assessment of calibration accuracy can be performed by calculating the corresponding gaze errors for the points shown during the calibration procedure (i.e., conducting validation of the calibration procedure).


                                 Recording validity: during an eye movement recording, the eye tracker sometimes fails to capture individual samples, due to device-specific faults or user-specific reasons (e.g., blinking, squinting, lose of attention, etc.). Recording validity is the total percent of the successfully captured (valid) samples for a particular recording and provides a direct measure of its quality.

The protocol used for recording the data employed in BioEye 2015 was the following: for each experimental trial, a new calibration was performed along with the corresponding validation of the calibration procedure. For poor values of calibration accuracy, the procedure was re-implemented. Furthermore, after the collection of the recordings, the recording validity for each individual recording was calculated. The subjects whom the recordings had low values of validity were removed from the database. From the initial subject pool of 322 subjects, 16 subjects were found to have one or more recordings with unacceptable levels of validity. The removal of these subjects led to the final database of 306 subjects. In Table 3
                        , we show the average values for the measured calibration accuracy and the calculated recording validity for all the sessions and all the cases of visual stimuli for the data used in BioEye 2015.

Several types of pre-processing can be applied on the raw eye movement recordings, usually aiming to filter any signal distortions arising from high-frequency noise and blinking artifacts. The presence of noise can affect the values of the extracted characteristics, and also, it should be noted that the effects from noise can be further emphasized due to the derivation (first and second order) of the signals, needed for the extraction of the velocity and acceleration features. Different types of smoothening filters can be employed for filtering the eye movement signals. Some examples used in the past include linear phase FIR filters (e.g., Parks-McClellan algorithm [64]), time-window weighted average filters
                        [65], Savitzky-Golay (SG) filter
                        [66], and Kalman filter
                        [67].

The followed pre-processing process for the data of BioEye 2015 involved the decimation of the signals from the original frequency of 1000Hz to 250Hz. The decimation was performed via the MATLAB function resample which uses an anti-aliasing FIR filter. This type of pre-processing was opted because it can provide satisfactory balance between noise filtering and preservation of the eye movement characteristics, and also, it can facilitate the database processing, storage, and transference, by keeping the size of the recordings (number of samples) at reasonable levels.

A simple approach for the extraction of features from eye movement recordings is to apply common signal processing techniques directly on the entire signal (or some parts of it). However, the most common approach is to initially classify the signal samples into sequences of fixations and saccades and then process the fixation and saccadic profiles for the extraction of biometric features. Next, we describe some basic techniques and examples of the features that can be extracted when following the previous two approaches. Then we present an overview of the specific features and methodologies used by the participants of BioEye 2015.

During the direct processing approach, time-series or frequency-domain analysis techniques are applied on the whole recording (or parts of it) without explicitly classifying fixations and saccades. These techniques usually rely on modifications of already established signal processing algorithms that have been used with success in other fields of research (e.g., speech recognition).

In the case of time-series analysis, examples of such techniques include the Dynamic Time Warping (DTW) algorithm [68] or the representation using Gaussian mixture models (GMM) [69]. In the case of frequency-domain analysis, the simplest option can be to apply the fast Fourier transform (FFT) on the signals. In Fig. 4
                        , we show an eye movement signal and the corresponding Fourier harmonics which can be used as frequency-based features. Other more sophisticated frequency-domain analysis options involve the use of Cepstrum transform [70] or Mel-frequency Cepstral coefficients [71], which have been originally used in the field of voice recognition. Direct signal processing techniques based on times-series and frequency-domain analysis have been employed by previous works in the field of eye movement biometrics (see Section 2
                        [29,30,32,37]), and they were also used by some of the participants of BioEye 2015 competition.

During this approach of processing, the eye movement samples are initially classified into sets of fixations and saccades. This type of processing is more intuitive than the direct processing of signals since it takes into consideration the specialized characteristics of the eye movement signals. For this reason, many of the previous research efforts have followed this approach (see Section 2
                        [31,34,36,40–43,47]), and also, this form of processing was adopted by most of the methodologies of BioEye 2015.

Several different algorithms have been presented in the literature for the classification of eye movements. The most important of them are the following:
                              
                                 •
                                 Velocity-threshold identification (I-VT) algorithm [72]
                                 

Hidden Markov model identification (I-HMM) algorithm [73]
                                 

Kalman filter identification (I-KF) algorithm [74]
                                 

Dispersion-threshold identification (I-DT) algorithm [75]
                                 

Minimal spanning tree identification (I-MST) algorithm [76]
                                 

The first three algorithms in the list (I-VT, I-HMM, and I-KF) are based on the velocity characteristics of the signals. The I-VT algorithm is the simplest one and uses a single threshold in order to classify the samples either as belonging to fixations (velocity≤threshold) or as belonging to saccades (velocity>threshold). The I-HMM algorithm provides a more sophisticated approach than the I-VT, by employing a probabilistic representation for the distributions of the samples belonging to fixations and saccades. The I-KF algorithm models the positions and velocities of the eye movement samples as states of a Kalman filter, and the classification is performed by comparing the actual eye movement velocities with the velocities predicted by the Kalman filter.

The latter two algorithms in the list (I-DT and I-MST) are based on the dispersion characteristics of the eye movement samples. The I-DT algorithm calculates the dispersion of a group of samples (window), and if the dispersion is lower than a threshold, then the samples are marked as belonging to a fixation. The window needs to be initialized at a minimum width (minimum fixation duration threshold). The I-MST algorithm uses a graph-based approach for modeling the dispersion of the eye movement samples. The samples are represented by constructing a minimal spanning tree on them, and the fixations are identified via a tree-search process applied on the constructed MST.

Apart from the algorithms used for the classification of the two main eye movement types (i.e., the fixations and saccades), there are also several algorithms developed for the classification of other types of eye movement events, such as smooth pursuit movements [77], glissades [78], and various types of corrective eye movements [79] (the latter have been tested as biometric features in [44]).

After the preliminary classification of eye movement samples into fixations and saccades, a large variety of features can be extracted from the corresponding amplitude, velocity, and acceleration profiles (higher-order profiles can also be used). In Fig. 5
                           , we present the amplitude, velocity, and acceleration profiles for a specific fixation and a specific saccade from the example eye movement signal shown on top. On these diagrams, we portray some of the basic features that can be extracted from fixation and saccadic profiles. These features can be used as the basis for the calculation of more complex features, e.g., various ratios of amplitude, velocity, and acceleration characteristics, and to model non-linear relationships (e.g., the main sequence [51]). Another possibility is the creation of more sophisticated statistical representations of the overall shape of the profile under consideration.

The extracted characteristics from the fixations and saccades along with their execution order can be used to represent the respective scanpaths formed by eye movements. Also, another approach for the representation of eye movements in the 2-D space is the employment of structures like graphs and activation maps. In this case, the biometric features are either the constructed structures themselves or some statistical attributes that can be calculated by them. In Fig. 6
                           , we present some examples of 2-D representations of the eye movements. Fig. 6(b) shows the captured positional eye movement samples (in the 2-D space) during the inspection of the visual scene shown in Fig. 6(a). In Fig. 6(c), we show the representation of these 2-D positional samples using a graph structure. The graph structure can provide a more robust representation if we assign weights to the edges (e.g., according to duration, sequence, etc.). In Fig.6(c) we show the representation of the same positional sample distributions using a different 2-D structure, an activation map. Such a structure represents the information in a more abstract way. Representations based on 2-D structures were proposed and evaluated in some previous works on eye movement-driven biometrics (see Section 2
                           [39,45,48]).

In this section, we present an overview of the basic characteristics of the methodologies developed by the participants of BioEye 2015. A more detailed description of the methodology that was ranked first in the competition was recently presented in [80]. In Table 4
                        , we present the general information related to the identity of the participants, their rank in the competition, and the methods' abbreviations used throughout the rest of the paper. In Table 5
                        , we present in a compact form the information regarding the utilization of basic types of features from the methodologies of BioEye 2015. It should be mentioned that the majority of methodologies (GER, ABE, KUB, GAL, BIX, and KUE) adopted the approach of first classifying the samples into fixations and saccades and then extracting features from the corresponding profiles. One methodology used the direct processing approach (KAS), and extracted features from signal chunks.

In all methodologies, the biometric templates were formed either by using directly the features or by building histograms over them. The participants employed a variety of techniques for performing the comparison and classification of the formed biometric templates. In Table 6
                        , we present the basic techniques utilized by the participants of BioEye 2015 for performing these tasks. Furthermore, some of the participants utilized some assistive information provided by the recordings. For example, the participants were provided with information regarding the validity of samples in a recording, and about the exact positioning and timing of the RAN stimulus presentation. Some of the participants also used the information of one-to-one correspondence, i.e., exactly one probe recording corresponded to exactly one of the gallery recordings. In Table 7
                        , we present the data regarding the utilization of different types of assistive information by the methodologies of BioEye 2015.

It should be noted that a simplified version of the methodology described in [43] was used in order to provide some baseline results to the competition participants. The distributions of fixation features (starting time, duration, and position-centroid horizontal/vertical) and saccadic features (starting time, duration, amplitude horizontal/vertical, mean velocity horizontal/vertical, and peak velocity horizontal/vertical) were compared with the use of the two-sample Cramer von Mises test. Then the resulting matching scores were combined using a simple mean fusion scheme.

This section presents an analysis of the results achieved by the methodologies of BioEye 2015. The measure that was used for the evaluation of the methods was the Rank-1 Identification Rate (Rank-1 IR), defined as the ratio of the correctly identified test samples to the total number of test samples in a data set. The identification rates were calculated for every data set separately, and the following formula was employed for combining the separate performances into the final Rank-1 IR that was used to rank the methodologies:
                        
                           
                              
                                 IR
                                 f
                              
                              =
                              
                                 w
                                 
                                    D
                                    1
                                 
                              
                              ∙
                              
                                 IR
                                 
                                    D
                                    1
                                 
                              
                              +
                              
                                 w
                                 
                                    D
                                    2
                                 
                              
                              ∙
                              
                                 IR
                                 
                                    D
                                    2
                                 
                              
                              +
                              
                                 w
                                 
                                    D
                                    3
                                 
                              
                              ∙
                              
                                 IR
                                 
                                    D
                                    3
                                 
                              
                              +
                              
                                 w
                                 
                                    D
                                    4
                                 
                              
                              ∙
                              
                                 IR
                                 
                                    D
                                    4
                                 
                              
                           
                        
                     where D1=“RAN_Short,” D2=“RAN_Long,” D3=“TEX_Short,” D4=“TEX_Long,” and wD1
                     =0.3, wD2
                     =0.2, wD3
                     =0.3, wD4
                     =0.2.

The utilization of smaller weight values for the long-time interval data sets was selected in order to provide a more balanced calculation of the final Rank-1 IR, given the smaller available amount of data (subjects) for these specific data sets.

In this section, we present the performance results for the original data sets of the competition. The reported results regard the evaluation sets, which contained the data from half of the subjects in every case (153 subjects for the short-time interval and 37 subjects for long-time interval data sets). In Table 8
                        , we present the achieved Rank-1 IR for the methodologies of BioEye 2015, showing the results for each data set separately and the final performance used for ranking. It should be noted that during the submission period the participants were given feedback only on 75% of the data. The table shows the results both on 75% and on 100% of the data, allowing thus for an inspection of the generalization capability of the methods. As it can be seen, for the final results on 100% of the data the three top ranked methods achieve rates over 50%, and as high as 96%. The algorithms perform better for the short-time interval data sets, even if the long-time interval data sets had a smaller amount of subjects (and thus the level of chance was higher). The difference between the short-time and long-time interval data sets reached a maximum of 29.4% (M
                        =13.2%, SD=9.8%) for the RAN stimulus and 27.2% (M
                        =13.9%, SD=11.2%) for the TEX stimulus. In overall, the performances across the different stimuli were relatively stable, however, most of the algorithms performed better for the TEX stimulus. The maximum between-stimulus difference was 18.9% (M
                        =5.5%, SD=6.3%) for the short-time interval data sets and 8.1% (M
                        =4.2%, SD=3.4%) for the long-time interval data sets.

In Table 9
                        , we present a post-analysis of the results showing—for each participant separately—the number of the commonly correctly identified subjects between the data sets corresponding to different visual stimuli. It can be observed that in all cases the number of commonly identified subjects is less that the maximum number of identified subjects for the RAN and TEX data sets separately. Given the fact that the same subject identities were used in both data sets (RAN and TEX), this specific result indicates the possibility of further improvement of performance by combining the information from data sets corresponding to different visual stimuli (i.e., perform multi-stimulus fusion).

Two additional data sets were given to the participants after the completion of the original submission period in order to assess the robustness of the developed algorithms. The data sets were formed using recordings captured for a random “jumping”-point-of-light stimulus. The basic characteristic of the first data set, “RAN_MultiRec_250Hz,” was that it provided more than one unlabeled recordings per subject. The number of unlabeled recordings could vary from 1 to 6, and it was randomly chosen for each subject following a homogeneous distribution. It should be noticed that these recordings were captured in different sessions than the recordings for the original data sets, and under different time intervals. The aim of this data set was to test the robustness of the algorithms in the presence of many biometric samples from each subject, which can possibly lead to larger variability of the feature values. Additionally, since the exact number of recordings per subject was unknown, this new data set cancelled the possibility of using the one-to-one correspondence (one gallery-one probe) to optimize results. The second data set, “RAN_MultiRec_1000Hz,” contained also multiple recordings per subject and additionally the recordings were in raw format, i.e., the eye tracker signals directly captured at 1000Hz.

In Table 10
                        , we present the technical characteristics of the additional data sets, such as the calibration accuracy, the recording validity, and the information about the time intervals. In Table 11
                        , we present the performance results of the BioEye 2015 methodologies for the additional data sets. In general, we can observe a relative drop in performance for all methodologies. The top Rank-1 IR is 70.1%, compared to the 96% for the original data sets. The difference between the results for the original data sets (for the RAN stimulus) and the RAN_MultiRec_250Hz data set can be as high as 44.6% (M
                        =21.1%, SD=13.8%), whereas the observed difference in results is even larger for the RAN_MultiRec_1000Hz data set, reaching a maximum of 58.5% (M
                        =22.6%, SD=16.9%).

The previous presentation of the cotemporary research and the achieved results in eye movement biometrics can serve as the basis for valuable discussion on the perspectives and the open challenges in the field. In this section, we discuss important topics related to the current research of eye movement biometrics under the scope of BioEye 2015 competition results, and we provide several suggestions for the future research in the field.

The examination of the previous works and of the methodologies of BioEye 2015 demonstrates that there is a variety of eye movement features that can potentially carry biometric information. It can be observed that many of the participants of BioEye 2015 used similar features, and this can be partially attributed to the previous research conducted in eye movement biometrics and partially on the discriminative power of some specific features. It can be hypothesized that the information enfolded in some of the features can be highly correlated, and this redundancy can in turn affect the discriminatory power of a methodology. This can explain, to some extent, the effectiveness of feature selection schemes used in some of the methods. Thus, a topic for future research in eye movement biometrics should be the investigation of features reliability in terms of discriminatory power and correlation properties.

Regarding the biometric accuracy in the field of eye movement biometrics, it should be noted that there is continuous improvement the last few years. Some of the identification rates achieved by the top performing methods of BioEye 2015 were particularly high; however, it should always be considered that they were achieved under the scope of a competition. Several steps can lead in optimization for the specific data sets, as, for example, the utilization of the one-to-one correspondence information (this can also partially explain the drop in performance for the multi-recording data sets). As a result, a second guideline for future research may involve the examination of features stability and the effects on performance in the case of multiple enrollments.

The effects of template aging on the eye movement biometrics performance have been demonstrated by previous research studies [38,81]. The results of BioEye 2015 confirmed the effects of template aging, as the performances for the long-time interval data sets were inferior compared to those for the short-time interval data sets. Thus, the future research in eye movement biometrics should strongly focus on a comprehensive evaluation of the long-term stability of biometric features and techniques. Such an investigation should involve short-time, medium-time, and long-time interval recordings from matched subject populations, and the exploration of the biometric features that remain relatively unaffected by physical and behavioral changes induced by time.

In real-world scenarios, there are several factors that can interfere with the functionality of the algorithms used to perform eye movement biometrics. Thus, there is a strong need for a systematic examination of the robustness of algorithms under different capturing specifications (e.g., sampling frequency, spatial accuracy, spatial resolution, environmental noise, etc.) and the inspection of features' stability under different capturing principles (e.g., optical tracking, infrared reflection, etc.). Another interesting topic of research concerns the exact investigation of any gender-specific or age-specific effects on the extracted biometric features, and their general impact on eye movement biometrics.

@&#CONCLUSION@&#

This work presented a general overview of the current research conducted in the field of eye movement biometrics. This presentation included a parallel description of the main features and techniques employed in previous research efforts and the major findings from “BioEye 2015: Competition on Biometrics via Eye Movements”. The provided descriptions and the presented examples form a compact manual for the current research in eye movement biometrics. Furthermore, this work highlights the emerging challenges in the field, and aims to further stimulate the research for improving the performance and trustworthiness of eye movement biometrics.

@&#ACKNOWLEDGMENTS@&#

We would like to express our gratitude to the participants of BioEye 2015 competition for their submissions, for providing their method descriptions, and for their valuable comments. We would like to thank SensoMotoric Instruments (SMI) for sponsoring BioEye 2015 competition prize in a form of REDn scientific eye tracker given to the winner of the competition. The recording of the competition database was supported in part by NSF CAREER grant no. CNS-1250718 and NIST grant nos. 60NANB12D234 and 60NANB14D274. The processing of the results of BioEye 2015 competition was made possible in part by the Google Research Award no. 2014_R1_308. Special gratitude is expressed to Dr. E. Abdulin, T. Miller, Ch. Heinich, and N. Myers for proctoring eye movement recordings.

@&#REFERENCES@&#

