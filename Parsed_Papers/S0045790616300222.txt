@&#MAIN-TITLE@&#Hardware implementation of real-time Extreme Learning Machine in FPGA: Analysis of precision, resource occupation and performance

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Extreme Learning Machine (ELM) on-chip learning is implemented on FPGA.


                        
                        
                           
                           Three hardware architectures are evaluated.


                        
                        
                           
                           Parametrical analysis of accuracy, resource occupation and performance is carried out.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

FPGA

Extreme Learning Machine - ELM

Neural network training

Neural network hardware

On-chip machine learning

Embedded systems

@&#ABSTRACT@&#


               
               
                  Extreme Learning Machine (ELM) proposes a non-iterative training method for Single Layer Feedforward Neural Networks that provides an effective solution for classification and prediction problems. Its hardware implementation is an important step towards fast, accurate and reconfigurable embedded systems based on neural networks, allowing to extend the range of applications where neural networks can be used, especially where frequent and fast training, or even real-time training, is required. This work proposes three hardware architectures for on-chip ELM training computation and implementation, a sequential and two parallel. All three are implemented parameterizably on FPGA as an IP (Intellectual Property) core. Results describe performance, accuracy, resources and power consumption. The analysis is conducted parametrically varying the number of hidden neurons, number of training patterns and internal bit-length, providing a guideline on required resources and level of performance that an FPGA based ELM training can provide.
               
            

@&#INTRODUCTION@&#

The Extreme Learning Machine (ELM) algorithm has revived the interest in Single Layer Feedforward Neural Networks (SLFN) since it eases the neural network training procedure. Hardware implementations have explored other computing paradigms, [1–3], but ELM makes SLFN possible in applications where it was not allowed before due to computational requirements, especially in on-line systems with real-time data processing and tight timing constraints.

ELM algorithm is rooted in the previous idea defined as Random Vector Functional Link (RVFL) networks, [4], but introduces new concepts and advantages that outperforms RVFL, [5]. Thus, ELM is based on random hidden layer weights and a linear adjustment for the output layer. As advantages, it provides similar or better results in generalization performance than previous iterative training methods without the need for manually adjusting parameters to find the best behavior. Further, ELM can use a wide range of activation functions, including those piece-wise linear. Finally, it is remarkable the combination of reduced computation requirements and extremely fast learning speed, surpassing previous training algorithms.

The reduced and fixed training time is one of the ELM properties that has contributed to its wide application in different fields, especially in battery operated devices where training time is a key factor. ELM has been successfully applied in the power energy area for failures and overload detection, in electricity distribution lines, and electricity price prediction. Other applications are related to biometrics, soldering inspection, or very fast object detection. In the field of control systems, ELM was already applied to real-time computation of friction in automobiles, new control strategies in non-linear control, or non-linear behavior compensation in fiber optic communications. Additional applications are reported for bank client classification, multi-categories classification applications, discrimination of evoked potentials, [6], or epileptic EEG detection, amongst others.

Moreover, the characteristic fixed and tight temporal requirements enable ELM to land in the hardware implementation arena to obtain efficient classifiers, usually implementing the forward phase. As an example, an FPGA implementation of the neural network forward phase was proposed in [7], which training phase was calculated on a PC. Others approaches have used VLSI techniques, [8]. However, its temporal requirements make ELM a good candidate to drive also on-chip training in real-time applications.

There is a current trend to implement on-chip learning for pattern recognition, facial recognition and complex learning behaviors. As an example, [9] proposed an on-line learning ANN performing a model training every new arriving data, to monitor and forecast the indoor temperature in a smart home, improving the energy efficiency of the HVAC (Heating, Ventilating and Air Conditioning); [10] focused the problem of sequential learning in mobile devices in real-time by a real-time learning algorithm for face recognition related applications; or [11], that proposed a real-time learning of neural networks to the prediction of future opponent robot coordinates in a specific environment.

The interest in hardware implementation of ELM training arises in the applications where input dataset does not increase excessively during the application, as in DBS (Deep Brain Stimulation) surgery, [12]. However, much more potential real-time applications can be driven by an on-line sequential learning algorithms as OS-ELM, which is a variation of basic ELM, and that anyway needs the implementation of the basic ELM training for initialization. Diverse OS-ELM sequential learning applications have been proposed to the date, as an example [5] adapted a gesture recognition model to new users automatically, getting high recognition accuracy.

As the expansion of on-chip learning systems is just beginning, and ELM is a good candidate to implement it, then arises the research question of how many resources usage the ELM training algorithm need, and which performance can it provide on current FPGA devices. Answering these questions parametrically (it is as a function of parameters like word width, number of training patterns or number of hidden neurons) would offer a quick sight to the needs and requirements of any application to be implemented on FPGA using on-chip ELM learning.

To answer this questions, we propose a hardware implementation of an SLFN, including ELM training, and the analysis of the performance, accuracy, resource occupation and power consumption that an FPGA implementation demands. The analysis was conducted parametrically varying the number of hidden neurons, number of training patterns and bit-length. The subject of the analysis was three proposed hardware implementations, all three under FPGA with a fixed-point implementation of the ELM training of a generic SLFN neural network. At the end, the results provide a guideline on expected resources, accuracy and performance that an FPGA implementation can provide for ELM batch training; proposes different hardware implementations for ELM training, describing a customizable core to fit multiple applications by simple parameter modification; and allows to compare between computational methods and parallelizing approaches, balancing among accuracy, logic occupation and performance. The results are applicable to any classification or regression application based on an SLFN neural network, providing a guideline on required resources and level of performance that an FPGA-based ELM learning can demand.

This paper is organized as follows. Details of the proposed architectures and computational methods are described in Section 2. Section 3 describes the hardware structures. Results of the analysis and discussion are presented in Sections 4 and 5, respectively. Finally, Section 6 concludes the paper.

The basic principle of ELM is that parameters of hidden nodes (the input weights and biases for additive hidden nodes or kernel parameters) need not be traditionally tuned by conventional learning algorithms (e.g. gradient descent-based method). The hidden nodes parameters can be randomly assigned. Right after, the output weights linking the hidden layer to the output layer can be analytically determined through simple generalized inverse operation of hidden layer output matrices [13].

We briefly delineate the principles of the ELM algorithm (Section 2.1) and explain and justify the computation method selected (Section 2.2).

Let 
                           
                              D
                              =
                              (
                              
                                 x
                                 i
                              
                              ,
                              
                                 o
                                 i
                              
                              )
                           
                        ; i = 1
                           
                              ,
                              …
                              ,
                           
                         N, be a set of N patterns where 
                           
                              
                                 {
                                 
                                    x
                                    i
                                 
                                 }
                              
                              ∈
                              
                                 R
                                 
                                    d
                                    1
                                 
                              
                           
                         are the input and 
                           
                              
                                 {
                                 
                                    o
                                    i
                                 
                                 }
                              
                              ∈
                              
                                 R
                                 
                                    d
                                    2
                                 
                              
                           
                         the output data for the training set, so that the goal is to find a relationship between {x
                        
                           i
                        } and {o
                        
                           i
                        }. If there are M nodes in the hidden layer, the general jth SLFN output is given by o
                        
                           j
                        :

                           
                              (1)
                              
                                 
                                    
                                       o
                                       j
                                    
                                    =
                                    
                                       ∑
                                       
                                          k
                                          =
                                          1
                                       
                                       M
                                    
                                    
                                       h
                                       k
                                    
                                    ·
                                    f
                                    
                                       (
                                       
                                          w
                                          k
                                       
                                       ,
                                       
                                          x
                                          j
                                       
                                       )
                                    
                                 
                              
                           
                        where 1 ≤ j ≤ N, w
                        
                           k
                         stands for the parameter of the kth element of the hidden layer (e.g. weights and biases in a SLFN) and can be denoted as the matrix W, f is the activation function generating the output of the hidden layer (applied to the scalar product of the input vector and the hidden weights), and hk
                         are the weights connecting the kth hidden element with the output layer (1 ≤ k ≤ M), which are the target values to obtain. For the output vector o, Eq. (1) can be expressed in matrix notation as 
                           
                              o
                              =
                              G
                              ·
                              h
                              ,
                           
                         where h is the vector of weights of the output layer and G is given by:

                           
                              (2)
                              
                                 
                                    G
                                    =
                                    
                                       (
                                       
                                          
                                             
                                                
                                                   f
                                                   
                                                      (
                                                      
                                                         w
                                                         1
                                                      
                                                      ,
                                                      
                                                         x
                                                         1
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                …
                                             
                                             
                                                
                                                   f
                                                   
                                                      (
                                                      
                                                         w
                                                         M
                                                      
                                                      ,
                                                      
                                                         x
                                                         1
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                          
                                             
                                                ⋮
                                             
                                             
                                                ⋱
                                             
                                             
                                                ⋮
                                             
                                          
                                          
                                             
                                                
                                                   f
                                                   
                                                      (
                                                      
                                                         w
                                                         1
                                                      
                                                      ,
                                                      
                                                         x
                                                         N
                                                      
                                                      )
                                                   
                                                
                                             
                                             
                                                ⋯
                                             
                                             
                                                
                                                   f
                                                   
                                                      (
                                                      
                                                         w
                                                         M
                                                      
                                                      ,
                                                      
                                                         x
                                                         N
                                                      
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

After random initialization of the M parameters of the hidden layer, w
                        
                           k
                        , the weights of the output layer are obtained by the Moore–Penrose generalized inverse [14] according to the expression 
                           
                              h
                              =
                              
                                 G
                                 †
                              
                              ·
                              o
                              ,
                           
                         where 
                           
                              
                                 G
                                 †
                              
                              =
                              
                                 
                                    (
                                    
                                       G
                                       t
                                    
                                    ·
                                    G
                                    )
                                 
                                 
                                    −
                                    1
                                 
                              
                              ·
                              
                                 G
                                 t
                              
                           
                         is the pseudo-inverse matrix (superscript t means matrix transposition). Once G† is calculated, the weights h are obtained and thus the system is trained according to the proposed set of training patterns 
                           D
                        .

Typically, the number of patterns is large compared to the number of hidden nodes (N > >M). In this case 
                           
                              h
                              =
                              
                                 G
                                 †
                              
                              ·
                              o
                           
                         becomes an over determined equations system as we have more equations than unknowns. They can be solved using generalized inverse matrices, which create the normal or least square equations where the unknowns are those minimizing the mean square error [15]. The obtained solution meets three important properties [16]: provides the least training error, the least norm of all possible solutions, and guarantees that the minimum norm solution is unique.

The main steps in hardware implementation of the ELM algorithm are those involved in computing the matrix 
                           
                              
                                 G
                                 †
                              
                              =
                              
                                 
                                    (
                                    
                                       G
                                       t
                                    
                                    ·
                                    G
                                    )
                                 
                                 
                                    −
                                    1
                                 
                              
                              ·
                              
                                 G
                                 t
                              
                              ,
                           
                         where G is known once input weights and hidden node biases W are randomly assigned. Thus, required computations are matrix inversion, matrix transposing and matrix multiplication.

Matrix inversion is one of the most important operations when dealing with ELM training. There exist different decomposition methods for matrix inversion computation, such as QR, LU or Cholesky. The selection of the decomposition method depends on the characteristics of the given matrix. For non-square matrices or when direct inversion to recover the data performs poorly, the QR decomposition is used to generate an equivalent upper triangular matrix.

All three architectures proposed in this work make use of QR decomposition (QRD) for matrix inversion, since it simplifies and reduces the computational complexity of the matrix inversion operation [17]. The other methods were discarded due to greater complexity ([18]), especially in the case of high dimension rectangular matrices as is the case of ELM.

The basic goal of QR decomposition is to factor a matrix as a product of two matrices typically called Q and R. For a given square matrix A, its inverted matrix can be computed as 
                           
                              
                                 A
                                 
                                    −
                                    1
                                 
                              
                              =
                              
                                 
                                    (
                                    Q
                                    ·
                                    R
                                    )
                                 
                                 
                                    −
                                    1
                                 
                              
                              =
                              
                                 R
                                 
                                    −
                                    1
                                 
                              
                              ·
                              
                                 Q
                                 t
                              
                           
                        . If A is a non-square Matrix, the pseudo-inverse of A is found as 
                           
                              
                                 A
                                 †
                              
                              =
                              
                                 R
                                 
                                    −
                                    1
                                 
                              
                              ·
                              
                                 Q
                                 t
                              
                              ,
                           
                         where Q is also a non-square matrix. In both cases, matrix Q is orthonormal with 
                           
                              Q
                              ·
                              
                                 Q
                                 t
                              
                              =
                              I
                           
                         (t shows transpose, I is the identity matrix) and R is a triangular matrix.

Three different algorithms are possible [19] to perform QR decomposition:

                           
                              •
                              Gram–Schmidt. Is one of the key algorithms in linear analysis of finite spatio vectorial systems. This method leads to polynomials and orthogonal functions in the function space.

Householder reflections. Is a linear transformation that describes a reflection about a plane or hyperplane containing the origin. It is typically used when least square methods need to be solved.

Givens rotations. Are orthogonal plane rotations used to eliminate elements within a matrix. By applying a series of successive Givens rotations, a matrix can be triangularized by eliminating the elements beneath the diagonal. It is used for adaptive filter implementation, requiring twice the computational cost compared to previous methods [20].

Givens rotations are frequently used to solve QR decomposition in hardware implementations due to the IP cores availability [20]. At the same time, Givens rotations is most accurate and stable than classical Gram–Schmidt algorithm. Moreover, the classical Gram–Schmidt algorithm is unstable under hardware implementation, [21]. On the other hand, Gram–Schmidt uses less resources than Givens Rotations or Householder reflections. Nevertheless, stability and optimization of resources can be combined using a slightly modified version of classical Gram–Schmidt, referred to as Modified Gram–Schmidt (MGS), [16,17]. This MGS algorithm has proven to be numerically equivalent to Givens Rotations QR factorization, [15], requiring less operations and being able to be implemented with fewer resources.

The proposed implementation follows a fixed-point architecture to optimize resources usage and boosting the maximum frequency of operation. This implies eventual quantization and round-off errors that induce losses in accuracy and hence, in orthogonality of Q. However, [22] established that Modified Gram–Schmidt (MGS) algorithm is still stable and accurate to the integer multiples of the machine precision under fixed-point precision, for a well conditioned non-singular matrix. For the above reason, the proposed implementation follows Modified Gram–Schmidt (MGS) to solve QR decomposition as other works before, [23].

Thus, the computation process to invert a square matrix (the pseudo-inverse matrix in the general case) is as follows:

                           
                              •
                              Apply the QR decomposition

Invert R through a Triangular Matrix Inversion (TMI). Since R is triangular, its inverse computation procedure is simpler than the inverse computation of A.

Use the matrix multiplication 
                                    
                                       
                                          A
                                          
                                             −
                                             1
                                          
                                       
                                       =
                                       
                                          R
                                          
                                             −
                                             1
                                          
                                       
                                       ·
                                       
                                          Q
                                          t
                                       
                                    
                                  (back-substitution) to obtain the required matrix inversion.

Once matrix inversion has been done, output layer weights h are obtained by a matrix multiplication between the resulting matrix G† and the known output values for the training set, 
                           
                              h
                              =
                              
                                 G
                                 †
                              
                              ·
                              o
                           
                        .

Usually, many applications use online learning algorithms due to their ability to learn by the sequentially arriving data. An algorithm that allows this continuous learning is OS-ELM, a variation of the ELM algorithm, [24,25]. If we pay attention to the hardware implementation of OS-ELM learning method, we can see that the implementation of the ELM training is also required. It is due to the necessary initial calculation of output weights h
                        0 as the first iteration of the algorithm, using batch learning for the initial training dataset.

Some applications use sequential learning during a short span, in which not a big amount of sequential patterns will incorporate during the process. These applications can take profit of ELM training either iteratively increasing the training dataset or replacing some less significant patterns in the original dataset. If this is the case, the new information can be learnt on-chip by using an IP core implementing the ELM algorithm proposed in this work.

An example are medical or surgery applications seeking to locate or detect the point in which an electrode must be placed to apply certain therapy. In this case, the use of a decision support tool is of great help. The learning starts from an initial dataset which is continuously changing by adding new collected data as surgery is taking place. This learning contributes to fine-tuning the location support tool for the patient through data obtained sequentially from himself, progressively increasing the power of the decision support. As it can be seen, these applications imply the collection of data, but the real-time retraining of the support tool lasts a short time span until the physicist/surgeon decides the optimal point applying the therapy.

A more specific example is Deep Brain Stimulation (DBS) surgery. In this therapy, a microelectrode stimulator is inserted into the brain to face neurological disorders such as Parkinson’s disease. But the key of the process is to find the adequate brain place where stimulation must be done. To obtain better precision and physiological validation of the target detection, surgeons use microelectrode recordings (MER) which are introduced step by step until the inner brain, recording action potential signals at each step. Fig. 1
                         illustrates the penetration trajectory of a MER microelectrode, first registers correspond to thalamus, then pass through the subthalamic nucleus and finally lasts registers corresponds to substantia nigra brain area. First and lasts registers from each trajectory can fed the system labelled automatically. Moreover, as several trajectories are normally done, registers from previous trajectories can be used to generate the patterns that increment the dataset. All this new information can be used to launch ELM training repeatedly, being its hardware implementation of great help to surgeons, [12].

In this work, we propose three hardware architecture alternatives for the implementation of the ELM training of a single layer feedforward neural network (SLFN). All three proposals use QR decomposition and Modified Gram–Schmidt methods to cope with matrix inversion since these methods optimize resources usage and stability, as seen in Section 2.2.

It is remarkable that all three proposals provide general hardware implementations. It implies that the hardware can be adapted for the application requirements modifying implementation parameters as: neural network size (number of hidden neurons), internal computation architecture, internal and external data bit-length and size of the training dataset (number of training patterns). Thus, all three architectures are fully parameterizable to be easily adapted to any classification or regression application requirements.

All three approaches use fixed-point arithmetic to reduce resource usage and increase performance. Typically, floating-point arithmetic is used when the computation takes place on a desktop PC. However, in embedded systems it is desirable to use fixed-point arithmetic to optimize resource usage and performance. Nevertheless, to obtain accurate results avoiding the use of floating-point arithmetic, the fixed-point arithmetic needs a flexible word-length scheme for internal computations. It is an important characteristic that helps to improve accuracy. Hence, all architecture proposals use bit-length of internal and external computations as adjustable parameters. The main objectives of the developed hardware implementation are the following:

                        
                           •
                           Parameterizable. The ELM characteristics can be defined for each problem. The matrix inversion computation is offered as a stand-alone computation module for general usage, easily parameterized so that different applications can make use of it.

Accurate. Accuracy must be comparable to floating-point arithmetic.

Balanced performance. Due to the code optimization for arithmetic computations, a good trade-off between accuracy, logic occupation and performance must be achieved.

To sum up, the aim of proposing three approaches is: to study how novel computation architectures cope with ELM training on FPGA; to parametrically measure accuracy, performance and logic occupation; and to outline a comparison between them. Thus, for all proposals it has been used novel computation architectures and optimized control data flow, in order to obtain a fast and accurate hardware processing unit, according to selected parameters.

All three versions of ELM training hardware implementations (from now named ELMv1, ELMv2 and ELMv3 versions) differ basically in the use of parallelism and the computation procedure. The computation procedures consist on different approaches to solve the equations system given by 
                           
                              G
                              ·
                              h
                              =
                              o
                           
                        .

Despite three implementation approaches having been proposed, they are based on two computation procedures:

                           
                              1.
                              
                                 Square Matrix Decomposition: Matrix 
                                    
                                       Z
                                       =
                                       
                                          G
                                          t
                                       
                                       ·
                                       G
                                    
                                  is built, and the QR decomposition of Z is done.


                                 Rectangular Matrix Decomposition: The QR decomposition of matrix G is directly calculated.

The difference between hardware implementations is in the training procedure referred as TRAIN_MODULE, Section 3.3. The rest of the modules, the external hardware interface and the configuration parameters are the same in all versions. Differences between hardware implementations are in the used computation procedure and in the parallelization effort. Table 1
                         lists the computation summary for each proposed method, readily:

                           
                              1.
                              ELMv1. Use serial computation procedure for square matrix. Computations are performed sequentially in order to share arithmetic units and reduce logic occupation.

ELMv2. A parallelized version of ELMv1. Parallelization efforts are focused on the most time consuming computations, i.e. the QR decomposition and the Triangular Matrix Inversion (TMI) algorithm. This implementation requires duplication of the matrix multiplication unit. Meanwhile, as shown in Table 1, step 5 (inverse Z matrix computation) and step 6 (G† matrix computation) are indirectly obtained by concurrent calculation of temp1 and QRD of Z in step3, and 
                                    
                                       R
                                       
                                          −
                                          1
                                       
                                    
                                  computation and temp2 in step 4. Since two computation algorithms are carried out in parallel, two computation steps are saved compared to ELMv1.

ELMv3. The computation procedure uses the rectangular matrix decomposition. As rectangular QR decomposition is a time consuming process, similar parallelization to ELMv2 is done with one more computation step reduction compared to ELMv2 since no square matrix is required (step 2).

As a summary, it is remarkable to note that first ELM version (ELMv1) is computed in a sequential fashion using square matrices; second ELM version (ELMv2) uses parallelization and computes using square matrices; and finally, third ELM version (ELMv3) uses parallelization and computes using rectangular matrices. Parallel processing in ELMv2 and ELMv3 allows faster processing by reducing the number of steps in the algorithms (Table 1).

The proposed hardware implementation is conceived as an IP core (Intellectual Property block). Hence, it can be used as an standalone module, or as a part of a more complex System on Chip with additional peripherals, embedded microprocessor, etc.

The hardware structure is based on four main blocks:

                           
                              1.
                              TRAIN_MODULE : ELM training block where weight calculation is carried out during batch learning.

ANN_MODULE : On-line working mode for output data calculation once the weights are obtained.

RAM memories for data storage. These blocks are shared by different computation units and consist of:
                                    
                                       (a)
                                       RAM_W: Stores hidden layer weight matrix W values.

RAM_T and RAM_D: Stores desired output values vector o for training input data x (T stands for target and D for data).

RAM_Wo: Stores weight vector h for the output layer values.

Data flow control logic.


                                 Run or On-Line mode. After the NN is trained, this mode is executed for classification or regression of the targeted application. Input data is accepted and computation procedure begins. When computation is done, output data is ready to be read and a new input data can be accepted.


                                 Training mode. Data required for training are loaded (matrix W and vectors x and o) and, after that, the training process is carried out.


                                 Memory Access. If no training or run mode is active, read/write of RAM stored data can be done. This is very useful for debugging and initialization.

Finally, as it was mentioned before, the system is configurable by modifying the implementation parameters. This reconfiguration capabilities allows the hardware implementation to suit any application. Thus, the hardware can be easily adapted to any training dataset, number of hidden neurons, number of outputs, and internal or external word length, amongst others. Hence, the training process and neural network structure can be parameterized by the following parameters:

                           
                              •
                              N_FIELDS: size of data input vector.

N_PATTERNS: number of training patterns (NP).

N_NEURONS: number of hidden neurons (NH).

N_OUTPUTS: number of outputs.

N_DATA, M_DATA: bit-length of input data (integer and fractional).

NBITS_INTEGER, NBITS_FRACTION: bit-length of internal data (integer and fractional).

WIDTH_RAMi, i = 1
                                    
                                       ,
                                       …
                                       ,
                                       7
                                    
                                 : bits needed for the defined RAM.

NORMALIZE: Activation of the scaling function.

NORM, NORM1, NORM2: size of scaling in different steps of ELM algorithm.

L, K1, K2: Constants for the fuzzy activation function.

These parameters must be specified by the IP core user to generate the desired hardware. From this point we will talk about bit-length to refer to the internal data representation word width.

This module performs the ELM training, calculating the weights that on-line working mode needs to run properly. TRAIN_MODULE is different for each implementation version. However, for code reuse, all versions use the same basic computation modules: matrix multiplication, QR decomposition and TMI.

The TRAIN_MODULE block is subdivided as Fig. 2
                         shows. Basically, it is composed of a general control unit TRAIN_CTRL, OPS_UNIT block containing the basic arithmetic units, ADDR_UNIT for RAM address generation, small size RAM blocks for temporary storage (in addition to external RAM blocks) and matrix operation state machines (FSM_MM for matrix multiplication, FSM_QRD for QRD and FSM_TMI for triangular matrix inversion).

Additionally, OPS_UNIT include low level computation blocks as simple multiplier, vector multiplier, division and square root units, used in different computation algorithms. Multiplication operation is designed to take advantage of embedded multipliers in FPGA devices. Vector multiplication is using dual port RAM and two parallel multipliers to perform two multiplications in a single clock cycle. Division and square root use sequential algorithms where N clock cycles are required for an N-bit operation. Additionally, bit scaling in the vector multiplication and division modules must be performed at certain stages of the calculation process to avoid overflow.

Note that, as fixed-point arithmetic is used, the accuracy is sensitive to the number of bits in the fractional part, and the integer part is important to avoid data value overflow.

One of the most important arithmetic modules is related to the QR decomposition algorithm (step 3 in Table 1). The block FSM_QRD allows the wise use of computation units and distributes the calculation process among different stages, obtaining a reduced calculation time and an intensive use of computation units for each of the states. Memory management is also very important during QRD calculation, which is solved by allocating specific memory areas and using them iteratively once the stored data are no longer needed. The logic resource usage is kept low by using specific memory addressing [26].

The decomposition process for a given matrix X is structured in different computation steps:

                           
                              1.
                              The diagonal elements of R matrix are obtained according to 
                                    
                                       
                                          R
                                          
                                             i
                                             i
                                          
                                       
                                       =
                                       
                                          
                                             
                                                X
                                                i
                                             
                                             ·
                                             
                                                X
                                                i
                                             
                                          
                                       
                                    
                                 
                              

The column vectors of matrix Q are calculated by 
                                    
                                       
                                          Q
                                          i
                                       
                                       =
                                       
                                          
                                             X
                                             i
                                          
                                          
                                             R
                                             
                                                i
                                                i
                                             
                                          
                                       
                                    
                                 .

The rest of R matrix elements are obtained in a loop.

Matrix X is updated as 
                                    
                                       
                                          X
                                          j
                                       
                                       =
                                       
                                          X
                                          j
                                       
                                       −
                                       
                                          R
                                          
                                             i
                                             j
                                          
                                       
                                       ·
                                       
                                          Q
                                          i
                                       
                                    
                                  and the process is repeated for all columns.

Matrix X is used to store initial values, serving also as intermediate memory storage during the computation process. This matrix, together with the vector multiplication and the square root module, is used to calculate the Euclidean norm. The result is stored in matrix R, then, division of the first column of X by the Euclidean norm is done to obtain the first column of matrix Q. The vector multiplication module is used to multiply Q and X matrices in order to calculate the first row of R. The signed multiplication and a subtraction unit updates the values of matrix X; this procedure is repeated for all the columns in Q and rows in R until the process is completed.

State machine FSM_QRD implements the steps of the algorithm and controls specific operation units inside the general block OPS_UNIT.

Along all the computation steps, the algorithm involves the calculation of vector modulus and vector division by a constant value, requiring computations as vector product, root square and division, being performed by the OPS_UNIT.

As mentioned before, this version use serial computation for square matrix, being the most time consuming steps the QR decomposition and the TMI algorithm (steps 3 and 4 in Table 1). Fig. 2 shows the block organization and control flow.

The most important block affecting occupation, performance and accuracy of results is related to the computation units. This block is called OPS_UNIT and it is controlled by the general control system TRAIN_CTRL (Fig. 2). Internally, it includes specific control machines for different arithmetic operations. The OPS_UNIT block contains the four main blocks to operate: Vector Multiplication (MultVec_Unit), Root Square (SQRT), Division (DIV) and Multiplication–Subtraction (SM), able to assess two operations in a single clock cycle. The DIV unit is used in QRD and TMI algorithm computation, SQRT and SM is used in QRD, and MultVec_Unit is used in QRD, TMI and matrix multiplications.

Data scaling is used to avoid data overflow or underflow when operation results are too high or low, reducing the accuracy but allowing the usage of reduced data bit-lengths. After QRD is finished, the Triangular Matrix Inversion (TMI) procedure is performed (step 4 in Table 1), involving basic multiplication and division operations. Finally, basic matrix algebra is intensively computed in steps 5 to 7 using the OPS_UNIT, specifically, MULT_VEC.

In the second version, parallelization of computations in ELMv1 are done when possible. In this case, the control module is modified (OPS_UNIT2) replicating some arithmetic units in the OPS_UNIT block from ELMv1, to add concurrency. The parallelization allows to perform different vector multiplications (states S3b and S4b) and the QRD and triangular matrix inversion (states S3 and S4) concurrently (Fig. 3
                        ). The nature of computation procedure prevents greater degree of parallelism.

The control module is modified by splitting address space and control signals. It assigns one entire address unit to the FSM_MM block and applies changes in the internal control machines for the TMI, QRD, memory access and other control signals for the synchronization of all processes in the modified data flow.

As stated in Table 1, the third ELM version operates on rectangular matrices. This fact makes a difference with previous versions as the QRD algorithm is applied directly to the matrix G, and no additional matrix multiplication is required to obtain the final result for output layer weights h. As a result, S2 and S3b states from ELMv2 (blue in Fig. 3) are removed in the training control scheme of ELMv3. This procedure reduces the computation steps, but the rectangular matrix has a higher dimension. It is necessary to modify the control unit TRAIN_CTRL for proper data addressing and flow into computation units.

States S4b and S4 are performed concurrently. The arithmetic block OPS_UNIT2 was adapted in order to adapt new control signals and data scaling. In this case, an important modification has to do with scaling. As it is not required to construct the square matrix, high values are not present in the input data matrix to the QRD algorithm. Thus, initial scaling is performed inside the QRD process.

This module was developed to work on-line, providing neural network output results efficiently. It uses the results of the ELM training to calculate the output according to the desired application for classification, prediction, estimation, etc. The calculation is done independently of the TRAIN_MODULE, allowing multiple tests of different learning algorithms. Thus, ANN_MODULE remains unchanged for all three ELM versions.

ANN_MODULE follows a serial procedure to obtain the output values. First, the hidden layer weight matrix W is multiplied by the input vector x and the neuron activation function is applied to the result, obtaining the hidden layer output. Second, the output layer weight vector h is multiplied by the hidden layer output, and the result is applied to the activation function to obtain the output values.


                        Fig. 4
                         shows the block diagram of ANN_MODULE, including the data flow of the ANN_CTRL synchronization module, the FSM_MM as the state machine controlling matrix multiplication performed by ‘MultVec_Unit’, the FUZZY_FUN module implementing a fuzzy activation function [27] for the neuron, and the ADDR_UNIT module which generates the proper addresses for data access to RAM memories: RAM_W for hidden layer weight values, RAM_Wo for output layer weights h, RAM_I for input data, and RAM_O for hidden layer output data.

@&#RESULTS@&#

As already mentioned before, this work aims to provide a generic analysis of the ELM training of a SLFN network, as different implementation parameters lead to different results. Thus, the results are obtained and presented in a parameterized way (as a function of parameters as number of hidden neurons, number of training patterns or bit-length), being applicable to any classification or regression application.

The object of study is a parametric one-output SLFN neural network, but the results remain valids for multiple-output SLFN networks. As it can be seen in Table 1, for ELMv2 and ELMv3 implementations, the use of output vector o is made in parallel with QRD decomposition and triangular matrix inversion, which are more time consuming than 
                        
                           m
                           p
                           1
                           a
                           temp
                           1
                           =
                           
                              G
                              t
                           
                           ·
                           o
                        
                      or 
                        
                           m
                           p
                           1
                           a
                           temp
                           1
                           =
                           
                              Q
                              t
                           
                           ·
                           o
                        
                      matrix multiplications. In this case, multiple output neurons are not an overhead. The difference is in the seventh computational step, Table 1, where the number of columns of o, temp1 and temp2 matrices depends on the number of output neurons. This influence is not significant due to two factors, the first is the low dimensionality of G† and the low complexity of step 7 matrix multiplication compared to previous steps (which have a much higher dimensionality, depending on the number of training patterns); and the second, and most important factor, is that the most consuming computation steps are related with matrix inversion, QRD decomposition and triangular matrix inversion, TMI. Thus, time and resource usage of training are not affected significantly by growing number of output neurons for a typical number of output neurons in classification and regression problems.

In view of this, it is used a standard regression problem with a one-output SLFN network to carry out the study. The results are a tight approach for typical multiple-output regression and classification problems.

Once defined, the three architectures were coded in VHDL. We used Xilinx ISE Design Suite 13.1 to carry out synthesis and implementation, and Xilinx XPower Analyzer 13 to perform power analysis. Analysis and results were based on an FPGA fixed-point implementation, 12 bit for external input and output data (sum of N_DATA and M_DATA parameters as described in Section 3.2) and a range from 14 to 40 for internal bit-length (integer and fractional parts, i.e. sum of NBITS_INTEGER and NBITS_FRACTION parameters as in Section 3.2) which will be referred throughout the paper as ‘bit-length’. The device used in this work was Xilinx Virtex6 XC6VCX75TL-1L FF484. It is important to note that this FPGA was selected to amply fit any of the three proposed architectures under all test conditions, allowing the analysis of computational burden without restrictions. Thus, resources usage was well below 100% of usage for any implementation option to avoid situations as implementing memory or multiplicators from logic slices or similar, which would distort the results. In case of any specific application, an FPGA device should be chosen accordingly to SLFN and ELM parameters (patterns, neurons, bit-length).

In order to validate results, a single output dataset is analyzed: the housing dataset, a standard dataset in the automatic learning area related to the price of houses around Boston. For this problem, 13 fields (number of inputs) and 500 samples were used, using up to half of this dataset for training of ELM versions (the actual number depends on the performed analysis when NP varies). The rest is used to validate the neural network performance using the online module ANN_MODULE according to the obtained weights from ELM training.

Testing is done generating 50 random repetitions for training and validation data as well as 50 random weight initialization. This procedure is repeated for a range of hidden layer neuron numbers (NH from 5 to 40), and bit-length (14 to 40 bits). The number of bits for the integer part (NBITS_INTEGER) was fixed according to the maximum value required to avoid overflow as higher number of neurons require higher number of bits in the integer part.

To assess the accuracy of each architecture, the resulting hardware output values were compared with floating-point implementation results using Matlab R2011b on Windows XP, running on a Core Duo T5600 1.8 GHz PC. Thus, accuracy results do not describe the error of the regression but the error that can be expected for a hardware implementation output using a set of implementation parameters. To measure this accuracy estimation the arithmetic mean absolute error value (MAE) is obtained for each ELM computation.

Each implementation reported data describing resource occupation as the amount of logic implementation using Look Up Tables (LUT) (none of implementations needed to use slice LUTs as memory); RAM memory for data storage, as RAMB18E blocks as much as RAMB36E blocks; and DSP48E for arithmetic units. Synthesis report also provided maximum frequency of operation. The procedure was repeated to obtain the performance and resource usage for a range of hidden layer neurons (NH from 5 to 40), internal bit-length (14 to 40) and number of training patterns (NP from 50 to 250).

The performance of a certain implementation is given by the maximum clock frequency and number of required clock cycles for computation. The number of clock cycles required for execution increases with bit-length for any number of hidden neurons (NH). This increment is clearly shown in Fig. 5
                         where clock cycles are represented as a function of bit-length for all three ELM architecture versions with fixed NH = 40 and NP = 250. It can be appreciated that linear increment is different depending on the architecture version (determination coefficient of 
                           
                              
                                 R
                                 2
                              
                              =
                              1
                           
                         in all cases).

This behavior can be modeled mathematically. Thus, execution cycles can be expressed as Eq. (3) for ELMv1, ELMv2 and ELMv3, being 
                           
                              n
                              b
                              i
                              t
                              s
                              =
                              N
                              B
                              I
                              T
                              S
                              _
                              I
                              N
                              T
                              E
                              G
                              E
                              R
                              +
                              N
                              B
                              I
                              T
                              S
                              _
                              F
                              R
                              A
                              C
                              T
                              I
                              O
                              N
                           
                        .

                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             c
                                             y
                                             c
                                             l
                                             e
                                             s
                                             _
                                             E
                                             L
                                             M
                                             v
                                             1
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                (
                                                1.5
                                                n
                                                b
                                                i
                                                t
                                                s
                                                +
                                                588
                                                )
                                             
                                             ·
                                             N
                                             
                                                H
                                                2
                                             
                                             +
                                             
                                                (
                                                1.5
                                                n
                                                b
                                                i
                                                t
                                                s
                                                +
                                                450
                                                )
                                             
                                             ·
                                             N
                                             H
                                             +
                                             14628
                                          
                                       
                                    
                                    
                                       
                                          
                                             c
                                             y
                                             c
                                             l
                                             e
                                             s
                                             _
                                             E
                                             L
                                             M
                                             v
                                             2
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                (
                                                1.5
                                                n
                                                b
                                                i
                                                t
                                                s
                                                +
                                                430
                                                )
                                             
                                             ·
                                             N
                                             
                                                H
                                                2
                                             
                                             +
                                             
                                                (
                                                1.5
                                                n
                                                b
                                                i
                                                t
                                                s
                                                +
                                                21
                                                )
                                             
                                             ·
                                             N
                                             H
                                             +
                                             12140
                                          
                                       
                                    
                                    
                                       
                                          
                                             c
                                             y
                                             c
                                             l
                                             e
                                             s
                                             _
                                             E
                                             L
                                             M
                                             v
                                             3
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                (
                                                0.5
                                                n
                                                b
                                                i
                                                t
                                                s
                                                +
                                                474
                                                )
                                             
                                             ·
                                             N
                                             
                                                H
                                                2
                                             
                                             +
                                             
                                                (
                                                254
                                                n
                                                b
                                                i
                                                t
                                                s
                                                +
                                                5120
                                                )
                                             
                                             ·
                                             N
                                             H
                                             +
                                             2900
                                          
                                       
                                    
                                 
                              
                           
                        
                     

As ELMv1 and ELMv2 use QR decomposition of square matrices, both have the same slope while intercept is greater for ELMv1, indicating that a reduced number of clock cycles is required for ELMv2. This feature reflects the performance optimization that parallelization introduces in ELMv2. On the other hand, greater slope and offset for ELMv3 curve reflects that QR decomposition of rectangular matrices have greater computational burden since the QR decomposition algorithm must compute a higher dimensional rectangular matrix, therefore requiring more division and square root operations, defined in a sequential process.

If we consider now to describe the number of clock cycles of execution in terms of the number of neurons in the hidden layer, NH, Fig. 6
                         provides this description for all three architecture versions and a range of bit-length between 16 and 30. Thus, measured cycles follow a quadratic law as a function of NH (with a determination coefficient of R
                        2 > 0.99991 in all cases).

In general, ELMv3 has the highest computational burden in number of clock cycles, followed by ELMv1, while ELMv2 has the lowest burden and its quadratic curve has the flattest, less progressive response. Thus, ELMv2 offers the best performance, saving nearly 30% of clock cycles compared to ELMv1 implementation and 40% of clock cycles in case of ELMv3.

Measured resources implement both the ELM training algorithm (TRAIN_MODULE) and the on-line working module (ANN_MODULE) required to properly run the neural system. The analysis of resources is based on the device internal blocks: DSP48E blocks (arithmetic units used, in this case, mainly for multiplications), slices containing LUT blocks for general logic and RAM blocks for memory.

The analysis of resources was performed depending on the number of hidden neurons NH and bit-length. In all cases, NP = 250 from housing dataset was used. Resources are shown as percentage of occupation, i.e. 100% denotes that all resources are occupied.

Concerning to the usage of LUT blocks, Table 2
                         describes occupation for NP = 250 and NH = 40. LUT usage grows slowly with the number of bits and it is the least restrictive resource for the ELM implementation. Note that ELMv2 and ELMv3 versions have identical occupation, being different from ELMv1. It can be explained by the parallelization procedure, which is the same in ELMv2 and ELMv3 versions and then, ELMv1 has fewer number of used slices for logic.

Concerning to the usage of DSP48E blocks, Table 3
                         describes how this usage increments with bit-length. The table shows that the number of DSP48E blocks remain constant until more units are needed, doubling resources then. It can be easily explained considering that DSP48E arithmetic units have fixed bit-length, and, when greater bit-length is required, more DSP48E blocks must be added to produce a longer bit-length arithmetic unit. A subsequent increase of bit-length does not require more DSP48E blocks until arithmetic unit bit-length is surpassed again. This behavior explains the discontinuities in Table 3.

It is important to remark that DSP48E arithmetic units usage remains independent of the number of training patterns, NP, and the number of hidden layer neurons, NH. In fact, neither NP nor NH contribute to the DSP48E usage because data are serially fed using the state machines and no additional resources are required when the number of patterns and/or hidden neurons grow, being the clock cycles the increased value. ELMv2 and ELMv3 use the same DSP48E resources, and ELMv1 uses less DSP48E blocks according to Table 3. It can be explained considering that ELMv2 and ELMv3 versions use parallelized computation, and ELMv1 use less DSP48E blocks because do not parallelize computation.

Concerning RAM block occupation, it grows linearly with bit-length for all three architectures (Fig. 7
                        ). The minimum determination coefficient of linear regression for all three versions is 
                           
                              
                                 R
                                 2
                              
                              =
                              0.9995
                              ,
                           
                         being a high value considering that RAM memory is assigned in blocks, not in bytes, thus discretizing the possible values.

RAM occupation is greater for ELMv3 followed by ELMv1, and the lowest occupation is given by ELMv2. ELMv1 and ELMv3 have similar results, with slopes being nearly the same, only existing between them a constant difference. RAM usage for ELMv2 is the lowest because the storage of matrix is not required whereas it is needed for ELMv1. Difference in RAM consumption between ELMv2 and ELMv3 is due to the usage of a square matrix in case of ELMv2, and a rectangular matrix for ELMv3.

On the other hand, RAM occupation grows with the number of hidden neurons (Fig. 8
                        ), but not proportionally. It behaves in a discontinuous manner, similarly to DSP48E blocks in Table 3. In this case, when bit-length surpasses a maximum, addition of RAM blocks is needed to be able to increase the bit-length. This addition implies a new maximum bit-length representation until which the RAM occupation will not need to be increased. Thus, RAM occupation behaves in a staired way, being constant versus NH until more blocks are needed, requiring allocation of more RAM blocks. The same situation is given in case of RAM occupation versus NP.

The maximum operation frequency of the synthesized circuit depends on parameters as bit-length, NH and NP. Fig. 9
                         represents the maximum operation frequency of two implementations, one is using a high number of hidden neurons and number of patterns, and a second implementation uses a minimum number of hidden neurons and number of patterns, all for ELMv2 architecture. It shows that, below 26 bits in bit-length, the maximum frequency is not much dependent on the number of hidden neurons and number of patterns (NH and NP) and, above 27 bits there is no NP neither NH dependency. In addition, the maximum operation frequency behavior is approximately the same for all three ELM versions. Fig. 10
                         shows that maximum frequency behaves similarly for all three architectures with the same parameters (NH = 40, NP = 250). Particularly ELMv2 and ELMv3 are more similar, and exactly the same up to 28 bits.

The staired shape of Figs. 9 and 10 is related to the increase of DSP48E blocks, provided that the minimum clock period is imposed by multiplication, acting as a bottleneck. Thus, when a step increment in DSP48E block usage arises, a decrease in speed is observed. As can be seen, the discontinuous behavior of DSP48E occupation versus bit-length (Table 3) corresponds to discontinuities in Figs. 9 and 10. Three vertical lines in Fig. 10 illustrate such discontinuities for DSP48E blocks in Table 3. As can be expected, these vertical lines match with the points where the maximum frequency shows discontinuities.

In any case, we must remark that discontinuities shown in performance and occupation are caused by the internal block structure and size of the selected device. Different FPGA families show discontinuities in different positions due to different block sizes. This situation would even disappear if ASIC technology was used, showing only the trend.

Accuracy analysis evaluates hardware accuracy by direct comparison of hardware results with floating-point Matlab implementation results. Thus, mean absolute value (MAE) errors were obtained from these output differences, for each ELM computation, when training and validation data are applied (validation error). In that case, the validation error as a function of bit-length and the number of neurons in the hidden layer (NH) are obtained.

An insight into the accuracy can be taken from the graphical representation of the validation error, shown in Fig. 11
                        , as the logarithmic representation of accuracy depending on bit-length and an intercept depending on NH, for ELMv2. A family of curves depending on the number of neurons in the hidden layer (NH), ranging from 5 to 40, are represented. As expected, the implementation provides better accuracy when using higher number of bits, being reduced as the number of neurons in the hidden layer is increased.

The regression for each NH experimental data value shown in Fig. 11 fits a straight line with a minimum determination coefficient of 
                           
                              
                                 R
                                 2
                              
                              =
                              0.99995
                           
                        . On the other hand, MAE variation with NH values fit a quadratic curve with a determination coefficient 
                           
                              
                                 R
                                 2
                              
                              =
                              0.9995
                           
                         as shown in Eq. (4). It can be observed that successive NH lines are not equispaced, doubling accuracy as the fractional bits increase by one.

                           
                              (4)
                              
                                 
                                    M
                                    A
                                    E
                                    =
                                    6.1586
                                    ·
                                    N
                                    
                                       H
                                       
                                          0.6326
                                          +
                                          1.1355
                                          l
                                          o
                                          g
                                          (
                                          N
                                          H
                                          )
                                       
                                    
                                    ·
                                    
                                       2
                                       
                                          −
                                          n
                                          b
                                          i
                                          t
                                          s
                                       
                                    
                                 
                              
                           
                        
                     

This result is formulated for ELMv2, but we can generalize it to ELMv1 and ELMv3 architecture versions provided that there are not significant differences between the mean of slopes in different architectures (ANOVA, F = 0.2, p = 0.6675).

Assuming an error level of 
                           
                              M
                              A
                              E
                              >
                              =
                              0.1
                              ,
                           
                         it appreciably affects the results of training and validation of the neural network, and 
                           
                              M
                              A
                              E
                              <
                              =
                              0.01
                           
                         provides acceptable results for training and validation. Fig. 11 represents a forbidden band for 
                           
                              M
                              A
                              E
                              >
                              =
                              0.1
                           
                         and a warning band for 
                           
                              0.10
                              <
                              M
                              A
                              E
                              <
                              =
                              0.01
                           
                        . Thus, any combination of bit-length and NH values resulting in an error below the warning band can be considered valid. This experimental model of accuracy allow us to explore a priori the best parameters of a neural network, fixing the accuracy we want to achieve and then calculating bit-length from NH, or vice versa.

Total power consumption analysis is divided in quiescent and dynamic power. Quiescent power is the power consumed by the device when it is powered up and configured with user logic, without any switching activity. On the other hand, dynamic power is the fluctuating power as the implemented system runs, representing the amount of power generated by the switching user logic and routing.

For the used device, quiescent power is almost independent of bit-length, NH, and NP, having a value of 
                           
                              1301
                              ±
                              3
                              
                              mW
                           
                         on the whole set of measures. Fig. 12
                         represents quiescent power as a flat basal line. On the other hand, dynamic power grows with bit-length, NP, and NH. In all cases, dynamic power was calculated considering user logic design switching at maximum operation frequency. Fig. 12 shows the total power consumption for ELMv2 implementation showing that total power consumption slightly grows with bit-length for both a high demanding system (NP = 250 and NH = 40) as a low demanding system (NP = 50 and NH = 5).

It can be considered that total power consumption is not very sensitive to bit-length, NP and NH parameters. Fig. 12 illustrates that the variation between represented cases is about 150 mW, which is around 10% of total power consumption. The highest part of power consumption is the quiescent power. It is greater than 82% in all cases. However, it is possible to reduce quiescent power consumption by selecting another FPGA device better fitting the implementation requirements (low power type device or best fit device).

@&#DISCUSSION@&#

ELM can face complex applications, being able to learn by thousands of times faster than traditional popular learning algorithms for feed-forward neural networks [13]. This feature is very interesting as a step forward to real-time training and classification. In certain problems this fact is essential, for example in price forecast in electricity market, in which ELM has shown to be able to capture the non-linearities from highly volatile data series [28], or in the sign language classification problem, accepting wearable sensor gloves with the ability to track hand and finger motion for sign language translation [29].

Programmable logic is the more efficient and effective device to implement a compact and fast circuitry supporting artificial neural network based system. FPGA devices are cost effective, have a much shorter design flow and privileges computational optimization. This study addresses the implementation of a parameterizable ELM training procedure on reconfigurable digital hardware since on-chip learning is not commonly included in hardware systems. However, its integration might lead to extend the range of applications where neural networks can be used: robotics, mobile systems, low power and small size devices, or medical applications, amongst others.

As a result, authors propose a flexible and parameterizable system using ELM for the training module. Three different architectures are developed, ELMv1 to ELMv3. ELMv2 and ELMv3 use coarse-grain parallelization of QR decomposition and calculation of inverse matrix. ELMv1 and ELMv2 use QR decomposition of square matrices, and ELMv3 use QR decomposition of rectangular matrices.

The present work pursues three main goals. First objective is to provide a parameterizable code for rapid implementation of any neural network structure, allowing code reuse in typical classification, prediction and regression applications. Second aim is to compare parallelized over sequential ELM batch training implementations in terms of performance, power and resource analysis. Third, provide a guideline on expected resources, accuracy and performance that an FPGA implementation can provide for ELM batch training. Thus, different system complexities (varying the number of hidden neurons, bit-length, and number of patterns) were explored.

Previous work of authors [30] was focused on the study of accuracy of proposed architectures. This work makes a step forward in terms of accuracy, providing more experimental data and obtaining an experimental model of accuracy as a function of the number of bits and number of hidden neurons, NH. Semi-logarithmic representation of obtained models result in a family of straight lines that provide an intuitive idea of the behavior of accuracy, doubling accuracy for each bit increment.

In the present work, results show that the limiting resources for the implementation are the number of RAM memory blocks and DSP48E arithmetic unit blocks, whereas logic LUT blocks usage is typically low. ELMv2 consumes 33% less RAM resources than ELMv1 and ELMv3, and ELMv1 consumes considerably less DSP48E blocks than ELMv2 and ELMv3 due to parallelized computation.

Hardware performance analysis reveals that the number of cycles to complete training grows quadratically with the number on hidden neurons, NH, and linearly with the number of bits. Again, ELMv2 architecture is better in terms of consuming less cycles to achieve training, as well as having the slowest increment of number of cycles when bit-length increases.

In terms of maximum clock frequency of operation, it decreases with bit-length in a staired shape. It is not much sensitive to NP and NH parameters and only below 26 bits some differences are appreciated. In terms of maximum clock frequency, the behavior of all three architectures is similar.

Concerning power analysis, using XPower Analyzer tool it has been found that values range from 1418 mW to 1691 mW depending on configuration parameters and ELM version. It has been found a fixed quiescent power consumption of approximately 1300 mW in all designs. Power consumption shows similar behavior in all three architectures. Note that the power consumption variation is not greater than 10% between different configurations, and, therefore, it can be stated that power consumption is not very sensitive to NP, NH, and bit-length.

The quiescent power can only be reduced by selecting a device from another Xilinx FPGA family. Each family has different power characteristics. Table 4
                      shows quiescent power calculated for some devices of different families. It has been used a device from the most quiescent consuming family (Virtex6) because it contains enough hardware resources to get all designs realizable, permitting performance and hardware resources occupation study without limitations.

To compare training in FPGA hardware versus PC computing, let us estimate performance parameters using some of the above results for ELMv2 implementation (NP = 250 and NH = 40). If we bound the validation error selecting the MAE accuracy as MAE=0.005, Eq. (4) gives a bit-length of 23.3 bits. Taking 24 bits, it defines 78.795 MHz of a maximum clock operation frequency (Fig. 10) and 760,335 clock cycles to complete training, according to Eq. (3), defining a total estimated time of 9.65 ms. Concerning power, the total estimated consumption is 1586 mW (dynamic+quiescent=286 mW+1300 mW).


                     Table 5
                      summarizes training time and power performance obtained for all three ELM implementations and PC implementation as the mean of 1000 executions. These values indicate that training time on PC (Core Duo T5600 1.8GHZ) doubles that on FPGA for ELMv2 implementation. On top of that, power consumption on FPGA is dramatically smaller than over PC.

Obtained results show the benefits of combining ELM batch learning strategy with a dedicated circuit approach. The combination yields a low power, high-end computing platform with superior speed performance without loss in accuracy.

@&#CONCLUSIONS@&#

Implementing the Extreme Learning Machine (ELM) training algorithm, a Virtex 6 based system can train a Single Layer Feedforward Neural Network (SLFN) using a large dataset (250 training patterns) up to 100 times per second while other computing platforms as a PC can perform 50 at 22x more power consumption. This fact, together with a high power efficiency, small size and portability, offers new possibilities for on-chip learning in neural networks with applications in many different fields.

Three computational methods for the ELM algorithm are proposed to fit real-time operation, offering a generic code to be implemented under FPGA and, in general, in any VLSI system. All three methods, including sequential and coarse-grain parallel implementations, are compared in performance. The analysis shows that the parallel version computing QR decomposition of square matrices (ELM_v2) offers the best balance among accuracy, logic occupation and performance.

Finally, this study provides a guideline to the implementation of neural network batch learning using the ELM algorithm. It shows the level of performance that can be obtained using ELM batch learning under embedded systems as FPGA for different neural network topologies, and the amount of required hardware resources.

@&#REFERENCES@&#

