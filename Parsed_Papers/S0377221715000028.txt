@&#MAIN-TITLE@&#CRM in social media: Predicting increases in Facebook usage frequency

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We assess the feasibility of predicting increases in Facebook usage frequency.


                        
                        
                           
                           We evaluate six classification algorithms and assess the importance of many predictors.


                        
                        
                           
                           Stochastic Boosting performs the best with an AUC of 0.66 and accuracy of 0.74.


                        
                        
                           
                           The top predictor is the deviation from regular usage patterns.


                        
                        
                           
                           Facebook can use our approach to customize its service (advertisements, recommendations).


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Decision support systems

Social media

Data mining

Predictive analytics

Facebook

@&#ABSTRACT@&#


               
               
                  The purpose of this study is to (1) assess the feasibility of predicting increases in Facebook usage frequency, (2) evaluate which algorithms perform best, (3) and determine which predictors are most important. We benchmark the performance of Logistic Regression, Random Forest, Stochastic Adaptive Boosting, Kernel Factory, Neural Networks and Support Vector Machines using five times twofold cross-validation. The results indicate that it is feasible to create models with high predictive performance. The top performing algorithm was Stochastic Adaptive Boosting with a cross-validated AUC of 0.66 and accuracy of 0.74. The most important predictors include deviation from regular usage patterns, frequencies of likes of specific categories and group memberships, average photo album privacy settings, and recency of comments. Facebook and other social networks alike could use predictions of increases in usage frequency to customize its services such as pacing the rate of advertisements and friend recommendations, or adapting News Feed content altogether. The main contribution of this study is that it is the first to assess the prediction of increases in usage frequency in a social network.
               
            

@&#INTRODUCTION@&#

With 1.23 billion active monthly users the social network site Facebook has grown one of the world’s largest user bases (Facebook, 2014). Of those monthly users 62 percent uses the site on a daily basis (757 million) (Facebook, 2014). While Facebook’s service is free, its business model is based on advertising. The more time users spend on the platform, the more page impressions Facebook can sell to advertisers. Revenues are directly determined by the number of active users and the time that they spend on the network. Hence, increasing user activity and as such generating ad impressions and clicks is one of Facebook’s primary objectives (Claussen, Kretschmer, and Mayrhofer, 2013).

One way of achieving this objective is predicting increases in usage frequency. More specifically, the binary problem of usage increase
                        2
                     
                     
                        2
                        Usage frequency increase prediction is equivalent to the more widely known up-sell prediction. Because Facebook is a free service, the term usage increase is more appropriate than the term up-sell.
                      prediction consists in predicting whether a user is going to increase usage of the product or service. If a user is predicted not to increase usage, appropriate actions can be taken. One possible action is to propose friendships with unknown users, in addition to the usual practice of proposing friendships with people the user is likely to know in real life (e.g., friends of friends). These new friendships will drive new content to the user’s personalized on-line newspaper called News Feed, and could provide a stimulation for increased usage.

Together with acquisition, cross-sell and retention modeling, usage increase modeling (up-sell) shapes the field of predictive modeling in analytical Customer Relationship Management (aCRM) (Ngai, Xiu, and Chau, 2009). To the best of our knowledge no aCRM study has been attempted in the social media industry. This leaves several questions unanswered such as (1) is predicting usage increases even feasible (i.e., is predictive performance high enough), (2) which algorithms perform best on these data and, (3) which predictors are most important? This study aims to fill this gap in literature by conducting an empirical study using Facebook data.

The remainder of this article is organized as follows. In the second section of this manuscript, we highlight our contribution by providing a literature review of aCRM studies per industry. Third, the data, time window, variables, algorithms, assessment criteria, and cross-validation will be detailed in the methodology section. Fourth, we discuss the results. The fifth section concludes this paper. The penultimate section discusses the managerial implications. In the seventh and final section we discuss the limitations and directions for future research.

@&#LITERATURE REVIEW@&#

There are two strategies to usage increase management: untargeted and targeted management (Burez and Van den Poel, 2007). An untargeted strategy relies on mass advertising to increase usage such as promoting new features and functionalities. In contrast, a targeted strategy comprises (1) identifying which users or customers are (not) going to increase usage and subsequently (2) taking appropriate actions to counter or facilitate this process (Hung, Yen, and Wang, 2006). Two targeted strategies exist: reactive and proactive. A reactive approach consists in waiting until a user decreases usage and subsequently trying to reverse the trend. In a proactive approach the company tries to predict whether users are going to decrease usage in advance. If a user is predicted to decrease usage appropriate actions can be taken such as reducing advertisements or recommending new friends. Analogously, if a user is predicted to increase usage, more advertisements could be directed to the user and friend recommendations could be paced and saved for later when needed.

As indicated, targeted proactive approaches can have many advantages but they can be very wasteful if predictions are inaccurate (Burez and Van den Poel, 2007). Hence the goal is to predict usage increases as accurately as possible. This study focuses on illustrating the feasibility of usage increase modeling in a major social network, Facebook. Specifically, this research uses data mining techniques to find the best predictive model and identify top predictors. This will allow the social network to increase usage of its services.

To highlight our contribution Table 1
                      provides an extensive literature review of predictive aCRM studies per industry. Popular industries are financial and insurance services, telecommunications and retail. To the best of our knowledge no other study has investigated the feasibility of usage increase modeling (i.e., up-sell modeling), or any other application for that matter, in the social media industry. It is important to note that Table 1 only contains aCRM studies. The search strategy for Table 1 is based on the requirement that all studies have to have a prediction focus aimed at one-to-one targeting of users or customers (as is the case with aCRM). While research on social relationships in Facebook has also studied users’ connectedness with the Facebook platform it focuses on emotional connectedness to Facebook and the degree to which Facebook is integrated into individuals’ daily lives (Clayton, Osborne, Miller, and Oberle, 2013) using surveys. For this purpose Ellison, Steinfield, and Lampe (2007) developed a survey scale, called the Facebook Intensity scale. Whereas aCRM focuses on large scale operational deployments of prediction models for one-to-one targeting, the social relationship research stream focuses mainly on description models for theory development and creating strategic insights and is therefore out of the scope of Table 1.

The social media industry is different from the industries listed in Table 1 in that much more variables are available. While predictive models are usually based on administrative, operational, or complaints data, profiles on social media contain geographical (e.g., hometown, current town, check-ins), demographical (e.g., birthday, gender), professional (e.g., job function, company), social (e.g., friends, family), and personal (e.g., interests, likes, political views, pictures, videos, albums) information. Hence empirically testing the feasibility of usage increase modeling and assessing which variables are most important in social media is a relevant undertaking.

Besides evaluating the importance of specific variables, it is key to understand which algorithms perform best. This study benchmarks the predictive performance of six algorithms that have proven to be top performers in literature: logistic regression (Berkson, 1944), Random Forest (Breiman, 2001), stochastic boosting (Friedman, 2001), Support Vector Machines (Cortes and Vapnik, 1995), Kernel Factory (Ballings and Van den Poel, 2013a), and Artificial Neural Networks (McCulloch and Pitts, 1943).

@&#METHODOLOGY@&#

To be able to extract data from Facebook user profiles we developed a Facebook application. In order to stimulate participation we offered a prize. When a user ran the application he or she was presented with an authorization box, which specified the data that were being collected. Immediately after giving permission, the application extracted the data. Next, users were presented with some questions that allowed us to determine the winner of the prize. The data were collected between May 13, 2012 and January 22, 2013.

Selection effects could occur when a user chooses to run the application. We took steps to mitigate these effects by targeting a recruitment campaign (i.e., Facebook advertisements) toward a representative sample of Facebook users. Confirming whether the sample is representative of the general Facebook population is challenging in that Facebook does not publish demographic statistics of its users. However, it is possible to obtain some official gender and age statistics through the use of Facebook’s advertisement targeting system. The extraction of these statistics is only possible for 25 countries, hence we chose the top 25 countries in terms of the absolute number of users. A χ
                        2 test indicates that our sample is not representative of the top 25 countries on both age and gender with respectively χ
                        2(5) = 237.01, p < 0.001 and χ
                        2(1) = 18.27, p < 0.001. To investigate this further Fig. 1
                         compares our recruited sample with the population. In terms of the age variable the sample is somewhat more representative for populations with a larger group of 20–29 year olds. The difference between men and women is also somewhat more pronounced. Despite the significant differences the demographics of our study sample are to a large degree comparable to the demographic characteristics of the top 25 Facebook countries. Other studies using Facebook data obtained comparable samples (e.g., Aral and Walker, 2011). The ranking of the age groups and genders are equivalent for this study and the Facebook population. One has to keep in mind though that the conclusions that we will draw about for example the feasibility of predicting usage frequency increases are somewhat more representative for populations with a larger group of 20–29 year olds and a higher proportion of males than the top 25 countries. It is important to note that ”Facebook is a standardized research instrument” (Lewis, Kaufman, Gonzalez, Wimmer, and Christakis, 2008). Results from subsequent data analysis are ”formally replicable in a way most ‘case study’ data are not” (Lewis et al., 2008) and given that our study is the first to compile and analyze such a rich data set, we believe the results are very valuable.

The main deliverable of this research is an individual-level model to target a user, yes or no. An unambiguous conclusion for each user needs to be made about his or her future behavior (Buckinx and Van den Poel, 2005). A natural approach is to build a binary classification model to classify a user as either a target or not. This methodology is often used in the literature on partial churning (e.g., Buckinx and Van den Poel, 2005; Migueis, Van den Poel, Camanho, and Falcao e Cunha, 2012a; Migueis, Van den Poel, Camanho, and Cunha, 2012b). The threshold of change in usage level, to classify a user or customer, is an input parameter given by the company’s management. The threshold should be rather high in order to target only the users or customers most prone to display the target behavior. A common threshold is 40 percent (see Buckinx and Van den Poel, 2005; Migueis et al., 2012a; Migueis et al., 2012b). Because the sample is relatively small and to avoid an imbalanced response variable we decided to select 25 percent as a threshold to classify the users in our sample. To determine usage increase we needed the users to run the application a second time at a later stage. If from the independent to the dependent period (see Fig. 2
                        ) usage increased by 25 percent we consider a user to increase usage. Usage is defined as the sum of the following user actions: likes, status updates, videos uploads, photo uploads, albums created, posted links, check-ins, posted notes and comments made. The final sample contains 921 Facebook users of which 24.6 percent were determined to increase usage in the dependent period.
                           3
                        
                        
                           3
                           We assessed the sensitivity of the performance of the benchmarked algorithms to two more cut-offs: 15 percent and 35 percent. These cut-offs result in respectively 25.9 percent and 22.8 percent of the sample to be classified as having increased usage.
                         We used SAS 9.3 and R 3.0.1 (R Core Team, 2013) to perform the necessary data preparations and statistical modeling.


                        Table 2
                         provides an overview of the 418 predictors used in this study. We mined as much information as possible from the users’ profiles. The predictors can be classified in the following categories: demographic and identification variables, geographical variables, professional and educational variables, social variables, personal variables, general Facebook account variables, likes, statuses, photos, videos, albums, events, links, check-ins, notes, games, tags, comments made and comments received. In Table 2 IND is short for INDicator and resolves to 1 if the profile’s field is used, otherwise it resolves to 0. REC is short for RECency and is the elapsed time since an event. SDIET is short for Standard Deviation Inter-Event Time. MIET is short for Mean Inter-Event Time. Any reference to ‘like’ in this study, unless otherwise stated, is not a ‘like’ of content generated by another user (e.g., a status update) but of a page, band, leisure, app, etc. The variable username refers to whether the second part of a user’s URL is upgraded to a name (created by the user) (the default is a numeric identifier created by Facebook). The term ‘user tags’ refers only to tags of the user himself/herself. In the case of photos and videos, the word ‘created’ refers to uploaded or created, and immediately uploaded, with the Facebook app. Facebook only allows extraction of the 25 most recent status updates, photo uploads, link uploads, album uploads, check-ins, video uploads and notes. To handle this problem we computed frequency by time as to no user in our database reaches these limits. Frequency (COUNT) of status updates, photo uploads and link uploads is for the last 7 days, album uploads and check-ins is for the last 4 months, and video uploads and notes is for the last year.

This section will cover our choices for the parameters of the classification algorithms used in this study. In the remainder of this study the acronyms LR, RF, AB, KF, NN and SV respectively denote Logistic Regression, Random Forest, AdaBoost, Kernel Factory, Neural Network, and Support Vector Machines.

In order to avoid overfitting we use the lasso approach to regularized logistic regression. Lasso imposes a bound on the sum of the absolute values of the coefficients. As such coefficients are shrunk toward zero (Guisan, Edwards, and Hastie, 2002). We determine the shrinkage parameter by cross validation. To fit the model the glmnet R package is used (Friedman, Hastie, and Tibshirani, 2010; 2013). The α parameter is set to one to obtain the lasso method and we let the function compute the sequence of λ by setting nlambda to 100 (the default).

Random Forest requires two parameters: the number of variables to try at each split and the number of trees in the ensemble. We follow Breiman’s recommendation (Breiman, 2001) and set the number of variables to the square root of the total number of predictors and use a large number of trees (500). To create the model the randomForest R package (Liaw and Wiener, 2002; 2012) is used.

Initial implementations of boosting (Freund and Schapire, 1996) are considered an approximation to deterministic weighting (Friedman, 2002). One of the most recent boosting variants is stochastic boosting and improves on the original algorithms by incorporating randomness as an integral part of the procedure (Friedman, 2002). Two important parameters are the number of terminal nodes in the base classifiers and the number of iterations. We set the maximum number of nodes to eight by setting the maximum depth of the trees to three which is in line with the recommendations of Friedman (2002). In addition we set the number of iterations to 500. Stochastic boosting is implemented with the ada R package (Culp, Johnson, and Michailidis, 2012).

An important parameter in Support Vector Machines (SVM) is the kernel function (Martin-Barragan, Lillo, and Romo, 2014). The most popular choices are the linear, polynomial, and radial basis (RBF) kernel (Ballings and Van den Poel, 2013a). In this study we choose the RBF because of the following reasons: (1) it can model non-linear relationships whereas the linear kernel cannot, (2) it has less hyperparameters than the polynomial function, and (3) it has less computational problems (such as numeric overflow) because the kernel values are bound by zero and one, while the polynomial kernel value may go to infinity (Coussement and Van den Poel, 2008). The RBF kernel requires the choice of only one hyperparameter γ, the width of the Gaussian (Ben-Hur and Weston, 2010). In addition the SVM penalty parameter C, also called the cost or soft margin constant, specifies the trade-off between hyperplane violations and the size of the margin. One cannot know in advance which C and γ are best for a given problem. We follow the recommendation of Hsu, Chang, and Lin (2010) to perform a grid search on C = [2−5, 2−4, ..., 215] and γ = [2−15, 2−13, ..., 23] to identify the best combination. To map the decision values to probabilities we used Platt’s method (Platt, 2000). Support Vector Machines are implemented through the svm function of the e1071 R package (Meyer, Dimitriadou, Hornik, Weingessel, and Leisch, 2012).


                           Ballings and Van den Poel (2013a) recommend the burn method for Kernel Factory. This method automatically selects the best kernel function. Furthermore, we use the values of one column partition and int(log10(N)) row partitions. Kernel Factory is implemented through the kernelFactory R package (Ballings and Van den Poel, 2013b).

The final base classifier is a feed-forward artificial neural network optimized by BFGS which is vastly more efficient, reliable and convenient than backpropagation. We use one layer of hidden neurons which is generally sufficient for classifying most data sets (Dreiseitl and Ohno-Machado, 2002). Before applying the neural network we rescale the numerical predictors to [0,1]. The binary variables are left untouched {0,1}. Scaling of the data is necessary to overcome numerical problems and to obtain training efficiency. The algorithm is implemented using the nnet R package (Ripley, 2013; Venables and Ripley, 2002). The network weights at the start of the iterative procedure are chosen at random (Ripley, 1996, p. 154), hence results of subsequent runs of nnet can differ. This mimics a newborn’s brain: developed but without any real knowledge (Venkatesh, Ravi, Prinzie, and Poel, 2014). The entropy parameter is set to use the maximum conditional likelihood as recommended by Spackman (1991) and Ripley (1996, p. 149). The rang parameter, controlling the range of the initial random weights parameter was left at the default of 0.5. We left the parameters abstol and reltol to respectively 1.0e − 4 and 1.0e − 8. We used weight decay to avoid overfitting (Dreiseitl and Ohno-Machado, 2002) and hence the maximum number of weights (MaxNWts) and the number of iterations (maxit) were set to very large values (5000) in order not to run into a situation of early stopping. Finally, the weight decay factor and the number of nodes in the hidden layer were determined by performing a grid search (Dreiseitl and Ohno-Machado, 2002). We tried all combinations of decay = {0.001, 0.01, 0.1} (Ripley, 1996, p. 163), and size = [1, 2, ..., 20] (Ripley, 1996, p. 170) and selected the optimal combination.

To assess the performance of the models we use accuracy and area under the receiver operator characteristic curve (AUC or AUROC). Accuracy is defined as follows:

                           
                              (1)
                              
                                 
                                    Accuracy
                                    =
                                    
                                       
                                          TP
                                          +
                                          TN
                                       
                                       
                                          P
                                          +
                                          N
                                       
                                    
                                 
                              
                           
                        with TP: True Positives, TN: True Negatives, P: Positives, N: Negatives

All classifiers were set to probabilistic output because a ranking is needed of which users are most likely to increase usage. To calculate accuracy a threshold then needs to be chosen to determine whether a user is classified as having increased usage or not. This cutoff should be chosen to correspond to the proportion of users that will be targeted (Hand, 2005) and is dictated by the marketing budget. Marketing analysts are often interested in the top 10 percent of users most likely to display the target behavior (Coussement, Benoit, and Van den Poel, 2010). Hence, in this study we will use the threshold value that corresponds to a proportion of 10 percent.

In contrast to accuracy, AUC is insensitive to the cutoff value of the posterior probabilities (Hanley and Mcneil, 1982; Thorleuchter and Van den Poel, 2012) that classifies a Facebook user as either increasing usage or not. More specifically, while accuracy measures the performance of the model at one specific cutoff value, AUC assesses performance across all possible cutoff values. AUC is considered an objective performance metric for classification models (Provost, Fawcett, and Kohavi, 1998) and is defined as follows (Ballings and Van den Poel, 2013a):

                           
                              (2)
                              
                                 
                                    AUC
                                    =
                                    
                                       ∫
                                       0
                                       1
                                    
                                    
                                       TP
                                       
                                          TP
                                          +
                                          FN
                                       
                                    
                                    d
                                    
                                       FP
                                       
                                          FP
                                          +
                                          TN
                                       
                                    
                                    =
                                    
                                       ∫
                                       0
                                       1
                                    
                                    
                                       TP
                                       P
                                    
                                    d
                                    
                                       FP
                                       N
                                    
                                 
                              
                           
                        with TP: True Positives, FN: False Negatives, FP: False Positives, TN: True Negatives, P: Positives, N: Negatives

AUC is bound between 0.5 and 1, where the former value denotes that predictions are not better than random, and the latter indicates perfect prediction. In some cases AUC can be lower than 0.5 on the test sample. This is almost always due to overfitting leading the model to generalize poorly.

To assess the importance of the predictors in usage increase prediction we use a permutation-based measure. This type of measure is often used in Random Forest and other ensembles but its usefulness is not limited to these algorithms. The decrease in accuracy if variables are permuted is a popular implementation but has the limitation that accuracy is not a good performance measure in imbalanced settings. Janitza, Strobl, and Boulesteix (2013) have compared permutation-based decrease in accuracy and AUC and conclude that the AUC-based importance measure is preferred to the standard accuracy-based measure whenever the two response classes have different class sizes. As this is the case in our study (see Subsection 3.1), we will follow their recommendation and use the decrease in AUC as the variable importance measure in this study.


                        Janitza et al. (2013) work with out-of-bag data, which is different for all trees. Hence they average the AUC over all trees per ensemble. Since we work with test data we compute the measure for the ensemble. This is essentially the same approach. The importance measures are averaged across the ten folds obtained through five times twofold cross-validation by taking the median.

All reported AUCs and accuracies are medians over five replications of twofold cross-validation (5 × 2f cv) (Alpaydin, 1999; Dietterich, 1998). In each replication all data instances are randomly assigned to one of the two parts that are equal in size. Each part is employed as both a training and test set. The entire process results in ten AUCs and accuracies. The same splits are used for all models. As a measure of dispersion we use the inter quartile range.

In order to test for significant differences we follow the suggestions of Demsar (2006) to use Friedman’s test with Nemenyi’s post hoc test (Nemenyi, 1963) for comparing the classifiers. Classifiers are ranked, within folds, with the top performing classifier receiving rank 1 and the worst performing classifier receiving a rank equal to the number of classifiers (if no ties are observed). Ties are handled by taking the average ranks. By using this approach the relatedness of the folds is incorporated (classifier ranks are calculated per fold and then the average rank is calculated per classifier). In contrast to when the median is computed, they are not treated as independent (Demsar, 2006). In summary, the average ranks of the AUCs preserve the order of the folds while the median of the AUCs does not. This is also the reason why results can sometimes differ to a limited extent (Ballings and Van den Poel, 2013a). Hence average ranks allow a stricter comparison than the median. We report both the average ranks per classifier and the median.

Two classifiers perform significantly different if their average ranks differ by at least the critical difference. The critical difference (CD) is defined as follows (Demsar, 2006):

                           
                              (3)
                              
                                 
                                    CD
                                    =
                                    
                                       q
                                       α
                                    
                                    
                                       
                                          
                                             k
                                             (
                                             k
                                             +
                                             1
                                             )
                                          
                                          
                                             6
                                             N
                                          
                                       
                                    
                                 
                              
                           
                        with qα
                         the critical value (which can be found in any statistical statistical book) for a given p-value and number of classifiers, k the number of classifiers, and N the number of folds. In this study the critical difference for a p-value of 0.05, 6 classifiers, 10 folds and critical value of 2.850 equals 2.384.

@&#RESULTS@&#

The results clearly indicate that predicting increases in usage frequency in the social media industry is a viable strategy with a best median AUC of 0.66 and best median accuracy of 0.74. Figs. 3
                         and 4
                         respectively display the median AUC and median accuracy of the cross validation folds. Based on AUC AdaBoost comes out on top followed by Random Forest, Logistic Regression, Kernel Factory, Support Vector Machines and Neural Networks. The same pattern can be observed for accuracy except that Random Forest shares its second place with Support Vector Machines and that Logistic Regression shares its third place with Kernel Factory. In sum, AdaBoost is the optimal choice for our data set. In Appendix B we provide the results of the sensitivity analysis for two other cut-offs. The results largely coincide.

These results are substantiated by the median ROC curves (Fig. 5
                        ) and the median accuracy curves (Fig. 6
                        ). There are no important ROC curve crossings except for Kernel Factory performing somewhat worse for lower false positive rates and Random Forest performing somewhat better on high false positive rates. In a similar fashion the accuracy curves show no important crossings. However, the performance gap between the leading classifier AdaBoost and the rest of the classifiers is more pronounced at higher proportions.


                        Table 3
                         displays the average ranks and the results of the Friedman significance test. The classifiers are divided into two groups: (1) classifiers that perform statistically worse to the top performer (AdaBoost) and (2) classifiers that perform statistically equal to AdaBoost (displayed in boldface in Table 3).

Based on AUC, Logistic Regression and Random Forest perform statistically equally well as AdaBoost. Based on accuracy Support Vector Machines does also perform equally well as AdaBoost. Kernel Factory and Neural Networks perform significantly worse than AdaBoost. These results indicate that Random Forest may be a good option whenever speed is an issue. The underlying reason is that Random Forest can be executed in parallel and AdaBoost is inherently sequential.

The feasibility of usage increase prediction is also confirmed when we look at the stability of the results. The inter quartile ranges (IQRs) are displayed in Table 4
                        . The IQRs are low for all classifiers, meaning that all classifiers produce stable results.

There are several possible reasons as to why AdaBoost and Random Forest are so strong in this case. First of all, both methods use decision trees. Trees outperform Logistic Regression if data are not normally distributed (which is often the case in real life data sets) (King, Feng, and Sutherland, 1995) and if data are non-linear. Trees even perform very well on data with extreme distributions (King et al., 1995). Similarly, Neural Networks will only result in the best possible linear combination of weight estimates if at least normality assumptions are met (Bishop, 2002; Matignon, 2005, p. 8). Our analyses suggest that indeed normality assumptions are not met (as is the case in many real life data sets). Kernel-based methods then, such as Support Vector Machines and Kernel Factory, are very sensitive to the choice of the kernel function and will either work very well, or very poorly depending on the data (and hence the right kernel) (Ballings and Van den Poel, 2013b). Given the large variety of variables (counts, standard deviations, means, indicators; see Table 2) it is highly likely that not all variables conform to one single kernel function. Kernels force the data to a novel data representation in that data are not represented individually anymore (per variable), but through a set of pairwise comparisons or similarities (Vert, Tsuda, and Schölkopf, 2004). This could be a possible reason why trees outperform kernel-based methods in this case. Moreover, the high performance of AdaBoost and Random Forest is not only due to their tree base-classifiers. Random Forest further improves upon the performance of single trees by reducing the variance of the predictions (Bauer and Kohavi, 1999). Boosting improves trees by reducing both bias and variance (Bauer and Kohavi, 1999). This makes AdaBoost and Random Forest all-round high performers. An extensive analysis of why AdaBoost and Random Forest outperform the other methods is out of the scope of this study. However, this paragraph offers plausible possible explanations of why AdaBoost and Random Forest outperform the other methods.

To investigate which predictors are driving predictive performance we made a scree plot (Fig. 7
                        ). In the scree plot the variables are sorted in descending order by the five times twofold cross-validated median decrease in AUC of the AdaBoost model when predictors are permuted. Predictors ranked lower than 50 add little to the predictive performance. To demonstrate this we included the top 200 variables in the plot (Fig. 7). The top 50 predictors are listed in Appendix A.

The top predictor is the time ratio. Time ratio is the standard deviation of the inter-event time (all events) relative to the mean inter-event time. Hence whether the user’s behavior deviates is a strong predictor of usage increase. The frequencies of liking behavior in the categories retail and consumer merchandise, food and beverage and public figures are also in the top 10 predictors. Next to that, photo album privacy settings play an important role. If users invest the time to create custom privacy settings this is predictive of usage increase. In addition, the frequency of photo uploads and the deviation in the creation of albums are important. Other important variables are the number of group memberships, the recency since the user has received comments on his or her content and the user’s age. Despite the importance of age, socio-demographics clearly do not play a significant role in the prediction of usage increase. This is in line with other studies in the field of aCRM (Rossi, McCulloch, and Allenby, 1996). The complete list of the top 50 predictors can be found in Appendix A. The disadvantage of these permutation measures is that they do not give the sign of the relationship. However this is not a problem since the focus is on prediction and not on description (Coussement and Van den Poel, 2008). For the sake of completeness we show how the relationship between the response variable and specific predictors can be described. For each value in the range of a given variable we iteratively impute that given variable for all the users and compute the median value of the predicted probability. A plot of each of these values and the median predicted probability then reveals the relationship between the predictor variable and the response variable. Fig. 8
                         describes the median trend for a selection of predictors.
                           4
                        
                        
                           4
                           The count and time ratio variables do have higher maximum values but these are equal to the value of the maximum predictor value displayed in the plots. Instead of the raw probabilities we used the percentile ranks since the purpose of the application is user targeting.
                         Consider the trend of age. The older a user is the higher the probability of usage increase. This is not true for users younger than 20: older means lower probability of usage increase. The other relationships are less complex. The number of group memberships and the number of likes from the retail and consumer merchandise category have an overall positive relationship with the probability to increase usage. The time ratio (deviation of regular usage in terms of inter event time) has an overall negative relationship with the probability to increase usage. This means that users that have more stable behavioral patterns have a higher probability to increase usage.

@&#CONCLUSIONS@&#

In this study we set out to (1) study the feasibility of usage increase prediction in social media, (2) evaluate which algorithms perform best, (3) and determine the top predictors.

The results clearly indicate that usage increase prediction is a viable strategy with AUCs up to 0.66 and accuracies up to 0.74. Based on AUC, AdaBoost is the best choice to model usage increase, followed by (in this order) Random Forest, Logistic Regression, Kernel Factory, Support Vector Machines and Neural Networks. A similar pattern can be observed when accuracy is used as a performance measure. AdaBoost is the top performer, with Random Forest and Support Vector Machines sharing a second place, Logistic Regression and Kernel Factory sharing the third place, and Neural Networks on the fourth place. Overall it is clear that AdaBoost is the best choice, closely followed by Random Forest.

The top predictor is the time ratio. This variable is operationalized to capture the user’s deviation from his or her regular usage patterns and has been shown to be negatively related to the probability to increase usage. This seems a plausible finding and is consistent with Buckinx and Van den Poel (2005) in that customers (or users) with higher time ratios (i.e., deviations from established behavioral patterns) are less loyal. This can in turn be translated to a lower probability to increase usage. Other important predictors are frequencies of specific liking behaviors, privacy settings of albums, frequencies of photo uploads and deviations in album creations. Groups memberships, recency since receiving comments, and a users’ age are also very important. A list of the top fifty predictors is provided in Appendix A and serves as a first direction for Facebook Inc. as to which predictors to include in their models. These findings are important for Facebook Inc. and other social media companies for that matter. The following section will discuss the managerial implications.

Facebook’s business plan is based on advertising. Therefore, one of Facebook’s primary goals is to increase the time users spend on the platform, increase user activity, and generate ad impressions (Claussen et al., 2013). The social network draws users to its platform by delivering social content to users’ customized on-line newspapers, called News Feeds. This process is governed by an algorithm with the absolute goal to deliver the right content to the right user at the right time (Facebook, 2013a). Every time someone visits Facebook the News Feed algorithm tries to filter out the posts that are important out of an average of 1500 potential stories from connections and pages (Facebook, 2013a).

The goal with the advertisements that are shown is identical: the right advertisement has to be delivered to the right user at the right time (Facebook, 2013b). Every time a user visits Facebook, the same algorithm chooses between thousands of advertisements to show the most relevant ones (Facebook, 2013b). However, sometimes users might not want to see any advertisements at all. Showing advertisements at that point might alienate users, decrease usage frequency, and decrease time spent on the platform for long periods of time. Once defective behavior has been observed, Facebook could react and turn off advertisements for that particular user. However it may take a while before the user returns to the platform and it even might be too late to recapture the user. Instead of a reactive strategy, we propose a proactive strategy.

More concretely we propose to improve the News Feed algorithm by introducing a predictive component. This will allow Facebook to proactively change tactics and increase revenues. An illustration may clarify our point. Users are attracted if more, new or better social content is delivered to their News Feeds and in contrast they are alienated if more advertisements are delivered. Facebook could employ usage increase prediction as part of the decision process regarding the balance of social content and advertisements. If a user is predicted to increase usage he or she is satisfied with the service (or at least uses the service more than other users) and hence more advertisements can be shown. Friend and page recommendations could be saved for later. In contrast, if a user is predicted not to increase usage, advertisements can be turned off, and new friends and content can be recommended in an attempt to attract and engage the user. Once engaged, advertisements can be turned on again. By proactively adapting its tactics, Facebook can avoid alienating potential unhappy users even further and prevent future defection. At the same time it can count on its satisfied users to create ad impressions and fuel revenue.

The predictive models that we have developed in this paper are viable and could be deployed on a large scale for such a proactive strategy. The News Feed algorithm could be adapted by introducing a switch that (1) turns on advertisements and saves friend and page recommendations for later if the user is predicted to increase usage and (2) turns off advertisements and activates friend and page recommendations if the user is predicted not to increase usage.

The first limitation is that we do not have access to full network data (i.e., data about all the connections of the recruited sample). Therefore network effects cannot be included in the analysis. There is a large body of research on social networks (Hellmann and Staudigl, 2014) reporting the influence of network effects on a wide range of behaviors (e.g., Bakshy, Eckles, Yan, and Rosenn, 2012). These effects are in part driven by homophily, also called endogenous group formation (Hartmann et al., 2008), and (social) influence (Aral, Muchnik, and Sundararajan, 2009). Failure to include network effects in the analysis could bias variable importance. Unfortunately, data of a user’s friends are very hard to obtain and we currently do not have these data. However, as Benoit and Van den Poel (2012) note, network- based predictors (e.g., Gómez, Figueira, and Eusébio, 2013) may only improve predictions and hence our conclusions about the feasibility of usage increase prediction in social media are substantiated. Future research could try to obtain such data and improve on this study.

The second limitation is selection effects. It might well be possible that the users that are unwilling to share their data (i.e., use our application) may be different from the users in our recruited sample. Web crawling, another method of obtaining Facebook data, also suffers from the limitation that private profiles cannot be crawled (Lewis et al., 2008). Although our approach (using an application with an authorization box) does extract data from users with a private profile, it suffers from the limitation that some of the privacy sensitive users will not be willing to share their data with an application. It can be argued that both groups, users with private profiles and users that do not want to give their data to applications, largely coincide. Although we recognize that this may impact the generalizability of our results we firmly believe that our approach is able to extract data from profiles that cannot be extracted using web crawling and consequently suffers less from generalizability issues than web crawling because of the following reasons. First, setting a profile to private is in part a mechanism for forcing candidate viewers of a user’s profile to identify themselves and state their purpose. Candidate viewers that are not deemed trustworthy are not allowed to see a user’s profile and become a connection. Hence, we identified ourselves by providing our contact information, and the university’s, for questions regarding privacy and stated the purpose of our research. We also informed the user that we encode usernames and that we do not extract personal messages. It can be argued that users may be more inclined to share their data to one application for academic research purposes than to the general, unidentified public. Second, given the massive adoption of Facebook applications that all ask the permission to access a user’s profile data, we believe that privacy is of lesser concern in sharing data to applications than it is in sharing data with the general public. Third, we offered an incentive in the form of a prize to share their data. In addition to these arguments, it is important to note that while previous research used a web-crawling approach to extract data from university students (e.g. Lampe, Ellison, and Steinfield, 2007; Lewis et al., 2008), with permission of the university in question and Facebook, it is important to note that our approach is more privacy aware, because we ask permission directly from the user. Moreover, Facebook is a standardized research instrument (Lewis et al., 2008) and results from subsequent data analysis are formally replicable in a way most case study data are not (Lewis et al., 2008).

The third limitation of this study is that some of the variables are limited in the number of values. Facebook limits the number of entries per variable that an application can extract to the 25 most recent entries. This limitation has mostly an impact on the frequency variables. To cope with this, we computed the frequency within a specific period of time. We determined the length of this time window per variable as to no user in our database reaches the maximum number of 25 entries. The frequency of status updates, photo uploads and link uploads was computed for the last 7 days, album uploads and check- ins for the last 4 months, and video uploads and notes for the last year. An interesting avenue for future work would be to find ways to alleviate this problem. A possible strategy might be to motivate people to revisit the application at different times and to stack the 25 last entries.

The fourth limitation is that private communication data are not included in the analysis. Some work has been done to analyze social relationships in Facebook. Arnaboldi, Guazzini, and Passarella (2013) study 28 Facebook users and all their relationships and determine the drivers of tie strength. Future research could integrate these insights. Fluctuations in tie strength with specific users may be a valuable predictor in usage increase models. Unfortunately our data do not allow us to incorporate individual communication-based variables. We also expect that these data are very difficult to obtain for a large sample in that users have to authorize data collection (the 28 users in Arnaboldi et al. (2013) are members of the authors’ research department).

The fifth limitation is that there might be some bias by not taking seasonality into account (e.g., if part of the users are celebrating Christmas and New year and part are not). It is however, very difficult to automatically detect which user is celebrating New Year and which user is not. This bias does not inflate our results: taking seasonality into account would only increase the predictive performance of our models meaning that we have a conservative result. Our finding that it is feasible to predict usage increase is hence valid but including seasonality would be an interesting avenue for future research.

The sixth limitation is that we do not model website visits or login events. As such we disregard passive users who visit the social network leaving valuable opportunities for advertising untapped. Unfortunately Facebook does not allow to extract visits or login events. If Facebook decides to share these data, an interesting avenue for future research would be to model passive usage and compare it with active usage. We do want to note that the advantage of active usage is that each activity on Facebook instills a user’s friends to also visit the website. Therefore it might have a greater impact to target active usage increase.

As a final remark we want to say that although this study has these data-related shortcomings, it is the first aCRM study using such a variety of data. Other published studies in top journals using Facebook data (for other purposes than this study) have similar limitations (e.g., Aral and Walker, 2011). Large investments have been made to create the extractor app and to the best of our knowledge this study is the first to provide insight into the feasibility of predicting usage increases. In sum, we feel that this is a valuable contribution to literature.

@&#ACKNOWLEDGMENTS@&#

We would like to thank Matthias Lelie for his help in this study. Funding for this research was provided by the Special Research Fund (BOF, Bijzonder Onderzoeksfonds), Ghent University, Belgium.


                     
                  

The results for the two alternative thresholds largely coincide. For a threshold of 15 percent and performance measure AUC (5 × 2f CV) the results are as follows (in decreasing order): AB = 0.6452, RF = 0.6091, LR = 0.5985, SV = 0.5748, KF = 0.5738, and NN = 0.5379. For the performance measure accuracy (5 × 2f CV) the results are as follows (in decreasing order): AB = 0.7348, RF = 0.722, LR & KF = 0.7137, NN = 0.7109, and SV = 0.7065. Both AdaBoost and Random Forest are again the top performers.

For a threshold of 35 percent and performance measure AUC (5 × 2f CV) the results are as follows (in decreasing order): AB = 0.6408, RF = 0.6040, LR = 0.6009, KF = 0.5797, SV = 0.5711, and NN = 0.5147. For the performance measure accuracy (5 × 2f CV) the results are as follows (in decreasing order): AB = 0.7570, LR = 0.7375, SV = 0.7354, RF = 0.7307, KF = 0.7242, and NN = 0.7166. The results based on AUC are identical to when a cut-off of 25 percent is used. When accuracy is used as a performance measure, Random Forest has to give way to Logistic Regression and Support Vector Machines.

In sum, the sensitivity analysis clearly indicates that the results are not very sensitive to the chosen cut-off. The results and conclusions are very similar when other thresholds are used. For all tested cut-offs the top performer is AdaBoost.

@&#REFERENCES@&#

