@&#MAIN-TITLE@&#Background modeling using Object-based Selective Updating and Correntropy adaptation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We propose a background modeling method to deal with non-stationary conditions.


                        
                        
                           
                           The object-based updating strategy allows to work with SFO and RFO.


                        
                        
                           
                           Non-Gaussian pixel dynamics are modeled using a Correntropy cost function.


                        
                        
                           
                           Analyzing the regions movement direction avoids tracking background objects.


                        
                        
                           
                           Object-based updating strategies improve the performance for indoor scenarios.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Background modeling

Learning rate

Correntropy-based adaptation

Moving object detection

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Development of computer vision techniques to support visual surveillance aims to facilitate analysis of video records, which due to their length are not suitable to be monitored by visual inspection. Particularly, some typical video-based surveillance applications include pedestrian and car traffic monitoring [1], unusual human activity detection [2], target counting [3], detection of abandoned objects [4], among others. Many applications are devoted to detect moving objects in the scene to support high-level stages in visual surveillance systems (object recognition and tracking). In practice, to efficiently detect moving objects (foreground) of video streams, background modeling is widely used, which contrasts every new input frame to identify as foreground those regions diverging the most from the static elements (background). Commonly, pixels related to foreground objects are assumed as having stochastic behaviors while the ones related to the background are expected as static [5]. Nonetheless, real-world scenarios tend to have time and spatial non-stationary fluctuations, where not all changing pixels must be associated with foreground entities nor static pixels must be associated with the background [6]. Therefore, the goal is to properly adapt the background model to video perturbations that may include pixel noise and illumination variations (local or global), visual artifacts having similar appearances with surrounding backgrounds (shadows, camouflage, bootstrapping, reflections), dynamic background scenarios, static foreground objects, removed foreground objects, among others [7].

@&#RELATED WORK@&#

Commonly, the pipeline of background modeling approaches can be divided into two key stages: the model representation and the updating strategy. The former stage aims to reveal relevant pixel-based features for discriminating between background and foreground entities. In turn, the latter stage looks for adaptive strategies to deal with time and/or spatial non-stationary fluctuations of the scene by updating the background model when a new frame is provided.

For the model representation stage, the most explored methods are the following [8], [5]: i) probability-distribution-based approaches, ranging from a single Gaussian [9] to Mixture of Gaussians [10], [11]; ii) clustering-based background modeling (e.g., Codebook [12]), iii) (kernel) subspace models [13]; and iv) heuristic background modeling introducing neural networks [14] or fuzzy concepts [15]. The above-enumerated background model representations have their own claimed merits and demerits that depend on the assumptions about the particular visual surveillance applications. Overall, these model representation approaches tend to decrease the foreground/background discrimination performance against non-stationary conditions, e.g. Static Foreground Objects SFO and Removed Foreground Objects RFO and non-Gaussian environments [16], [17]. Also, the probabilistic-based and the codebook based approaches require adding and pruning schemes that are not a straightforward task in real-world video scenarios. Therefore, the challenge is to devise updating strategies able to adapt the chosen model representation parameters to tackle the aforementioned real-world video perturbations.

In this line of analysis, a variety of background updating schemes aiming to maximize speed and to limit the memory requirements have been presented in the past [18], including mainly different recursive update parameter rules [19]. However, background modeling can be also considered as an adaptive online learning task, where frame pixel samples are available one by one and the newest existing information updates the model parameters according to fixed process conditions [20], [21]. In online learning tasks, both the adaptation strategy (termed as the cost function) and the learning rate parameter must be properly configured to enable a trade-off between learning stability and quick update. The applied adaptation strategy penalizes the deviation between estimated and observed values, while the learning rate is a weighting parameter that determines how much an updating step influences current values. The most popular scheme of background updating is the exponentially weighted moving average (EWMA) that is based on ideally fitting a Gaussian probability density function on the last considered pixel values. This scheme employs the Mean Square Error (MSE) as a cost function that minimizes the mean squared error between the desired signal and the filter output [22]. Nonetheless, the EWMA algorithm may infer inappropriate background model estimations when foreground and/or background pixels are corrupted by non-Gaussian noise. Furthermore, the use of this quadratic cost function fails to characterize the entire structure of the data beyond just the second-order moments [23]. As a result, there is a need for choosing a proper cost function that should provide robustness when the knowledge of the underlying data density is not available along with the presence of non-gaussian noise. Yet, the cost function to be selected must have an algorithmic simplicity as to be suitable in a wide range of background modeling applications demanding detection of moving objects.

In the case of the learning rate parameter, the effectiveness of the background modeling adaptation highly depends on the stated relationship between the model learning rate and sensitivity to abnormalities of the static foreground elements, abbreviated as the R–S trade-off [24]. One way to deal with this compromise is allowing the learning rate to include spatial information about the performed foreground/background discrimination of observed samples. Therefore, the learning rate can be set in accordance to either selective or blind updating scheme [25]. The former scheme discards all samples from observations that have been previously inferred as foreground by setting the learning rate as zero. On the contrary, the latter rate uses all the samples extracted from the reference pixels by setting a constant learning rate regardless the attained discrimination. Although the blind updating is easier to implement, it leads to significant amounts of misdetections since it can easily include foreground information as part of the background model [26]. By contrast, due to samples of foreground pixels are not used to update the background model, the former strategy performs detections more accurately. Still, the danger of selective updating is that an incorrectly identified foreground pixel may continually be misclassified since the background model will never adapt to it. To overcome this issue, an object-based selective learning rate strategy is proposed in [27]. In contrast to pixel-based strategies contemplating the processes among pixels as totally independent, this kind of strategies investigates the inter-pixel relationships for the background modeling adaption. These spatio-temporal relationships are measured for those pixels that belong to coherent objects (people, cars, etc.). So, high-level computer vision techniques, such as object detection and tracking, are used to distinguish detected objects aiming to estimate suitably the learning rate for a given pixel. Thus, the accomplished discrimination gets superior performance in scenarios with static and removed foreground objects, that is, stationary objects that were in the scene during the training phase and then move away. However, since a single frame is used as model representation to encode temporal variability of pixels, this approach only works in scenarios under firmly stationary assumptions. Moreover, in order to reduce the computational cost, the employed approaches for object detection and tracking are formulated in a simple way, leading to tracking video recordings which may not be suitable for supporting higher level processing tasks. Hence, a background modeling approach achieving a feasible R-S trade-off remains an open issue under non-stationary assumptions.

@&#PROPOSED METHOD@&#

To resolve the above issues, this paper develops a novel adaptive background modeling approach, termed Object-based Selective Updating with Correntropy (OSUC), to support video-based surveillance systems detecting online moving objects by using static cameras. Our approach estimates existing pixel spatio-temporal relationship as an online learning task by using as model representation a simple Single Gaussian. So, the temporal statistical pixel distribution is inferred by using an updating rule based on the stochastic gradient algorithm with Correntropy-based cost function, being able to measure relevance of non-stationary pixel value fluctuations influencing the background model. Particularly, the Correntropy tends to be superior to minimum square error measure if the residual error is non-symmetric or with nonzero mean [28]. Also, we present an automatic tuning strategy of Correntropy-based cost function bandwidth parameter, allowing to analyze the model error distribution shape within a fixed time window that is suitable for either Gaussian or non-Gaussian noise environments.

Furthermore, to include pixel spatial relationships into the background modeling processing, we propose to use an object-based selective learning rate strategy. To make clear the proposed strategy, Fig. 1
                         depicts the pipeline of a traditional video-based surveillance system that is in accordance with the employed learning rate estimation: i) blind strategies do not use any feedback regarding detected objects, ii) selective strategies use the foreground information as feedback (red dashed line), and iii) object-based selective strategies consider the information of detected and tracked objects as feedback (green dashed line). Since surveillance applications must include a tracking stage in their pipeline, object-based strategies take advantage of this information to update the parameters of the used background model representation. Therefore, they use a single tracking procedure so that the computational load of the system is not severely affected. Furthermore, we incorporate a particle filter-based tracking algorithm that is widely applied in video surveillance systems. As such, our proposed strategy uses an object motion analysis stage to detect and track foreground entities based on pixel intensities and motion direction attained via optical flow computation. In this sense, our object motion analysis enhances the detection and tracking of foreground regions against real-world challenges: occlusions, camouflage, scale and orientation variations, etc., being suitable to be used in further high-level computer vision stages. Hence, unlike most of the-state-of-the-art algorithms, OSUC can deal not only with stationary, but also with non-stationary foregrounds/backgrounds, highlighting relevant Spatio-temporal pixel fluctuations. Accuracy results accomplished upon well-known datasets show that OSUC outperforms some of the-state-of-the-art algorithms.

The remainder of this work is organized as follows: In Section 2, we describe the theoretical background of proposed adaptive learning approach to support video-based surveillance systems. Experiments and Discussion are presented in Section 4. Lastly, in Section 5, we conclude about the achieved results.

We will assume that considered video surveillance scenarios, recorded by a single static video camera, constitute a stochastic process composed by two main dynamics: foreground and background. So, our goal is to learn a mapping function 
                           f
                           :
                           X
                           →
                           Y
                         based on pixel information extracted from a given sample sequence 
                           
                              x
                              t
                           
                           ∈
                           X
                           ⊂
                           
                              ℝ
                              N
                           
                        , with N features and t
                        =1,…,
                        T; being T
                        ∈ℕ the number of captured frames. In real-world scenarios, value T tends to infinity. Basically, estimation of f can be seen as an unsupervised learning problem, where 
                           Y
                           ∈
                           
                              0
                              1
                           
                         denotes either foreground or background labels. Besides, we deal with online learning systems where frame pixel samples are available one by one, so that the developed learning algorithm produces the termed sequence of hypothesis {f
                        1,…,
                        f
                        
                           t
                           +1}. Thus, we aim to take advantage of new existing information at each time instant to learn a discrimination function f
                        
                           t
                        
                        ∈ℝ, while adapting the system in accordance to the provided non-stationary process conditions. Then, the sequence of dient descent within the conventional online learning framework as follows [21]:
                           
                              (1)
                              
                                 
                                    f
                                    t
                                 
                                 :
                                 =
                                 
                                    f
                                    
                                       t
                                       −
                                       1
                                    
                                 
                                 −
                                 
                                    η
                                    t
                                 
                                 ∂
                                 
                                    f
                                    
                                       t
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       l
                                       
                                          
                                             x
                                             t
                                          
                                          
                                             
                                                f
                                                t
                                             
                                             −
                                             1
                                          
                                       
                                    
                                 
                              
                           
                        being η
                        
                           t
                        
                        ∈ℝ+ the learning rate, 
                           l
                           :
                           X
                           →
                           ℝ
                         is a given cost function, ∂
                           f
                        (⋅) is the gradient with respect to f and f
                        0 is a certain initial condition.

Owing to computational cost and model mathematical tractability, pixel statistical distribution is commonly assumed as either Gaussian or Gaussian mixture models. Thus, given a pixel x
                        
                           t
                         at time instant t, we state that the hypothesis f
                        
                           t
                         is ruled by a normal function 
                           
                              f
                              t
                           
                           :
                           =
                           N
                           
                              
                                 μ
                                 t
                              
                              
                                 Σ
                                 t
                              
                           
                        , being μ
                        
                           t
                        
                        ∈ℝ
                           N
                         a mean vector with elements μ
                        
                           t
                        
                        
                           n
                        
                        ∈ℝ and Σ
                        
                           t
                        
                        ∈ℝ
                           N
                           ×
                           N
                         a diagonal covariance matrix with elements (σ
                        
                           t
                        
                        
                           n
                        )2
                        ∈ℝ. Therefore, as a scheme of the background updating, the gradient descent based algorithm in Eq. (1) can be used to adjust dynamically f, taking into account measured pixel variations along the time. Moreover, f can be adapted by updating simultaneously μ
                        
                           t
                         and Σ
                        
                           t
                        . Particularly, we use the following updating rule:
                           
                              (2)
                              
                                 
                                    μ
                                    t
                                 
                                 =
                                 
                                    μ
                                    
                                       t
                                       −
                                       1
                                    
                                 
                                 −
                                 
                                    η
                                    t
                                 
                                 
                                    ∂
                                    
                                       μ
                                       t
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       l
                                       
                                          
                                             x
                                             t
                                          
                                          
                                             μ
                                             
                                                t
                                                −
                                                1
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Although online learning models described in Eq. (1) allow dealing with temporal dependencies, most of the sample pixels in video-based analysis are related through an unknown spatio-temporal relationship that must be estimated to improve accuracy of the mapping function f, in terms of discriminating between background and foreground. Bearing this in mind, we further propose to make clear this spatio-temporal pixel relationship in Eq. (2), by introducing the cost function l(x
                        
                           t
                        ,
                        μ
                        
                           t
                           −1) as well as the learning rate factor η
                        
                           t
                        . Then, we also renew the parameter Σ
                        
                           t
                         by using an updating rule based on Middleton's non-Gaussian interference models.

Correntropy is a localized measure estimating the probabilistic similarity between two given random variables. So, if both variables are very close to each other, their Correntropy value yields the 2-norm distance, while it asymptotically evolves to the 1-norm distance when variables tend to get apart. Furthermore, Correntropy falls to the zero-norm as given variables become very far apart. Thus, provided two concrete random variables, U
                        ∈ℝ and V
                        ∈ℝ, their Correntropy value is computed in the form:
                           
                              (3)
                              
                                 
                                    c
                                    ϕ
                                 
                                 
                                    U
                                    V
                                 
                                 =
                                 E
                                 
                                    
                                       
                                          κ
                                          ϕ
                                       
                                       
                                          
                                             U
                                             −
                                             V
                                          
                                       
                                       :
                                       n
                                       ∈
                                       N
                                    
                                 
                                 ,
                              
                           
                        where 
                           E
                           
                              ⋅
                           
                         stands for the expectation operator, N is the random variable sample size, and κ
                        
                           ϕ
                        (⋅) is a symmetric positive definite kernel (commonly assumed as Gaussian), which is scaled by the bandwidth parameter ϕ
                        ∈ℝ+ within the assessed similarity window [29].

With this in mind, we introduce the Correntropy into the proposed adaptive learning algorithm in Eq. (2), so that the stochastic gradient descent maximizes the c
                        
                           ϕ
                        (x
                        
                           t
                        ,
                        μ
                        
                           t
                           −1). As in case of the MSE cost function, however, we also search the optimal solution of Eq. (2) as follows:
                           
                              (4)
                              
                                 
                                    μ
                                    t
                                 
                                 =
                                 
                                    μ
                                    
                                       t
                                       −
                                       1
                                    
                                 
                                 +
                                 
                                    η
                                    t
                                 
                                 
                                    ∂
                                    
                                       μ
                                       
                                          t
                                          −
                                          1
                                       
                                    
                                 
                                 
                                    
                                       
                                          c
                                          ϕ
                                       
                                       
                                          
                                             x
                                             t
                                          
                                          
                                             μ
                                             
                                                t
                                                −
                                                1
                                             
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

Also, taking into account Eq. (3) and Eq. (4), the concrete gradient computation with respect to μ
                        
                           t
                           −1 yields:
                           
                              (5)
                              
                                 
                                    μ
                                    t
                                 
                                 =
                                 
                                    μ
                                    
                                       t
                                       −
                                       1
                                    
                                 
                                 +
                                 
                                    
                                       η
                                       t
                                    
                                    
                                       ϕ
                                       2
                                    
                                 
                                 E
                                 
                                    
                                       
                                          e
                                          t
                                          n
                                       
                                       
                                          κ
                                          ϕ
                                       
                                       
                                          
                                             x
                                             t
                                             n
                                          
                                          
                                             μ
                                             
                                                t
                                                −
                                                1
                                             
                                             n
                                          
                                       
                                       :
                                       n
                                       ∈
                                       N
                                    
                                 
                              
                           
                        where 
                           
                              x
                              t
                           
                           ,
                           
                              μ
                              t
                           
                           ∈
                           X
                           ⊂
                           
                              ℝ
                              N
                           
                        , and e
                        
                           t
                        
                        
                           n
                        
                        =
                        x
                        
                           t
                        
                        
                           n
                        
                        −
                        μ
                        
                           t
                           −1
                        
                           n
                        .

Here, we assume a single model for each pixel feature, e.g., each RGB color channel is considered aside, making 
                           
                              f
                              t
                           
                           =
                           N
                           
                              
                                 μ
                                 t
                                 n
                              
                              
                                 
                                    
                                       σ
                                       t
                                       n
                                    
                                 
                                 2
                              
                           
                        . For the sake of simplicity, we will employ x
                        
                           t
                        , μ
                        
                           t
                         and σ
                        
                           t
                        
                        2 notations for a single model. Consequently, the introduced adaptive learning model that infers the pixel temporal relationship between the actual sample, x
                        
                           t
                        , and the previous model, μ
                        
                           t
                           −1, is as follows:
                           
                              (6)
                              
                                 
                                    μ
                                    t
                                 
                                 =
                                 
                                    μ
                                    
                                       t
                                       −
                                       1
                                    
                                 
                                 +
                                 
                                    
                                       η
                                       t
                                    
                                    
                                       ϕ
                                       2
                                    
                                 
                                 
                                    e
                                    t
                                 
                                 
                                    κ
                                    ϕ
                                 
                                 
                                    
                                       x
                                       t
                                    
                                    
                                       μ
                                       
                                          t
                                          −
                                          1
                                       
                                    
                                 
                                 .
                              
                           
                        
                     

As a result, the introduced Correntropy-based cost function deals with the non-Gaussian noise fluctuations because of non-stationarity of the measured process. It is worth noting that the robustness of the cost function is ruled by the parameter, ϕ,. Moreover, its proper tuning influences the learning algorithm performance even more than the choice of the same kernel in terms of reaching the local optimum, rate of convergence, and robustness to impulsive noise during adaption.

Mostly, the kernel bandwidth is selected as a compromise between outlier rejection and estimation efficiency. In the particular case of video background modeling tasks, the kernel value is fixed by estimating the sample standard deviation through a considered time window [30]. When dealing with non-Gaussian conditions, however, this moment is not accurate enough. We rather use the adaptive kernel bandwidth selection algorithm based on Middleton's non-Gaussian interference models to dynamically adjust the Correntropy-based cost function in Eq. (6) as well as the variance of the normal based pixel model 
                           
                              f
                              t
                           
                           =
                           N
                           
                              
                                 μ
                                 t
                              
                              
                                 σ
                                 t
                                 2
                              
                           
                        . Thus, given a time window of size T
                        
                           e
                        , the proposed algorithm aims to evaluate the error distribution shape e
                        
                           t
                        
                        =
                        x
                        
                           t
                        
                        −
                        μ
                        
                           t
                           −1 by estimating the Kurtosis of e
                        
                           t
                         at time t as 
                           
                              β
                              
                                 e
                                 r
                              
                           
                           =
                           E
                           
                              
                                 
                                    
                                       
                                          e
                                          r
                                       
                                       −
                                       
                                          μ
                                          
                                             e
                                             r
                                          
                                       
                                       /
                                       
                                          σ
                                          
                                             e
                                             r
                                          
                                       
                                    
                                 
                                 4
                              
                           
                         with β
                        
                           e
                           
                              r
                           
                        
                        ∈ℝ+, being μ
                        
                           e
                           
                              r
                           
                         and σ
                        
                           e
                           
                              r
                           
                         the mean and the standard deviation of e
                        
                           r
                        , respectively, with r
                        ∈{t
                        −
                        T
                        
                           e
                        ,
                        t
                        −
                        T
                        
                           e
                        
                        +1,…,
                        t}. The kurtosis provides information about the distribution shape that may discriminate between Gaussian and non-Gaussian dynamics. Specifically, a distribution holding high kurtosis has a sharper peak and heavy tails, while a low kurtosis implies shorter, thinner tails. Therefore, the Correntropy kernel bandwidth is updated as proposed in [31]:
                           
                              (7)
                              
                                 
                                    ϕ
                                    t
                                 
                                 =
                                 α
                                 
                                    ϕ
                                    
                                       t
                                       −
                                       1
                                    
                                 
                                 +
                                 
                                    
                                       1
                                       −
                                       α
                                    
                                 
                                 
                                    σ
                                    
                                       e
                                       r
                                    
                                 
                                 
                                    
                                       
                                          β
                                          G
                                       
                                       /
                                       
                                          β
                                          
                                             e
                                             r
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        where α
                        ∈ℝ[0,1] is a forgetting factor and β
                        
                           G
                         is the kurtosis of the Gaussian distribution. It must be quoted that we manage the tradeoff between robustness and convergence speed by properly fixing the initial kernel bandwidth ϕ
                        0 and α values in Eq. (7).

As a result, if the computed distribution error within the given window lasting T
                        
                           e
                         has longer and fatter tails (see Fig. 2(a)), the update rule in Eq. (7) tends to yield a bandwidth value smaller than σ
                        
                           e
                           
                              t
                           
                        , rejecting misleading error information (by instance because of existing burst-like samples). Otherwise, in case of light tail shapes (see Fig. 2(b)), ϕ
                        
                           t
                         value tends to be larger than σ
                        
                           e
                           
                              t
                           
                        , extracting more suitable updating information to improve algorithm speed convergence.

Due to the updating rule in Eq. (7) is computed over time windows where pixel dynamics are assumed as stationary, the kernel bandwidth ϕ
                        
                           t
                         allows considering those main video backgrounds varying over time. Specifically, illumination changes and background motion objects can be modeled by the normal function 
                           
                              f
                              t
                           
                           =
                           N
                           
                              
                                 μ
                                 t
                              
                              
                                 σ
                                 t
                                 2
                              
                           
                         by updating μ
                        
                           t
                         according to Eq. (6) and after fixing ϕ
                        =
                        ϕ
                        
                           t
                        
                        ,
                        σ
                        
                           t
                        
                        2
                        =
                        ϕ
                        
                           t
                        
                        2. Hence, illumination variations (either Gaussian or non-Gaussian) as well as background motion objects influence directly on the error distribution shape that, in turn, rules the system adaptation.

To deal with this situation, we develop an object motion analysis stage aiming to identify accurately and track the available foreground objects. Further, we make use of the information about the tracked objects to set the learning rate parameter of the employed Gaussian model. Consequently, we provide a functional f conveniently describing pixel dynamics related to moving or static foreground entities. So, the proposed strategy can be divided into three main steps: i) Motion detection that detects regions that change between two consecutive frames, computing the optical flow only for these regions. ii) Object detection and modeling. The regions with optical flow presenting a clear movement direction are considered as objects and are modeled using color intensity and movement angle. Iii) Motion-based object tracking. Modeled objects are tracked by a particle filter based on kernel similarities. For alleviating the computational cost, the motion information is also employed to slant the spreading of the particles depending on the motion trajectory of each object. Following, each step is described in detail.

Provided a frame set {X
                        
                           t
                        
                        :
                        t
                        =1,…,
                        T} where each element X
                        
                           t
                        
                        ∈ℝ
                           w
                           
                              R
                           
                           ×
                           w
                           
                              C
                           
                           ×
                           N
                         represents the t-th pixel intensity matrix with w
                        
                           R
                         rows, w
                        
                           C
                         columns, and N color channels (i.e., RGB color codes). We detect the regions changing between two consecutive frames, X
                        
                           t
                         and X
                        
                           t
                           −1. To this end, we split these neighboring frames into patches of size w
                        
                           Ω
                        
                        ×
                        w
                        
                           Ω
                        , with w
                        
                           Ω
                        
                        ≪
                        w
                        
                           C
                        . Grounded on the obtained patches, we further compute the matrix H
                        
                           t
                        
                        ∈ℝ
                           w
                           
                              R
                           
                           ×
                           w
                           
                              C
                           
                         containing elements in the form of the following Sum of Absolute Differences (SAD) [32]:
                           
                              (8)
                              
                                 
                                    h
                                    t
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             1
                                             ,
                                          
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               Ω
                                                               t
                                                               
                                                                  i
                                                                  ,
                                                                  j
                                                               
                                                            
                                                         
                                                         −
                                                         
                                                            
                                                               Ω
                                                               
                                                                  t
                                                                  −
                                                                  1
                                                               
                                                               
                                                                  i
                                                                  ,
                                                                  j
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                                F
                                                2
                                             
                                             <
                                             
                                                ξ
                                                H
                                             
                                          
                                       
                                       
                                          
                                             0
                                             ,
                                          
                                          
                                             Otherwise
                                          
                                       
                                    
                                 
                              
                           
                        where both patches Ω
                        
                           t
                        
                        
                           i
                           ,
                           j
                        , Ω
                        
                           t
                           −1
                        
                           i
                           ,
                           j
                        
                        ∈ℝ
                           w
                           
                              Ω
                           
                           ×
                           w
                           
                              Ω
                           
                         hold spatial pixels neighboring element (i,
                        j) and belonging to X
                        
                           t
                         and X
                        
                           t
                           −1, respectively; the value ξ
                        
                           H
                        
                        ∈ℝ+ is an a priori threshold parameter, and notation ||⋅||
                           F
                         stands for the Frobenius norm operator. Afterward, we estimate from H
                        
                           t
                         the movement direction of each patch. Though this movement direction can be calculated based on optical flow, the majority of optical flow procedures require high computational burden since they compute it for the whole frame [33]. To reduce the needed computational burden, we propose a selective optical flow approach that searches in the frame X
                        
                           t
                         only the moving patches detected at the time instant t
                        −1. Then, the search is carried out within the neighborhood of patches sizing k
                        ∈ℕ. Resulting from the search, we get the position (i
                        ⁎,
                        j
                        ⁎) of the best matching patch to calculate the displacement coordinates (Δ
                           i
                        
                        =
                        i
                        −
                        i
                        ⁎,Δ
                           j
                        
                        =
                        j
                        −
                        j
                        ⁎) that are further stored in matrices A
                        
                           t
                        
                        ∈ℤ
                           w
                           
                              R
                           
                           ×
                           w
                           
                              C
                           
                         and B
                        
                           t
                        
                        ∈ℤ
                           w
                           
                              R
                           
                           ×
                           w
                           
                              C
                           
                        , respectively. As seen in 
                        Fig. 3 illustrating the proposed motion detection procedure, the detected moving patches of H
                        
                           t
                         emphasize in this case the walker's silhouette, while the computed optical flow, encoded in matrices A
                        
                           t
                         and B
                        
                           t
                        , shows the subject movement direction. Lastly, to get a more compact description, object movement is directly computed from the optical flow angle matrix Q
                        
                           t
                        
                        ∈ℝ
                           w
                           
                              R
                           
                           ×
                           w
                           
                              C
                           
                         holding elements q
                        
                           t
                        
                        
                           i
                           ,
                           j
                        
                        =tan−1(b
                        
                           t
                        
                        
                           i
                           ,
                           j
                        /a
                        
                           t
                        
                        
                           i
                           ,
                           j
                        ), with q
                        
                           t
                        
                        
                           i
                           ,
                           j
                        
                        ∈ℝ[0,360].

At this stage, we aim to identify from H
                        
                           t
                        , the patches highlighted as moving which describe foreground objects in the scene. To this end, the first step is to identify regions as groups of spatially connected patches highlighted as moving. Thus, a connectivity-based operation over H
                        
                           t
                         is carried out, labeling spatially connected patches. There may be moving regions, which do not correspond to foreground objects in the scene. That is the case of regions which changed due to intrinsic scene artifacts, such as illumination changes or dynamical such regions as foreground objects, we propose to impose a spatial smoothness constraint on the movement region information encoded in Q
                        
                           t
                        . In particular, we search for regions having a low-movement direction variability in terms of the estimated optical flow. Regarding this, we compute for the d
                        ‐th region the optical flow angle histogram from Q
                        
                           t
                        . As a result, the region d
                        
                           t
                         is assumed to be a moving object if the ratio θ
                        1
                        
                           d
                           
                              t
                           
                        /θ
                        2
                        
                           d
                           
                              t
                           
                         is larger than ρ
                        ∈ℝ+, being θ
                        1
                        
                           d
                           
                              t
                           
                        
                        ,
                        θ
                        2
                        
                           d
                           
                              t
                           
                        
                        ∈ℝ[0,360] the first and second most frequent angle direction values, respectively. This assumption is added to avoid connected regions having high-movement direction variability, which should be rather related to the above-mentioned scenario artifacts.

Afterwards, each one of the detected objects is enclosed into a bounding box generating the set O
                        
                           t
                        
                        ={o
                        
                           t
                        
                        
                           p
                        
                        :
                        p
                        =1,…,
                        P
                        
                           t
                        } where o
                        
                           t
                        
                        
                           p
                        
                        ∈ℕ1×4 encodes the (i,
                        j) up-left position, the width w
                        
                           C
                        
                        
                           p
                        
                        ∈ℕ, and the height w
                        
                           R
                        
                        
                           p
                        
                        ∈ℕ of object p, being P
                        
                           t
                        
                        ∈ℕ the number of the detected moving objects at a time instant t. Lastly, to model these detected objects, we characterize both object shape (encoded in the color intensity matrix X
                        
                           t
                        ) and movement dynamics (in Q
                        
                           t
                        ). So, the sets Ψ
                           t
                        
                        ={Y
                        
                           t
                        
                        
                           p
                        } and Υ
                           t
                        
                        ={ε
                        
                           t
                        
                        
                           p
                        } are built to model the detected objects, where Y
                        
                           t
                        
                        
                           p
                        
                        ∈ℝ
                           w
                           
                              R
                           
                           
                              p
                           
                           ×
                           w
                           
                              C
                           
                           
                              p
                           
                           ×
                           N
                         is the matrix obtained by mapping the bounding box of object p into X
                        
                           t
                         and ε
                        
                           t
                        
                        
                           p
                        
                        ∈ℝ[0,360] corresponds to θ
                        1
                        
                           p
                        .


                        
                        Fig. 4 depicts proposed object detection and modeling methodology. The angle histogram of the moving region (colored in dark gray in the 8-connected matrix) points out a clear motion direction trend, since the ratio θ
                        1
                        1/θ
                        2
                        1 gets a high value. Therefore, the dark gray region is assumed as a foreground object and is modeled by the intensity RGB matrix Y
                        
                           t
                        
                        1 and the angle value ε
                        
                           t
                        
                        1.

Once we compute both object model sets (Ψ
                           t
                         and Υ
                           t
                        ) as well as the bounding box set O
                        
                           t
                        , we must keep the tracking for every single detected object to incorporate its spatial information into the mapping function f
                        
                           t
                        . Inspired by particle-based filter approaches, we introduce a tracking algorithm that reduces the searching area by spreading particles depending on the movement angle model of each object ε
                        
                           t
                        
                        
                           p
                        . Consequently, the following state transition function is stated:
                           
                              (9)
                              
                                 
                                    g
                                    
                                       t
                                       ,
                                       p
                                    
                                    
                                       n
                                       p
                                    
                                 
                                 =
                                 N
                                 
                                    
                                       o
                                       t
                                       p
                                    
                                    
                                       Λ
                                       t
                                    
                                 
                                 ,
                              
                           
                        being g
                        
                           t
                           ,
                           p
                        
                        
                           n
                           
                              p
                           
                        
                        ∈ℝ4 with n
                        
                           p
                        
                        =1,…,
                        N
                        
                           p
                        , the estimated particle from o
                        
                           t
                        
                        
                           p
                        , assuming additive Gaussian noise conditions with covariance matrix Λ
                        
                           t
                        
                        ∈ℝ4×4. The number of particles N
                        
                           p
                        
                        ∈ℕ and Λ
                        
                           t
                         can be selected according to some object motion constraints a priori imposed by the user. Thus, to model each particle, each g
                        
                           t
                           ,
                           p
                        
                        
                           n
                           
                              p
                           
                         is mapped into X
                        
                           t
                           +1, obtaining an RGB intensity model Γ
                        
                           t
                           ,
                           p
                        
                        
                           n
                           
                              p
                           
                        
                        ∈ℝ
                           w
                           
                              R
                              
                                 t
                                 ,
                                 p
                              
                           
                           
                              n
                              
                                 p
                              
                           
                           ×
                           w
                           
                              C
                              
                                 t
                                 ,
                                 p
                              
                           
                           
                              n
                              
                                 p
                              
                           
                           ×
                           N
                        . Lastly, the particle movement direction γ
                        
                           t
                           ,
                           p
                        
                        
                           n
                           
                              p
                           
                        
                        ∈[0,360] is estimated from the angle between both the centroids of o
                        
                           t
                        
                        
                           p
                         and g
                        
                           t
                           ,
                           p
                        
                        
                           n
                           
                              p
                           
                        .

Then, we search for the particle g
                        
                           t
                           ,
                           p
                        
                        
                           n
                           
                              p
                           
                         that best matches the object o
                        
                           t
                        
                        
                           p
                        . Thus, we contrast their respective RGB and movement direction models using respectively two kernel functions of similarity. Particularly, we propose to employ a multiple-kernel approach to compare the p object against the n
                        
                           p
                         particle [34], yielding:
                           
                              (10)
                              
                                 
                                    κ
                                    ϑ
                                 
                                 
                                    
                                       o
                                       t
                                       p
                                    
                                    
                                       g
                                       
                                          t
                                          ,
                                          p
                                       
                                       
                                          n
                                          p
                                       
                                    
                                 
                                 =
                                 
                                    ν
                                    
                                       t
                                       ,
                                       p
                                    
                                 
                                 
                                    
                                       
                                          ϑ
                                          1
                                       
                                       
                                          κ
                                          
                                             σ
                                             c
                                          
                                       
                                       
                                          
                                             Y
                                             t
                                             p
                                          
                                          
                                             
                                                Γ
                                                ˆ
                                             
                                             
                                                t
                                                ,
                                                p
                                             
                                             
                                                n
                                                p
                                             
                                          
                                       
                                       +
                                       
                                          ϑ
                                          2
                                       
                                       
                                          κ
                                          
                                             σ
                                             ε
                                          
                                       
                                       
                                          
                                             ε
                                             t
                                             p
                                          
                                          
                                             γ
                                             
                                                t
                                                ,
                                                p
                                             
                                             
                                                n
                                                p
                                             
                                          
                                       
                                    
                                 
                                 ,
                              
                           
                        
                        
                           
                              
                                 s
                                 .
                                 t
                                 .
                                 :
                                 
                                    ϑ
                                    1
                                 
                                 +
                                 
                                    ϑ
                                    2
                                 
                                 =
                                 1
                              
                           
                        where κ
                        
                           σ
                           
                              c
                           
                        (⋅,⋅) and κ
                        
                           σ
                           
                              ε
                           
                        (⋅,⋅) are the Gaussian kernels included to measure the RGB and movement direction similarities, respectively; ϑ
                        1
                        ,
                        ϑ
                        2
                        ∈ℝ+ are the weighting factors of the multiple-kernel approach. The term ν
                        
                           t
                           ,
                           p
                        
                        ∈{0,1} avoids all examined particles with a similar color intensity to the background. The similarity is measured using the same Gaussian kernel κ
                        
                           σ
                           
                              c
                           
                        (⋅,⋅), notation ν
                        
                           t
                           ,
                           p
                        
                        =0 stands for a kernel value greater than ζ
                        
                           b
                        
                        ∈ℝ+ (particle similar to the background), otherwise, ν
                        
                           t
                           ,
                           p
                        
                        =1.

As a result, the particle g
                        
                           t
                           ,
                           p
                        
                        ⁎ that has the highest similarity value with object p, at time instant t, can be found as:
                           
                              
                                 
                                    g
                                    
                                       t
                                       ,
                                       p
                                    
                                    *
                                 
                                 =
                                 arg
                                 
                                    max
                                    
                                       g
                                       
                                          t
                                          ,
                                          p
                                       
                                       
                                          n
                                          p
                                       
                                    
                                 
                                 
                                    κ
                                    ϑ
                                 
                                 
                                    
                                       o
                                       t
                                       p
                                    
                                    
                                       g
                                       
                                          t
                                          ,
                                          p
                                       
                                       
                                          n
                                          p
                                       
                                    
                                 
                                 ,
                                 s
                                 .
                                 t
                                 .
                                 :
                                 
                                    κ
                                    ϑ
                                 
                                 
                                    
                                       o
                                       t
                                       p
                                    
                                    
                                       g
                                       
                                          t
                                          ,
                                          p
                                       
                                       *
                                    
                                 
                                 <
                                 
                                    ζ
                                    
                                       κ
                                       ϑ
                                    
                                 
                              
                           
                        with ζ
                        
                           κ
                           
                              ϑ
                           
                        
                        ∈ℝ+. The imposed restriction is added to improve the tracking accuracy, matching only particles similar enough to the object p. Therefore, if the restriction does not hold, the tracking updating is carried out as: o
                        
                           t
                           +1
                        
                           p
                        
                        =
                        o
                        
                           t
                        
                        
                           p
                        , Y
                        
                           t
                           +1
                        
                           p
                        
                        =
                        Y
                        
                           t
                        
                        
                           p
                        , and ε
                        
                           t
                           +1
                        
                           p
                        
                        =
                        ε
                        
                           t
                        
                        
                           p
                        , otherwise, the following updating rules are employed:
                           
                              (11a)
                              
                                 
                                    o
                                    
                                       t
                                       +
                                       1
                                    
                                    p
                                 
                                 =
                                 
                                    g
                                    
                                       t
                                       ,
                                       p
                                    
                                    *
                                 
                              
                           
                        
                        
                           
                              (11b)
                              
                                 
                                    Y
                                    
                                       t
                                       +
                                       1
                                    
                                    p
                                 
                                 =
                                 E
                                 
                                    
                                       
                                          Y
                                          
                                             t
                                             o
                                          
                                          p
                                       
                                       +
                                       
                                          Γ
                                          
                                             t
                                             ,
                                             p
                                          
                                          *
                                       
                                       :
                                       
                                          t
                                          o
                                       
                                       =
                                       t
                                       −
                                       
                                          T
                                          o
                                       
                                       +
                                       1
                                    
                                    …
                                    t
                                 
                              
                           
                        
                        
                           
                              (11c)
                              
                                 
                                    ε
                                    
                                       t
                                       +
                                       1
                                    
                                    p
                                 
                                 =
                                 E
                                 
                                    
                                       
                                          ε
                                          
                                             t
                                             o
                                          
                                          p
                                       
                                       +
                                       
                                          γ
                                          
                                             t
                                             ,
                                             p
                                          
                                          *
                                       
                                       :
                                       
                                          t
                                          o
                                       
                                       =
                                       t
                                       −
                                       
                                          T
                                          o
                                       
                                       +
                                       1
                                    
                                    …
                                    t
                                 
                              
                           
                        where T
                        
                           o
                         is the size of the time window employed to infer object model variations.

Finally, to incorporate the estimated foreground dynamics into the background updating rule (see Eq. (6)), we calculate the set Z
                        ∈ℝ
                           L
                           
                              t
                           
                           ×2 holding all L
                        
                           t
                         pixels of tracked moving objects according to the object bounding box set O
                        
                           t
                        . Thus, to avoid inclusion of false information into the background model generated by complex foreground object dynamics, the proposed strategy of object-based selective learning rate sets the learning factor η
                        
                           t
                         of pixel x
                        
                           t
                         as follows:
                           
                              (12)
                              
                                 
                                    η
                                    t
                                 
                                 =
                                 
                                    {
                                    
                                       
                                       
                                          0
                                          ,
                                          
                                          O
                                          therwise
                                       
                                       
                                          λ
                                          
                                             ϕ
                                             t
                                             2
                                          
                                          ,
                                          
                                          
                                             x
                                             t
                                          
                                          ∉
                                          Z
                                       
                                    
                                 
                              
                           
                        
                     


                        
                        Fig. 5 shows the general scheme of the proposed object tracking that determines the particle having the highest similarity with the object models Y
                        
                           t
                        
                        1 and a
                        
                           t
                        
                        1. Here, the new estimated models are estimated as a result of the updating rules given in Eq. (11b) and Eq. (11c).

The Object-based Selective Updating with Correntropy (OSUC) algorithm is summarized in 
                        Fig. 6, considering the proposed Correntropy based cost function for adaptive learning (see Section 2.2) and the object-based selective learning rate strategy (Section 3). It is worth saying that objects detected by the proposed object detection and modeling stage are compared at each time instant against all the previously detected and tracked objects to achieve a unique representation. Namely, detected objects at a time instant t that occupy the same location in the scenario as previously detected and tracked objects are not included in sets O
                        
                           t
                        , Ψ
                           t
                         and Υ
                           t
                        .

In order to asses the convenience of the proposed OSUC algorithm for the discrimination between foreground and background in video surveillance systems, we split testing into the following three stages: i) Evaluation of the Correntropy-based adaptive learning cost function in static and dynamic background scenarios. That is, we aim to analyze by visual inspection the temporal evolution of the cost function, as well as its contribution in the OSUC-based background modeling. ii) Evaluation of the performed accuracy of the object-based selective learning rate in cases with more complex foreground dynamics. Thus, we validate the tracking ability of the object motion analysis stage on situations holding both static and moving objects as well as object occlusions and crossing cases. iii) Testing of the OSUC algorithm for background and foreground discrimination against a given ground-truth set.

All experiments are performed using videos recorded in real-world video settings, including the following typical challenges: dynamical background, bootstrapping, static and moving objects, foreground object occlusions, shadows, and camouflages. Namely, the following four databases are employed:
                        
                           •
                           DBa-Change Detection
                                 1
                              
                              
                                 1
                                 
                                    http://www.changedetection.net/
                              : This data collection (used in the CVPR 2012 Change Detection Workshop challenge) holds 31 different video sequences of typical indoor and outdoor environments, where a spatial and a temporal region of interest (ROI) are provided for each video sequence foreground masks. Hand segmented ground-truths are also available and hold the following five labels: background, hard shadow, outside region of interest, unknown motion, and foreground.

DBb-A-Star-Perception
                                 2
                              
                              
                                 2
                                 
                                    http://perception.i2r.a-star.edu.sg
                                 
                              : This collection recorded in both indoor and outdoor scenarios contains nine image sequences with resolution ranging from a low (160×120) to a medium value (320×240). The hand-segmented ground-truths, which are available for random frames in the sequence, hold two labels: background and foreground.

DBc-Left-Packages
                                 3
                              
                              
                                 3
                                 
                                    http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1
                                 
                              : The main purpose of this database is the identification of abandoned objects (a box and a bag) and holds five different image sequences with medium resolution value (388×244), recorded in an interior scenario having several illumination changes. We provide the hand-segmented ground-truths of randomly selected frames with labels: background and foreground.

DBd-Activity Recognition
                                 4
                              
                              
                                 4
                                 
                                    http://www.wisdom.weizmann.ac.il/~vision/SpaceTimeActions.html
                                 
                              : This collection that contains several video recordings of people performing different activities has been also used for testing of tracking algorithms [35]. The ground-truth images are available with labels: background and foreground.

To make clear the contribution of the Correntropy-based cost function, we conduct a visual inspection of the temporal evolution of the μ
                        
                           t
                         parameter for two scenes with different background dynamics. Namely, the DBb-ShoppingMall video sequence (see 
                        Fig. 7(a)) that is an indoor surveillance scene with several foreground moving objects, while the background remains mostly static; and the outdoor DBa-Fall sequence with a highly dynamical background of a tree leaf movement (see Fig. 7(c)). The learning rate is fixed by using λ
                        =0.5, so that we focus only on the information provided by the cost function as a blind update strategy, that is, without considering the object motion analysis. To estimate the performed error, we also adjust the forgetting factor α and the size of the non-overlapping time window equal to 0.5 and T
                        
                           e
                        
                        =25, respectively.


                        Fig. 7(b) shows the absolute gradient calculated for the 378-th frame and selected from the DBb-ShoppingMall sequence. As seen, the Correntropy-based background model provides gradient values for the foreground regions (i.e., moving people) close to zero due to the ability of the introduced kernel bandwidth tuning criterion to learn pixel distributions along the time. In fact, due to the static nature of this scene, the bandwidth ϕ
                        
                           t
                         becomes thinner, making the foreground pixels be outlier values so that their gradient becomes close to zero. In contrast, for the 195-th frame of DBa-Fall, the kernel bandwidth becomes wider for those pixels located on the tree leaves, significantly increasing the computed gradient of the updating background modeling (see Fig. 7(d)).

The proposed Correntropy based background modeling against the baseline MSE cost function algorithm are visually contrasted in 
                        Fig. 8 that shows an example of the temporal evolution of the background model parameter, μ
                        
                           t
                        , calculated for a single pixel of the red color channel. As seen, the pixel value for the DBb-ShoppingMall video (see Fig. 8(a)) tends to be mostly static, although there are some abrupt changes (outliers) caused by several moving objects. Accordingly, we infer that the MSE cost function does not consider error distribution changes along the time and is sensitive to outlier values. Therefore, the MSE tends to generate noisy models leading to undesired information. On the contrary, the proposed Correntropy based model is able to discover the main sample dynamics due to its ability to encode the error distribution shape along the time. That is, our approach better models the pixel intensity by fixing a small Correntropy kernel bandwidth rejecting outlier values.

On the other hand, the MSE can not accurately observe the non-stationary video dynamics as it is the case of the DBa-Fall video with pixels highly changing through the time (see Fig. 8(b)). This flaw makes the MSE mislead in false foreground/background discrimination results. In turn, the kernel bandwidth of the Correntropy model becomes wider to include bigger model parameter changes. Thus, the adaptive kernel bandwidth strategy makes easier develop background models to follow fast changes. To make clear this situation, the estimated temporal evolution by the Correntropy-based kernel is shown in fig:Kernels. Particularly, 
                        Fig. 9(a) shows that even for small errors, the kernel value for DBb-ShoppingMall so vanishes that significantly reduces the computed stochastic gradient. In contrast, the estimated kernel allows following bigger changes of the DBd-Fall sequence (see Fig. 9(b)).

As said before in Section 3, we aim to discover the spatio-temporal relationship between pixels encoding foreground dynamics to be further incorporated into the background model through the object-based selective learning rate strategy. Testing of this stage is carried out for accurate detection and tracking of moving objects to avoid false foreground/background discrimination results. Besides, we seek for tracking records which could be used to support higher level processing stages of a complete video-based surveillance system. Table 1
                         shows the free parameter values that are experimentally selected, assuming that detected objects have small and medium sizes in comparison to the frame resolution and looking for a trade-off between system accuracy and computational cost. As such, 
                           
                              w
                              Ω
                           
                           =
                           0.02
                           
                              
                                 
                                    w
                                    R
                                 
                                 ×
                                 
                                    w
                                    C
                                 
                              
                           
                         and the elements of the diagonal of covariance tracking matrix Λ
                        
                           t
                         are set as [0.02w
                        
                           R
                        0.02w
                        
                           C
                        0.1w
                        
                           R
                        
                        
                           p
                           
                              t
                              −1
                        0.1w
                        
                           C
                        
                        
                           p
                           
                              t
                              −1
                        ].

In particular, the object motion analysis is tested on the following three tracking challenging tasks:


                        Detection and tracking of moving objects
                        . 
                        
                        Fig 10 shows some tracked objects in the DBa-StreetLight (top row) and DBb-ShoppingMall (bottom row) sample videos, respectively. The OSUC algorithm includes a particle filter based tracker that takes into account both movement direction and color intensity of the objects, therefore, the object labels (remarked by colored boxes) are detected and tracked accurately even though both scenarios hold quite different dynamics.


                        Tracking of crossing and/or occluding foreground objects. In some particular video surveillance conditions, foreground dynamics may be complex due to either the presence of crossing objects and/or occlusions, making difficult to carry out a suitable moving object tracking. In the DBa-Pedestrians video, some objects appear with different dynamics, namely, bicycle and people crossing each other (see 
                        Figs. 11(a) to 11(d)). For each frame, the tracked objects are delineated by lines with different styles and colors. Also, the corresponding angle trajectories (with the same styled and colored lines) are displayed in Fig. 11(e), where the data-tips indicate each corresponding frame. So, the inclusion of the angle path in the foreground model improves separability among occluded objects. As seen in Fig. 11(a), Fig. 11(c) some objects cross each other, each calculated angle trajectory becomes discriminant and allows to accurately track the label set.


                        Tracking of foreground objects with dynamics similar to the background
                        . To appraise the influence of object motion on the background model estimation, we carry out analysis of foreground objects, which remain static during long time intervals. To illustrate this case, we compare the model obtained by OSUC using the object-based selective learning rate strategy against the one obtained by its version using a blind updating strategy, that is, we set the learning rate as a constant value, λ
                        =0.5. Particularly, the DBc-LeftBox video sequence is employed where a person enters into the scene and then stops twice at the beginning of the video recording, making hard to accomplish background/foreground discrimination (see 
                        Fig. 12). In the blind strategy version, obtained results indicate that if both the foreground and background dynamics become similar, the Correntropy-based cost function is not anymore able to reject foreground pixels as outliers, introducing false information into μ
                        
                           t
                         after a while. As a result, the estimated model is notably affected by the presence of static foreground objects as shown in Fig. 12(a). Conversely, inclusion in the OSUC model of object motion information (see in Fig. 12(b) the tracked object colored in blue) allows distinguishing between considered foreground and background dynamics, even if foreground objects keep static during a long time interval as seen in Fig. 12(c).

Lastly, the proposed OSUC framework is tested for the background subtraction of real video sequences. Thus, we compute both parameters (μ
                        
                           t
                         and σ
                        
                           t
                        ) of the mapping function f, so that each pixel x
                        
                           t
                         is mapped into the matrix, S
                        
                           t
                        
                        ∈ℤ
                           w
                           
                              R
                           
                           ×
                           w
                           
                              C
                           
                        , holding elements expressed as follows:
                           
                              (13)
                              
                                 
                                    s
                                    t
                                 
                                 =
                                 
                                    
                                       
                                          
                                             1
                                             ,
                                          
                                          
                                             
                                                
                                                   ∏
                                                   
                                                      n
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                             
                                             f
                                             
                                                
                                                   x
                                                   t
                                                   n
                                                
                                                
                                                   μ
                                                   t
                                                   n
                                                
                                                
                                                   σ
                                                   t
                                                   n
                                                
                                             
                                             <
                                             
                                                ζ
                                                S
                                             
                                          
                                       
                                       
                                          
                                             0
                                             ,
                                          
                                          
                                             Otherwise
                                          
                                       
                                    
                                 
                              
                           
                        where ζ
                        
                           S
                        
                        ∈ℝ+ is a given discrimination threshold and s
                        
                           t
                        
                        ∈{0,1} corresponds to the (i,
                        j) pixel label, where s
                        
                           t
                        
                        =1 stands for pixels labeled as foreground, otherwise, the label is background.

With the aim to include a wide variety of dynamics, we select 23 video sequences excluding scenes that hold either camera jitter or thermal images. Depending on the challenging level to discriminate between background and foreground dynamics, we also rank all testing videos in the following categories of challenge: a) Foreground and background dynamics are clearly distinguishable, b) Background dynamics tends to be similar to the foreground dynamics, e.g., moving leaves, water flowing, etc. c) Foreground dynamics tends to be similar to the background dynamics, e.g., static people. All videos also include different kind of artifacts, e.g., illumination changes, shadows, occlusions, among others.

In this case, the needed free parameters of Correntropy-based cost function are fixed as in Section 4.1 while the values of the carried out object motion analysis are as in Section 4.2. Also, the OSUC-based background and foreground discrimination free parameter is heuristically fixed as ζ
                        
                           S
                        
                        =0.6. It is worth noting that to avoid false segmented regions due to illumination changes, we implement the shadow removal post-processing as given in [36].

With the purpose of demonstrating the advantage of the proposed Correntropy-based cost function for background modeling, we compare the foreground masks resulting from the OSUC algorithm against the ones attained by its selective updating version, without the help of the tracking module. To this end, we introduce OSUC-2 that uses the spatial information included in the segmented matrix S
                        
                           t
                         as a traditional selective based updating approach, instead of using the object-based selective learning rate strategy as in Eq. (12). Thus, provided a pixel (i,
                        j), if its label s
                        
                           t
                        
                        
                           i
                           ,
                           j
                         equals to 1, the learning rate η
                        
                           t
                         will be 0, otherwise, η
                        
                           t
                        
                        =
                        λϕ
                        
                           t
                        
                        2.

Additionally, the following four state-of-the-art algorithms are also compared in terms of foreground/background discrimination performance:
                           
                              •
                              Zivkovic Gaussian Model Mixture (Z-GMM): uses a blind updating strategy and can automatically tune the number of models needed to describe each pixel, so reducing the computational cost [19].

Tracking closed loop GMM (GMM-T) that uses an object-based updating strategy to adapt the GMM parameters. A Multiple Hypotheses Tracker is employed to follow the regions marked as foreground by the subtraction [37]. This algorithm is included in the benchmark since it has a similar purpose as the proposed OSUC method. For the implementation, all parameters are set using the default values suggested by the authors.

Spatial Coherence Self-Organizing Background Subtraction (SC-SOBS): builds a background codebook during the training phase and incorporates spatial coherence based on a selective updating strategy to provide robustness against false detections [38]. This algorithm is publicly available
                                    5
                                 
                                 
                                    5
                                    
                                       http://www.na.icar.cnr.it/~maddalena.l/
                                    
                                  and for our concrete testing all its parameters are left as default.

Pixel Based Adaptive Segmenter (PBAS): models the background by constructing a codebook with a fixed number of observed values. Besides, the PBAS updating strategy can be classified as blind due to the employed misdetection neighboring post-processing [39].

The Z-GMM, SC-SOBS, and PBAS had been included in the Change Detection Challenge 2012 [40], where the last two are labeled as top algorithms. For the implementation of the Z-GMM and the PBAS algorithms, the BGS library [41] is employed that provides a C++ framework to perform background subtraction techniques and is publicly available.
                           6
                        
                        
                           6
                           
                              https://code.google.com/p/bgslibrary/
                           
                         We left the Z-GMM and PBAS parameters given by default by the authors. The foreground/background discrimination performance is evaluated in terms of the F
                        1 pixel-based measure:
                           
                              (14)
                              
                                 
                                    F
                                    1
                                 
                                 =
                                 2
                                 pr
                                 /
                                 
                                    
                                       p
                                       +
                                       r
                                    
                                 
                                 ,
                                 
                                    F
                                    1
                                 
                                 ∈
                                 ℝ
                                 
                                    0
                                    1
                                 
                                 ;
                              
                           
                        where r is the recall and p is the precision that are pixel-based measures defined respectively as follows:
                           
                              (15a)
                              
                                 r
                                 =
                                 
                                    t
                                    p
                                 
                                 /
                                 
                                    
                                       
                                          t
                                          p
                                       
                                       +
                                       
                                          f
                                          n
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (15b)
                              
                                 p
                                 =
                                 
                                    t
                                    p
                                 
                                 /
                                 
                                    
                                       
                                          t
                                          p
                                       
                                       +
                                       
                                          f
                                          p
                                       
                                    
                                 
                                 ,
                              
                           
                        here, t
                        
                           p
                         is the number of true positives, f
                        
                           p
                         is the false positives, and f
                        
                           n
                         is the false negatives; all of them obtained during comparison against a given hand-segmented ground truth. So, the higher F
                        1 value, the better the foreground/background discrimination performance.

The foreground/background discrimination results for all 6 compared algorithms are shown in Table 2
                        . Here, we show the attained F1 measure (rescaled to the interval 0−100) for the three considered categories (a, b, c). Besides, 
                        Fig. 13 displays the foreground masks obtained for some of the relevant frames relating the three considered categories. The first column of the Fig. 13 shows the frame of analysis and the tracking blobs of the OSUC algorithm. The foreground masks estimated for OSUC-2 are not displayed since the results of Table 2 show, reasonably, the advantage of the OSUC. Thus, the discussion of OSUC-2 now regards only the supervised measure results of Table 2. Following the results are shown and discussed by category order.

In the case of category of challenge a, all six compared algorithms reach high F1 measures, but the PBAS gets, in average, the best results as seen in Table 2 showing the estimated F
                        1 values. Certainly, Figs. 13(b) to 13(f) imply that all the algorithms can infer the relevant pixel dynamics of the DBa-Pedestrians video; yet, this sequence is not a complex challenge of discrimination between foreground and background. As for more complex dynamics as presented by the DBa-Backdoor video, where a background illumination change occurs in the middle of the record, the Z-GMM, GMM-T and SC-SOBS algorithms generate noisy foreground masks as seen in Figs. 13(j) to 13(l). Here, Z-GMM is not able to react to fast illumination changes and the SC-SOBS algorithm as a selective updating based approach does not update its model for dynamics that has not been present in the initial learning stage. Due to GMM-T uses as tracking seeds the obtained foreground regions, it misclassifies these regions as foreground persistently throughout the whole video sequence. Although the OSUC-2 benefits from the selective updating strategy, this particular segmented region (caused by the illumination change) cannot be accurately adjusted in the model, reducing the performance as seen in Table 2. In contrast, both algorithms OSUC and PBAS, learn the new dynamics derived from the non-stationary illumination change as seen Fig. 13(i) Fig. 13(m), respectively. The OSUC tracking stage does not consider the region produced for the illumination change as an object since it does not have a coherent movement as explained in Fig. 4.

In the category of challenge b, the SC-SOBS and OSUC approaches produce the best results in terms of the performed F
                        1 values; the former method gets the highest F
                        1 average, while the latter reaches a bit less value, but with a lower dispersion as seen in Table 2. Nonetheless, a closer visual inspection to the foreground masks for the DBa-Overpass video (see Fig. 13(o) to 13(u)) shows that both the Z-GMM and PBAS algorithms fail to detect foreground objects staying at the same location for a long time. Due to the employed blind updating strategies, those foreground objects are slowly incorporated into the background model. OSUC-2 behaves a bit better since it uses a selective updating. However, objects with similar chromaticity as the background are wrongly modeled, leading to false foreground masks, which is reflected in the obtained low F
                        1 measures. Further, the OSUC and the GMM-T algorithms do not fully encode the background variations if foreground objects stay in front of an extremely dynamical background region. As a result, a noisy foreground mask appears inside the tracking blobs as seen in Fig. 13(o) and Fig. 13(r). Another aspect of interest is whether the background has highly dynamical regions that can be detected as foreground elements, as is the case of the DBa-Fountain01. As shown in Fig. 13(x) to 13(z) the Z-GMM, GMM-T and SC-SOBS methods detect all fountains mistakenly as the foreground. In contrast, OSUC and PBAS can perform the foreground/background discrimination properly since their updating rules involve the pixel variability (see Fig. 13(w) and Fig. 13(aa)). Furthermore, the introduced Correntropy-based cost function in OSUC allows including highly dynamical background information.

In the case of category of challenge c, Table 2 shows that OSUC algorithm attains, in average, the highest F
                        1 value with a low standard deviation, outperforming all compared algorithms. The first challenging aspect to consider is the detection of the foreground objects that stop and then get static for a long period of time, tending to have similar behavior to the background dynamics as it is the case of DBa-CopyMachine and DBc-LeftBag videos (see Figs. 13(ac) to 13(ad)). The OSUC and GMM-T algorithms can successfully track such objects, facilitating the discrimination between foreground and background as shown in Fig. 13(ad), Fig. (af) and Fig. 13(ak), Fig. (am). On the contrary, the remaining three algorithms barely deal with the task. The second challenging aspect is when one foreground object keeps static for a while at the beginning of the recording and then suddenly starts moving as in the case of DBa-WinterDriveway and DBb-ShoppingMall sequences (see Figs. 13(aq) to 13(bd)). Here, the OSUC-2 and SC-SOBS perform the worst discrimination since the uncovered areas are not incorporated into the background model (see Fig. 13(au) and Fig. 13(bb) SC-SOBS). Likewise, the ZGMM and the PBAS algorithms cannot manage to quickly include sudden changes in the background model, leading to false segmentations (see Fig. 13(as), Fig. 13(av), Fig. 13(az), and Fig. 13(bc)). Because of the included tracking stage, GMM-T outperforms the Z-GMM approach. However, some foreground regions are wrongly tracked and persistently miss-classified as foreground. This result may be explained due to the seeds for the tracking initialization are the self-foreground regions without any motion analysis. On the other hand, due to the ability of the model Section 3.2 to detect moving objects, the OSUC algorithm correctly identifies the objects that start moving, regardless the time they have been static as seen in Fig. 13(aq). Moreover, the Correntropy-based cost function makes the OSUC adapt the model to the object absence, improving the segmentation task (see Fig. 13(ar) and Fig. 13(ay)). Overall, attained results show that coupling the Correntropy-based background modeling together with the object motion analysis allows improving discrimination between foreground and background dynamics.


                        Sensitivity analysis of the OSUC parameters
                        . We evaluate the influence of OSUC parameters on the performed background and foreground discrimination in terms of the F
                        1 measure. To this, we identify the most important parameters of the three stages described in Section 4 as: The time window size T
                        
                           e
                         (Correntropy-based cost function), the number of particles N
                        
                           p
                         (object motion analysis), and the threshold segmentation parameter ζ
                        
                           S
                         (foreground and background discrimination). Taking into account the common frame rate of security cameras, we select the analysis interval of the time window size T
                        
                           e
                         ranging within the values {5,15,25,35,50}. It can be seen in 
                        Fig. 14(a) that attained results for category a are quite consistent for considered values of T
                        
                           e
                        ; this can be explained by the fact that these videos include simple and almost stationary dynamics. For the category b, the F
                        1 increases asymptotically and then become constant at T
                        
                           e
                        
                        ≥25; this result is explained by the fact that for small T
                        
                           e
                         values the automatic Correntropy-based cost function bandwidth has not enough information to learn pixel dynamics. Category c reaches the maximum F
                        1 values within the interval T
                        
                           e
                        
                        ∈[20−40], then results decrease since high T
                        
                           e
                         values do not allow to rightly describe scenarios with non-stationary foreground dynamics. Overall, T
                        
                           e
                        
                        =25 seems to be an adequate time window size value.

Besides, to get a trade-off between computational cost and accuracy, we vary the number of particles N
                        
                           p
                        , within the value set {10,50,100,200,400,600,800}. As shown in Fig. 14(b), a low number of particles decreases the object tracking accuracy, which directly affects the final segmentation results obtained for all categories. Then, after values of approx. N
                        
                           p
                        
                        =400 the segmentation performance for all categories becomes steady, therefore, N
                        
                           p
                        
                        =400 is an adequate value to fulfill the desired trade-off.

Finally, OSUC based video segmentation is tested by varying the threshold segmentation parameter ζ
                        
                           S
                         within the set {0.05,0.2,0.4,0.6,0.8,0.95}. Particularly, for category a, the higher ζ
                        
                           S
                         the better F
                        1 average. On the other hand, a ζ
                        
                           S
                         value from 0.6 to 0.8 is suitable for complex background and foreground dynamics (categories b and c), as it can be seen in Fig. 14(c). Indeed, low ζ
                        
                           S
                         values tend to estimate under-segmented results, while high ζ
                        
                           S
                         values attain over-segmented outcomes.


                        OSUC computational burden
                        . The computational cost is calculated for a concrete prototype of OSUC using a C
                        ++ implementation on an Intel Xeon 3.4GHz processor. Table 3
                         shows the average FPS for the 23 videos and their respective resolution. It can be seen that the computational cost of OSUC highly depends on the image resolution since for the highest resolution videos DBa-Fall, DBa-CopyMachine and DBa-PETS2006, the lowest FPS is attained. Otherwise, the highest FPS is obtained for the lowest resolution video DBb-WaterSurface. It also can be appreciated that for videos with simple background and foreground dynamics such as DBd-MosheWalk, DBd-DariaWalk, the FPS is high. However, the relationship between foreground and background dynamics becomes more complex for DBa-Canoe, DBa-Fountain01, DBb-Bootstrap, DBb-ShoppingMall videos. This situation decreases the FPS significantly since the computational cost over the object motion analysis stage is directly proportional to the number of objects to track. To overcome this issue, OSUC can be easily parallelized by performing the tracking of each object in different threads. In general, the proposed OSUC exhibits an affordable computational cost (21.64 fps) for medium resolution videos (348×253). Some OSUC based video segmentation results are online available.
                           7
                        
                        
                           7
                           
                              https://vimeo.com/channels/osucsm
                           
                        
                     

Additionally, we calculate the computational cost of the main stages of OSUC algorithm (see blocks of Fig. 6). Given that videos have different resolutions, the computational cost in this case is measured as the percentage that takes each stage to process a frame. As seen from Table 4
                        , the most expensive stages are the Motion Detection, Object Tracking and Correntropy Background. However, the last one is only computed every T
                        
                           e
                         frames. Here, the need of a multithread implementation is even more evidenced since the stages taking the most time are the ones related to the number of foreground regions. Moreover, the Correntropy Background stage can be computed in a thread aside to avoid increasing the computational cost of the system every T
                        
                           e
                         frames.

@&#CONCLUSIONS@&#

A methodology for background and foreground discrimination in video surveillance scenarios is proposed. Our approach, called OSUC analyzes pixel spatio-temporal relations as an adaptive learning algorithm. OSUC infers the temporal statistical distribution of each pixel by using a Correntropy-based cost function, which is able to weight the relevance of each new input pixel to build a background model. Moreover, an automatic tuning algorithm is proposed in order to suitably set the Correntropy kernel bandwidth by evaluating the error distribution shape, achieving good performance under both Gaussian and non-Gaussian pixel distribution conditions. By including an object motion analysis stage, proposed OSUC is able to properly detect, model, and track foreground objects. Such information is weighted afterwards into the background model, avoiding false segmentations when working with the presence of moving and static foreground objects in the scene. Besides, OSUC object motion analysis stage is able to give extra information about the foreground object dynamics which could be further used to solve other video surveillance tasks.

From the obtained results, we infer that proposed OSUC is able to handle well known issues from video surveillance systems. Furthermore, achieved performance is comparable with state of the art algorithms, obtaining even better results when the challenge of the segmentation relies on the complexity of the relations between foreground and background dynamics. Furthermore, it was shown with the experiments performed by OSUC-2 that even using a single Gaussian model, the Correntropy cost function is able to support the background modeling process just as good as the Z-GMM. The main OSUC free parameters are experimentally studied, to validate algorithm's performance and stability under different video conditions. Proposed approach, is a suitable alternative to support video surveillance tasks achieving good segmentation results, while having an affordable computational burden.

As future work, it would be interesting to consider more robust approaches to model and track the detected foreground objects. In addition to F-measure in the experimental validation, the authors plan to employ more elaborate metrics (like D-Score or Structural Similarity [5]) that take into account more information about the segmented image structure. Finally, an OSUC extension should be developed to deal with moving cameras, and optimized implementation will be carried out in order to improve the OSUC scalability.

@&#ACKNOWLEDGEMENTS@&#

Research supported by a PhD. scholarship funded by Colciencias and the projects “Plataforma tecnológica para los servicios de teleasistencia, emergencias médicas, seguimiento y monitoreo permanente de pacientes y apoyo a los programas de prevención” - ARTICA and project 16882 funded by Universidad Nacional de Colombia and Universidad de Caldas.

@&#REFERENCES@&#

