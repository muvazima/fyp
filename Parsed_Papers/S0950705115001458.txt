@&#MAIN-TITLE@&#Integration of graph clustering with ant colony optimization for feature selection

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel supervised filter-based feature selection method using ACO is proposed.


                        
                        
                           
                           Our method integrates graph clustering with a modified ant colony search process.


                        
                        
                           
                           Each feature set is evaluated using a novel measure without using any learning model.


                        
                        
                           
                           The sizes of the final feature set is determined automatically.


                        
                        
                           
                           The method is compared to the state-of-the-art filter and wrapper based methods.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Feature selection

Ant colony optimization

Filter method

Graph clustering

@&#ABSTRACT@&#


               
               
                  Feature selection is an important preprocessing step in machine learning and pattern recognition. The ultimate goal of feature selection is to select a feature subset from the original feature set to increase the performance of learning algorithms. In this paper a novel feature selection method based on the graph clustering approach and ant colony optimization is proposed for classification problems. The proposed method’s algorithm works in three steps. In the first step, the entire feature set is represented as a graph. In the second step, the features are divided into several clusters using a community detection algorithm and finally in the third step, a novel search strategy based on the ant colony optimization is developed to select the final subset of features. Moreover the selected subset of each ant is evaluated using a supervised filter based method called novel separability index. Thus the proposed method does not need any learning model and can be classified as a filter based feature selection method. The proposed method integrates the community detection algorithm with a modified ant colony based search process for the feature selection problem. Furthermore, the sizes of the constructed subsets of each ant and also size of the final feature subset are determined automatically. The performance of the proposed method has been compared to those of the state-of-the-art filter and wrapper based feature selection methods on ten benchmark classification problems. The results show that our method has produced consistently better classification accuracies.
               
            

@&#INTRODUCTION@&#

In recent years, with the advance of science and technology, the amount of data has been growing rapidly and thus pattern recognition methods often deal with samples consisting of thousands of features. This problem is called curse of dimensionality and reduction of the datasets’ dimensionality becomes crucial to make them tractable [7,45,61]. Dimensionality reduction methods provide a way of understanding the data better, improving prediction performance and reducing the computation time in pattern recognition applications. As a general rule, for a classification problem with 
                        
                           D
                        
                      dimensions and 
                        
                           C
                        
                      classes, a minimum of 
                        
                           10
                           ×
                           D
                           ×
                           C
                        
                      training samples are required [9]. While it is practically impossible to acquire the required number of training samples, reducing features reduces the size of the training sample required and consequently helps to improve the overall performance of the classification algorithm.

A common way to deal with such problems is the feature selection technique. The feature selection methods can be classified into four categories including filter, wrapper, embedded, and hybrid models [51]. The filter approach requires the statistical analysis of the feature set without utilizing any learning algorithm. In contrast, wrapper-based feature selection methods apply a learning algorithm to evaluate the quality of feature subsets in the search space iteratively. In the embedded model the feature selection procedure is considered as a part of the process of building models. On the other hand, the goal of the hybrid-based methods is to use the computational efficiency of the filter model and the proper performance of the wrapper model.

In recent years many evolutionary and swarm-based methods such as Genetic Algorithm (GA) ([8,56], ant colony optimization (ACO) [1,11,17,54,60]), particle swarm optimization (PSO) [27,58] and Artificial Bee Colony (ABC) [36,44] and Harmony Search algorithm (HSA) [62] have been utilized to tackle the feature selection problem. Among the swarm intelligence-based methods, ACO has been successfully used in the feature selection area of research. The ACO is a metaheuristic algorithm for solving hard combinatorial optimization problems [35]. This algorithm has been successfully applied to a large number of difficult combinatorial problems such as vehicle routing [47], graph coloring [16], and communication network [68]. ACO is a multi-agent system and it has some advantages such as positive feedback, the use of a distributed long-term memory, nature implementation in a parallel way, functions similar to those of reinforcement learning schemata, and a good global and local search capability due to stochastic and greedy components in the algorithm. Moreover, ACO has been successfully applied for feature selection problem [1,11,12,17,26,31,32,37,52,54,67]. Although, ACO has been shown as an effective approach to finding optimal (or near optimal) feature subsets while it suffers from several shortcomings which are listed as follows:
                        
                           1.
                           
                              Graph representation: In many ACO-based feature selection methods the problem space is represented by a fully connected graph except a work [11] in which the problem space was represented by a directed graph with only 
                                 
                                    2
                                    n
                                 
                               arcs where 
                                 
                                    n
                                 
                               denotes the number of features. In the case of fully connected graphs, in each step (i.e., step 
                                 
                                    t
                                 
                              ) each ant should compute the probability rule for unselected features (i.e., 
                                 
                                    n
                                    -
                                    t
                                    +
                                    1
                                 
                              , where 
                                 
                                    n
                                 
                               denotes the number of features) which leads to increase the time complexity of the algorithm. For example if the ant needs to traverse 
                                 
                                    m
                                 
                               numbers of nodes in the graph, 
                                 
                                    (
                                    n
                                    )
                                    !
                                    /
                                    (
                                    n
                                    -
                                    m
                                    )
                                    !
                                 
                               computations are needed; therefore one can reduce these computations.


                              Updating pheromone: Most ACO-based feature selection methods employed a learning model in their search process to evaluate a constructed feature subset, and thus they are classified as the wrapper model [1,37,60]. While, the wrapper based methods need high computational time especially on the datasets with a large number of features. On the other hand, only in three cases instead of the learning model, information theoretic measures are used to update the pheromone [32,48,52,54].


                              Selecting redundant features: In most of ACO-based feature selection methods, the possible dependency between the features is ignored in the search process [1,11,60]. These methods assume that the features are conditionally independent and thus, while the ant selects the next feature, the dependency of the feature on previously selected ones is ignored in the computations. Therefore, the constructed subset may contain the redundant features, which reduces the classifier performance.


                              Final subset size: The number of selected features which defines traverse path length of the ants, imposes another challenge on ACO-based methods. In most of the ACO-based feature selection methods the number of traversed nodes should be pre-determined before the ant starts their search processes [48,52,54]. Moreover the accuracy of these methods depends on optimally defining the size of feature subset.

To overcome the mentioned shortcomings, in this paper we propose a novel filter-based feature selection method based on ACO algorithm. The method attempts to select high-quality features within a reasonable time. The proposed algorithm which is called Graph Clustering based ACO feature selection method, in short GCACO, works in three steps. In the first step, the problem space is represented as a graph in which each node denotes a feature and the edges weights are similarities between features. In the second step, features are divided into several clusters by employing an efficient community detection algorithm [59]. Finally in the third step, a novel ACO-based search strategy is proposed for selecting the final feature subset. In this strategy an ant which is placed on a randomly selected cluster, in each step, decides to select the next position in the current cluster or move to another cluster. In the case of remaining in the same cluster, the probability values are computed only for the features of this cluster. In contrast for the other cases the probability values are computed for the features of the next selected cluster. This process is continued until all of the clusters are visited. Therefore, the number of features which are selected by each ant in each cycle and also the final feature subset can be automatically determined based on the number of clusters in the problem space.

This approach is quite different from those of the existing schemes [48,52,54], where the size of the constructed subset is defined by a fixed number. Furthermore, the aim of using community-based representations of the problem space is to group highly correlated features into the same cluster. Therefore the ACO-based search process is guided in such a way that relatively less correlated features are injected in a high proportion with respect to more correlated features to the consecutive iteration. Besides, the similarity between features is considered in computation of feature relevance, which minimizes the redundancy between selected features. Therefore, the clustering-based strategy of the proposed method has a high probability of identifying a subset of useful and independent features. Moreover, clustering the features in the problem space results in reduction of the computation complexity of probability values because when an ant is placed in a given cluster, the probability value is computed only for features in the current cluster. Furthermore, unlike most of the existing ACO-based feature selection methods which use a learning algorithm [1,11,37] to evaluate the constructed subsets, in this paper a feature subset is evaluated by means of a separability index matrix without using any learning models. Therefore the proposed method can be classified as a filter-based approach and thus it will be computationally efficient for high-dimensional datasets.

The rest of this paper is organized as follows. Section 2 reviews related works on feature selection. A detailed description of our proposed method, including the complexity analysis of the different steps, is presented in detail in Section 3. In Section 4 we compare the proposed algorithm with other existing feature selection methods. Finally, Section 5 summarizes the present study.

@&#RELATED WORKS@&#

The main idea behind feature selection is to choose a subset of available features, by eliminating irrelevant features with little or no predictive information, as well as redundant features that are strongly correlated. To find the optimal feature subset one needs to enumerate and evaluate all the possible subsets of the features. The entire search space contains all the possible subsets of features, meaning that the search space size is 
                        
                           
                              
                                 2
                              
                              
                                 n
                              
                           
                        
                      where 
                        
                           n
                        
                      is the dimensionality of the problem (i.e., the number of original features). Therefore, the problem of finding the optimal feature subset is NP-hard [10,19]. Since evaluating the entire feature subsets is computationally expensive, time consuming and also impractical even for moderate sized feature sets, the final solution should be found in a feasible computational time with a reasonable trade-off between the quality of the found solution and time–space cost. Therefore, many feature selection algorithms involve heuristic or random search strategies to find the optimal or near optimal subset of features in order to reduce the computational time. Feature selection is a fundamental research topic in machine learning with a long history since the 1970s, and there are a number of attempt to review the feature selection methods [10,51].

The feature selection methods can be classified into four categories including filter, wrapper, embedded, and hybrid models. The filter approach requires only a statistical analysis on a feature set for solving the feature selection task without utilizing any learning algorithms. Therefore, the methods in this approach are typically fast. The filter-based feature selection methods can be classified into univariate and multivariate methods. In the univariate methods, the informativeness of each feature is evaluated individually, according to a specific criterion, such as the Information gain [33], Gain Ratio [39], Term Variance [55], Gini index [42], Laplacian Score (L-Score) [64] and Fisher Score (F-Score) [41]. This means that each feature is considered separately, thereby ignoring feature dependencies, which may lead to worse classification performance when compared to other types of feature selection methods. In order to overcome the problem of ignoring feature dependencies, a number of multivariate filter methods were introduced, aiming at the incorporation of feature dependencies to some degree. Multivariate approaches, on the contrary, evaluate the relevance of the features considering how they function as a group, taking into account their dependencies.

The wrapper model uses the learning algorithm as a predictor and the predictor performance as the objective function to evaluate the feature subset. The wrapper-based methods can be broadly classified into sequential feature selection algorithms (SFS) and heuristic search algorithms. The sequential forward (backward) selection algorithms start with an empty set (full set) and add features (remove features) until the maximum objective function is obtained. On the other hand, the heuristic search algorithms evaluate different subsets to optimize the objective function. Different subsets are generated either by searching around in a search space or by generating solutions to the optimization problem. The main drawback of the wrapper model is the number of required computations to obtain the feature subset. To evaluate the feature subset, a learning algorithm is trained for each subset and tested to obtain the classifier accuracy. Therefore, most of the execution time of the algorithm is spent on the training of the predictor when dealing with high-dimensional datasets.

The hybrid model is a combination of the filter and the wrapper models and attempts to take advantage of both approaches. The hybrid model mainly focuses on combining the filter and the wrapper-based methods to achieve the best possible performance with a particular learning algorithm with time complexity similar to that of the filter-based methods. Moreover, the embedded model tries to include the feature selection as a part of the classifier training process, like inherently binary decision tree classifiers do. In other words, the feature selection process is embedded into the training of the learning algorithm.

Feature selection methods are modeled using different sorts of optimization algorithms such as swarm intelligence (SI) or evolutionary algorithms (EAs). Swarm intelligence has become a research interest to many research scientists of related fields in recent years. Swarm intelligence is a computational intelligence-based approach which is made up of a population of artificial agents and inspired by the social behavior of animals in the real world. Each agent performs a simple task, while the colony’s cooperative work will solve a hard problem. In this section, feature selection algorithms relying on the swarm intelligent methods such as particle swarm optimization (PSO), artificial bee colony optimization (ABC), differential evolution (DE), gravitational search algorithm (GSA), harmony search algorithm (HSA) and ant colony optimization (ACO) are reviewed and outlined in Table 1
                     .

Particle swarm optimization is a powerful swarm-based metaheuristic method, proposed by Kennedy and Eberhart in 1995 [28]. PSO is motivated by social behaviors such as bird flocking and fish schooling. The particle swarm optimization method has recently gained more attention for solving the feature subset selection problem [6,25,27,58,65]. Unler et al. [58] proposed a hybrid method called maximum relevance minimum redundancy PSO (mr2PSO), which integrates the mutual information-based filter model within the PSO-based wrapper model. Another hybridized approach was proposed by Inbarani et al. [27] to solve the selection of appropriate features for the medical diagnosis problems. In this method, two hybrid supervised PSO-based feature selection methods called PSO-based Relative Reduct (PSO-RR) and PSO based Quick Reduct (PSO-QR) are proposed. Moreover, Huang and Dun [25] proposed a PSO–SVM model that hybridizes the PSO and support vector machines (SVMs) to improve the classification accuracy with a small and appropriate feature subset. Furthermore, Xue et al. [65] proposed three initialization strategies and several updating mechanisms in PSO to develop feature selection approaches. The goal of the method was to select smaller numbers of features as well as to achieve better classification performance.

Genetic algorithm (GA) is one of the most widely used techniques for feature selection problem [2,13–15,29,38,43,46,50,53,56,57,63,66]. In [46], a genetic algorithm is used simultaneously for feature selection and extraction and training classifier. Ho et al. [24] designed an intelligent genetic algorithm (IGA) to tackle both instance and feature selection problems simultaneously by introducing a special orthogonal cross operator. Again, in Ramirez-Cruz et al. [43] GA and evolution strategies are combined to select instances and weight the features. Similarly, in Ros et al. [50] a hybrid genetic approach is proposed which treats feature and instance selection problems as a single optimization problem. Ahn and Kim [2] used a genetic algorithm method to simultaneously optimize feature weighting and instance selection for case-based reasoning in the bankruptcy prediction problem. In Kabir et al. [38] a hybrid genetic algorithm with a specific local search is proposed for feature selection. In Uğuz [57] a two-stage feature selection method is proposed for text categorization by using information gain, principal component analysis and genetic algorithm. A more comprehensive study of feature selection algorithms based on genetic algorithm is provided in Tsai et al. [13].

The artificial bee colony algorithm is one of the recently introduced swarm-based algorithms. This algorithm was proposed to simulate the intelligent foraging behavior of honey bee swarms. This method is also used for solving feature selection tasks in several studies [36,44]. Schiezaro and Pederini [36] proposed a feature selection method for data analysis based on the ABC algorithm that can be used in several knowledge domains through wrapper and forward strategies. Moreover, in Forsati et al. [44] the feature selection task is formulated as an optimization problem and a feature selection procedure based on Bee Colony Optimization (BCO) is proposed in order to achieve better classification results. The Differential Evolution (DE) algorithm is a well-known population-based algorithm which has been successfully applied to a variety of pattern recognition applications as well as the feature selection problem. Al-Ani et al. [3] proposed a wrapper-based feature selection method using differential evolution. The aim of the method is to reduce the search space using a simple, yet powerful, procedure that involves distributing the features among a set of wheels. Moreover, Han et al. [21] presented a feature subset selection search procedure using a modified gravitational search algorithm (GSA). Recently, a feature selection method based on harmony search algorithm (HS) is proposed for email classification [62].

In recent years, some ACO-based methods for feature selection have been reported. Aghdam et al. [1] proposed an ACO-based feature selection algorithm to improve the performance of the text categorization problems. In their method, the classifier performance and the length of the selected feature subset are used to adopt the heuristic information of the ACO. Moreover, Kabir et al. [37] proposed a hybrid ACO-based feature selection algorithm, called ACOFS, which uses the information gain method to define the heuristic information of the ACO for each feature and the neural network predictor to evaluate the results of each ant. Furthermore, Vieira et al. [60] proposed an ACO-based feature selection algorithm which uses two cooperative ant colonies. The goal of the first colony is to determine the number of features while the aim of the second one is to select the features based on cardinality given by the first colony. In Li et al. [67], a two-stage ACO-based feature selection called ACO-S was proposed to apply to microarray datasets. In the first stage of the method an ant system is used to filter the non-significant genes and in the second stage an improved ant colony system is applied to gene selection. Most of the existing ACO-based feature selection methods need to traverse a complete graph with 
                        
                           O
                           (
                           
                              
                                 n
                              
                              
                                 2
                              
                           
                           )
                        
                      edges where n is the number of original features. However, Chen et al. [11] present a different feature ACO-based algorithm in which the artificial ants traverse on a directed graph with only 
                        
                           O
                           (
                           2
                           n
                           )
                        
                      arcs. Recently, in Forsati et al. [17], a new variant of ACO, called enRiched Ant Colony Optimization (RACO) was proposed for the feature selection task. In this method the information contained in the traversals of the previous iterations is modeled as a rich source that guides the ant’s further path selection and pheromone updating stages. The mentioned ACO-based methods are based on the wrapper model which needs a learning algorithm to evaluate the results of each ant. On the other hand, in Ke et al. [32] a filter ACO-based algorithm called ACOAR was proposed to deal with attribute reduction in rough set theory. Recently, in Tabakhi et al. [54] a filter method based on the ACO algorithm, called UFSACO, was proposed. The method seeks the optimal feature subset through several iterations without using any learning algorithms.

@&#PROPOSED METHOD@&#

In this section a novel method is described which can efficiently and effectively deal with both irrelevant and redundant features. The proposed method consists of three steps: (1) Graph representation of the problem space, (2) Feature clustering and (3) Searching for the optimal feature subset based on ACO. In the first step the feature set is represented as a graph in which each node in the graph denotes a feature and each edge weight indicates the similarity value between its corresponding features. In the second step, the features are divided into several clusters using a community detection method [59]. The goal of features clustering is to group most correlated features into the same cluster. In the third step a novel feature selection algorithm based on the search strategy of the ACO is proposed to select the final feature subset. Fig. 1
                      illustrates the overall schema of the proposed three-step method. In Fig. 1(a) the feature space is represented as a weighted graph in which the nodes represent the features, and also the edges denote that the similarity value between the corresponding two features. After applying the graph clustering method, the features were grouped into three clusters. Fig. 1(b) shows the clustering result for the graph. Finally Fig. 1(c) shows that the ants search for the optimal feature subset in the different groups of the features. For example, as can be seen from Fig. 1(c) the ant starts to move from cluster 1 and selects feature 
                        
                           
                              
                                 F
                              
                              
                                 1
                              
                           
                        
                      from this cluster. It should be noted that the features in the clusters are selected based on their probability values which can be obtained by applying the fisher score measure and considering the similarity with the previously selected features. Then in the next step, the ant has two choices: remaining in the same cluster or going to another cluster. In the case of remaining in the same cluster, the ant starts to choose remaining features based on their probability values. On the other hand while the ant wants to go to another cluster, the next feature will be selected in this cluster. It can be seen from Fig. 1(c) that the ant goes to the cluster 2 and selects feature 
                        
                           
                              
                                 F
                              
                              
                                 6
                              
                           
                        
                      from this cluster. Moreover, in the third step the ant decides to select the next position in the current cluster (i.e., cluster 2) and selects feature 
                        
                           
                              
                                 F
                              
                              
                                 5
                              
                           
                        
                     . Then in the fourth step feature 
                        
                           
                              
                                 F
                              
                              
                                 8
                              
                           
                        
                      from cluster 3 is selected. Finally, all of the clusters have been traversed and the ant has constructed the feature subset (i.e., 
                        
                           {
                           
                              
                                 F
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 F
                              
                              
                                 6
                              
                           
                           ,
                           
                              
                                 F
                              
                              
                                 5
                              
                           
                           ,
                           
                              
                                 F
                              
                              
                                 8
                              
                           
                           }
                           }
                        
                     ). The obtained subset is evaluated using the separability index matrix method and there is no need for any learning models. The additional details are described in the corresponding subsections.

In general, to apply the ACO algorithm, the search space of the feature selection problem should be represented by a fully connected undirected graph. Thus, we attempt to model the feature selection problem using a graph theoretic representation. To this end, the feature set is mapped into its equivalent graph 
                           
                              G
                              =
                              (
                              F
                              ,
                              E
                              ,
                              
                                 
                                    w
                                 
                                 
                                    F
                                 
                              
                              )
                           
                        , where 
                           
                              F
                              =
                              {
                              
                                 
                                    F
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    F
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              }
                           
                         is a set of original features, 
                           
                              E
                              =
                              {
                              (
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                              )
                              :
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                              ∈
                              F
                              }
                           
                         denotes the edges of the graph and 
                           
                              
                                 
                                    w
                                 
                                 
                                    ij
                                 
                              
                           
                         indicates the similarity between two features 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                           
                         connected by edge 
                           
                              (
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                              )
                              .
                           
                         The methods for measuring the similarity values (i.e., edge weights) critically determine the performance of the subsequent graph-based feature selection algorithm. There are different similarity measures that can be used to determine the edge weights and different methods may lead to different results. Therefore, we need to carefully select the most suitable measure. Generally, the Euclidean distance and Pearson’s correlation coefficient are both widely used as similarity measures. In this work, the Pearson correlation coefficient measure is used to measure the similarity value between different features of a given training set. The correlation between two features 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                           
                         is defined as follows:
                           
                              (1)
                              
                                 
                                    
                                       w
                                    
                                    
                                       ij
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      p
                                                   
                                                
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      i
                                                   
                                                
                                                -
                                                
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                   
                                                   
                                                      ‾
                                                   
                                                
                                                )
                                                (
                                                
                                                   
                                                      x
                                                   
                                                   
                                                      j
                                                   
                                                
                                                -
                                                
                                                   
                                                      
                                                         
                                                            x
                                                         
                                                         
                                                            j
                                                         
                                                      
                                                   
                                                   
                                                      ‾
                                                   
                                                
                                                )
                                             
                                             
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            p
                                                         
                                                      
                                                      
                                                         
                                                            (
                                                            
                                                               
                                                                  x
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                            -
                                                            
                                                               
                                                                  
                                                                     
                                                                        x
                                                                     
                                                                     
                                                                        i
                                                                     
                                                                  
                                                               
                                                               
                                                                  ‾
                                                               
                                                            
                                                            )
                                                         
                                                         
                                                            2
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            ∑
                                                         
                                                         
                                                            p
                                                         
                                                      
                                                      
                                                         
                                                            (
                                                            
                                                               
                                                                  x
                                                               
                                                               
                                                                  j
                                                               
                                                            
                                                            -
                                                            
                                                               
                                                                  
                                                                     
                                                                        x
                                                                     
                                                                     
                                                                        j
                                                                     
                                                                  
                                                               
                                                               
                                                                  ‾
                                                               
                                                            
                                                            )
                                                         
                                                         
                                                            2
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                           
                         denote the vectors of features 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                           
                        , respectively. Variables 
                           
                              
                                 
                                    
                                       
                                          x
                                       
                                       
                                          i
                                       
                                    
                                 
                                 
                                    ‾
                                 
                              
                           
                         and 
                           
                              
                                 
                                    
                                       
                                          x
                                       
                                       
                                          j
                                       
                                    
                                 
                                 
                                    ‾
                                 
                              
                           
                         represent the mean values of vectors 
                           
                              
                                 
                                    x
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    x
                                 
                                 
                                    j
                                 
                              
                           
                        , averaged over 
                           
                              p
                           
                         samples. It is clear that the similarity value between two features which are completely similar will be equal to 1, and on the other hand for completely dissimilar features this value will be equal to 0. In most cases, the similarity values between features in these datasets are so close to each other. To overcome this situation, a nonlinear normalization method called softmax scaling [55] is used to scale the edge weight into the range [0 1] as follows:
                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             w
                                          
                                          
                                             ˆ
                                          
                                       
                                    
                                    
                                       ij
                                    
                                 
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       1
                                       +
                                       exp
                                       
                                          
                                             
                                                -
                                                
                                                   
                                                      
                                                         
                                                            w
                                                         
                                                         
                                                            ij
                                                         
                                                      
                                                      -
                                                      
                                                         
                                                            w
                                                         
                                                         
                                                            ¯
                                                         
                                                      
                                                   
                                                   
                                                      σ
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    w
                                 
                                 
                                    ij
                                 
                              
                           
                         the similarity value between features 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                           
                        , 
                           
                              
                                 
                                    w
                                 
                                 
                                    ¯
                                 
                              
                           
                         and 
                           
                              σ
                           
                         are respectively the mean and variance of indicates all of the similarity values, and 
                           
                              
                                 
                                    
                                       
                                          w
                                       
                                       
                                          ˆ
                                       
                                    
                                 
                                 
                                    ij
                                 
                              
                           
                         indicates the normalized value of the similarity between features 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    F
                                 
                                 
                                    j
                                 
                              
                           
                        .

The main idea behind feature clustering is to group the original features into several clusters based on the similarity values between features. Therefore, the features in the same cluster are similar to each other. Most of the existing feature clustering methods suffer from some shortcomings [30]. First, indicating the desired number of clusters, the parameter 
                           
                              k
                           
                        , has to be specified in advance. Generally defining the proper number of clusters needs exhaustive trial-and-error. Second, the distribution of the data in a cluster is an important factor and the existing methods do not consider the variance of the underlying cluster in similarity computation. Third, all features in a cluster have the same degree of contribution to the resulting extracted feature. To deal with these issues in this paper a community detection method is applied to cluster the features.

Detection of communities in the weighted graph is significant for understanding the graph structures and analysis of the graph properties. The goal of community detection in this study is to cluster the similar highly correlated features into the same community and separate them from the others. In this work, we have used the Louvain community detection algorithm to identify the feature clusters [59]. This algorithm detects the communities in the graph by maximizing a modularity function. This is a simple, efficient and easy-to-implement method for identifying communities in large networks. The computational complexity of the algorithm is 
                           
                              O
                              (
                              n
                              log
                              n
                              )
                           
                        , where 
                           
                              n
                           
                         is the number of the nodes in the graph, so it can be used to detect communities in very large networks within short computing times. The method detects communities of a network in two steps. In the first step each node is assigned to a community chosen in order to maximize specific network modularity; and the second step simply makes a new network by merging those of the previously found communities. Then the process iterates until a significant improvement of the network modularity is obtained. This method has two advantages. First, its steps are intuitive and easy to implement, and second, the algorithm is extremely fast. It should be noted that in the previous step the problem space was represented by a fully connected graph. Each edge in the graph was associated with a value which denoted the similarity value between every two nodes. Therefore, before using the clustering method, the edges with associated weights lower than the 
                           
                              θ
                           
                         parameter will be removed to improve the performance of the clustering method. The 
                           
                              θ
                           
                         parameter can be set to any value in the range [0 1], and thus when its value is small (large), more (fewer) edges will be considered in the graph clustering algorithm and the number of obtained clusters will be low (high). Fig. 2
                         illustrates the feature clustering algorithm for Sonar dataset. In Fig. 2(a) the feature space is represented as a weighted complete graph. After removing edge with associated weights lower than the θ parameter the complete graph converted into sparse graph. Fig. 2(b) shows this sparse graph. Finally Fig. 2(c) shows the clustering result for the graph.

In this subsection a novel ACO-based search strategy is proposed to select a feature subset from a clustered graph. The algorithm is composed of several iterations. Before the iterations start, the amount of pheromone assigned to each node is initialized to a constant value 
                           
                              γ
                           
                        . Also, the discrimination power of feature 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                         is evaluated using the Fisher score as follows:
                           
                              (3)
                              
                                 F
                                 -
                                 Score
                                 (
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             c
                                          
                                       
                                       
                                          
                                             n
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               ¯
                                                            
                                                         
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   -
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                            
                                                            
                                                               ¯
                                                            
                                                         
                                                      
                                                      
                                                         i
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             2
                                          
                                       
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             c
                                          
                                       
                                       
                                          
                                             n
                                          
                                          
                                             i
                                          
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         σ
                                                      
                                                      
                                                         i
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             2
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              c
                           
                         is the number of classes of the dataset, 
                           
                              
                                 
                                    n
                                 
                                 
                                    i
                                 
                              
                           
                         is the number of samples in class 
                           
                              i
                           
                        , 
                           
                              
                                 
                                    
                                       
                                          x
                                       
                                       
                                          ¯
                                       
                                    
                                 
                                 
                                    i
                                 
                              
                           
                         shows the mean of all the patterns corresponding to feature 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                        , and also 
                           
                              
                                 
                                    
                                       
                                          x
                                       
                                       
                                          ¯
                                       
                                    
                                 
                                 
                                    i
                                 
                                 
                                    k
                                 
                              
                           
                         and 
                           
                              
                                 
                                    σ
                                 
                                 
                                    i
                                 
                                 
                                    k
                                 
                              
                           
                         denote the mean and variance of class 
                           
                              k
                           
                         corresponding to feature 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                        . A larger 
                           
                              F
                           
                        -
                           
                              Score
                              (
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                              )
                           
                         value implies that feature 
                           
                              
                                 
                                    F
                                 
                                 
                                    i
                                 
                              
                           
                         has a greater discriminative ability. Similarly to correlation values between features, all 
                           
                              F
                           
                        -
                           
                              Scores
                           
                         are normalized using softmax scaling method [55]. Then, in each iteration, each ant starts to move from a random cluster and then it selects a feature from a given cluster. In this case the ant decides to choose the next feature from the current cluster or go to the different cluster. To this end a random value is generated and then if the generated value is lower than a predefined 
                           
                              ε
                           
                         parameter, the ant chooses the next feature from a different cluster; otherwise the ant will remain in the current cluster. The ant continues to move until all the clusters are selected. In the case of remaining in the current cluster, the ant selects the next feature based on a specified ACO-based probability rule. When all of the ants have finished their traverse on the graph, the quality of each solution (i.e., feature subset) is evaluated by applying a separability index and then the amount of pheromone of each node is updated by applying a specified ACO-based updating rule.

The process is repeated until a given number of iterations; then, the features are sorted based on their pheromone values in decreasing order. Finally, the top 
                           
                              k
                              ×
                              ω
                           
                         features with the highest pheromone values are selected as the final feature subset, where 
                           
                              k
                           
                         denotes the number of clusters and 
                           
                              ω
                           
                         is a user-specified parameter that controls the size of the final feature subset. The details of our ACO-based search strategy algorithm are provided in Fig. 3
                        .

In ACO a probabilistic decision rule, denoting the probability of selection of the nodes, is designed by combining the heuristic desirability and the pheromone density values of the nodes. In the proposed method, each ant traverses the graph using both greedy and probabilistic state transition rules. In the greedy method, the 
                              
                                 k
                              
                           th ant chooses the next feature 
                              
                                 
                                    
                                       F
                                    
                                    
                                       j
                                    
                                 
                              
                            applying the following formula:
                              
                                 (4)
                                 
                                    
                                       
                                          F
                                       
                                       
                                          j
                                       
                                    
                                    =
                                    
                                       
                                          
                                             arg
                                          
                                          
                                             
                                                
                                                   F
                                                
                                                
                                                   u
                                                
                                             
                                             ∈
                                             
                                                
                                                   UF
                                                
                                                
                                                   i
                                                
                                                
                                                   k
                                                
                                             
                                          
                                       
                                    
                                    
                                       max
                                    
                                    {
                                    
                                       
                                          [
                                          
                                             
                                                τ
                                             
                                             
                                                u
                                             
                                          
                                          ]
                                       
                                       
                                          α
                                       
                                    
                                    
                                       
                                          [
                                          η
                                          (
                                          
                                             
                                                F
                                             
                                             
                                                u
                                             
                                          
                                          ,
                                          
                                             
                                                VF
                                             
                                             
                                                k
                                             
                                          
                                          )
                                          ]
                                       
                                       
                                          β
                                       
                                    
                                    }
                                    ,
                                    
                                    if
                                    
                                    q
                                    ⩽
                                    
                                       
                                          q
                                       
                                       
                                          0
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       UF
                                    
                                    
                                       i
                                    
                                    
                                       k
                                    
                                 
                              
                            is the set of unvisited features by ant 
                              
                                 k
                              
                            from the current cluster 
                              
                                 i
                              
                           , 
                              
                                 
                                    
                                       τ
                                    
                                    
                                       u
                                    
                                 
                              
                            denotes the pheromone intensity value associated with feature 
                              
                                 
                                    
                                       F
                                    
                                    
                                       u
                                    
                                 
                              
                           , 
                              
                                 
                                    
                                       VF
                                    
                                    
                                       k
                                    
                                 
                              
                            is denotes the previously selected features (visited features) by ant 
                              
                                 k
                              
                           , 
                              
                                 η
                                 (
                                 
                                    
                                       F
                                    
                                    
                                       u
                                    
                                 
                                 ,
                                 
                                    
                                       VF
                                    
                                    
                                       k
                                    
                                 
                                 )
                              
                            indicates the heuristic information function, parameters α and β determine the importance of the pheromone versus the heuristic information value, 
                              
                                 q
                              
                            is a random number in the range [0, 1], and 
                              
                                 
                                    
                                       q
                                    
                                    
                                       0
                                    
                                 
                              
                            is a predefined constant parameter (
                              
                                 0
                                 ⩽
                                 
                                    
                                       q
                                    
                                    
                                       0
                                    
                                 
                                 ⩽
                                 1
                              
                           ).

In this paper a specific heuristic information function is proposed. According to Eq. (5), features selected by ants are those with both minimum similarity to previously selected features and maximum dependency on the target class. This selection rule results in lower probability for redundant and irrelevant features to be selected. This function is defined as follows:
                              
                                 (5)
                                 
                                    η
                                    (
                                    
                                       
                                          F
                                       
                                       
                                          i
                                       
                                    
                                    ,
                                    
                                       
                                          VF
                                       
                                       
                                          k
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             F
                                             -
                                             Score
                                             (
                                             
                                                
                                                   F
                                                
                                                
                                                   i
                                                
                                             
                                             )
                                             -
                                             
                                                
                                                   1
                                                
                                                
                                                   |
                                                   
                                                      
                                                         VF
                                                      
                                                      
                                                         k
                                                      
                                                   
                                                   |
                                                
                                             
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      
                                                         
                                                            F
                                                         
                                                         
                                                            x
                                                         
                                                      
                                                      ∈
                                                      
                                                         
                                                            VF
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                   
                                                
                                             
                                             sim
                                             (
                                             
                                                
                                                   F
                                                
                                                
                                                   i
                                                
                                             
                                             ,
                                             
                                                
                                                   F
                                                
                                                
                                                   x
                                                
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 sim
                                 (
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       F
                                    
                                    
                                       x
                                    
                                 
                                 )
                              
                            denotes the similarity value between feature 
                              
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                              
                            and features 
                              
                                 
                                    
                                       F
                                    
                                    
                                       x
                                    
                                 
                              
                           . In the probabilistic method, the 
                              
                                 k
                              
                           th ant selects the next feature 
                              
                                 
                                    
                                       F
                                    
                                    
                                       j
                                    
                                 
                              
                            with a probability of 
                              
                                 
                                    
                                       P
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 
                                    
                                       VF
                                    
                                    
                                       k
                                    
                                 
                                 ,
                                 
                                    
                                       F
                                    
                                    
                                       j
                                    
                                 
                                 )
                              
                            which is calculated as follows:
                              
                                 (6)
                                 
                                    
                                       
                                          P
                                       
                                       
                                          k
                                       
                                    
                                    (
                                    
                                       
                                          F
                                       
                                       
                                          j
                                       
                                    
                                    ,
                                    V
                                    
                                       
                                          F
                                       
                                       
                                          k
                                       
                                    
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         
                                                            
                                                               
                                                                  [
                                                                  
                                                                     
                                                                        τ
                                                                     
                                                                     
                                                                        j
                                                                     
                                                                  
                                                                  ]
                                                               
                                                               
                                                                  α
                                                               
                                                            
                                                            
                                                               
                                                                  [
                                                                  η
                                                                  (
                                                                  
                                                                     
                                                                        F
                                                                     
                                                                     
                                                                        j
                                                                     
                                                                  
                                                                  ,
                                                                  
                                                                     
                                                                        VF
                                                                     
                                                                     
                                                                        k
                                                                     
                                                                  
                                                                  )
                                                                  ]
                                                               
                                                               
                                                                  β
                                                               
                                                            
                                                         
                                                         
                                                            
                                                               
                                                                  ∑
                                                               
                                                               
                                                                  u
                                                                  ∈
                                                                  
                                                                     
                                                                        UF
                                                                     
                                                                     
                                                                        i
                                                                     
                                                                     
                                                                        k
                                                                     
                                                                  
                                                               
                                                            
                                                            
                                                               
                                                                  [
                                                                  
                                                                     
                                                                        τ
                                                                     
                                                                     
                                                                        u
                                                                     
                                                                  
                                                                  ]
                                                               
                                                               
                                                                  α
                                                               
                                                            
                                                            
                                                               
                                                                  [
                                                                  η
                                                                  (
                                                                  
                                                                     
                                                                        F
                                                                     
                                                                     
                                                                        u
                                                                     
                                                                  
                                                                  ,
                                                                  
                                                                     
                                                                        VF
                                                                     
                                                                     
                                                                        k
                                                                     
                                                                  
                                                                  )
                                                                  ]
                                                               
                                                               
                                                                  β
                                                               
                                                            
                                                         
                                                      
                                                      ,
                                                      
                                                      if
                                                      
                                                      j
                                                      ∈
                                                      
                                                         
                                                            UF
                                                         
                                                         
                                                            i
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      ,
                                                   
                                                   
                                                      if
                                                      
                                                      q
                                                      >
                                                      
                                                         
                                                            q
                                                         
                                                         
                                                            0
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      0
                                                      ,
                                                   
                                                   
                                                      otherwise
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           The probabilistic decision rules (i.e., Eqs. (4) and (6)) depend on parameters 
                              
                                 q
                              
                            and 
                              
                                 
                                    
                                       q
                                    
                                    
                                       0
                                    
                                 
                              
                           , which aims to provide a proper balance between exploration and exploitation. If 
                              
                                 q
                                 ⩽
                                 
                                    
                                       q
                                    
                                    
                                       0
                                    
                                 
                              
                            the ants select the best feature in the greedy way (i.e., exploitation); otherwise, each feature has a chance of being selected corresponding to its probability value which is computed using equation Eq. (6) (i.e., exploration). It should be noted that the exploration property of the search process prohibits the ants from converging on a common path.

At the end of each iteration, when all ants have completed their traverses on the graph, the pheromone level of each feature is updated by applying the following updating rule:
                              
                                 (7)
                                 
                                    
                                       
                                          τ
                                       
                                       
                                          i
                                       
                                    
                                    (
                                    t
                                    +
                                    1
                                    )
                                    =
                                    (
                                    1
                                    -
                                    ρ
                                    )
                                    
                                       
                                          τ
                                       
                                       
                                          i
                                       
                                    
                                    (
                                    t
                                    )
                                    +
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             k
                                             =
                                             1
                                          
                                          
                                             A
                                          
                                       
                                    
                                    
                                       
                                          Δ
                                       
                                       
                                          i
                                       
                                       
                                          k
                                       
                                    
                                    (
                                    t
                                    )
                                 
                              
                           where 
                              
                                 ρ
                              
                            is a pheromone decay parameter, 
                              
                                 
                                    
                                       τ
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 t
                                 )
                              
                            and 
                              
                                 
                                    
                                       τ
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 t
                                 +
                                 1
                                 )
                              
                            represent the amounts of pheromone on feature 
                              
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                              
                            at times 
                              
                                 t
                              
                            and 
                              
                                 t
                                 +
                                 1
                              
                           , respectively, 
                              
                                 A
                              
                            is the number of ants, and 
                              
                                 Δ
                                 
                                    
                                       τ
                                    
                                    
                                       i
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 t
                                 )
                              
                            is the extra pheromone increment to feature 
                              
                                 
                                    
                                       F
                                    
                                    
                                       i
                                    
                                 
                              
                            by ant 
                              
                                 k
                              
                           , and is defined as follows:
                              
                                 (8)
                                 
                                    
                                       
                                          Δ
                                       
                                       
                                          i
                                       
                                       
                                          k
                                       
                                    
                                    (
                                    t
                                    )
                                    =
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      γ
                                                      (
                                                      
                                                         
                                                            FS
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      (
                                                      t
                                                      )
                                                      )
                                                   
                                                   
                                                      if
                                                      
                                                      
                                                         
                                                            F
                                                         
                                                         
                                                            i
                                                         
                                                      
                                                      ∈
                                                      
                                                         
                                                            FS
                                                         
                                                         
                                                            k
                                                         
                                                      
                                                      (
                                                      t
                                                      )
                                                   
                                                
                                                
                                                   
                                                      0
                                                   
                                                   
                                                      Otherwise
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       FS
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 t
                                 )
                              
                            is the feature subset founded by ant 
                              
                                 k
                              
                            at iteration 
                              
                                 t
                              
                           , and 
                              
                                 γ
                                 (
                                 
                                    
                                       FS
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 t
                                 )
                                 )
                              
                            is the evaluation function which measures the quality of solution 
                              
                                 
                                    
                                       FS
                                    
                                    
                                       k
                                    
                                 
                                 (
                                 t
                                 )
                              
                           .

In this paper the quality of each ant’s solution is evaluated by means of a specifically devised separability index. The separability index has been derived from the multiple discriminant analysis (MDA) approach. MDA is an extension to 
                              
                                 c
                              
                           -class problems (
                              
                                 c
                                 >
                                 2
                              
                           ) of the Fisher’s Linear Discriminant [23], which has been defined for finding the best linear combination of features in case of two class problems. The class separability index is defined by 
                              γ
                              (
                              FS
                              )
                              =
                              trace
                              
                                 
                                    
                                       
                                          
                                             
                                                W
                                             
                                             
                                                T
                                             
                                          
                                          
                                             
                                                S
                                             
                                             
                                                B
                                             
                                          
                                          W
                                       
                                       
                                          
                                             
                                                W
                                             
                                             
                                                T
                                             
                                          
                                          
                                             
                                                S
                                             
                                             
                                                W
                                             
                                          
                                          W
                                       
                                    
                                 
                              
                           
                           , where 
                              
                                 W
                              
                            is the transformation matrix from the original 
                              
                                 n
                              
                           -dimensional space to the 
                              
                                 l
                              
                           -dimensional subspace corresponding to the selected subset 
                              
                                 FS
                              
                           , 
                              
                                 
                                    
                                       S
                                    
                                    
                                       w
                                    
                                 
                              
                            is the within scatter matrix, 
                              
                                 
                                    S
                                 
                                 
                                    B
                                 
                              
                            is the between scatter matrix which are calculated as:
                              
                                 (9)
                                 
                                    
                                       
                                          S
                                       
                                       
                                          w
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             c
                                          
                                       
                                    
                                    
                                       
                                          π
                                       
                                       
                                          j
                                       
                                    
                                    
                                       
                                          Σ
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                           
                           
                              
                                 (10)
                                 
                                    
                                       
                                          S
                                       
                                       
                                          B
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             j
                                             =
                                             1
                                          
                                          
                                             c
                                          
                                       
                                    
                                    (
                                    
                                       
                                          μ
                                       
                                       
                                          j
                                       
                                    
                                    -
                                    
                                       
                                          M
                                       
                                       
                                          O
                                       
                                    
                                    )
                                    
                                       
                                          (
                                          
                                             
                                                μ
                                             
                                             
                                                j
                                             
                                          
                                          -
                                          
                                             
                                                M
                                             
                                             
                                                O
                                             
                                          
                                          )
                                       
                                       
                                          T
                                       
                                    
                                 
                              
                           where 
                              
                                 
                                    
                                       π
                                    
                                    
                                       j
                                    
                                 
                              
                            is the a priori probability that a pattern belongs to a particular class j, 
                              
                                 
                                    
                                       Σ
                                    
                                    
                                       j
                                    
                                 
                              
                            is the sample covariance matrix of class j, 
                              
                                 
                                    
                                       μ
                                    
                                    
                                       j
                                    
                                 
                              
                            is the sample mean vector of that class and 
                              
                                 
                                    
                                       M
                                    
                                    
                                       O
                                    
                                 
                              
                            is the sample mean vector of the entire data points calculated as 
                              
                                 
                                    
                                       M
                                    
                                    
                                       o
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                    
                                    
                                       j
                                       =
                                       1
                                    
                                    
                                       c
                                    
                                 
                                 
                                    
                                       π
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       μ
                                    
                                    
                                       j
                                    
                                 
                              
                           .

This evaluation function presents two main advantages: (1) This index measures statistical properties of the feature subset and does not depend on any specific learning algorithms and (2) it does not require that the dimensionality of the searched subspace (i.e., the actual number of features to be used) is a priori fixed.

In the first step of the proposed method to represent the graph it is needed to compute the similarity values between each two features, so the time complexity of this step is 
                           
                              O
                              (
                              
                                 
                                    n
                                 
                                 
                                    2
                                 
                              
                              p
                              )
                           
                         where 
                           
                              n
                           
                         is the number of the original features and 
                           
                              p
                           
                         denotes the number of patterns. Moreover, in the second step, the Louvain community detection algorithm is used to group the features into several clusters and the time complexity of this algorithm is 
                           
                              O
                              (
                              n
                              log
                              n
                              )
                           
                        . Furthermore, in the third step, first of all, the relevance values of the features are evaluated using the Fisher score measure while the time complexity is 
                           
                              O
                              (
                              ncp
                              )
                           
                        , where 
                           
                              c
                           
                         is the number of classes. Then a specific ant colony based search strategy is used to select the final features. Based on this strategy each ant starts to search the solution space from different points. The search process will be repeated for a number of iterative cycles (i.e., 
                           
                              I
                           
                        ). Thus, the time complexity of this part is 
                           
                              O
                              (
                              
                                 
                                    IAkf
                                 
                                 
                                    k
                                 
                              
                              )
                           
                        , where 
                           
                              A
                           
                         is the number of the ants, 
                           
                              k
                           
                         is the number of the clusters and 
                           
                              
                                 
                                    f
                                 
                                 
                                    k
                                 
                              
                           
                         denotes the average number of features in each cluster which approximates 
                           
                              n
                              /
                              k
                           
                        , so this complexity can be represented by 
                           
                              O
                              (
                              IAn
                              )
                           
                        . In addition if the ants run in a parallel way, the computational complexity will be reduced to 
                           
                              O
                              (
                              In
                              )
                           
                        . Moreover, at the end of each iteration in the third step, the quality of each constricted subset is evaluated by means of a separability index. The time complexity of the separability index is 
                           
                              O
                              (
                              
                                 
                                    s
                                 
                                 
                                    2
                                 
                              
                              np
                              )
                           
                        , where 
                           
                              s
                           
                         is the cardinality of the selected subset. When the ACO algorithm is over, all of the features are sorted based on their pheromone values with the time complexity of 
                           
                              O
                              (
                              nlogn
                              )
                           
                         and then the 
                           
                              m
                           
                         features with highest values are selected as the final subset of features. Therefore, the time complexity of the third step is 
                           
                              O
                              (
                              ncp
                              +
                              In
                              +
                              
                                 
                                    s
                                 
                                 
                                    2
                                 
                              
                              np
                              +
                              n
                              log
                              n
                              )
                           
                        . Consequently, the final time complexity of the GCACO method is 
                           
                              O
                              (
                              
                                 
                                    n
                                 
                                 
                                    2
                                 
                              
                              p
                              +
                              n
                              log
                              n
                              +
                              ncp
                              +
                              In
                              +
                              
                                 
                                    s
                                 
                                 
                                    2
                                 
                              
                              np
                              +
                              n
                              log
                              n
                              )
                           
                        . When the number of features which are selected by each ant (i.e., 
                           
                              s
                           
                        ) is much smaller than the number of original features (
                           
                              
                                 
                                    s
                                 
                                 
                                    2
                                 
                              
                              ≪
                              n
                              )
                           
                        , the computational complexity of the proposed method can be reduced to 
                           
                              O
                              (
                              
                                 
                                    n
                                 
                                 
                                    2
                                 
                              
                              p
                              +
                              In
                              )
                           
                        .

@&#EXPERIMENTAL RESULTS@&#

In order to evaluate the proposed method several experiments were conducted in terms of the classification accuracy, the number of selected features and the execution time to obtain the final feature subset. Generally, the classification accuracy is used as a measure to evaluate the performance of the feature selection algorithms. This is due to the fact that the relevant features are usually not known in advance, and we cannot directly evaluate how good a feature selection algorithm is by the features selected. The classification accuracy is defined as the proportion of the total number of predictions that were correct. Moreover, for each dataset, the classification accuracy is obtained over ten independent runs to achieve relatively accurate and stable estimations. In each run, first of all, the normalized datasets were randomly split into a training set (2/3 of dataset) and a test set (1/3 of dataset). The training set was used to select the final feature subset while the test set was used to evaluate the selected features using a learning model. Then to attain fair results, all the methods will be performed on the same train/test partitions. According to the randomness of the datasets and also the randomness in the proposed method, we reported both the average and the standard deviation of the classification accuracy. The experiments have been run on a machine with a 3.2GHz CPU and 2GB of RAM. Moreover, the proposed method was compared to the well-known and state of the art filter-based feature selection methods which are listed below:
                        
                           
                              Laplacian Score (L-Score) 
                              [64] is a feature selection method that has been designed for serving both supervised and unsupervised learning. The basic idea behind L-Score is to evaluate the features according to their locality preserving power.


                              Fisher Score (F-Score) 
                              [41] is a supervised feature selection algorithm that seeks features with the best discrimination abilities. Moreover, similarly to the other filter-based methods, the F-Score selects some top high-ranked features that have maximum locality preserving power computed in terms of the Laplacian score.


                              Relevance–redundancy feature selection (RRFS) 
                              [4] is an efficient feature selection technique based on relevance and relevance/redundancy analyses, which uses a specific criterion to choose an adequate number of features. RRFS has log-linear time complexity, with respect to the number of features.


                              Minimal-redundancy–maximal-relevance (mRMR) 
                              [22] is a solid multivariate filter approach which returns a feature subset with features that are mutually far away from each other (minimizing the redundancy) as well as highly correlated with the classification label (maximizing the relevance).


                              ReliefF 
                              [49] is an extension of the Relief algorithm, which applies a feature weighting scheme and searches for several nearest neighbors. A key idea behind the original ReliefF algorithm is to estimate the quality of attributes according to their ability to distinguish between instances that are close to each other.


                              Unsupervised feature selection method based on ant colony optimization (UFSACO) 
                              [54] is an ACO-based method, proposed to find an optimal solution to the feature selection problem, which tries to minimize the redundancy of the selected features without using any learning models. The UFSACO can be classified as a filter-based multivariate method.

In addition, the proposed method was compared to state of the art wrapper-based feature selection methods which are listed below:
                        
                           
                              Hybrid genetic algorithm for feature selection (HGAFS) 
                              [38] is a method that integrates two quite different new techniques in genetic algorithm for feature selection problem. This method limits the number of selected features and the local search operation.


                              Ant colony optimization algorithm for feature selection (ACOFS) 
                              [37] is a hybrid ant colony optimization algorithm for feature selection. ACOFS uses a hybrid search technique that combines the advantages of wrapper and filter approaches.


                              Particle swarm optimization for feature selection (PSOFS) 
                              [6] is PSO-based feature selection method to selecting a smaller number of features and achieving similar or even better classification performance than using all features.

Moreover, the detailed descriptions of the datasets, employed classifiers, user-specified parameters, experimental results, and sensitivity analysis are discussed in the following subsections.

In this paper, several datasets with different properties were used in the experiments to show the effectiveness of the proposed method. These datasets include Wine, Hepatitis, WDBC, Ionosphere, Spambase, Sonar, Arrhythmia, Madelon, Colon, and Arcene. The basic characteristics of these ten datasets are summarized in Table 2
                        . The detailed descriptions of these datasets except Madelon and Arcene datasets are available in the University of California Irvine machine learning repository [5]. Moreover, Madelon and Arcene datasets are collected from NIPS2003 feature selection challenge, available at its website
                           1
                           
                              http://www.nipsfsc.ecs.soton.ac.uk/.
                        
                        
                           1
                         Some of these datasets contain attributes with missing values; therefore, to deal with these types of values in the experiments, each missing value was replaced with the mean of the available data on the respective feature [55]. Moreover, in many practical situations a designer is confronted with features whose values lie within different ranges. Thus, the features associated with large range values dominate those associated with small range values. To overcome this problem, a nonlinear normalization method called softmax scaling [55] is used to scale the datasets.

To show the generality of the proposed method, the classification prediction capability of the selected features was tested using several well-known classical classifiers, i.e., Support Vector Machine (SVM), Decision Tree (DT), Naïve Bayes (NB), k-Nearest Neighbor (kNN) and Random Forest (RF). These classifiers are the most influential algorithms that have been widely used in the data mining community. Moreover, the Weka (Waikato Environment for Knowledge Analysis) software Hall et al. [20], which is a collection of machine learning algorithms for data mining tasks, was used as a workbench in the experiments to evaluate the selected feature. In this work, SMO, J48 (implementation of the C4.5 algorithm), Naïve Bayes, IBk and RandomForest as WEKA implementation of SVM, DT, NB, kNN and RF were used, respectively. Furthermore, the parameters of the mentioned classifiers for each experiment were set to the default values of the Weka software.

There are several user-specified parameters used in the methods of the experiments and thus their corresponding values should be determined by the user. Note that some of these parameters are not specific to the proposed method, and are generally required for most of the ACO-based feature selection methods. These parameters were chosen after a number of preliminary runs, and were not meant to be optimal. Since 
                           
                              α
                           
                         and 
                           
                              β
                           
                         parameters in the ACO-based methods determine respectively the relative importance of the pheromone and the heuristic information, proper selection of these parameters is needed to achieve an effective balance between exploitation and exploration. Table 3
                         shows the parameters and their corresponding values.

@&#RESULTS@&#

In this subsection, we present the experimental results in terms of the classification accuracy, the number of selected features and the execution time. Also, for the purpose of exploring the statistical significance of the results, we performed a nonparametric Friedman test [18] to statistically compare different methods on multiple datasets.

In the experiments, first of all the performance of the proposed method was evaluated over different classifiers. Tables 4–8
                           
                           
                           
                           
                            show the average classification accuracy (in %) of the proposed method (i.e., GCACO) with ten independent runs over the SVM, DT, NB, kNN and RF classifiers, respectively. The best result for each dataset is shown in bold face and the numbers in the parentheses show the rank of the algorithms. Moreover, the results are compared with those of the filter methods including L-Score, F-Score, RRFS, mRMR, ReliefF and UFSACO.


                           Table 4 compares the classification accuracy of the proposed method with those of the other filter based methods over SVM classifier. From the results it can be observed that in most cases the GCACO obtained the highest classification accuracy compared to those of filter-based methods. For example, for the Colon dataset, GCACO obtained a 81.42% classification accuracy while for L-Score, F-Score, RRFS, mRMR, ReliefF and UFSACO this value was reported 67.14%, 69.52%, 71.91%, 73.34%, 70.48% and 72.86%, correspondingly. Moreover, compared to the original dataset (i.e., that all features), the average classification accuracy over all datasets using the SVM classifier improved by 5.17 (i.e., 82.26–77.09), 0.64 (i.e., 77.73–77.09), 0.51 (i.e., 77.60–77.09) and 1.56 (i.e., 78.65–77.09) percentage points, where GCACO, F-Score, RRFS, and UFSACO were respectively used for selecting the final feature subset. Unfortunately, in this case the average classification accuracy of L-Score, mRMR and ReliefF decreased by 1.38, 0.82 and 2.15 percentage points, respectively. Furthermore, the results show that the GCACO obtained 82.26 average classification accuracy and achieved the first rank with a margin of 3.61% compared to the UFSACO which obtained the second best average classification accuracy (i.e., 81.23).

Furthermore, Table 5 reports the results over DT classifier. It can be seen from Table 5 results that in most cases the GCACO obtained the highest classification accuracy compared to those of filter-based methods and acquired the second place only for the WDBC dataset. For example, for the Colon dataset, GCACO obtained an 80.00% (5.38) classification accuracy (standard deviation) while for L-Score, F-Score, RRFS, mRMR, ReliefF and UFSACO this value was reported 66.67% (5.03), 69.05% (6.05), 73.33% (5.11), 71.43% (4.49), 69.53% (6.05) and 71.91% (6.52), respectively. Moreover, Tables 6–8 reported similar results for the NB, kNN and RF classifiers, respectively. It can be seen from the results that the proposed methods achieved the best classification accuracy compared to the other methods. Consequently, it can be concluded from Tables 4–8 results that the proposed method obtained the best results for the datasets with large numbers of features. For example, from Table 8 results we can see that the differences between the obtained classification accuracy of the proposed method and that of the second best ones were reported 5.13 (i.e., 71.45–66.32) and 4.42 (i.e. 82.28–77.86) for the Arcene (10,000) and Colon (2000) datasets (number of features), respectively. Moreover, for the proposed method, the best result was reported for the NB classifier. The NB classifier supposes that the features are conditionally independent from each other. On the other hand, in univariate methods (i.e., L-Score, F-Score and ReliefF), each feature is considered separately, thereby ignoring feature dependencies. While in these datasets, there are redundancies between features and thus, univariate methods will simply fail in the case of NB classifier. However, the proposed GCACO method considers the dependency of the selected features and selects a subset of features with minimum redundancy between them, thus in this case the best results were reported for the NB classifier.

For the purpose of exploring the relationship between the feature selection methods and the classifiers, i.e., which method is more suitable for which classifier, seven feature selection methods is ranked according to their obtained classification accuracy for a given dataset and for a specific classifier as reported in Tables 4–8. To this end, the rank values of the feature selection methods for each dataset are provided in these tables. Moreover, the average ranks of each method over all the datasets are reported. The results show that the proposed GCACO method achieved the best rank among the other methods for all of the employed classifiers. For example the GCACO obtained the average 1.1 rank for the DT classifier while this value was reported 6.3, 4.1, 3.8, 3.7, 5.6 and 3.2 for L-Score, F-Score, RRFS, mRMR, ReliefF and UFSACO, respectively.


                           Table 9
                            shows the number of selected features of the different feature selection methods over ten independent runs. From the results it can be observed that, generally all the feature selection methods achieve significant reduction of dimensionality by selecting only a small portion of original features. Moreover, the results show that the GCACO, on average, selected the lowest number of features. For example from the results it can be seen that the proposed method selected the average number of 23.1 features, while, the other feature selection methods, on average selected 28.5 features.

Moreover, several experiments were conducted to compare the accuracy of the proposed method with the other feature selection methods based on the different numbers of selected features. Figs. 4 and 5
                           
                            plot the classification accuracy (average over 10 independent runs) curves of SVM and DT classifiers on Arrhythmia and Colon datasets, respectively. In these plots, the x-axis denotes the subset of selected features, while the y-axis is the average classification accuracy. Fig. 4(a) shows that the GCACO is superior to the other methods applied on the SVM classifier when the number of features is less than 40. Moreover it can be seen from the results that for higher numbers of features the proposed method obtained the second place after the F-Score method. In addition, Fig. 4(b) represents similar results when the GCACO was applied on the DT classifier.

Moreover, Fig. 5(a) illustrates that the performance of the proposed method is superior to the performances of all methods for different numbers of selected features when the SVM classifier and Colon dataset are used in the experiments. The results in Fig. 5(b) report similar results and demonstrate that the GCACO is significantly superior to all of the other methods. Especially, when 40 features were selected, the classification error rates were around 66%, 69%, 71%, 71%, and 80% for L-Score, F-Score, mRMR, UFSACO and GCACO, respectively.

The performance of the proposed method has been compared to those of the wrapper-based feature selection methods including; HGAFS [38], ACOFS [37] and PSOFS [65] on the different datasets. Table 10
                            reports the average classification accuracy over ten independent runs for HGAFS, ACOFS, PSOFS and GCACO methods using SVM and NB classifiers. It can be seen from the results that the proposed method obtained the highest classification accuracy while it was applied on the Wine, Hepatitis, Ionosphere, Spambase and Madelon datasets for the NB classifier. Moreover the GCACO method acquired the second best results for WDBC and Arcene datasets. While for the other cases the wrapper based methods achieved the best results compared to the proposed method. Consequently, it can be concluded from the reported results that the overall performance of the GCACO method is comparable with those of the state of the art wrapper feature selection methods.

The aim of this section is to compare the computational complexity and execution time of the aforementioned methods. Table 11
                            shows the computational complexity of filter and wrapper based feature selection methods. Moreover, descriptions of notations are also provided in the table. It should be noted that wrapper based feature selection methods require a classifier (i.e. a learning model) to evaluate the selected feature subset in each iteration. Up to now, these methods have been employed different classifiers such as DT, NN, kNN, SVM and RF. For example the HGAFS a specific NN classifier to calculate the fitness value of the particles. The computational complexities of these classifiers are different from each other. Therefore into report the computation complexity of the wrapper based methods (i.e. ACOFS, HGAFS and PSOFS) we referred to 
                              
                                 O
                                 (
                                 classifiere
                                 )
                              
                            in their corresponding computational complexity formula.

Furthermore, several experiments were performed in order to compare the execution time of the feature selection methods. In these experiments, the average execution times (in milliseconds) of filter and wrapper based methods are respectively reported in Tables 12 and 13
                           
                           . Form Table 12 results, it can be seen that the univariate feature selection methods (i.e., F-Score, and ReliefF) are much faster than the multivariate feature selection methods (i.e., mRMR, UFSACO and GCACO). This is due to the fact that in the univariate methods, the possible dependency between features is ignored in the feature selection process; thus, these methods can be computationally less expensive than the multivariate methods. Moreover, the reported results show that the proposed method is faster than other ACO-based feature selection method (i.e., UFSACO).

Additionally, the execution time of the proposed method has been also compared to those of the wrapper-based feature selection methods on the different datasets and the obtained results are reported in Table 13. The reported results show that in most cases the execution time of the proposed method is lower than the wrapper based methods. For example it can be seen from the results that the GCACO selected the final subset of features for Spambase dataset after 6846ms. While in this case the HGAFS, ACOFS and HGAFS selected the final subset after 98,328, 87,193 and 531,439ms respectively. In this case the results show the proposed method is nearly 88 times faster than the PSOFS method. While the results of Table 10 show that the accuracy of PSOFS method is only 0.5 times higher than GCACO method. Moreover, the results show that for very large datasets such as Arcene the execution time of the HGAFS and ACOFS methods are lower than the proposed method. This is due to the fact that these methods are executed in two steps. In the first step they applied a filter based method (such as Information gain or Gini-index) to rank the features independently and then a small size subset (i.e. a subset with only 100 top ranked features) are selected to be incorporated a wrapper based method in the second step. While in this case the proposed method is applied over all of the original features. On the other hand Table 10 results show that in this case the classification accuracy of the proposed method is much higher than the HGAFS and ACOFS methods.

Furthermore, from the reported results on classification accuracy (i.e. Tables 4–8 and Table 10) and also the execution time (i.e. Tables 12 and 13) of the filter and wrapper based methods, it can be concluded that the computational complexity and the quality of the selected feature subset are two main goals of the search methods. These goals are generally in conflict with each other and improving one of them causes the others to worsen. In other words, the filter-based feature selection methods have paid much attention to the computational time, while the wrapper feature selection methods usually consider the quality of the selected features. Therefore, a trade-off between these two issues has become an important and necessary goal to providing a good search method.

In the experiments, a specific significance test known as the Friedman test [18] was employed to further compare the mentioned feature selection methods on multiple datasets, with different classifier. The Friedman test can be used to compare 
                              
                                 k
                              
                            methods over 
                              
                                 N
                              
                            datasets by ranking each method on each dataset separately. The method that obtains the best performance gets the rank of 1, the second best ranks 2, and so on. We have used the SPSS statistics acquired by IBM [40] for this purpose. The reported results show that the Friedman test reported a p-value of 0.000008 for the classification accuracy values of the SVM classifier in Table 4; since this value is below 0.05, we can claim the statistical significance of the proposed method results. Moreover, the Friedman test reported 0, 0, 0.000003 and 0.000001 p-values for the other classifiers including DT, NB, kNN and RF, respectively. Since, these classifiers have p-values below 0.05 making these results statistically significant. Also, the Friedman test is also performed for the reported results of the wrapper based methods (i.e. Table 10). In this case the statistical test reported the p-values of 0.000026 and 0. 085801 for SVM and NB classifiers, respectively. In SVM classifier, the obtained p-value lower than 0.05, thus, it can be concluded that the achieved classification accuracy results over the SVM classifier are statistically significant. In contrast in the case of the NB classifier, the corresponding results are not statistically significant and it can be inferred that none of the feature selection methods performed consistently better than the others.

Furthermore, additional statistical tests are also conducted for other measures such as number of features and also the execution time. The conducted Friedman test on the reported execution time results (i.e. Table 12) achieved a p-value of 6.4036E−8≪0.05 and thus these results are statistically significant. The performed Friedman statistical test on the results of Table 9, reported a p-value of 0.423, thus it cannot be concluded the statistically significant of the results.

Like many other feature selection methods, the proposed method requires the 
                              
                                 ω
                              
                            and 
                              
                                 θ
                              
                            parameters. The 
                              
                                 ω
                              
                            is a user-specified parameter that controls the size of the final feature subset. Accurate setting of the 
                              
                                 ω
                              
                            parameter substantially influences the results of the GCACO method. This parameter can be set to any value in the range 
                              
                                 [
                                 1
                                 .
                                 .
                                 n
                                 ]
                              
                            and still 
                              
                                 ω
                                 ×
                                 k
                              
                            should be smaller than the number of original features (i.e., 
                              
                                 ω
                                 ×
                                 k
                                 ⩽
                                 n
                              
                           ). If its corresponding value is set to a high value, thus too many features will be selected and natural patterns in the data will be blurred by noise and the redundant features can be selected with a high probability. On the other hand when the parameter is set to a small value, too few features are chosen and thus, there will not be enough information for the classification task. Moreover, in order to explore the proper determination of the 
                              
                                 ω
                              
                            parameter values, several experiments were performed to reveal how the classification accuracy is changing with different values of the parameter. Fig. 6
                           (a) and (b) shows the 
                              
                                 ω
                              
                            parameter sensitivity analysis for SVM and DT classifiers respectively. The results compare the classifier accuracy on the Hepatitis, WDBC, Ionosphere, Spambase and Sonar datasets for different values of 
                              
                                 ω
                              
                            in the range of 
                              
                                 [
                                 2
                                 .
                                 .
                                 6
                                 ]
                              
                           . The obtained results indicated that in most cases, when the parameter was set to 4 (i.e., 
                              
                                 ω
                                 =
                                 4
                              
                           ), the proposed method obtained the best results.

Furthermore, in the second step (i.e., feature clustering) the edges with associated weights lower that the 
                              
                                 θ
                              
                            parameter are removed. The result of the feature clustering algorithm depends on the value of 
                              
                                 θ
                              
                           . This parameter can be set to any value in the range of [0 1]. When this value is set to a small value, more edges are considered in the graph clustering algorithm and the number of obtained clusters is low. On the other hand when the parameter is set to a high value, the graph clustering algorithm identifies more clusters. In this study several experiments were also performed to analyze the 
                              
                                 θ
                              
                            parameter. Table 14
                            shows the average SVM classification accuracy results of over 10 independent runs for different values of the 
                              
                                 θ
                              
                            parameter for the Hepatitis, WDBC, Ionosphere and Sonar datasets. Moreover, the number of obtained clusters is also reported in Table 14. The obtained results indicated that in most cases when the parameter was set to 0.6, the proposed method obtained the best results. It can be concluded from the results that when the parameter was set to a small value, the graph clustering algorithm identified a lower number of clusters. Therefore in this case the proposed method selects a smaller number of features and thus most representative features cannot be selected, which reduces the classifier accuracy.

@&#DISCUSSION@&#

This subsection briefly explains why the performance of the GCACO is better than those of the other feature selection methods. There are three differences that might contribute to better performance of GCACO compared to the other methods.
                           
                              1.
                              Irrelevant features, along with redundant features, severely affect the accuracy of the learning algorithm [8,10,34]. Thus, feature selection should be able to identify and remove as many of the irrelevant and redundant features as possible. Of the many feature selection methods, some can effectively remove irrelevant features but fail to handle the redundant features. In the univariate methods (i.e., L-Score, F-Score and RelifF) the relevance of a feature is measured individually and the possible dependency between features is ignored in the feature selection process; thus, these methods cannot remove the redundant features precisely. On the other hand, some of the multivariate feature selection methods only eliminate the redundant features. For example, the main goal of the UFSACO method is to select a subset of features with minimum redundancy and there is no guarantee to select the optimal feature set. This is due to the fact that the selected features may not constitute the best representative set although they are highly dissimilar to each other. To this end, we have developed a novel method which can efficiently and effectively deal with both irrelevant and redundant features. In the proposed method each ant selects the features with minimum similarity with those of the previously selected ones while it maximizes the dependency on the target class. By applying this kind of selection rules, the redundant and the irrelevant features have a lower probability to be selected.

One of the main shortcomings of existing univariate filter-based feature selection methods is ranking the features independently without considering their dependency on the other features. In this case some of the features with lower ranks may have strong discriminatory power while they are considered with the other features in a group. To overcome this problem, we focus on exploring a new framework to retain the useful structure among features as many as possible. In the proposed method, based on the probabilistic decision rule, to avoid being trapped into a local optimum, each feature has a chance of being selected corresponding to its probability value which is computed using Eq. (6). Also, the proposed updating rule takes into account all of the features based on their quality in the constructed feature subset. Pheromone updating rule is intended to allocate a greater amount of pheromone to better solution. Thus, individually weak and collectively strong features increase their selection probability.

The GCACO method integrates the graph clustering method with the search process of the ACO algorithm. Using the feature clustering method improves the performance of the proposed method in several aspects. First, the time complexity is reduced compared to those of the other ACO-based feature selection methods. This is due to the fact that the ant does not need to traverse a complete graph; thus, the probability computation in a clustered graph is reduced compared to that of the completed graph. Second, the search process is guided in such a way that at least one feature is selected per cluster, along the search process of each ant, and also relatively fewer correlated features are injected in a high proportion with respect to more correlated features to the consecutive iteration.

@&#CONCLUSION@&#

Feature selection plays an important role in the world of machine learning and more specifically in the classification task. Moreover, the computational cost is reduced and on the other hand, the model is constructed from the simplified data and this improves the general abilities of classifiers. In this paper, a novel feature selection method has been developed by integrating the concept of graph clustering with the search process of the ant colony optimization. The proposed method works in three steps: in the first step, the problem space is represented as a graph by considering the entire feature set as the vertex set and having feature similarity as the corresponding edge weight. In the second step, features are divided into several clusters by employing a community detection method. Finally, in the third step, we use a novel ACO-based feature selection algorithm that has been developed by utilizing the feature clusters. The proposed method can deal with both irrelevant and redundant features. This is because of the fact that each ant in the clustered graph tries to search for the features with minimum similarity and it maximizes the dependency on the target class. The proposed method has been compared to the six well-known and state-of-the-art filter-based feature selection methods including L-Score, F-Score, RRFS, mRMR, ReliefF and UFSACO and three state of the art wrapper based methods including HGAFS, ACOFS and PSOFS from the three different aspects of classification accuracy, size of subset selected features and execution time. The reported results show that in most cases the proposed method obtained the best classification accuracy. Furthermore, the results indicate that the execution time of the proposed method is comparable to those of feature selection methods. Moreover, the performed statistical test over three different measures including classification accuracy, number of selected features and execution time, show that the obtained results are statistically significant.

@&#REFERENCES@&#

