@&#MAIN-TITLE@&#Efficient multi-target tracking via discovering dense subgraphs

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A multi-target tracking is formulated as a dense subgraph discovering problem.


                        
                        
                           
                           Both local and global cues are exploited to represent the tracklet affinity model.


                        
                        
                           
                           The distinguishable appearance based models are learned for the targets.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Multi-target tracking

Dense subgraphs

Relation affinity graph

Discriminative target appearance model

@&#ABSTRACT@&#


               
               
                  In this paper, we cast multi-target tracking as a dense subgraph discovering problem on the undirected relation graph of all given target hypotheses. We aim to extract multiple clusters (dense subgraphs), in which each cluster contains a set of hypotheses of one particular target. In the presence of occlusion or similar moving targets or when there is no reliable evidence for the target’s presence, each target trajectory is expected to be fragmented into multiple tracklets. The proposed tracking framework can efficiently link such fragmented target trajectories to build a longer trajectory specifying the true states of the target. In particular, a discriminative scheme is devised via learning the targets’ appearance models. Moreover, the smoothness characteristic of the target trajectory is utilised by suggesting a smoothness tracklet affinity model to increase the power of the proposed tracker to produce persistent target trajectories revealing different targets’ moving paths. The performance of the proposed approach has been extensively evaluated on challenging public datasets and also in the context of team sports (e.g. soccer, AFL), where team players tend to exhibit quick and unpredictable movements. Systematic experimental results conducted on a large set of sequences show that the proposed approach performs better than the state-of-the-art trackers, in particular, when dealing with occlusion and fragmented target trajectory.
               
            

@&#INTRODUCTION@&#

Multi-target tracking in real world scenarios is a crucial problem for many computer vision tasks. Some of its potential applications include anomaly detection, visual surveillance, human–computer interaction and sports analysis.

Generally speaking, a multi-target tracking framework takes a set of target detections in each frame from any pre-trained detector as its input and aims to recover the trajectories of all targets as well as maintaining their consistent identities. However, the problem and its difficulty depend on conflicting challenges, such as occlusion, drastically varying illumination and viewpoint for the target during the tracking, and therefore its trajectory is expected to be fragmented into multiple tracklets across video sequence. In addition, recovering the true target trajectories is an ambiguous problem when several visually similar targets move closely together. Apart from these challenges, the detector cannot be assumed to be accurate and may produce false negatives (missed targets) or extra detections. In such scenarios, revealing each individual’s identity results in an extreme complexity in the number of tracks and measurements, and thus quickly becomes infeasible.

Recently, tracking-by-detection methods [1,2] have attracted much attention due to the promising improvements on object detection. They usually seek proper association between the target hypotheses by constructing their mutual similarities (affinities) based on multiple cues, such as motion similarity.

In the proposed framework, multi-target tracking is formulated as a dense subgraph extraction problem, in which each recovered dense subgraph specifies the trajectory of the one particular target during the tracking. Here, the higher order correspondences between the observations (targets’ hypotheses across video sequence) are considered. With respect to the literature, this approach has the following advantages:

                        
                           1.
                           An iterative strategy is devised to partition the spatio-temporal graph of target hypotheses into different subgraphs. Each reveals a different target trajectory. At each iteration, the most coherent vertices of the constructed spatio-temporal graph in a short time period of the video will be added to the existing subgraphs (tracklets) until reaching the densest subgraphs, which are more likely to be the true trajectories of the targets. Since the proposed method works on small graph partitions (in the few video frames), it is much more efficient in terms of computational expense.

In the presence of occlusion of the target, whether caused by another target or visual obstacles where the target trajectories are only partially observable due to a failure of the pre-trained detector, a discriminative scheme is devised via learning the targets’ appearance models. In addition, target hypotheses are sparse in time (false negative), which prevents obtaining good target motion estimates. Therefore, the smoothness characteristic of the target trajectory is utilised by suggesting a smoothness tracklet affinity model to increase the power of the proposed tracker to associate the fragmented target tracklets to produce persistent target trajectories revealing different targets’ moving paths (see Fig. 1
                              ).

@&#RELATED WORK@&#

Most of the recent approaches to tracking which pursue a tracking-by-detection strategy, comprises two steps: (i) obtaining a set of independent target candidates in each frame (detection) and (ii) assigning the target identities to these candidates (data association). Although there are some related work [3] which address both problems (object detection and data association) via proposing these two steps in a single framework, but the majority of state-of-the-art methods considered these two problems separately and the priority is usually given to the data association design. Therefore, the related work in this tracking step is extensively described here:

These methods consider correspondence between the target observations (detections’ responses) in the temporally local frames which yields the polynomial complexity. The best example of such approaches are Bi-partite matching methods [4,5]. Brendel et al. [5] presented an approach to formulate data association problem as finding the Maximum-Weight Independent Set (MWIS) of the graph of detections’ responses. Shu et al. [4] suggested part-based model to obtain human part detections, which is robust against partial occlusion and appearance changes. Some methods devise a greedy strategy for the data association by solving a series of Bi-partite assignment problems to assign an evolving set of target trajectories to the target hypotheses in each video frame. Shafique and Shah [6] proposed a k-partite complete graph where the connections in the graph are not limited to only two consecutive frames. Bae and Yoon [7] proposed an online multi-object tracking system with a new data association technique considering the track existence probability. Although this class of methods are computationally efficient, but they fail in tracking visually similar moving targets or when there exists heavy occlusion or pose variations, due to their limited-temporal-locality strategies.

Unlike local association based methods, recently, another category of data association based techniques (namely global association) have been proposed which are by far, the most explored strategy for multi-target tracking. The global association approaches operate on the batch of frames and sometimes the whole temporal sequence at once [8,9]. These approaches formulate the higher order relationships between different target observations (detection responses).

Multi-object tracking has been recently formulated as a network flow problem where a set of object tracks are resolved simultaneously by solving min-cost flow techniques. Different approaches to minimum cost flow have been presented recently. Zhang et al. [10] proposed a global optimal solution for the network flow optimisation using push-relabel algorithm. Pirsiavash et al. [11] utilise the same graph as in [10], and use a fast greedy shortest path algorithm for the tracking problem. Berclaz et al. [12] suggested a globally tracking by relying on the k-shortest paths (KSP) algorithm to solve the flow problem. To handle the difficulties in object localisation caused by occlusion and clutter in the video, recently Chari et al. [13] proposed to add pairwise costs to the min-cost network flow framework where a convex relaxation strategy with an efficient rounding heuristic is designed to solve the problem. The main drawback of these approaches is that, they do not consider acceleration information for computing trajectories which is required for trajectories’ smoothness in spatio-temporal domain.

Work by Schindler [14] and Milan et al. [15] proposed multi-target tracking in crowded scenarios by incorporating mutual exclusion between the targets at both: trajectory estimation where any two trajectories should remain spatially separated as well as the data association step, in which each trajectory should be assigned at most one detections per frame. However, for this purpose, they employed an already-performed tracking from [11] as an input to their approach. Hence, these work should rather be considered as trajectory-refinement procedure.

Recently, graph partitioning based methods have been proposed for multi-target tracking problem. Here, given a set of target hypotheses produced by a generic object detector, the aim is to build a graph of all target hypotheses and to devise a strategy to associate them across the time. Kumar et al. [16] formulated multiple object tracking as a graph partitioning problem, in which the sum of weighted edges connecting vertices with the same label must be maximised. Then, a Conditional Random Field (CRF) is defined and optimised using an efficient combination of message passing and move making algorithms.

The work of Zamir et al. [9] and Wen et al. [17] are most relevant to the proposed approach. Zamir et al. [9] proposed a sequential Generalised Minimum Clique Problem (GMCP) where the computed cliques on a graph of detection responses specify the target trajectories. For handling the occlusion, hypothetical nodes were devised for the missing targets assuming constant velocity in a short period of time. In addition, similar to the proposed framework, the input video sequence is divided into a number of segments and the final trajectory of each person obtained by stitching together the obtained target tracklets in each segment.

However, the proposed approach differs in two aspects. First, unlike their work that focus on one target at a time, rather than dealing with all targets simultaneously, here, all targets are considered jointly in the presented framework. Second, the proposed framework is more workable in cases where there are fragmented trajectories and the targets only are observable in a short temporal span. We extend the dense subgraph extraction algorithm [18] to formulate our proposed tracking framework in an efficient way in which the number of initialisations of the algorithm has been significantly reduced due to the number of constructed initial target tracklets while not sacrificing for the tracker’s accuracy. In addition, we propose a new graph density by defining several affinity models based on different cues between the tracklets which promises a high probability of obtaining all significant dense subgraphs (potential tracklets). Similar to the proposed contribution, Wen et al. [17] formulated the multi-target tracking as a hierarchical dense neighbourhood searching problem on the multiple relation affinity hypergraphs. However, they consider each target tracklet (graph node) as an initialisation for the optimisation, which increases the results complexity. In contrast, the proposed framework starts with the initial tracklets (less reliable tracklets) and iteratively grows and condenses the tracklets to obtain more reliable tracklets, which are more likely to be true trajectories of the targets.

Briefly, the contributions of this paper are summarised as below:

                           
                              1.
                              In this paper, multi-target tracking is formulated as an efficient subgraph extraction problem, which considers all the relations, e.g. motion similarity between any pair of target tracklets apart from their closeness in the temporal domain. Different from constrained-temporal-locality methods, which are not well suited to consider the target observations outside of the temporal neighbourhood they are focused on, this approach processes much longer temporal segments to link and cluster target hypotheses jointly across space and time. An iterative two-step tracking scheme is proposed as shown in Fig. 2. It operates on the initial target tracklets (subgraphs). First in the trajectory growth step, the most correlated vertices to the current subgraph are chosen and added to grow the target tracklets. Further, in each subgraph, the uncorrelated vertices, which are more likely to be false positive or belong to the occluded targets, are penalised, allowing for subgraph condensation. Therefore, multiple dense subgraphs
                                    1
                                 
                                 
                                    1
                                    Points to those subgraphs where the affinities between their vertices are large.
                                  robustly revealed. The proposed framework yields a better formulation of the data association problem, which is shown to lead to superior results.

In addition, to handle false positive detections and the difficulties caused by the presence of interacting targets which confuse association between the target hypotheses, a target-specific metric learning model is proposed to obtain effective appearance cues for reliable association between different tracklets. For this purpose, a discriminative projection space is learned effectively where the appearance features of tracklets are projected onto a low-dimensional subspace to lead to more distinguishable target representations, while keeping the computational complexity low. Moreover, to address the specific problems caused by unreliable detection responses or occlusion, the target trajectories’ smoothness is exploited via proposing the tracklet smoothness affinity model to take advantage of point trajectories. Meanwhile, in order to penalise an abrupt gap between the tracklets belonging to the same target, motion similarities between the tracklets are utilised to enforce merging them through (e.g. occlusion).

Given a set of target object hypotheses (target states) O for the image sequences , where the state of target object j at frame t is given as 
                        
                           
                              O
                              
                                 t
                              
                              j
                           
                           =
                           
                              {
                              
                                 p
                                 
                                    t
                                 
                                 j
                              
                              ,
                              
                                 s
                                 
                                    t
                                 
                                 j
                              
                              ,
                              
                                 v
                                 
                                    t
                                 
                                 j
                              
                              }
                           
                        
                      if an object j appears at frame t (
                        
                           
                              b
                              
                                 t
                              
                              j
                           
                           =
                           1
                        
                     ) and 
                        
                           
                              p
                              
                                 t
                              
                              j
                           
                           ,
                        
                     
                     
                        
                           s
                           
                              t
                           
                           j
                        
                      and 
                        
                           v
                           
                              t
                           
                           j
                        
                      are the position, scale and velocity, respectively
                        2
                     
                     
                        2
                        The presence of the j-th target object is denoted via the binary function 
                              
                                 b
                                 
                                    t
                                 
                                 j
                              
                           .
                     . The aim is to find the most likely set 
                        
                           T
                           =
                           {
                           
                              τ
                              1
                           
                           ,
                           
                              τ
                              2
                           
                           ,
                           …
                           ,
                           
                              τ
                              N
                           
                           }
                        
                      of the target object tracklets. The tracklet τi
                      of the i-th target object is defined as a set of states up to frame t, and is represented as 
                        
                           
                              τ
                              i
                           
                           =
                           
                              {
                              
                                 O
                                 
                                    
                                       t
                                       
                                          s
                                       
                                       i
                                    
                                    :
                                    
                                       t
                                       
                                          e
                                       
                                       i
                                    
                                 
                                 i
                              
                              ∣
                              
                                 b
                                 
                                    
                                       t
                                       
                                          s
                                       
                                       i
                                    
                                    :
                                    
                                       t
                                       
                                          e
                                       
                                       i
                                    
                                 
                                 i
                              
                              =
                              1
                              ,
                              1
                              ≤
                              
                                 t
                                 
                                    s
                                 
                                 i
                              
                              ≤
                              
                                 t
                                 
                                    e
                                 
                                 i
                              
                              ≤
                              t
                              }
                           
                           ,
                        
                      which actually is a temporal sequence of i-th target detection responses starting at time stamp 
                        
                           t
                           
                              s
                           
                           i
                        
                      and ending at the end frame 
                        
                           t
                           
                              e
                           
                           i
                        
                     . Building the spatio-temporal hypergraph 
                        
                           G
                           =
                           (
                           V
                           ,
                           E
                           )
                        
                      for the current video segment, whose vertices 
                        
                           V
                           =
                           {
                           
                              v
                              1
                           
                           ,
                           …
                           ,
                           
                              v
                              m
                           
                           }
                        
                      correspond to target tracklets and the edges E include more than just two vertices as 
                        
                           E
                           ⊂
                           
                              
                                 
                                    V
                                    ×
                                    ⋯
                                    ×
                                    V
                                 
                                 ︷
                              
                              l
                           
                        
                     
                     
                        3
                     
                     
                        3
                        
                           l is the number of graph vertices involved in each hyperedge.
                     , the goal is to partition the spatio-temporal graph into dense subgraphs such that each subgraph corresponds to the trajectory of one person. The term ‘dense’ refers to the most correlated set of vertices of the spatio-temporal graph, which is more likely to be a consistent target trajectory. The notations used in this paper are presented in Table 1.

In the proposed approach, the tracking task is formulated as an iterative algorithm starting from the initial tracklets obtained by the initialisation and then adding the most similar graph vertices based on the average affinity between each vertex and the current tracklet. In continue, considering relationships between all target tracklets within the current video segment, the less coherent tracklets are excluded, which are more likely to be fragmented or belong to the different targets. These two steps are iterated until convergence is reached, when the final target tracklets are revealed from the constructed relation affinity graph among the tracklets. For this purpose, the input video is divided into a few temporal segments and a set of target tracklets is produced within each segment using the proposed iterative method. Then, the tracklets obtained in all segments are stitched together to obtain the final trajectory of each target by carrying out the same framework (dense subgraph extraction) again for the entire video time span (see Fig. 3
                     
                     ).

To evaluate the similarity of any two tracklets, local cues such as dense motion trajectories and global cues such as appearance features, as well as intermediate ones such as motion smoothness are considered. The proposed method matches the tracklets of one target across the full video duration, while incorporating the remaining targets using the 
                        
                           
                              
                                 m
                                 ×
                                 ⋯
                                 ×
                                 m
                              
                              ︷
                           
                           l
                        
                      symmetric affinity array A, exhibits the mutual similarities between the tracklets. Mathematically, A(τi, τj
                     ) is used to denote the probability of τi
                      and τj
                      belonging to the same target, which is computed as follows:

                        
                           (1)
                           
                              
                                 A
                                 
                                    (
                                    
                                       τ
                                       i
                                    
                                    ,
                                    
                                       τ
                                       j
                                    
                                    )
                                 
                                 =
                                 
                                    Λ
                                    S
                                 
                                 
                                    (
                                    
                                       τ
                                       i
                                    
                                    ,
                                    
                                       τ
                                       j
                                    
                                    )
                                 
                                 
                                    Λ
                                    M
                                 
                                 
                                    (
                                    
                                       τ
                                       i
                                    
                                    ,
                                    
                                       τ
                                       j
                                    
                                    )
                                 
                                 
                                    Λ
                                    A
                                 
                                 
                                    (
                                    
                                       τ
                                       i
                                    
                                    ,
                                    
                                       τ
                                       j
                                    
                                    )
                                 
                              
                           
                        
                     where ΛS
                     (τi, τj
                     ), ΛM
                     (τi, τj
                     ) and ΛA
                     (τi, τj
                     ) are the tracklet smoothness affinity, tracklet motion affinity and the tracklet appearance affinity, respectively (see Section 4). Considering the spatio-temporal constraint, if the two tracklets have overlap, they should not correspond to the same target. Thus, their mutual affinity
                        4
                     
                     
                        4
                        The affinity value lies in [0, 1].
                      is set to zero as 
                        
                           A
                           (
                           
                              τ
                              i
                           
                           ,
                           
                              τ
                              j
                           
                           )
                           =
                           0
                        
                     . The multiplicative formulation for the tracklets’ affinity array is more effective than the alternative additive scheme in the proposed tracking framework. For example, the appearance affinity value ΛA
                      gives higher scores to the distinguishable tracklets, which are discriminated easily from the background or other targets. Thus, it can be considered as an importance weight for other two tracklet affinity models. Moreover, the appearance affinity value of the interacting targets, when they are closely moving and/or partially occluding each other, is close to 1 and it has no effect on the total tracklet affinity function.

Considering the index set of all graph vertices 
                           
                              I
                              =
                              {
                              1
                              ,
                              …
                              ,
                              m
                              }
                              ,
                           
                         taking any subset S⊆I into account, a subgraph GS
                         of G with the vertex set 
                           
                              
                                 V
                                 S
                              
                              =
                              
                                 {
                                 
                                    v
                                    i
                                 
                                 ∣
                                 i
                                 ∈
                                 S
                                 }
                              
                           
                         is denoted. Given a relation affinity graph on the vertices, the aim is to produce the dense subgraphs (partitions of the graph vertices) that agree as much as possible to constitute the high average affinity in the form of the following quadratic optimisation problem:
                           5
                        
                        
                           5
                           
                              y is relaxed into the continuous space Δ.
                        
                        
                           
                              (2)
                              
                                 
                                    
                                       max
                                       y
                                    
                                    f
                                    
                                       (
                                       y
                                       )
                                    
                                    =
                                    
                                       y
                                       T
                                    
                                    A
                                    y
                                    ,
                                    
                                    s
                                    .
                                    t
                                    .
                                    
                                    y
                                    ∈
                                    Δ
                                 
                              
                           
                        where y represents a set of vertices by a probabilistic subgraph (target tracklet), which is a unit vector in the space of standard simplex as 
                           
                              Δ
                              =
                              {
                              y
                              ∈
                              
                                 R
                                 m
                              
                              :
                              y
                              ≥
                              0
                              ,
                              
                                 ∑
                                 
                                    i
                                    =
                                    1
                                 
                                 m
                              
                              |
                              
                                 y
                                 i
                              
                              |
                              =
                              1
                              }
                           
                        .

To this end, considering a set of all subgraphs 
                           G
                         in the graph G and a simplex projection as 
                           
                              F
                              :
                              G
                              →
                              Δ
                              ,
                           
                         a mapping vector over the subgraph GS
                         is defined as 
                           
                              F
                              (
                              
                                 G
                                 S
                              
                              )
                              =
                              y
                           
                         such that 
                           
                              
                                 y
                                 i
                              
                              =
                              
                                 1
                                 n
                              
                           
                         if the i-th vertex is included in a probabilistic subgraph GS
                         and 
                           
                              
                                 v
                                 i
                              
                              =
                              0
                           
                         otherwise. n is the number of vertices in the subgraph GS
                         and 
                           
                              F
                              (
                              G
                              )
                           
                         is denoted as 
                           
                              Δ
                              G
                           
                         where 
                           
                              
                                 Δ
                                 G
                              
                              =
                              
                                 {
                                 y
                                 ∈
                                 Δ
                                 ∣
                                 ∃
                                 n
                                 >
                                 0
                                 
                                 ∀
                                 i
                                 =
                                 1
                                 ,
                                 …
                                 ,
                                 m
                                 
                                 
                                    y
                                    i
                                 
                                 ∈
                                 
                                    {
                                    0
                                    ,
                                    
                                       1
                                       n
                                    
                                    }
                                 
                                 }
                              
                           
                        . The indices of all nonzero elements of y ∈ Δ comprise its compactness, represented as 
                           
                              ϕ
                              
                                 (
                                 y
                                 )
                              
                              =
                              
                                 {
                                 i
                                 ∣
                                 
                                    y
                                    i
                                 
                                 ≠
                                 0
                                 }
                              
                           
                        . Here, if the local maximiser y
                        * of Eq. (2) belongs to the space of simplex 
                           
                              
                                 Δ
                                 G
                              
                              ,
                           
                         then the optimal dense subgraph is recovered as 
                           
                              G
                              
                                 ϕ
                                 (
                                 
                                    y
                                    *
                                 
                                 )
                              
                           
                        ; Otherwise, a greedy strategy is proposed to approximate the optimal solution via considering the subgraph’s dense neighbourhood (see Algorithm in Fig. 4
                        ). The suggested iterative algorithm consists of two iterative steps, including condensation and trajectory growth, as follows:


                        
                           Condensation step.
                         Given the initialised tracklets (subgraphs), the well-known algorithm, replicator dynamic 
                        [19] is utilised to efficiently solve the optimisation problem of Eq. (2) on the small subgraph GS
                         ⊆ G as:

                           
                              (3)
                              
                                 
                                    
                                       
                                          (
                                          
                                             y
                                             S
                                          
                                          )
                                       
                                       i
                                    
                                    
                                       (
                                       t
                                       +
                                       1
                                       )
                                    
                                    =
                                    
                                       
                                          (
                                          
                                             y
                                             S
                                          
                                          )
                                       
                                       i
                                    
                                    
                                       (
                                       t
                                       )
                                    
                                    
                                       
                                          
                                             (
                                             
                                                A
                                                S
                                             
                                             
                                                y
                                                S
                                             
                                             
                                                (
                                                t
                                                )
                                             
                                             )
                                          
                                          i
                                       
                                       
                                          
                                             y
                                             S
                                          
                                          
                                             
                                                (
                                                t
                                                )
                                             
                                             T
                                          
                                          
                                             A
                                             S
                                          
                                          
                                             y
                                             S
                                          
                                          
                                             (
                                             t
                                             )
                                          
                                       
                                    
                                    
                                    i
                                    ∈
                                    S
                                 
                              
                           
                        where yS
                         and AS
                         are the probabilistic indicator vector and the affinity matrix of the subgraph GS
                        , respectively. Here, the aim is to condense the current subgraph to form a denser subgraph. However, the probabilistic cluster of vertices 
                           
                              y
                              
                                 S
                              
                              *
                           
                         obtained by Eq. (3) may not indicate an optimal solution y
                        * of the graph G in the current video segment.


                        
                           Trajectory growth.
                         In this step, the aim is to add the most relevant dense neighbours of the current subgraph based on the affinity value between the candidate dense neighbour vertex and the current subgraph. In fact, this affinity value indicates the confidence score of the neighbourhood vertices to measure their compactness with respect to other vertices within the current cluster (subgraph).


                        
                           Theorem.
                         According to Liu et al. [18], the reward for those neighbourhood vertices not associated to the current subgraph GS
                        , is not bigger than f(y
                        *) in Eq. (2). Therefore, the corresponding target tracklet is grown into its similar dense neighbours with the larger confidence scores to the current tracklet (subgraph) such as: {k∣Score(y*, vk
                        ) > f(y*), k ∈ I}. Thus, the aim is to find the update vector Δy in which 
                           
                              
                                 y
                                 
                                    n
                                    e
                                    w
                                 
                              
                              =
                              
                                 y
                                 
                                    o
                                    l
                                    d
                                 
                              
                              +
                              Δ
                              y
                           
                        . The update vector Δy was efficiently computed as in [18] (For more details, please see [18]). By this way, the densest subgraphs are gradually revealed from the initial subgraphs. First, the initial subgraphs merge to a more compact subgraph, and then expand to its neighbours. These two steps iterate until the final target tracklets are discovered in each video segment.

The target trajectories should be continuous and smooth in the spatio-temporal domain. For example, when a significant part of the target trajectory is occluded due to severe occlusion or extreme target object deformations, it is difficult to judge whether the tracklets (partial trajectories) extracted by the tracker within the video segment correspond to the unique target trajectory or should be considered as the fragmented tracklets, which belong to other targets. Therefore, reliable tracklets are needed, which are smooth in the spatio-temporal domain and more likely being associated with the same target. Here, similar to [20], dense point trajectories are utilised, which can be extended to those frames without any detection responses and properly distinguish targets under heavy occlusion caused by the background or other interacting targets. The goal is to find the common dense point trajectories overlapping between the set of bounding boxes of the detections within the tracklets Tr(τi, τj
                        ) to build their mutual affinities ΛS
                        (τi, τj
                        ) as shown in Fig. 5 . Thus, the smooth affinity between their frames closest in time (t
                        1, t
                        2) is computed as:

                           
                              (4)
                              
                                 
                                    
                                       Λ
                                       S
                                    
                                    
                                       (
                                       
                                          τ
                                          i
                                       
                                       ,
                                       
                                          τ
                                          j
                                       
                                       )
                                    
                                    =
                                    e
                                    x
                                    p
                                    
                                       (
                                       −
                                       
                                          ∑
                                          
                                             o
                                             ∈
                                             T
                                             r
                                             
                                                (
                                                
                                                   τ
                                                   i
                                                
                                                ,
                                                
                                                   τ
                                                   j
                                                
                                                )
                                             
                                          
                                       
                                       
                                          
                                             
                                                |
                                                l
                                                o
                                                c
                                                
                                                   (
                                                   t
                                                   
                                                      r
                                                      o
                                                   
                                                   ,
                                                   
                                                      D
                                                      
                                                         i
                                                      
                                                      
                                                         t
                                                         1
                                                      
                                                   
                                                   )
                                                
                                                −
                                                l
                                                o
                                                c
                                                
                                                   (
                                                   t
                                                   
                                                      r
                                                      o
                                                   
                                                   ,
                                                   
                                                      D
                                                      
                                                         j
                                                      
                                                      
                                                         t
                                                         2
                                                      
                                                   
                                                   )
                                                
                                                |
                                             
                                             2
                                          
                                          
                                             σ
                                             
                                                l
                                             
                                             2
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        
                     

where 
                           
                              D
                              
                                 i
                              
                              t
                           
                         is the detection response at frame t for the tracklet τi
                        , 
                           
                              l
                              o
                              c
                              (
                              t
                              
                                 r
                                 o
                              
                              ,
                              
                                 D
                                 
                                    i
                                 
                                 t
                              
                              )
                           
                         specifies the Euclidean distance between the dense (point) trajectory tro
                         and the centre of detection bounding box Di
                         at frame t. The parameter 
                           
                              σ
                              
                                 l
                              
                              2
                           
                         controls the sensitivity of the smoothness affinity value to the abrupt target motion change
                           6
                        
                        
                           6
                           Tracklet smoothness affinity value is set to 
                                 
                                    
                                       σ
                                       
                                          l
                                       
                                       2
                                    
                                    =
                                    4
                                    ,
                                 
                               experimentally.
                        . The affinity score for the two tracklets is higher when their common dense trajectories are able to keep up with their detection responses. 
                           
                              
                                 A
                                 S
                              
                              
                                 (
                                 
                                    τ
                                    i
                                 
                                 ,
                                 
                                    τ
                                    j
                                 
                                 )
                              
                              =
                              0
                              ,
                           
                         if there are no common dense point trajectories between two tracklets, denoting the absence of effective information to measure their mutual affinities. Thus, the proposed affinity links those sparse target detections through their gaps, which are more likely consistent in the spatio-temporal domain.

The motion similarity between any two tracklets ΛM
                        (τi, τj
                        ) is computed similar to [1], which is based on the relative distance between the predicted positions with the linear motion model and the real positions. Considering positions of the two tracklets 
                           
                              P
                              
                                 j
                              
                              
                                 h
                                 e
                                 a
                                 d
                              
                           
                         and 
                           
                              P
                              
                                 i
                              
                              
                                 t
                                 a
                                 i
                                 l
                              
                           
                         with the frame gap Δt between them, the relative distances 
                           
                              Δ
                              
                                 P
                                 i
                              
                              =
                              
                                 (
                                 
                                    P
                                    
                                       j
                                    
                                    
                                       h
                                       e
                                       a
                                       d
                                    
                                 
                                 +
                                 
                                    v
                                    
                                       j
                                    
                                    B
                                 
                                 Δ
                                 t
                                 )
                              
                              −
                              
                                 P
                                 
                                    i
                                 
                                 
                                    t
                                    a
                                    i
                                    l
                                 
                              
                           
                         and 
                           
                              Δ
                              
                                 P
                                 j
                              
                              =
                              
                                 (
                                 
                                    P
                                    
                                       i
                                    
                                    
                                       t
                                       a
                                       i
                                       l
                                    
                                 
                                 +
                                 
                                    v
                                    
                                       i
                                    
                                    F
                                 
                                 Δ
                                 t
                                 )
                              
                              −
                              
                                 P
                                 
                                    j
                                 
                                 
                                    h
                                    e
                                    a
                                    d
                                 
                              
                           
                         are considered, in which the forward velocity 
                           
                              v
                              
                                 i
                              
                              F
                           
                         is evaluated from the head to the tail of tracklet i, while the backward velocity 
                           
                              v
                              
                                 j
                              
                              B
                           
                         is evaluated from the tail to the head of tracklet j (see Fig. 6
                        )
                           7
                        
                        
                           7
                           The positions and velocities along with x and y coordinates are predicted and updated in the Kalman filtering process.
                        . Finally, the motion affinity is measured with the estimated forward and backward motions of tracklets as:

                           
                              (5)
                              
                                 
                                    
                                       Λ
                                       M
                                    
                                    
                                       (
                                       
                                          τ
                                          i
                                       
                                       ,
                                       
                                          τ
                                          j
                                       
                                       )
                                    
                                    =
                                    N
                                    
                                       (
                                       Δ
                                       
                                          P
                                          i
                                       
                                       ,
                                       Σ
                                       )
                                    
                                    N
                                    
                                       (
                                       Δ
                                       
                                          P
                                          j
                                       
                                       ,
                                       Σ
                                       )
                                    
                                 
                              
                           
                        where N( ·, Σ) is a zero-mean Gaussian function
                           8
                        
                        
                           8
                           The covariance for forward and backward motions is set to 
                                 
                                    Σ
                                    =
                                    d
                                    i
                                    a
                                    g
                                    [
                                    
                                       25
                                       2
                                    
                                    
                                    
                                       75
                                       2
                                    
                                    ]
                                 
                               and fixed for all experiments.
                        . Then, consistency along a target trajectory is considered by assuming linear motion between the closest detected bounding boxes on either side of the temporal axis.

The appearance model of the target object is a critical part for tracking. A reliable appearance model needs to establish a good balance between being adaptive, to account for appearance change caused by (e.g. pose changes), and being moderate in the case of partial occlusion, where we need to keep track of the target after being lost during tracking. Here, the goal is to learn the appearance model for the target object, which effectively makes it distinguishable not only from the background but also from other interacting targets. In order to learn the discriminative appearance models, a metric learning problem is defined to project high-dimensional tracklet features onto the learned low dimensional subspace in a way that each target tracklet is distinguishable from other tracklets.

Unlike the previous approaches [7], in which ensemble learning was utilised to combine a number of weak classifiers via boosting to learn a decision boundary for distinguishing each target object from the background or other target object, a computationally efficient metric function is designed to compute the appearance affinities between the tracklets. First, the training samples (high confidence detection responses) are collected from the initial tracklets. For each target candidate (detection response), an appearance descriptor obtained by concatenating feature histograms of the HSV colour and the Histogram of Oriented Gradient (HOG) to take advantage of both colour and shape information.

Given the training samples 
                           
                              
                                 
                                    {
                                    
                                       (
                                       
                                          f
                                          i
                                       
                                       )
                                    
                                    ,
                                    
                                       c
                                       i
                                    
                                    }
                                 
                                 
                                    i
                                    =
                                    1
                                 
                                 T
                              
                              ,
                           
                         where fi
                         is the feature vector belonging to the i-th tracklet and ci
                         is its corresponding tracklet label, learning the target appearance model is formulated as a distance metric function, which can enhance discrimination between feature vectors by ensuring that distances are smaller when features are extracted within a tracklet of the same target, and larger otherwise. The distance function is modelled using the Mahalanobis distance as follows:

                           
                              (6)
                              
                                 
                                    d
                                    
                                       (
                                       
                                          f
                                          i
                                       
                                       ,
                                       
                                          f
                                          j
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          (
                                          
                                             f
                                             i
                                          
                                          −
                                          
                                             f
                                             j
                                          
                                          )
                                       
                                       T
                                    
                                    M
                                    
                                       (
                                       
                                          f
                                          i
                                       
                                       −
                                       
                                          f
                                          j
                                       
                                       )
                                    
                                 
                              
                           
                        where M is the parameter of the Mahalanobis distance, which can be efficiently learned using an adaptation of the Large Margin Nearest Neighbour (LMNN) framework [21]. In fact, by solving the parameter M, as a decomposition problem 
                           
                              M
                              =
                              L
                              
                                 L
                                 T
                              
                              ,
                           
                         a mapping L is learned to transform feature vectors into the new space as 
                           
                              
                                 f
                                 ^
                              
                              =
                              L
                              .
                              f
                           
                        . To compute the affinity score between the two tracklets, the relative distance between their learned discriminative projection is calculated as:

                           
                              (7)
                              
                                 
                                    
                                       Λ
                                       A
                                    
                                    
                                       (
                                       
                                          τ
                                          i
                                       
                                       ,
                                       
                                          τ
                                          j
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             L
                                             T
                                          
                                          f
                                          
                                             (
                                             
                                                τ
                                                i
                                             
                                             )
                                          
                                          .
                                          
                                             L
                                             T
                                          
                                          f
                                          
                                             (
                                             
                                                τ
                                                j
                                             
                                             )
                                          
                                       
                                       
                                          
                                             ∥
                                          
                                          
                                             L
                                             T
                                          
                                          f
                                          
                                             (
                                             
                                                τ
                                                i
                                             
                                             )
                                          
                                          
                                             ∥
                                             .
                                             ∥
                                          
                                          
                                             L
                                             T
                                          
                                          f
                                          
                                             (
                                             
                                                τ
                                                j
                                             
                                             )
                                          
                                          
                                             ∥
                                          
                                       
                                    
                                 
                              
                           
                        
                     

In order to obtain the initial target tracklets, a heuristic approach is used to create a set of short tracklets as the input of the proposed approach. For each detection at time t, if it belongs to the set of non-collision detection responses of one target, it should correspond to the closest detection among the detection responses of the next video frame, while the second closest detection would correspond to a different target. Thus, for each detection, its closest and second closest detection for the next frame are considered and the ratio of their distance
                           9
                        
                        
                           9
                           Position and object size features are used.
                         is measured. A ratio smaller than a threshold
                           10
                        
                        
                           10
                           The threshold is empirically set to 0.25 for a less crowded scene (less than 8 persons/frame) and 0.6 for a high density crowded scene (greater than 10 persons/frame).
                         means that both detections are more likely representing the same target.

@&#EXPERIMENTS AND RESULTS@&#

In this paper, the performance of the proposed tracking framework is evaluated with several state-of-the-art trackers, such as globally tracking on discrete grid (KSP) [12], GMCP tracker [9], GMMCP-tracker [8], linear programming multiple people tracker [22], continuous energy minimisation based tracker [14], DCO tracker [15], globally greedy tracker [11], H
                     2
                     T tracker [17], identity-aware network flow tracker [3], pairwise network flow tracker [13,23] and online multi-object tracker proposed by Bae and Yoon [7] on the standard dataset. The performance of the proposed tracker on the sports videos is also examined. Table 2
                      shows a quantitative comparison of the proposed system to previous methods. For a fair comparison, all results are computed using the code provided by the authors and the same pre-trained detector and ground truth are used. Fig. 7
                      shows some tracking results of the proposed method. In the majority of cases, the proposed framework outperforms state-of-the-art methods in terms of the mentioned performance measures (Section 5.2).

The parameters of the proposed tracking algorithm are fixed in all experiments. Some parameters are found empirically, and most parameters fixed for all datasets. The parameters do not affect the overall performance of the proposed tracker much. For the appearance model, the HSV colour histogram with 192 bins is utilised and to capture the shape information, the HOG feature is implemented by setting the cell size to be 8 and concatenate these two feature elements into one vector. Every 8–12 frames are combined to generate one segment of the video.

Quantitative evaluation of multiple target tracking is a difficult task due to different assignment strategies, annotation and metric descriptions. In this paper, different performance measures are utilised, including the CLEAR MOT metrics [24] and metrics from [25,26]:

                           
                              •
                              
                                 MOTA
                                 
                                    
                                       
                                          (
                                          ↑
                                          )
                                       
                                       =
                                       1
                                       −
                                       
                                          
                                             
                                                ∑
                                                t
                                             
                                             
                                                m
                                                t
                                             
                                             +
                                             f
                                             
                                                p
                                                t
                                             
                                             +
                                             m
                                             
                                                m
                                                t
                                             
                                          
                                          
                                             
                                                ∑
                                                t
                                             
                                             
                                                g
                                                t
                                             
                                          
                                       
                                       :
                                    
                                  Combines three error types: missed targets mt
                                 , false positive fpt
                                  and the identity switches or mismatches mmt
                                 , respectively at frame t.


                                 MOTP (↑): Measures the localisation error of the tracker computed by intersection areas between bounding boxes of tracking results and ground truth over union areas of the bounding boxes.


                                 FP (↓): The false positives.


                                 IDS (↓): The number of identity switches.


                                 GT: The number of targets in the ground truth.


                                 MT (↑): The number of mostly tracked targets, which their trajectories are successfully tracked for more than 80% of their ground truth time span.


                                 ML (↓): The number of mostly lost targets, which the trajectories are only tracked in less than 20% of their ground truth time span.


                                 FG (↓): The number of fragmented tracklets.


                                 REC (↑): Recall, the ratio of the correctly matched detections over the total number of detections in ground truth.


                                 PRE (↑): Precision, the ratio of the correctly matched detections over the total number of output detections.


                                 FAF (↓): The number of false alarms per frame.

Here, the symbol ↑ means that higher scores indicate better results, and ↓ means that lower scores specify better performances.

We evaluate the performance of the proposed approach on sports video sequences.

                           
                              •
                              
                                 Sports Videos. Two new sports video datasets, namely Women Soccer and Field Hockey, are introduced. The former is collected via a panning camera (1444 × 806 pixels) and the latter contains high resolution frames (1132 × 638 pixels) from four views (only one view is used in our experiments). There exist different tracking difficulties such as mutual occlusion between the soccer players in Women Soccer dataset. In addition, the challenging sport video sequence (AFL) is utilised, in which there are complex interactions among a crowd of players. For the main baseline, SMOT [27] is used, which is specifically proposed for tracking visually similar players in sports videos by relying on motion similarity and utilising a generalised linear assignment to recover long-term tracks. In these videos, SMOT [27] exhibits a good performance in the absence of detection noise. However, it fails in recovering the true trajectories of the targets in the realistic scenarios. In DCO [15], since the optimisation reaches only a moderate local minimum, therefore, it produces many fragmented target tracklets leading to a high number of interrupted trajectories (FG).

The proposed approach is also examined on challenging publicly available video sequences.


                                 PETS 2009. The PETS 2009 dataset [28] is captured from multiple cameras, and only sequences (PETS S2.L1 and PETS S2.L2) captured from view-1 are used with frame size of 768 × 576 pixels in the experiments. PETS S2-L1 includes 795 frames showing up to 8 similar walking pedestrians. The dataset’s difficulty stems from the presence of nonlinear motion. PETS S2.L2 is even more difficult than PETS S2.L1, due to its crowded density of the pedestrians and frequent mutual occlusions between them. The proposed system is capable to reach the lowest ID changes especially for the targets leaving and re-entering the scene. In majority of the used metrics, the proposed tracker performs better.


                                 TUD. 
                                 TUD-Crossing and TUD-Campus 
                                 [29] are two short sequences with frequent occlusions of the side-view pedestrians, containing 201 and 71 frames, respectively. Compared to other methods such as [15], which requires an explicit parameter estimation step, the proposed framework is much faster with fewer parameters. The results of the proposed tracker are averaged over campus and crossing sequences.


                                 Parking Lot. This sequence includes 1, 000 frames of a largely crowded scene with up to 14 pedestrians walking in parallel. It includes missed detections and parallel motion with similar appearances. The proposed tracker achieves the lowest number of ID switches as a result of correctly linking the fragmented trajectories. Furthermore, with the apt trajectories recovering strategy, the quantitative result outperforms others in terms of recall and overall accuracy.


                                 TownCentre. The HD dataset, namely TownCentre sequence of size 1920 × 1080 pixels is captured from a busy town, where there is varying number of people who are walking
                                    11
                                 
                                 
                                    11
                                    In average, 16 people are walking simultaneously.
                                 . In addition, some people become partially occluded during this video.

To verify the effectiveness of the main parts of the proposed approach, two different experiments are devised to compare the performance of the system with different constraints:

                           
                              •
                              
                                 Exp. 1: Eliminate the learned appearance models of the targets.


                                 Exp. 2: Eliminate the trajectory smoothness part in the proposed relation affinity graph.

The average results of evaluation metrics are provided for the datasets include PETS.L1, PETS.L2, AFL and Women Soccer sequences, respectively in Table 3
                        . From the accuracy results, the effectiveness of each different part of the proposed tracking framework is examined.

In Exp. 1, the proposed discriminative target appearance models are replaced by the appearance model of using the Bhattacharyya distance of multi-cue feature histograms. From the observations, the MOTP and MOTA measures have been declined noticeably since the discrimination power is reduced. The tracking results of the proposed tracker without considering the trajectory smoothness part (Exp. 2) is demonstrated in Fig. 8
                        . As anticipated, the overall system improves tracking framework performance for the two metrics, while the other experiments (Exp. 1) and (Exp. 2) are still comparable with the benchmark.

Moreover, the proposed tracker’s sensitivity to the chosen parameters including Σ and σ is examined for the different sequences and the obtained results are shown in Tables 4
                         and5
                        ,
                         respectively. Since the size of tracked target will be changed from frame to frame, the standard deviation σ for the target size noises has been set to 2 experimentally to handle the size variations. This scale has be determined for a target height of 160 pixels and automatically adjusted by the estimated height of tracked object. With increasing target’ size deviation σ and target tracklets’ forward/backward motion covariance Σ, the tracking performance will slightly decrease due to the more pairwise relationships between incorrect target tracklets in the defined tracklet affinity model. However, these changes are quite small and do not affect the proposed tracker’s performance.

To demonstrate the effectiveness of the designed tracklet affinity measurements, which fit into the iterative subgraph extraction framework, the following criteria are considered:

                           
                              •
                              (C
                                 1) Tracklet-target hypothesis similarity: Higher similarity between a target tracklet and the corresponding target hypothesis in each video frame represents the reliability of the tracklet.

(C
                                 2) Robustness to occlusions: The target tracklets should be robust to occlusion and missed detections.

(C
                                 3) Tracklet cardinality: A long tracklet is more likely to be a true trajectory of the target object since it contains more statistical target information.

Therefore, the tracklet reliability (confidence score) can be formulated based on the above requirements according to Eq. (8), which can be explicated as how well the obtained target tracklet by the proposed framework, matches the real trajectory of the target object during different scenarios (e.g. occlusion, appearance changes):

                           
                              (8)
                              
                                 
                                    R
                                    e
                                    l
                                    
                                       (
                                       
                                          τ
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          L
                                          e
                                          n
                                          g
                                          t
                                          h
                                          
                                             (
                                             
                                                τ
                                                i
                                             
                                             )
                                          
                                          −
                                          w
                                       
                                       
                                          L
                                          e
                                          n
                                          g
                                          t
                                          h
                                          
                                             (
                                             
                                                τ
                                                i
                                             
                                             )
                                          
                                       
                                    
                                    .
                                    A
                                    
                                       (
                                       
                                          τ
                                          i
                                       
                                       ,
                                       
                                          O
                                          
                                             k
                                          
                                          i
                                       
                                       )
                                    
                                    
                                    ,
                                    k
                                    ∈
                                    
                                       [
                                       
                                          t
                                          
                                             1
                                          
                                          i
                                       
                                       :
                                       
                                          t
                                          
                                             2
                                          
                                          i
                                       
                                       ]
                                    
                                 
                              
                           
                        where w is the number of the frames the target i is lost or confused due to the occlusion (target–target occlusion). 
                           
                              A
                              (
                              
                                 τ
                                 i
                              
                              ,
                              
                                 O
                                 
                                    k
                                 
                                 i
                              
                              )
                           
                         is the affinity score (Eq. (1)) between the target hypothesis and the corresponding tracklet. The obtained confidence score lies in [0 1]. The reliability of different female soccer player trajectories from the Women Soccer dataset, is illustrated in Fig. 9. The darker the colour of the bounding box is, the more reliable the specific target tracklet has been for the frame sequences.

In sports videos, such as the Field Hockey dataset, the players exhibit complex interactions and frequent occlusions occur within the group of players. Each of these difficulties violates the assumption of smooth movement for a relatively long period of time. Therefore, the video segment size should be chosen in a way that this assumption is not violated. As it shown in Fig. 10 , the tracking accuracy and precision for the Field Hockey dataset increased slightly by a rise in the number of segment’s frames from 6 to 10, then decreased to less than 88% MOTP for 20 frames in a video segment.

In contrast, for some other video sequences, where interactions among the group of targets may be relatively simple, such as parallel walking of the pedestrians in the Parking Lot dataset, with increasing the number of frames per segment, the performance of the tracker will improve constantly from 82.5% MOTP to 83.8% (black curve in Fig. 10) and from 91.3% MOTP to 92.9% (pink curve in Fig. 10). This is due to having more statistics on colour and appearance features of the targets for the proposed tracking framework to link target tracklets jointly across space and time, which results in more robust associations over the sequence. However, in general, the tracking performance remained steady at about 8–12 frames per segment and is not very sensitive to the video segment size.

The proposed framework was implemented using a single 3.1 GHz core without any parallel programming. In the proposed iterative algorithm, the two steps (trajectory growth and condensation step) iterate till reaching the optimal solutions (dense subgraphs).

In the trajectory growth step, for each initial subgraph, the number of nearby vertices can be handled and the correlated vertices will always be added to the current subgraph. In the condensation step, some irrelevant vertices will be discarded, and only a very dense cluster of vertices will remain. Both the run time and convergence of the proposed algorithm depend on the density of the spatio-temporal graph of vertices
                           12
                        
                        
                           12
                           The number of targets per frame.
                         in the different videos. However, the proposed system performs on small subgraphs and thus is fast in run time. The main computational cost lies in the condensation phase. Considering the number of vertices in the largest subgraph as N and the number of iterations for the replicator equation Eq. (3) is tr
                        , then the time complexity of the condensation step is O(Ntr
                        ). The total time complexity is O(Ntrts
                        ), where ts
                         is the number of iterations for both steps. The computation expense of the proposed tracking system is not significantly increased by the number of the temporal segments. The reason is that there is a trade-off between the number of frames in each temporal segment (batch size) and its time processing. The smaller the number of frames in each segment, the faster the condensation step is obtained. In the experiments, for the less crowded scenes, such as PETS S2.L1 (8 persons/frame), the processing time is ∼ 0.05 s/frame, which is 20x faster than the best result of Milan et al. [15]. For crowded sequences, such as AFL1, the run time is ∼ 0.09 s/frame. Moreover, in comparison with the best average run time results of Dehghan et al. [8] for a batch of 50 frames of Parking Lot sequence (1.57 s), the proposed tracker is 10x faster.

@&#CONCLUSION@&#

In this paper, a multi-target tracking framework is formulated as a dense subgraph discovering problem where the aim is to iteratively recover dense clusters of target hypotheses, each belongs to the different targets. Unlike the previous approaches, which rely on the computational expensive optimisation algorithms without any guarantee to obtain the optimal solutions, in this paper, a general framework is proposed, which can automatically find the optimal target tracklets. Both local and global cues are exploited to model the relationships between the tracklets. To increase the discriminative ability of the suggested system, the distinguishable appearance based models are learned for the targets, which would be beneficial for the performance of the proposed approach. In addition, the effectiveness of each tracking main part is evaluated. The experimental results on the challenging tracking datasets have shown the improved performance of the proposed approach, compared to other state-of art tracking systems.

Supplementary material associated with this article can be found, in the online version, at 10.1016/j.cviu.2015.11.013.
                  


                     
                        
                           Supplementary Data S1
                           
                              Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/
                              
                           
                           
                        
                     
                  


                     
                        
                           Supplementary Data S2
                           
                              Supplementary Raw Research Data. This is open data under the CC BY license http://creativecommons.org/licenses/by/4.0/
                              
                           
                           
                        
                     
                  

@&#REFERENCES@&#

