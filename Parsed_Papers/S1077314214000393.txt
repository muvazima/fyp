@&#MAIN-TITLE@&#Image classification by non-negative sparse coding, correlation constrained low-rank and sparse decomposition

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We use non-negative sparse coding with max pooling to represent images.


                        
                        
                           
                           Use correlation constrained low-rank matrix recovery to decompose image features.


                        
                        
                           
                           Locality-constrained linear coding is used to recode image representation.


                        
                        
                           
                           We achieve the state-of-the-art performances on several public datasets.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Sparse coding

Image classification

Low-rank decomposition

Non-negative

Correlation constrained

@&#ABSTRACT@&#


               
               
                  We propose an image classification framework by leveraging the non-negative sparse coding, correlation constrained low rank and sparse matrix decomposition technique (CCLR-Sc+SPM). First, we propose a new non-negative sparse coding along with max pooling and spatial pyramid matching method (Sc+SPM) to extract local feature’s information in order to represent images, where non-negative sparse coding is used to encode local features. Max pooling along with spatial pyramid matching (SPM) is then utilized to get the feature vectors to represent images. Second, we propose to leverage the correlation constrained low-rank and sparse matrix recovery technique to decompose the feature vectors of images into a low-rank matrix and a sparse error matrix by considering the correlations between images. To incorporate the common and specific attributes into the image representation, we still adopt the idea of sparse coding to recode the Sc+SPM representation of each image. In particular, we collect the columns of the both matrixes as the bases and use the coding parameters as the updated image representation by learning them through the locality-constrained linear coding (LLC). Finally, linear SVM classifier is trained for final classification. Experimental results show that the proposed method achieves or outperforms the state-of-the-art results on several benchmarks.
               
            

@&#INTRODUCTION@&#

As a fundamental problem in computer vision, image classification has attracted a lot of attention in recent years. Among many image representation models, the-bag-of-visual-words (BoW) model [1] has been widely used by many researchers [2–4] and shown very good performance. The BoW model contains mainly two modules: (i) codebook generation and quantization of features extracted from local image patches; (ii) histogram based image representation and prediction. Recently, it has been shown that combining the two modules with sparse representation is very effective and can achieve the state-of-the-art performance.

As to the first module of the BoW model, k-means is usually used to generate codebook and quantize visual descriptors extracted from local image patches by nearest-neighbor search. A histogram is then computed to represent each image by counting the occurrence number of each visual word within this image. Recently, Yang et al. [4] developed an extension by generalizing vector quantization to sparse coding. By using sparse coding instead of k-means, they tried to learn the optimal codebook and coding parameters for local features simultaneously. The use of sparse coding helps to reduce the quantization loss. Multi-scale max pooling is then used to get the feature representation of images. However, sparse coding has no constraints on the signs of coding coefficients. To satisfy the objective of sparse coding, negative coefficients are sometimes needed, while large numbers of zero coefficients are inevitable. Since non-zero components typically provide useful information, the encoding process with max pooling will bring the loss in terms of those negative components, and further degrade the classification performance.

Instead of learning sparse representations for local features [4], the use of sparse representation for the final classification has also been widely applied to many visual applications and achieves the state-of-the-art performances, e.g., image restoration [5] and classification tasks [6–12]. These holistically sparse representations on the whole image ensure robustness to occlusions and image corruptions. Training images are often chosen as the bases for sparse representation and testing images are then classified by assigning the class with the lowest reconstruction error. Ideally, a testing image can be reconstructed by the training samples and only the coefficients of the samples within the same class may be non-zero. This means a test image can be sufficiently reconstructed by the training images of the same class. However, images are often contaminated with noise. We use noise to denote the perturbing terms for the classification task. For example, the object to be classified in an image may be occluded by other objects; the background around the target object may hinder the recognition of this object. Besides, there are often multiple objects in an image with different poses and occlusions. Sometimes using the training images as the bases is not discriminative enough to boost the final classification performance. Moreover, images with similar visual features often share a lot of similarities and correlate with each other, hence exhibit degenerated structure 
                     [6]. This semantic information can help make correct classification.

In this paper, we propose a new image classification framework by leveraging the non-negative sparse coding, correlation constrained low-rank and sparse matrix decomposition techniques (CCLR-Sc+SPM). Fig. 1
                      shows the flowchart of the proposed method. Our proposed framework consists of two contributions. First, we extend the recent work on image classification [4] by using non-negative sparse coding along with max pooling to reduce the information loss during the encoding process of local features. The second is our main contribution. We propose a new image classification method by using the correlation constrained low-rank and sparse matrix decomposition technique. Our work is motivated by the observations that: Although images may be captured under various conditions with varied poses and occlusions, (i) the visual features of images within the same class are often similar and related. That means images of the same class exhibit some degenerated structure 
                     [6]. Ideally, if we stack the BoW representation of images of the same class into a matrix, this matrix will probably be low-rank. The ideal case means images are represented without any disturbance and can be well separated without any mistake; (ii) when one particular image is captured, it only contains a limited number of disturbances for classification. This results in the characteristics of noise sparsity for the stacked BoW matrix of the same class. (iii) images with similar BoW representations are more likely to have similar objects and related. This visual similarity consistency should also be preserved for their corresponding low-rank component. This correlation information should be jointly combined with the low-rank and noise components of images for better representation than directly using the BoW representation of training images.

Specially, to get more discriminative sparse coding bases with the BoW representation of images, we leverage the correlation constrained low-rank and sparse matrix decomposition technique to decompose the BoW representation of images into a low rank matrix and a sparse error matrix. We then use these bases to encode the BoW representation of images with sparsity and locality constraints. These coding parameters are used to represent images and linear SVM classifier is then utilized to predict the category labels of images. Experimental results demonstrate the effectiveness of the proposed method.

Comparing with our previous work [12], we extend the low-rank and sparse matrix decomposition per class with correlation constraints for all classes. This correlation constraints combine the relationship of images for low-rank and sparse decomposition, hence are more effective and robust to noise. Under certain conditions, [12] can be viewed as a special case of the proposed non-negative sparse coding, correlation constrained low-rank and sparse matrix decomposition technique (CCLR-Sc+SPM). In addition, more experiments are added to clarify the effectiveness of CCLR-Sc+SPM.

The rest of this paper is organized as follows. Section 2 introduces some related work. Section 3 presents the proposed non-negative sparse coding spatial pyramid matching method (Sc+SPM). Section 4 shows the proposed image classification method by correlation constrained low-rank and sparse matrix decomposition method. Experimental results are given in Section 5. Finally we conclude in Section 6.

@&#RELATED WORK@&#

The use of the bag-of-visual-words (BoW) model [1] has been proven very useful for image classification. Over the past few years, many works have been done to improve the performance of the BoW model. Some tried to learn discriminative visual codebook for image classification [13,14]. Co-occurrence information of visual words was also modeled in a generative framework [15,16]. Others tried to learn discriminative classifiers by considering the spatial information and correlations among visual words [2–4,7,10,11]. To overcome the loss of spatial information in the BoW model, motivated by Grauman and Darrell’s [3] pyramid matching in feature space, Lazebnik et al. [2] proposed the spatial pyramid matching (SPM). Since its introduction, SPM has been widely used and proven very effective.

Recently, Yang et al. [4] proposed a sparse coding based approach for soft coding and achieved the state-of-the-art performance for image classification when only one type of local feature (SIFT) is used. This method can automatically learn the optimal codebook and search for the optimal coding weights for each local feature. After this, max pooling along with SPM is used to get the feature representation of images. Inspired by this, Wang et al. [17] proposed to use locality to constrain the sparse coding process which can be computed faster and yields better performance. [11,18] also tried to jointly learn the optimal codebooks and classifiers. However, sparse coding [19] has no constraints on the sign of the coding coefficients, negative parameters are sometimes needed to satisfy the sparse coding constrains. For some particular applications [20], non-negative sparse coding [21] is needed.

Not only has sparse coding been used with local features, but also it has been widely used holistically on the entire image. Wright et al. [6] tried to view face recognition as finding a sparse representation of the test image by treating the training set as the bases and impressive results were achieved. Bradley and Bagnell [9] tried to train a compact codebook using sparse coding. Yuan and Yan [7] made visual classification with multi-task joint sparse representation by fusing different types of features. Liu et al. [20] tried to learn sparse and non-negative representations of images by solving a set of regression type non-negative matrix factorization problems. However, because images are often corrupted with noise and there are often multiple objects in one image with different poses and occlusions, sparse coding by directly using the training images as the bases is not discriminative enough to boost the final classification performance.

There has been a lot of work on how to learn good bases for visual applications, e.g., clustering and classification. Some [6–8,20,22] tried to use the training samples as the bases directly. To code a new sample, [6–8,20] used all the training samples while locally linear embedding (LLE) [22] used the k nearest neighbors. Others [18,21,23–26] utilized the training data to learn the bases, e.g., k-means, Gaussian mixture model (GMM) and sparse coding [19,21].

Over the past few years, the low-rank matrix recovery problem has been widely studied [23–26] and successfully applied to many applications, such as image processing [23], web data mining [27], and bioinformatic data analysis [28]. It tries to recover a low-rank matrix with an unknown fraction of its entries being arbitrarily corrupted. Under surprisingly broad conditions, this problem can be exactly solved via convex optimization which minimizes a combination of the nuclear norm and the norm [23,24].

To preserve more information during the local feature encoding process, the use of large dense vectors has also become popular, such as the Fisher kernels [29] and super vectors [30]. By capturing the high order differences between local features and the anchor points for coding, the Fisher kernels and super vectors can encode more information which eventually helps to improve the classification performance. Chatfield et al. [31] also conducted a rigorous evaluation of various types of encoding methods.


                     k-means clustering has been widely used for codebook generation in the BoW model. Let 
                        
                           X
                           =
                           [
                           
                              
                                 x
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 x
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 x
                              
                              
                                 N
                              
                           
                           ]
                           ,
                           
                              
                                 x
                              
                              
                                 i
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 D
                                 ×
                                 1
                              
                           
                        
                      be the set of N local image descriptors of D dimensions. The vector quantization (VQ) method uses the k-means clustering method to learn the codebook by solving the following optimization problem as:
                        
                           (1)
                           
                              
                                 
                                    
                                       min
                                    
                                    
                                       U
                                       ,
                                       V
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       n
                                       =
                                       1
                                    
                                    
                                       N
                                    
                                 
                              
                              ‖
                              
                                 
                                    x
                                 
                                 
                                    n
                                 
                              
                              -
                              
                                 
                                    u
                                 
                                 
                                    n
                                 
                              
                              V
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                           
                        
                     
                     
                        
                           
                              s
                              .
                              t
                              .
                              
                              Card
                              (
                              
                                 
                                    u
                                 
                                 
                                    n
                                 
                              
                              )
                              =
                              1
                              ,
                              |
                              
                                 
                                    u
                                 
                                 
                                    n
                                 
                              
                              |
                              =
                              1
                              ,
                              
                                 
                                    u
                                 
                                 
                                    n
                                 
                              
                              ⪰
                              0
                              ,
                              ∀
                              n
                           
                        
                     where 
                        
                           V
                           =
                           [
                           
                              
                                 v
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 v
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 v
                              
                              
                                 K
                              
                           
                           ]
                           (
                           
                              
                                 v
                              
                              
                                 i
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 D
                                 ×
                                 1
                              
                           
                           )
                        
                      are the K cluster centers to be learned and 
                        
                           U
                           =
                           [
                           
                              
                                 u
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 u
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 u
                              
                              
                                 N
                              
                           
                           ]
                           (
                           
                              
                                 u
                              
                              
                                 i
                              
                           
                           ∈
                           
                              
                                 R
                              
                              
                                 K
                                 ×
                                 1
                              
                           
                           )
                        
                      are the cluster membership indicators. 
                        
                           Card
                           (
                           
                              
                                 u
                              
                              
                                 n
                              
                           
                           )
                           =
                           1
                        
                      is the cardinality constraint. However, this constraint is too strict because each local feature can be assigned to only one visual word, especially for the local features located at the boundary of clusters. To alleviate the quantization loss of VQ, Yang et al. [4] relaxed the constraint by using a 
                        
                           
                              
                                 ℓ
                              
                              
                                 1
                              
                           
                        
                     -norm regularization and turned the VQ into sparse coding as:
                        
                           (2)
                           
                              
                                 
                                    
                                       min
                                    
                                    
                                       U
                                       ,
                                       V
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       n
                                       =
                                       1
                                    
                                    
                                       N
                                    
                                 
                              
                              ‖
                              
                                 
                                    x
                                 
                                 
                                    n
                                 
                              
                              -
                              
                                 
                                    u
                                 
                                 
                                    n
                                 
                              
                              V
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                              +
                              λ
                              ‖
                              
                                 
                                    u
                                 
                                 
                                    n
                                 
                              
                              
                                 
                                    ‖
                                 
                                 
                                    1
                                 
                              
                           
                        
                     
                     
                        
                           
                              s
                              .
                              t
                              .
                              
                              ‖
                              
                                 
                                    v
                                 
                                 
                                    k
                                 
                              
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                              ⩽
                              1
                              ,
                              ∀
                              k
                           
                        
                     where 
                        
                           λ
                        
                      is the regularization parameter. Spatial pyramid matching with max pooling is then used to obtain the final feature representation, which achieved the state-of-the-art performances on several datasets when only one type of local feature is used.

However, there is one problem with this sparse coding plus max pooling strategy. To satisfy the objective of sparse coding, negative coefficients are sometimes needed. This coding strategy is suboptimal because max pooling is then used to extract the feature representation of images. Zero (or small positive) coefficients of sparse coding indicate the corresponding bases have no (or very small) influences. However, since zero (or positive value) is always larger than negative values, max pooling strategy will choose zero (or positive value) instead of negative values. Because most of the coefficients in sparse coding are zero, this phenomenon will happen with high probability. Simply using the absolute values of sparse coding parameters is also sub-optimal because positive parameter and negative parameter have contrary influences to minimizing the reconstruction error for sparse coding. Using the absolute of the parameters values may result in the comparison of contrary influences directly. That means some useful information is lost which hinders the final classification performance. Fig. 2
                      shows a toy example of this problem.

To alleviate the information loss of the sparse coding plus max pooling strategy [4], we propose to use non-negative sparse coding instead. The non-negative sparse coding tries to solve the following optimization problem as:
                        
                           (3)
                           
                              
                                 
                                    
                                       min
                                    
                                    
                                       U
                                       ,
                                       V
                                    
                                 
                              
                              
                                 
                                    
                                       ∑
                                    
                                    
                                       n
                                       =
                                       1
                                    
                                    
                                       N
                                    
                                 
                              
                              ‖
                              
                                 
                                    x
                                 
                                 
                                    n
                                 
                              
                              -
                              
                                 
                                    u
                                 
                                 
                                    n
                                 
                              
                              V
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                              +
                              λ
                              ‖
                              
                                 
                                    u
                                 
                                 
                                    n
                                 
                              
                              
                                 
                                    ‖
                                 
                                 
                                    1
                                 
                              
                           
                        
                     
                     
                        
                           
                              s
                              .
                              t
                              .
                              
                              ‖
                              
                                 
                                    v
                                 
                                 
                                    k
                                 
                              
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                              ⩽
                              1
                              ,
                              
                                 
                                    u
                                 
                                 
                                    n
                                 
                              
                              ⪰
                              0
                              ,
                              ∀
                              k
                              ,
                              n
                           
                        
                     
                  

In this way, the non-negative sparse coding will be consistent with the max pooling strategy. We use “consistent” to mean that the non-negative sparse coding parameters can be extracted by max pooling for image representation directly. We follow the same optimization procedure as did in [18] and solve it iteratively by alternatively optimizing over U or V while keeping the other fixed. When U is fixed, this problem is reduced to a least square problem with quadratic constraints as:
                        
                           (4)
                           
                              
                                 
                                    
                                       min
                                    
                                    
                                       V
                                    
                                 
                              
                              ‖
                              X
                              -
                              UV
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                           
                        
                     
                     
                        
                           
                              s
                              .
                              t
                              .
                              
                              ‖
                              
                                 
                                    v
                                 
                                 
                                    k
                                 
                              
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                              ⩽
                              1
                           
                        
                     This can be efficiently solved by using the Lagrange dual method. When V is fixed, we solve the optimization problem (3) by optimizing over each local feature individually as:
                        
                           (5)
                           
                              
                                 
                                    
                                       min
                                    
                                    
                                       
                                          
                                             u
                                          
                                          
                                             n
                                          
                                       
                                    
                                 
                              
                              ‖
                              
                                 
                                    x
                                 
                                 
                                    n
                                 
                              
                              -
                              
                                 
                                    u
                                 
                                 
                                    n
                                 
                              
                              V
                              
                                 
                                    ‖
                                 
                                 
                                    2
                                 
                              
                              +
                              λ
                              ‖
                              
                                 
                                    u
                                 
                                 
                                    n
                                 
                              
                              
                                 
                                    ‖
                                 
                                 
                                    1
                                 
                              
                           
                        
                     
                     
                        
                           
                              s
                              .
                              t
                              .
                              
                              
                                 
                                    u
                                 
                                 
                                    n
                                 
                              
                              ⪰
                              0
                           
                        
                     This is a linear regression problem with 
                        
                           
                              
                                 ℓ
                              
                              
                                 1
                              
                           
                        
                      norm regularization and non-negative constraints on the coefficients. We adopt the feature-sign search algorithm with projected gradient descent to solve this problem.

Due to the large amount of local features, we only sample some features to learn the codebook. We choose around 45,000 SIFT features randomly to train the codebook by iteratively solving the optimization of problem (4) and problem (5). The iteration number is set to 50. After the codebook has been learned, we can code the local features for each image. Spatial pyramid matching with max pooling is then used to get the BoW representation of images.

In this section, we first introduce the correlation constrained low-rank and sparse matrix decomposition and then use it for image classification.

Not only has sparse coding been used for local features, but also it has been widely used holistically on the entire image. Training samples are often chosen as the bases for sparse coding or its variants when it is applied on the entire image. However, images are often corrupted with noise and there are often multiple objects in one image with different poses and occlusions, even if they are of the same class. That is, there exist the correlated (or common) items and the specific (or noisy) items among images. The both parts are more robust and discriminative for image classification than directly using the feature representation of training images because the two parts capture the correlated and specific attributes of images in the same class. The use of low-rank and sparse matrix decomposition technique can help to separate the two components apart. Besides, images with similar BoW representations are more likely to have similar objects and related. This visual similarity consistency should also be preserved for their corresponding low-rank component. If we can take these constraints into consideration, we will be able to further improve the image classification performance.

Motivated by these observations, we propose to use the correlation constrained low-rank and sparse matrix decomposition technique to decompose the features of images into a low-rank matrix and a noise matrix. Because images with similar visual features share a lot of similarities and often correlate with each other, as shown in [4,6,10,17]. Besides, images often undergo gross corruptions (such as occlusion or illumination change) which often happen in modern visual applications. This means noises in images may have arbitrarily large magnitude. Here we consider an idealized version and assume the noise is sparse but unknown. Let 
                           
                              H
                              =
                              [
                              
                                 
                                    h
                                 
                                 
                                    1
                                 
                              
                              ,
                              
                                 
                                    h
                                 
                                 
                                    2
                                 
                              
                              ,
                              …
                              ,
                              
                                 
                                    h
                                 
                                 
                                    p
                                 
                              
                              ]
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    m
                                    ×
                                    p
                                 
                              
                           
                         be the stacked column vectors for the BoW representation of p training images, each training image is a 
                           
                              m
                              ×
                              1
                           
                         vector. We try to decompose it as:
                           
                              (6)
                              
                                 H
                                 =
                                 L
                                 +
                                 N
                              
                           
                        where L and N are the low-rank matrix and the noise matrix. This problem can be posed as
                           
                              (7)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          L
                                          ,
                                          N
                                       
                                    
                                 
                                 rank
                                 (
                                 L
                                 )
                                 +
                                 γ
                                 ‖
                                 N
                                 
                                    
                                       ‖
                                    
                                    
                                       0
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 s
                                 .
                                 t
                                 .
                                 
                                 H
                                 =
                                 L
                                 +
                                 N
                              
                           
                        Here the 
                           
                              ‖
                              
                              .
                              
                              
                                 
                                    ‖
                                 
                                 
                                    0
                                 
                              
                           
                         counts the non-zero elements in the error matrix and 
                           
                              γ
                              >
                              0
                           
                         is the parameter that balances the rank term and the sparsity error term. However, this problem is non-convex and is very hard to solve. Recently, it is shown that under certain conditions [23], solving
                           
                              (8)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          L
                                          ,
                                          N
                                       
                                    
                                 
                                 ‖
                                 L
                                 
                                    
                                       ‖
                                    
                                    
                                       ∗
                                    
                                 
                                 +
                                 γ
                                 ‖
                                 N
                                 
                                    
                                       ‖
                                    
                                    
                                       1
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 s
                                 .
                                 t
                                 .
                                 
                                 H
                                 =
                                 L
                                 +
                                 N
                              
                           
                        exactly recovers the low-rank matrix L and the sparse matrix N. 
                           
                              ‖
                              
                              .
                              
                              
                                 
                                    ‖
                                 
                                 
                                    ∗
                                 
                              
                           
                         is the nuclear norm defined as the sum of all singular values. The augmented lagrange multiplier method (ALM) proposed by Lin et al. [25] can be adopted to solve this problem.

Besides, let W be a 
                           
                              p
                              ×
                              p
                           
                         similarity matrix which measures the similarity between images with its elements defined as:
                           
                              (9)
                              
                                 
                                    
                                       W
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 =
                                 exp
                                 (
                                 -
                                 ‖
                                 
                                    
                                       h
                                    
                                    
                                       i
                                    
                                 
                                 -
                                 
                                    
                                       h
                                    
                                    
                                       j
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 /
                                 
                                    
                                       θ
                                    
                                    
                                       2
                                    
                                 
                                 )
                              
                           
                        and 
                           
                              θ
                           
                         is the median value of the entries in W. We define the diagonal matrix P as:
                           
                              (10)
                              
                                 
                                    
                                       P
                                    
                                    
                                       i
                                       ,
                                       i
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          ,
                                          j
                                          ≠
                                          i
                                       
                                    
                                 
                                 
                                    
                                       W
                                    
                                    
                                       ij
                                    
                                 
                                 ,
                                 ∀
                                 i
                              
                           
                        If two images 
                           
                              
                                 
                                    h
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    h
                                 
                                 
                                    j
                                 
                              
                           
                         are similar, the corresponding low rank component 
                           
                              
                                 
                                    l
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    l
                                 
                                 
                                    j
                                 
                              
                           
                         of 
                           
                              
                                 
                                    h
                                 
                                 
                                    i
                                 
                              
                              ,
                              
                                 
                                    h
                                 
                                 
                                    j
                                 
                              
                           
                         are also similar. We add an additional term in problem (8) and try to solve a new problem as:
                           
                              (11)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          L
                                          ,
                                          N
                                       
                                    
                                 
                                 =
                                 ‖
                                 L
                                 
                                    
                                       ‖
                                    
                                    
                                       ∗
                                    
                                 
                                 +
                                 γ
                                 ‖
                                 N
                                 
                                    
                                       ‖
                                    
                                    
                                       1
                                    
                                 
                                 +
                                 
                                    
                                       λ
                                    
                                    
                                       1
                                    
                                 
                                 Tr
                                 
                                    
                                       
                                          L
                                          (
                                          P
                                          -
                                          W
                                          )
                                          
                                             
                                                L
                                             
                                             
                                                T
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 s
                                 .
                                 t
                                 .
                                 
                                 H
                                 =
                                 L
                                 +
                                 N
                              
                           
                        
                        
                           
                              
                                 
                                    λ
                                 
                                 
                                    1
                                 
                              
                           
                         is a balancing parameter. In this way, we are able to combine the correlation information between images into the low rank and sparse matrix decomposition.

Comparing with our previous work [12], we extend the low-rank and sparse matrix decomposition per class with correlation constraints for all classes. If we add an additional constraint to (9) with 
                           
                              
                                 
                                    h
                                 
                                 
                                    i
                                 
                              
                           
                         and 
                           
                              
                                 
                                    h
                                 
                                 
                                    j
                                 
                              
                           
                         should belong to the same class and let 
                           
                              
                                 
                                    λ
                                 
                                 
                                    1
                                 
                              
                           
                         to be large enough number, our proposed CCLR-Sc+SPM will degenerate to the LR-Sc+SPM in [12]. Hence, the LR-Sc+SPM can be viewed as a special case of CCLR-Sc+SPM. We adopt the accelerated proximal gradient algorithm in [32] to solve problem (11).

After the low-rank matrix L and the sparse matrix N have been learned, we can use them to encode the histogram information of images. We define the new bases for sparse coding the BoW representation of images as 
                           
                              B
                              =
                              [
                              L
                              ,
                              N
                              ]
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    m
                                    ×
                                    2
                                    p
                                 
                              
                           
                        . We use the Locality-constrained Linear Coding (LLC) method [17] to reconstruct the pooled feature representation of images by leveraging the bases learned above. As shown by Yu et al. [10], locality has been shown to lead to good performance. LLC uses local bases instead of all the bases for reconstruction. Formally, LLC tries to solve:
                           
                              (12)
                              
                                 
                                    
                                       
                                          min
                                       
                                       
                                          C
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          p
                                          =
                                          1
                                       
                                       
                                          P
                                       
                                    
                                 
                                 ‖
                                 
                                    
                                       h
                                    
                                    
                                       p
                                    
                                 
                                 -
                                 
                                    
                                       Bc
                                    
                                    
                                       p
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                                 +
                                 λ
                                 ‖
                                 
                                    
                                       d
                                    
                                    
                                       p
                                    
                                 
                                 ⊙
                                 
                                    
                                       c
                                    
                                    
                                       p
                                    
                                 
                                 
                                    
                                       ‖
                                    
                                    
                                       2
                                    
                                 
                              
                           
                        
                        
                           
                              
                                 s
                                 .
                                 t
                                 .
                                 
                                 
                                    
                                       1
                                    
                                    
                                       T
                                    
                                 
                                 
                                    
                                       c
                                    
                                    
                                       p
                                    
                                 
                                 =
                                 1
                                 ,
                                 ∀
                                 p
                              
                           
                        where 
                           
                              ⊙
                           
                         denotes the element-wise multiplication which favors near-by bases. 
                           
                              
                                 
                                    d
                                 
                                 
                                    p
                                 
                              
                           
                         is defined as:
                           
                              (13)
                              
                                 
                                    
                                       d
                                    
                                    
                                       p
                                    
                                 
                                 =
                                 exp
                                 (
                                 dist
                                 (
                                 
                                    
                                       h
                                    
                                    
                                       p
                                    
                                 
                                 ,
                                 B
                                 )
                                 /
                                 σ
                                 )
                              
                           
                        where 
                           
                              dist
                              (
                              
                                 
                                    h
                                 
                                 
                                    p
                                 
                              
                              ,
                              B
                              )
                              =
                              [
                              dist
                              (
                              
                                 
                                    h
                                 
                                 
                                    p
                                 
                              
                              ,
                              
                                 
                                    b
                                 
                                 
                                    1
                                 
                              
                              )
                              ,
                              …
                              ,
                              dist
                              (
                              
                                 
                                    h
                                 
                                 
                                    p
                                 
                              
                              ,
                              
                                 
                                    b
                                 
                                 
                                    T
                                 
                              
                              )
                              ]
                              ,
                              T
                           
                         is the number of bases and 
                           
                              dist
                              (
                              
                                 
                                    h
                                 
                                 
                                    p
                                 
                              
                              ,
                              b
                              )
                           
                         is the Euclidean distance between 
                           
                              
                                 
                                    h
                                 
                                 
                                    p
                                 
                              
                           
                         and b. 
                           
                              σ
                           
                         is the weight decay speed for the locality adaptor. We follow the same approximated method in [17] by firstly choose the k-nearest neighbors of 
                           
                              
                                 
                                    h
                                 
                                 
                                    p
                                 
                              
                           
                         and then use these nearest neighbors for reconstruction. This reduces the computation complexity and speeds up the coding phase. Linear SVM classifier is then used to predict the category of images due to its advantages in speed and good performances for the sparse coding parameters, as shown in [4,11,17,33].

@&#EXPERIMENTS@&#

In this section, we evaluate the proposed non-negative sparse coding, correlation constrained low-rank and sparse matrix decomposition method (CCLR-Sc+SPM) on four public datasets: The Scene-15 dataset [2], the UIUC-Sport dataset [34], the Caltech-101 dataset [35] and the PASCAL VOC 2007 dataset [36]. The codebook size for non-negative sparse coding is set to 1024, as in [4,17,33]. As to feature extraction for Scene-15 dataset, the UIUC-Sport dataset and the Caltech-101 dataset, we use the same setup as [4] did because this setup has been proven very effective on these datasets. We densely compute SIFT descriptors on overlapping 
                        
                           16
                           ×
                           16
                        
                      pixels with an overlap of 6 pixels. All images are processed in gray scale. These extracted features are then normalized with 
                        
                           
                              
                                 L
                              
                              
                                 2
                              
                           
                        
                      norm. For SPM, we follow Lazebnik et al. [2] and use the first 3 layers (
                        
                           1
                           ×
                           1
                           ,
                           2
                           ×
                           2
                           ,
                           4
                           ×
                           4
                        
                     ) with the same weight for each layer. For the PASCAL VOC 2007 dataset, we follow the spatial partition as [31] did and with 
                        
                           1
                           ×
                           1
                           ,
                           3
                           ×
                           1
                           
                           and
                           
                           2
                           ×
                           2
                        
                      grids. We use the multi-class linear SVM provided by Yang et al. [4] for its advantages in speed and good performance in max pooling based image classification. Following the common benchmarks procedures, we repeat the experimental process with randomly selecting the training and testing images to obtain reliable results. We record the average per-class classification rates for each run and report our final results by the mean and standard deviation of the classification rates.

In our method, the most important parameters are 
                        
                           γ
                        
                      and 
                        
                           
                              
                                 λ
                              
                              
                                 1
                              
                           
                        
                     . Fig. 3
                      shows the performance changes with varied 
                        
                           γ
                        
                      and 
                        
                           
                              
                                 λ
                              
                              
                                 1
                              
                           
                        
                     . With the increase of 
                        
                           γ
                        
                     , the noise matrix N gets more and more sparse. In the experiments, we found that the performance is good when a small 
                        
                           γ
                        
                      (0.05) is used for the PASCAL VOC 2007 dataset and a relatively larger 
                        
                           γ
                        
                      (0.2) is used for the other three datasets. As to the parameter 
                        
                           
                              
                                 λ
                              
                              
                                 1
                              
                           
                        
                     , it controls the smoothness of the low-rank parts. The larger 
                        
                           
                              
                                 λ
                              
                              
                                 1
                              
                           
                        
                      is, the smoother the low-rank parts. In the experiment, we set 
                        
                           
                              
                                 λ
                              
                              
                                 1
                              
                           
                        
                     =0.1:0.05:0.5 and use fivefold cross-validation to choose the best parameter. As to the 
                        
                           σ
                        
                     , we use the median value of 
                        
                           dist
                           (
                           
                              
                                 h
                              
                              
                                 p
                              
                           
                           ,
                           
                              
                                 b
                              
                              
                                 t
                              
                           
                           )
                           ,
                           p
                           =
                           1
                           ,
                           …
                           ,
                           P
                           ,
                           t
                           =
                           1
                           ,
                           …
                           ,
                           T
                        
                     . The 
                        
                           λ
                        
                      in Eq. (12) should be chosen carefully so that the influences of the reconstruction error could match the neighbor’s number for image representation. Finally, 100 is used in this paper. The k used for LLC is empirically set to 20 accordingly.

The major sources of pictures in the Scene-15 dataset include the COREL collection, personal photographs and Google Image Search. Each category has 200 to 400 images with the average image size of 
                           
                              300
                              ×
                              250
                           
                         pixels. The total image number is 4485. Fig. 4
                         shows some example images of the Scene-15 dataset. We use the same number of training images per category as [2,4,33] did and randomly choose 100 images per category and test on the rest. This process is repeated for ten times to obtain reliable results.


                        Table 1
                         gives the performance comparison of the proposed method and several other methods [2,4,12,37–39] on the Scene-15 dataset. The proposed method outperforms the ScSPM [4] by about 12.8 percent and MWLP [39] by 10.2 percent respectively which demonstrate the effectiveness of our method. The MWLP [39] made use of the local constraints in the feature space during the coding process. Since we use non-negative sparse coding instead of sparse coding along with spatial pyramid matching and max pooling, we are able to preserve more information and reduce the quantization loss. Besides, by leveraging the correlation constrained low-rank matrix recovery technique, we are able to learn better bases which considers the similarity between images hence outperforms LR-Sc+SPM. This makes the final image representation more discriminative, hence is able to improve the image classification performance.

To evaluate the relative contribution of each components of the proposed method, we also give the classification performances of S
                           
                              
                                 
                                    c
                                 
                                 
                                    +
                                 
                              
                           
                        SPM (non-negative sparse coding), LR-ScSPM (low-rank and sparse matrix decomposition), CCLR-ScSPM (correlation constrained sparse coding, low-rank and sparse matrix decomposition), CCLR-S
                           
                              
                                 
                                    c
                                 
                                 
                                    +
                                 
                              
                           
                        SPM (no LLC) (correlation constrained non-negative sparse coding, low-rank and sparse matrix decomposition (without using LLC)). We can have the conclusion that the using of non-negative sparse coding instead of sparse coding can improve the classification accuracy. Besides, the use of low-rank and sparse matrix decomposition technique can help to get more discriminative image representation than BoW. Moreover, the exploration of correlation information between images can further improve the classification rate.

The UIUC-Sport dataset contains eight categories with 1792 images. The eight categories are: badminton, bocce, croquet, polo, rock climbing, rowing, sailing and snow boarding. The number of images ranges from 137 to 250. Fig. 5
                         shows some example images of this dataset. We randomly select 70 images from each class for training [34] and test on the rest images. We repeat this process for ten times for fair comparison.


                        Table 2
                         gives the performance comparison of the proposed method and several other methods [4,12,33,37] on the UIUC-Sport dataset. We also give the performances of only using the non-negative sparse coding (Sc+SPM) for image classification. We can see that the proposed method can achieve or outperform the state-of-the-art performances on this dataset. [4] used the absolute value to solve the negative coefficients problem during max pooling. The problem with this strategy is that the inconsistency with the sparse coding and max pooling. By imposing non-negative constraints, we can make the sparse coding and max pooling parts consistent with each other. We can see from Table 2 that the Sc+SPM performs better than ScSPM [4]. This demonstrates the effectiveness of non-negative sparse coding over naive absolute value based method.

The Caltech-101 dataset contains 101 classes with high intra-class appearance shape variability. The number of images per category varies from 31 to 800 images and most of these images are medium resolution, i.e. 
                           
                              300
                              ×
                              300
                           
                         pixels. We follow the common experimental setup as did in [4,7,18,20], and randomly choose 15 and 30 images per category for training and up to 30 images for test. This process is repeated for ten times for fair comparison. Fig. 6
                         shows some example images for the Caltech-101 dataset.


                        Table 3
                         gives the performance comparison of the proposed method and several other methods [2,4,7,12,17,38–41] on the Caltech-101 dataset. As shown, our method achieves the state-of-the-art performance and outperforms ScSPM by 3.8 percent for 15 training images and 3.4 percent for 30 training images. Besides, our method also outperforms the KMTJSRC [7] which used the BoW representation of images for sparse representation directly. One work worth mentioning is the NBNN [40], where the authors employed nearest neighbor distances in the space of local image features and used the ‘Image-to-Class’ distance instead of ‘Image-to-Image’ distance. This scheme improves the image classification performance with heavy computational cost and some approximation algorithm has to be used to speed up the calculation. Moreover, by combining correlation constraints with low-rank and sparse matrix decomposition, we are able to further improve the performance of LR-Sc+SPM [12]. The performances of the proposed method is also comparable with the MWLP [39] which also used the locality information in the feature space.

Our method performs well on classes with little clutter (like watch and motorbikes) or represent coherent natural scenes (like sunflower) and less successful on classes with large intra-class variation (like dolphin and lobster). Besides, images of some classes are manually rotated to face one direction which also influences the classification performance (like accordion and barrel).

The PASCAL VOC 2007 dataset contains around 10,000 images which are divided into the train, validation and test sets with 20 classes of objects (person, bird, cat, cow, dog, horse, sheep, airplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, soft and tv/monitor). This dataset is very difficult to classify because images are daily photos. Fig. 7
                         shows some example images of this dataset. We densely extract SIFT features with an overlap of 4 pixels on various patch sizes. The patch size varies from 16 to 64 pixels with a step of 4 pixels. The performance is measured by the mean average precision (mAP).

We compared the proposed method with LLC [17], the best result of VOC07 competition [42] and the re-implementation of Fisher kernel [29] and Super vector [30] by [31]. Table 4
                         gives the performance comparison. The proposed CCLR-S
                           
                              
                                 
                                    c
                                 
                                 
                                    +
                                 
                              
                           
                        SPM achieves better performance than LLC [17,42]. This again proves the effectiveness of the proposed method. Besides, the classification performance of CCLR-S
                           
                              
                                 
                                    c
                                 
                                 
                                    +
                                 
                              
                           
                        SPM is comparable with fisher vector based method. The fisher vector can preserve more information than nearest neighbor based assignment and sparse coding as it combines the power of generative probability model and discriminative classifier. Note that the proposed method CCLR-S
                           
                              
                                 
                                    c
                                 
                                 
                                    +
                                 
                              
                           
                        SPM can also be combined with fisher vector by replacing the sparse coding for BoW representation with fisher vector based coding. On analyzing the per-class performance, we can see that the CCLR-S
                           
                              
                                 
                                    c
                                 
                                 
                                    +
                                 
                              
                           
                        SPM performs better on rigid objects than on non-rigid objects compared with LLC. We believe this is because the intra-class variance of rigid objects is smaller than non-rigid objects. This means the non-rigid objects is more difficult than rigid objects which is proven by the results of LLC [17], SV [30] and FK [29].

Since sparse coding has no constraints on the signs of the coding coefficients, information loss is unavoidable when max pooling is then used. To measure this information loss, we firstly take the absolute value of sparse coding coefficients and then apply max pooling to get a BoW representation (denoted as 
                           
                              
                                 
                                    p
                                 
                                 
                                    1
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    q
                                    ×
                                    1
                                 
                              
                           
                        ) of each image and compare with the BoW representation (denoted as 
                           
                              
                                 
                                    p
                                 
                                 
                                    2
                                 
                              
                              ∈
                              
                                 
                                    R
                                 
                                 
                                    q
                                    ×
                                    1
                                 
                              
                           
                        ) generated by sparse coding plus max pooling. We use the discrepancy percentage to measure the information loss for each image as:
                           
                              (14)
                              
                                 discrepancy
                                 
                                 percentage
                                 =
                                 
                                    
                                       q
                                       -
                                       sum
                                       (
                                       
                                          
                                             p
                                          
                                          
                                             1
                                          
                                       
                                       =
                                       =
                                       
                                          
                                             p
                                          
                                          
                                             2
                                          
                                       
                                       )
                                    
                                    
                                       q
                                    
                                 
                              
                           
                        
                     


                        Table 5
                         shows the average discrepancy percentage on the Scene-15 dataset, the UIUC-Sports dataset, the Caltech-101 datset and the PASCAL VOC 2007 dataset. This is achieved by taking the average discrepancy percentage of about 150 randomly chosen images per dataset. The exact image number varies depending on the dataset. Since we use the first 3 layers of SPM with the codebook size 1024, q is 1024×21=21,504 in our experiments for the Scene-15 dataset, the UIUC-Sports dataset and the Caltech-101 datset. We can see from Table 5 that the sparse coding plus max pooling strategy losses about 25–30 percent information which will hinder the final classification performance.

@&#CONCLUSION@&#

In this paper, we introduce a novel image classification framework by leveraging the the non-negative sparse coding, correlation constrained low-rank and sparse matrix decomposition technique (CCLR-Sc+SPM). Specifically, to reduce the information loss, we propose to use non-negative sparse coding along with max pooling and spatial pyramid matching (Sc+SPM) to get the BoW representation of images. Besides, we use the correlation constrained low-rank and sparse matrix decomposition technique to get more discriminative bases for sparse representation than directly using the training images as the bases. Experimental results on several public datasets demonstrate the effectiveness of the proposed method.

In our future work, we will try to model the low-rank information and noise information more extensively during the matrix decomposition process. Besides, how to encode local features more finely will also be studied.

@&#ACKNOWLEDGEMENT@&#

This work is supported by National Basic Research Program of China (973 Program): 2012CB316400, National Natural Science Foundation of China: 61303154, 61272329, 61303114, 61202234, 61332016. The Open Project Program of the National Laboratory of Pattern Recognition (NLPR). The President Fund of University of Chinese Academy of Sciences (UCAS). Specialized Research Fund for the Doctoral Program of Higher Education (No. 20130141120024). Beijing Municipal Natural Science Foundation of China No. 4132010.

@&#REFERENCES@&#

