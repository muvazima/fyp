@&#MAIN-TITLE@&#Sentiment analysis system adaptation for multilingual processing: The case of tweets

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We study different strategies to classify sentiment from tweets, using supervised learning with hybrid features.


                        
                        
                           
                           We experiment with English and Spanish data and compare against benchmark competitions.


                        
                        
                           
                           We employ machine-translated data from other languages for training.


                        
                        
                           
                           We show that the use of multilingual data improves the sentiment classification accuracy.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Subjectivity analysis

Sentiment analysis

Multilingual resources

Social media mining

Chat analysis

@&#ABSTRACT@&#


               
               
                  Nowadays opinion mining systems play a strategic role in different areas such as Marketing, Decision Support Systems or Policy Support. Since the arrival of the Web 2.0, more and more textual documents containing information that express opinions or comments in different languages are available. Given the proven importance of such documents, the use of effective multilingual opinion mining systems has become of high importance to different fields. This paper presents the experiments carried out with the objective to develop a multilingual sentiment analysis system. We present initial evaluations of methods and resources performed in two international evaluation campaigns for English and for Spanish. After our participation in both competitions, additional experiments were carried out with the aim of improving the performance of both Spanish and English systems by using multilingual machine-translated data. Based on our evaluations, we show that the use of hybrid features and multilingual, machine-translated data (even from other languages) can help to better distinguish relevant features for sentiment classification and thus increase the precision of sentiment analysis systems.
               
            

@&#INTRODUCTION@&#

In the past decade, the quantity of user-generated contents on the Internet has been growing exponentially. Social Media platforms, such as Facebook, Twitter, Flickr and LinkedIn, as well as commercial sites, like Amazon and Booking.com offer their users the possibility to share their experiences and opinions on topics ranging from economics, to politics, products, VIPs and globally-critical events. The value of such unbiased, real-time user-generated content has been shown to be tremendous, with applications in Marketing, Decision Support Systems, Politics and Public Policy support, disaster and crisis management, etc. Since the high volume of opinionated information makes its manual processing virtually impossible, systems have been developed to treat texts and process the opinions they contain automatically, in the context of the Subjectivity and Sentiment Analysis tasks, within the field of Natural Language Processing (NLP).

Subjectivity and Sentiment Analysis typically aim at detecting subjective, “private” states (i.e. opinions, emotions, sentiments, evaluations, beliefs, and speculations) in texts (Pang & Lee, 2008; Pang, Lee, & Vaithyanathan, 2002; Wiebe, 2000) and subsequently classifying them according to their polarity.

A lot of research has concentrated on developing methods for subjectivity and sentiment analysis in different types of texts and with different applications in mind. Nonetheless, the majority of the research has concentrated on texts written in English, since it is the language with most processing tools and annotated resources. While for some applications analyzing only opinions written in English is enough, for others, such as news monitoring, being able to detect and comparatively analyze opinions expressed in different sources is an important requirement.

This paper presents the experiments carried out for the Sentiment Analysis in Tweets task of SemEval 2013
                        1
                        
                           http://www.cs.york.ac.uk/semeval-2013/task2/.
                     
                     
                        1
                      (Balahur, 2013) and the modifications performed to the English system for the participation in the TASS
                        2
                        Workshop on Sentiment Analysis at SEPLN, http://www.daedalus.es/TASS2013.
                     
                     
                        2
                      competition (Balahur & Perea-Ortega, 2013). SemEval 2013 Task 2 was entitled “Sentiment Analysis in Tweets” and is focused on English. TASS is an experimental evaluation workshop for sentiment analysis and online reputation management systems developed with a focus on Spanish. We present the approaches followed in these competitions and their adaptation to new languages, as well as additional experiments carried out to improve the results obtained.

In the SemEval 2013 task 2, participating systems had to classify snippets from or entire tweets according to their polarity – into positive, negative and neutral (or objective). In the TASS 2013 edition, we only participated in the first task, entitled “sentiment analysis at global level”. In this task, the participants were asked to assess the global polarity of short texts extracted from Twitter by using 5 levels of sentiment (very positive, positive, neutral, negative and very negative), plus discriminate them from the objective ones. To tackle this task, we applied an approach based on machine learning by trying different feature combinations, using dictionary-based features and adding external data for training obtained through machine translation. The main motivation for the experiments in the TASS competition was to evaluate the manner in which our approach (applied for English and combinations of data from different languages) could perform for Spanish. It followed previous efforts to test the viability of using machine-translated data for sentiment analysis in newspaper articles (Balahur & Turchi, 2014, 2012) and tweets (Balahur & Turchi, 2013) and showed promising results. The evaluations in the two competitions and in the subsequent experiments show that the use of supervised learning with additional dictionary features and external training data obtained from machine translated texts might be considered good strategies to generate learning data for polarity classification systems.

The rest of the paper is organized as follows: the following section deals with the state of the art in sentiment analysis. The main features of the proposed approaches are presented in Section 3. Section 4 describes the data used for learning while the different experiments carried out are detailed in Section 6. Finally, the results obtained and the conclusions are discussed in Section 7 and Section 8, respectively.

The work presented herein is related to research in NLP on short informal text classification and multilingual text classification.

As regards short informal text classification, Go, Bhayani, and Huang (2009) performed one of the first studies involving sentiment analysis applied to tweets. The authors introduced emoticons (e.g. “:)”, “: (”, etc.) as markers of positive and negative tweets. Following their initial findings, Read (2005) employed the method to generate a corpus of sentiment-annotated tweets. They considered that positive tweets were the ones containing positive emoticons 
                        
                           (
                           e
                           .
                           g
                           .
                           
                           “
                           :
                           )
                           ”
                           )
                        
                     , and negative tweets were the ones with negative emoticons 
                        
                           (
                           e
                           .
                           g
                           .
                           
                           “
                           :
                           (
                           ”
                           )
                        
                     . In their subsequent experiments, they introduce different supervised approaches (SVM, Naïve Bayes and Maximum Entropy) and various sets of features and conclude that the simple use of unigrams leads to good results, but it can be slightly improved by the combination of unigrams and bigrams.

In the same line of thinking, Pak and Paroubek (2010) also generated a corpus of tweets for sentiment analysis, by selecting positive and negative tweets based on the presence of specific emoticons. Subsequently, they perform different experiments to classify sentiment in the obtained corpus and conclude that the best settings include the use of a Naïve Bayes classifier with unigrams and part-of-speech tags.

Another approach on sentiment analysis in tweets is that of Zhang, Ghosh, Dekhil, Hsu, and Liu (2011). Here, the authors adopt a hybrid approach, combining supervised learning with the knowledge on sentiment-bearing words, which they extract from the DAL sentiment dictionary (Whissell, 1989). Their pre-processing stage includes the removal of retweets, translation of abbreviations into original terms and deleting of links, a tokenization process, and part-of-speech tagging. They employ various supervised learning algorithms to classify tweets into positive and negative, using n-gram features with SVM and syntactic features with Partial Tree Kernels, combined with the knowledge on the polarity of the words appearing in the tweets. The authors conclude that the most important features are those corresponding to sentiment-bearing words. Finally, Jiang, Yu, Zhou, Liu, and Zhao (2011) classify sentiment expressed on previously-given “targets” in tweets. They add information on the context of the tweet to its text (e.g. the event that it is related to). Subsequently, they use SVM and General Inquirer and perform a three-way classification (positive, negative, neutral).

In SemEval 2013, a task was organized on sentiment analysis in tweets (Wilson et al., 2013). Here, the best-performing systems used additional dictionaries that were built from large data sets and word-emotion association dictionaries built from millions of tweets. From here, we can see that the use of dictionaries to improve the features used in supervised learning is a good strategy.

The TASS evaluation campaign has also been organized in 2012. The best participating system employed 5 classifiers to distinguish among the 5 classes of polarity (among themselves) and the objective class (separately). The training and test data were both in Spanish.

With regard to sentiment analysis in different languages, several authors have proposed approaches to deal with resource scarcity and/or domain adaptation. As such, in order to produce multilingual resources for subjectivity analysis, Banea et al. Banea, Mihalcea, and Wiebe (2008) apply bootstrapping to build a subjectivity lexicon for Romanian, starting with a set of 60 words which they translate and subsequently filter using a measure of similarity to the original words, based on Latent Semantic Analysis (LSA) Deerwester, Dumais, Furnas, Landauer, and Harshman (1990) scores. Another approach to mapping subjectivity lexica to other languages is proposed by Xiaojun (2009), who uses co-training to classify un-annotated Chinese reviews using a corpus of annotated English reviews. He first translates the English reviews into Chinese and subsequently back to English, then performs co-training using all generated corpora. Kim, Li, and Lee (2010) create a number of systems consisting of different subsystems, each classifying the subjectivity of texts in a different language. They translate a corpus annotated for subjectivity analysis (MPQA), the subjectivity clues (Opinion Finder) lexicon and re-train a Naive Bayes classifier that is implemented in the Opinion Finder system using the newly generated resources for all the languages considered. Banea, Mihalcea, and Wiebe (2010) translate the MPQA corpus into five other languages (some with a similar etymology, others with a very different structure). Subsequently, they expand the feature space used in a Naïve Bayes classifier using the same data translated to 2 or 3 other languages. Finally, Steinberger et al. (2011) and Steinberger, Lenkova, Kabadjov, Steinberger, and van der Goot (2011) create sentiment dictionaries in other languages using a method called “triangulation”. They translate the data, in parallel, from English and Spanish to other languages and obtain dictionaries from the intersection of these two translations.

For the classification of English tweets, we employ two different approaches: (a) one based on supervised learning using Support Vector Machines Sequential Minimal Optimization (SVM SMO) using unigram and bigram features; and (b) a hybrid approach, based on supervised learning with a SVM SMO linear kernel, on unigram and bigram features, but exploiting as features sentiment dictionaries, emoticon lists, slang lists and other social media-specific features. SVM SMO was preferred due to the computation speed. We do not employ any specific language analysis software. The aim is to be able to apply, in a straightforward manner, the same approach to as many languages as possible. The approach can be extended to other languages by using similar dictionaries that have been created in our team (Steinberger et al., 2011; Steinberger, Lenkova, Kabadjov et al., 2011).

Two main approaches were proposed for the experiments carried out in TASS. For the 
                           dictionary-based
                         approach, we took into account the linguistic peculiarities of tweets, regarding spelling, use of slang, punctuation, etc., and also the sentiment-bearing words from the training data were replaced with a unique label. In this way, the sentence “I love roses.” will be equivalent to the sentence “I like roses.”, because “like” and “love” are both positive words according to the General Inquirer
                           3
                           
                              http://www.wjh.harvard.edu/inquirer.
                        
                        
                           3
                         dictionary. If the first sentence is contained in the training data and the second sentence is contained in the test data, replacing the sentiment-bearing word with a general label increases the chance to have the second sentence classified correctly.

In the same line of thought, we also replaced modifiers with unique corresponding labels. The sentiment dictionary generated by Steinberger, Lenkova, Kabadjov et al. (2011) was used to replace the sentiment-bearing words contained in the tweets with unique labels describing their polarity. As such, words that were found in the “High positive” category in the dictionary were replaced with the label “HPOSITIVE”, those that were in the “Positive” category were replaced with the label “POSITIVE” and similarly for those in the “High negative” and “Negative” categories, with the labels “HNEGATIVE” and “NEGATIVE” respectively. In the same manner, negators, intensifiers and diminishers, as identified by the sentiment dictionary, were replaced with the labels “NEGATOR”, “INTENSIFIER” and “DIMINISHER”. Finally, we replaced the emoticons with the sentiment value they had, giving them positive, high positive, negative or high negative labels and replaced the repeated punctuation signs “!”, “?”, “.” (more than 2 appearances) with the unique labels “MULTIEXCLAMATION”, “MULTIINTERROGATION” and “MULTISTOP”. As can be seen from the results obtained, although the dictionaries used (for Spanish) have less entries than the ones (for English) originally employed in our SemEval participation (Balahur, 2013), this approach was the best-performing one.

For the second approach we also tested the performance of the sentiment classification by applying 
                           4 separate pairs of classifiers
                         (for objective versus subjective, positive versus negative versus neutral, positive versus very positive and negative versus very negative). Our aim was to see if the use of separate classifiers (in a cascade) could improve the performance, by fitting the data more appropriately.

Finally it is noteworthy that for both approaches we used simple heuristics to select the features. Although feature selection algorithms are easy to apply when a data mining environment is used, the final choice is influenced by the data at hand and it is difficult to apply on new sets of data. After performing various tests, we chose to select the features to be employed in the classification model based on the condition that they should occur at least twice in the training set.

Several data sets have been used to carry out the experiments. They are briefly presented in the next subsection. Further on, different pre-processing steps applied to these data sets are also explained.

The data set of tweets put forward for the SemEval 2013 Task 2 covered a wide range of topics (Wilson et al., 2013). Topics included a mixture of entities (e.g., Gadafi, Steve Jobs), products (e.g., Kindle, Android phone), and events (e.g., Japan earthquake, NHL playoffs). It had the following structure (see Table 1
                           ):

Two main data sets for learning purposes have been used in our experiments: the general corpus training set of TASS 2013 and the data set of tweets used in Task 2 (B) of the SemEval 2013 evaluation campaign. The first one was provided by the TASS 2013 organizers for the sentiment analysis at global level task.
                              4
                              
                                 http://www.daedalus.es/TASS2013/corpus.php.
                           
                           
                              4
                            This corpus contains 7219 Twitter messages written in Spanish about well-known personalities in politics, economics, communication or culture, between November 2011 and March 2012. Each message is tagged with its global polarity, indicating whether the text expresses a positive, negative or neutral sentiment, or no sentiment at all. Five levels have been defined: strong positive (P+), positive (P), neutral (NEU), negative (N), strong negative (N+) and one additional no sentiment tag (NONE). In addition, there is also an indication of the level of agreement or disagreement of the expressed sentiment within the content, with two possible values: AGREEMENT and DISAGREEMENT. This is especially useful to make out whether a neutral sentiment comes from neutral keywords or else the text contains positive and negative sentiments at the same time. On the other hand, a set of topics has been defined based on the thematic areas covered by the corpus. Some examples are politics, soccer, literature or entertainment. Each message has been assigned to one or several of these topics. Table 2
                            shows the statistics of the general corpus provided by TASS 2013.

The sentiment-annotated tweets in the SemEval 2013 Task 2(B) were provided by the task’s organizers.
                              5
                              
                                 http://www.cs.york.ac.uk/semeval-2013/task2.
                           
                           
                              5
                            This corpus consists of about 12,000 twitter messages covering a wide range of topics, such as known entities (e.g., Gadafi, Steve Jobs), products (e.g., kindle, android phone), and events (e.g., Japan earthquake, NHL playoffs). This data set was provided in English but we obtained the Spanish translation by applying Machine Translation (MT) using the Google MT engine.
                              6
                              
                                 http://translate.google.com.
                           
                           
                              6
                            Therefore, this translated version was also included in the training data set for our TASS experiments.

The sentiment analysis process for English contains two stages: preprocessing and sentiment classification. The preprocessing stage contains the following steps:
                           
                              •
                              Repeated punctuation sign normalization (RPSN). In the first step of the preprocessing, we detect repetitions of punctuation signs (“.”, “!” and “?”). Multiple consecutive punctuation signs are replaced with the labels “multistop”, for the fullstops, “multiexclamation” in the case of exclamation sign and “multiquestion” for the question mark and spaces before and after.

Emoticon replacement (ER). In the second step of the preprocessing, we employ the annotated list of emoticons from SentiStrength
                                    7
                                    
                                       http://sentistrength.wlv.ac.uk/
                                    
                                 
                                 
                                    7
                                  and match the content of the tweets against this list. The emoticons found are replaced with their polarity (“positive” or “negative”) and the “neutral” ones are deleted.

Lower casing and tokenization (LC&N). Subsequently, the tweets are lower cased and split into tokens, based on spaces and punctuation signs.

Slang replacement (SR). The next step involves the normalization of the language employed. In order to be able to include the semantics of the expressions frequently used in Social Media, we employed the list of slang expressions from dedicated sites.
                                    8
                                    
                                       www.noslang.com/dictionary, www.smsslang.com.
                                 
                                 
                                    8
                                  This step is especially relevant to SMS texts, whose language in their original form has little to do with language employed in ordinary texts.

Word normalization (WN). At this stage, the tokens are compared to entries in Roget’s Thesaurus. If no match is found, repeated letters are sequentially reduced to two or one until a match is found in the dictionary (e.g. “perrrrrrrrrrrrrrrrrrfeeect” becomes “perrfeect”, “perfeect”, “perrfect” and subsequently “perfect”). The words used in this form are maked as “stressed”.

Affect word matching (AWM). Further on, the tokens in the tweet are matched against three different sentiment lexicons: General Inquirer, LIWC and MicroWNOp, which were previously split into four different categories (“positive”, “high positive”, “negative” and “high negative”). Matched words are replaced with their sentiment label – i.e. “positive”, “negative”, “hpositive” and “hnegative”.

Modifier word matching (MWM). Similar to the previous step, we employ a list of expressions that negate, intensify or diminish the intensity of the sentiment expressed to detect such words in the tweets. If such a word is matched, it is replaced with “negator”, “intensifier” or “diminisher”, respectively.

User and topic labeling (U&TL). Finally, the users mentioned in the tweet, which are marked with “@”, are replaced with “PERSON” and the topics which the tweet refers to (marked with “#”) are replaced with “TOPIC”.

Once the texts are preprocessed, they are passed on to the sentiment classification module. We employed supervised learning using Support Vector Machines Sequential Minimal Optimization (SVM SMO) (Platt, 1998) with a linear kernel, employing boolean features – the presence or absence of unigrams and bigrams determined from the training data (tweets that were previously preprocessed as described above) that appeared at least twice. Bigrams are used especially to spot the influence of modifiers (negations, intensifiers, diminishers) on the polarity of the sentiment-bearing words. We tested different parameters for the kernel and modified only the C constant to the best value determined on the training data (5.0).

The training data have been preprocessed by discarding the stop words and applying a stemming process. For removing the stop words, the list for the Spanish language provided by Snowball
                           9
                           
                              http://snowball.tartarus.org/algorithms/spanish/stop.txt.
                        
                        
                           9
                         has been used. This list was revised manually, by discarding some words that might have influenced in the polarity, such as no, sí, más, and mucho. In total, from the 325 stop words included in the original Snowball list, 228 were removed, resulting in a final list of 97 stop words.

Regarding the stemming process, the 3.2 version of TreeTagger
                           10
                           
                              http://www.cis.uni-muenchen.de/schmid/tools/TreeTagger.
                        
                        
                           10
                         for Spanish has been used. This tool allows to unify different variants of the same type of word by using one unique lemma even for different gender. Some examples of these conversions are: salgo
                        ⇒
                        salir, ayudar me/nos
                        ⇒
                        ayudar yo nosotros, pensando
                        ⇒
                        pensar, la que se va
                        ⇒
                        el que se ir, lo intento
                        ⇒
                        el intento, buen día
                        ⇒
                        bueno día, etc.

Taking into account these pre-processing steps, several training data sets were generated:
                           
                              •
                              
                                 tassTrain-base: original training TASS data without applying any pre-processing step.


                                 tassTrain-lemma: original training TASS data without removing stop words but applying the stemming process.


                                 tassTrain-lemmaStop: original training TASS data+stopper+stemmer.


                                 semevaltassTrain-base: tassTrain-base+tweets of SemEval 2013 Task 2 (B), without applying any pre-processing step.

We participated in Task 2 of SemEval 2013 with two versions of the system. This task called “Sentiment Analysis in Twitter” was divided into two sub-tasks: Contextual Polarity Disambiguation (A) and Message Polarity Classification (B). Both sub-tasks provided the same data set for training and development but with different annotations. For subtask A the organizers provided annotations at expression-level, while for subtask B, the typical annotations at global message level were generated. In addition, for “unconstrained” submissions, we added to the joint training and development data provided by SemEval the set of MySpace comments provided by Thelwall, Buckley, Paltoglou, Cai, and Kappas (2010).

The main difference among the two experiments submitted to SemEval 2013 was the use of dictionaries for affect (AWM) and modifier word matching (MWM) and replacement. As such, in the first method (denoted as “Dict”), we perform all the preprocessing steps mentioned above, while the second method (denoted as “NoDict”) is applied on the data on which the AWM and MWM are not performed (i.e. words that are associated with a sentiment in a lexicon are not replaced with labels). Table 3
                      summarizes the experiments submitted to SemEval 2013 only for the test sets related to Twitter messages.

Different experiments have been carried out in our first participation in the TASS workshop. They are mainly based on the machine learning approach, combining different results or even by using external semantic resources like dictionaries. WEKA
                        11
                        
                           http://www.cs.waikato.ac.nz/ml/weka.
                     
                     
                        11
                      has been used as a tool for generating the different learning models, by applying Support Vector Machines Sequential Minimal Optimization (SVM SMO) (Platt, 1998) as learning algorithm. SVM has been proven to be highly effective in traditional text categorization and has been applied successfully in many opinion mining tasks, performing better than other machine learning techniques (Esuli & Sebastiani, 2005; O’Keefe & Koprinska, 2009).

Taking into account the learning data sets explained in the previous section, three main experiments were proposed:
                        
                           •
                           
                              SVM. For the first experiment, SVM SMO was used as learning algorithm. Thus, these experiments are represented by containing the word SVM in their title or id.


                              DICT. For the second experiment, we applied the dictionary-based approach (see Section 3). Specifically, we have used external semantic resources such as the dictionaries provided by Steinberger, Lenkova, Kabadjov et al. (2011) and General Inquirer.


                              4CLS. For the third experiment we applied the second approach in which we combined the results of 4 separate pairs of classifiers to obtain the final polarity value. These 4 classifiers combined objective versus subjective, positive versus negative versus neutral, positive versus very positive and negative versus very negative labeled tweets from the training data sets.

According to these approaches and the different learning data sets generated, a total of 18 experiments were submitted to the TASS 2013 workshop. Table 4
                      summarizes what each experiment represents. For each experiment and tweet, the global polarity using 5-levels (P+, P, NEU, N, N+) was obtained. Then, the 3-level version of each experiment was obtained by considering as P and N those tweets classified as P+ and N+, respectively. The rest remained with identical labels than those obtained for the 5-level experiments.

@&#RESULTS AND DISCUSSION@&#


                        Table 5
                         shows the results obtained in SemEval 2013 Task 2 in terms of F1-measure, applying the approach explained in Section 5. Please note that word normalization, which affected 2% of the words in the SemEval 2013 tweets, was applied to all the entire dataset.

As can be seen in Table 5, the best F1 results obtained were 0,69 for the test set “A” and 0,62 for the test set “B”. According to these results, we can say that the joint use of slang replacement, dictionaries and punctuation sign mark-up for tweets leads to significantly lower results, meaning that this step (at least with the resources we employed for slang treatment), is not necessary for the treatment of tweets. Instead, for these texts, the use of affect dictionaries and modifier lists, together with punctuation sign generalization lead to better results. This proves that such a generalization, in the context of “legible” texts, is a useful tool for sentiment analysis. The higher impact is also due to the fact that slang replacement affected 8% of the words in tweets, 1053 question marks, 3037 exclamation marks, 360 positive words and 88 negative words.

In addition, Table 6
                         shows the comparison of our best results obtained in SemEval with the best ones obtained by other participants in the same task (Wilson et al., 2013). The average of F1 score from all the participants is also shown. A total of 23 teams participated in the sub-task “A” and 38 teams in the sub-task “B”.

The official results for the 18 experiments submitted to TASS 2013 are shown in Tables 7 and 8
                        
                        . Table 7 shows the results for the 5-level way and Table 8 for the 3-level way. The typical measures in classification tasks, such as precision (P), recall (R) and F1 have been applied to obtain these results. It is noteworthy that the official evaluation results considered the successes and failures globally, i.e., without taking into account each class, and therefore averaging P, R and F1 in total.

As can be seen from the results, our approach (that was initially tailored for English data) performed well. Although the calculation of the systems’ performance as accuracy is debatable, given that the classes evaluated were highly unbalanced, we can conclude that our system is robust and the performance is relatively stable. Further analysis on the per-class performance is required in order to establish which of the classes were less well distinguishable, leading to improved features in our system. Surprisingly, the 4-classifiers approach performed the lowest. In this sense, further analysis must be done to determine at what step of the cascade the mis-classification of the examples has led to a loss in accuracy. Finally, Table 9
                         shows the comparison of our best results obtained in TASS 2013 with the best ones obtained by other participants in the same task (Villena-Román, Lana-Serrano, García-Morera, & González-Cristóbal, 2014). The average of F1 score from all the participants is also shown. A total of 12 teams participated in Task 1 called “Sentiment Analysis at Global Level”.

Subsequent to the SemEval 2013 and TASS 2013 competitions, we extended our experiments with the aim of improving the sentiment classification performance in the two languages.

As such, subsequent to the tweet normalization phase, we translated the Twitter data (the training and development data in the SemEval Task 2 campaign) using the Google machine translation system to four languages – Italian, Spanish, French and German (Balahur & Turchi, 2013). The reason for choosing the development dataset for testing is that this set is smaller and allows us to manually check and correct it, to obtain a Gold Standard (and ensure that performance results are not biased by the incorrect translation in both the training, as well as the development data).

Further on, we extract the same features as in the case of the system working for English – unigrams and bigrams – from these obtained datasets. We employ the features to train an SVM SMO classifier, in the same manner as we did for English.

The results of the classification experiments are presented in Table 10
                        . We consider the measure of accuracy and do not compare to the SemEval official results, because in the competition, the results did not take into account the “neutral” class.

The results show that the use of joint classifiers for the different languages improves the overall sentiment classification performance. The explanation for this is that by adding multilingual features (thus increasing the feature space), for languages that are closer to one another, the features that are important will have an even higher weight. At the same time, for languages that are further apart, thus their features will not overlap, the use of SVM will succeed in classifying examples more accurately since it reduces the expected probability of error, thus generalizing better over new examples.

In this article, we presented our participation to the SemEval 2013 and TASS 2013 evaluation campaigns. We participated with a system that was designed for English for the SemEval campaign and we showed how we adapted it to Spanish, by employing in-house built dictionaries and machine-translated data for training. Additionally, we tested the manner in which 4 separate classifiers could be used in cascade to determine the sentiment, from the general “subjective” versus “objective”, to the finer-grained classes of polarity. Subsequently, we combined the multilingual data and obtained a multilingual classifier, that performed overall better than the monolingual classifiers taken separately.

Several conclusions can be inferred from the experiments carried out. One of them concerns with the use of minimal linguistic processing, which makes the approach easily portable to other languages. On the other hand, from the results obtained, it has shown that the use of linguistic processing (e.g. lemmatization, stopword removal) actually worsen the performance. Finally, the use of unigrams and bigrams to spot modifications in the polarity of the sentiment expressed, allowed us to learn general patterns of sentiment expression (e.g. “negation positive”, “intensifier negative”, etc.). This pattern was successfully applied for English and, as we could see from the results obtained, also for Spanish. From the experiments combining the two languages, we can see that the use of joint classifiers can help to improve the performance of monolingual classifiers, by eliminating noisy features and reinforcing valuable ones.

In further work, we would like to tune our classifiers for the Spanish data employed and use additional language-specific features. We also plan to enrich the sentiment dictionaries used for the TASS experiments, so that more informal sentiment expressions can be captured and adopted as features for the polarity classification. Finally, we plan to include text normalization techniques adapted to Spanish. In this sense, specific linguistic processing steps, such as slang dictionaries, must be built specifically for Spanish, in order to be able to spot the same phenomena as in English.

@&#REFERENCES@&#

