@&#MAIN-TITLE@&#Network Hypervisors: Enhancing SDN Infrastructure

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new “HyperNet” abstraction to help an average user create special-purpose SDNs.


                        
                        
                           
                           A HyperNet architecture is developed to support a HyperNet.


                        
                        
                           
                           A Network Hypervisor service that provides high level HyperNet APIs.


                        
                        
                           
                           A MobileNet HyperNet is implemented to demonstrate our design.


                        
                        
                           
                           MobileNet greatly enhances the communication performance for mobile devices.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

HyperNet

Network Hypervisor

SDN

@&#ABSTRACT@&#


               
               
                  Software-Defined Networking (SDN) has been widely recognized as a promising way to deploy new services and protocols in future networks. The ability to “program” the network enables applications to create innovative new services inside the network itself. However, current SDN programmability comes with downsides that could hinder its adoption and deployment. First, in order to offer complete control, today’s SDN networks provide low-level API’s on which almost any type of service can be written. Because the starting point is a set of low-level API calls, implementing high-level complex services needed by future network applications becomes a challenging task. Second, the set of emerging SDN technologies that are beginning to appear have little in common with one another, making it difficult to set up a flow that traverses multiple SDN technologies/providers.
                  In this paper we propose a new way to set up SDN networks spanning multiple SDN providers. The key to our approach is a Network Hypervisor service. The Network Hypervisor offers high-level abstractions and APIs that greatly simplify the task of creating complex SDN network services. Moreover, the Network Hypervisor is capable of internetworking various SDN providers together under a single interface/abstraction so that applications can establish end-to-end flows without the need to see, or deal with, the differences between SDN providers.
               
            

@&#INTRODUCTION@&#

It is widely agreed that software-defined networks (SDNs) will play a major role in future network architectures. A rapidly growing list of network equipment vendors are scrambling to add SDN support to their portfolio of network offerings, each with their own distinct value-added features [1,2]. Moreover, various network providers (i.e., ISPs) are now beginning to add SDN-like services into their network infrastructure and making these services available to customers [3,4].

There is no-doubt that software-defined networks are a promising way to develop and deploy new network protocols and services in the future. However, writing down a definition of software-defined networking, one that everyone could agree with and work toward, is difficult if not impossible. To some, SDN implies programmability of the network infrastructure. To others, it simply implies user control of the network resources. To some, it implies virtualization of the infrastructure. To others, virtualization is not a requirement. These, and other differences, are reflected in the wide range of (so-called) SDN technologies that are beginning to appear, not to mention the wide range of SDN services being offered by providers. This chaos, combined with the added complexity and responsibility of having to program the network in order to achieve the benefits of SDN, could significantly hinder its adoption and use.

Creating an SDN network begins by discovering and then gaining access to the (programmable) “resources” offered by the network. At present, the APIs available for discovering, allocating, and accessing resources vary widely across providers as do the authentication/authorization schemes used to protect the resources. As a result, constructing an end-to-end SDN network requires an application to interact with a variety of different APIs offered by the various network providers that make up the resulting network. While there has been some work to develop standards, such as the Global Environment for Networking Innovation (GENI) [5] Aggregate Manager (AM) APIs [6] or the OpenFlow [7] standards, the abstractions (and resulting APIs) they define differ considerably. Even the various implementations of a standard can differ significantly. Anyone who has tried to create a programmable network that spans multiple GENI aggregates and/or OpenFlow domains can attest to the challenges of dealing with the heterogeneity and differences in APIs that can exists across SDN providers. A related issue is the problem of establishing a network connection between resources from different providers – what the GENI community refers to as the “stitching” problem. In short, while it is clear that SDN network infrastructure and services will be offered by providers, it is also clear that providers will offer such services using differing levels of abstraction and a variety of APIs, authentication, and billing mechanisms. (Note that SDN as a “paid service” raises additional challenges that we do not have space to address in this paper. Instead, we refer the interested reader to [8]).

An even bigger challenge associated with SDN networks arises from one of its key features; namely the ability to program the network. In order to offer programmers maximum control over, and programmability of, the resources that comprise the network, SDN networks often present programmers with low-level, fundamental, building blocks and abstractions on which users can build any type of network they desire. Take for example Openflow’s pattern matching and forwarding rules, or GENI’s (general purpose) VMs on which users can execute any code they want. While these low-level, basic, APIs ofter complete control and are needed in certain situations, more often than not, the fact that the abstraction is at such a low, foundational, level actually increases the difficulty of programming these networks.

In this paper, we present a new abstraction layer, called the Network Hypervisor layer, that hides the underlying details of the various SDN providers and their particular APIs from SDN applications. The Network Hypervisor allows SDN applications to run seamlessly across the sea of emerging SDN Providers, enabling Internet-scale (end-to-end) SDN applications and services that would otherwise be difficult or impossible to deploy across a disparate group of SDN Providers. The Network Hypervisoralso provides a high-level set of API calls that simplifies the task of creating and initializing commonly occurring network topologies and services, thereby enabling SDN applications to avoid the task of building the entire network “from the ground up”.

In what follows, we present our new Network Hypervisor service layer and describe the types of SDN applications that it is designed to support. Sections 3 and 4 then discuss existing software-defined networking systems and introduces the concept of an SDN Provider
                     
                     (SP). Section 5 then describes our new Network Hypervisor, the abstractions it offers, and its relationship to SPs. We then describe a working prototype in Section 6 and present an example use case in Section 7. Related work is discussed in Section 8, followed by our conclusions in Section 9.

Despite the growing popularity of SDN networks, there are relatively few SDN applications available to run on SDN networks (i.e., applications specifically designed to leverage the virtualization and/or programmability features of SDN networks). While there are multiple factors that contribute to this lack of SDN application development, a key factor is the dire need for a new abstraction layer that SDN applications can code to.

To address this need, we have developed a new abstraction layer called the Network Hypervisor
                     
                     layer. The Network Hypervisorlayer not only provides a single common interface by which SDN applications can control and leverage the various underlying SDN network technologies, but it also provides a high-level abstraction designed to make it easy for SDN applications to create commonly used SDN networks.

One of the key contributions of the Network Hypervisorlayer is that it allows SDN applications to view the SDN layer as a single logical network, masking the fact that it actually consists of many SDN providers, each having a different service offering, including differences in the geographic region that they cover. Using the Network Hypervisorlayer, applications can create end-to-end programmable networks that cross several distinct SDN providers each offering a different API for accessing, reserving, controlling, and programming the SDN provider’s resources.

Another important contribution of the Network Hypervisorlayer is that it offers a higher-level API for interacting with SDNs than is currently available with any of the existing SDN technologies or providers. The goal of the Network HypervisorAPI is to make it easy for SDN applications to do things that SDN applications often need to do such as specifying a commonly-used topology (e.g., tree, star, ring, etc.), discovering the best set of SDN resources to include in a topology (e.g., links, routers, processing/storage nodes, etc.), efficiently mapping virtual topologies to physical topologies, statically and dynamically adding in network “participants”, and reserving and initializing SDN resources.

The ultimate goal of the Network Hypervisoris to enable HyperNets 
                     [9]. One can think of a HyperNet as a prepackaged, special-purpose, network application that can be “run on the Network Hypervisor”. HyperNetsenable ordinary users to dynamically create an SDN network tailored to a particular application and set of users (called “participants”). Ordinary users simply download a HyperNet and then “run” it on the Network Hypervisor.

For example, consider a video conferencing HyperNet in which the video conference leader downloads a HyperNet from the web (e.g., from a HyperNet “App Store”) and “runs” it to create an SDN network specifically designed for video conferencing and tailored to the participants of the conference. The video conferencing HyperNet might, for example, be highly optimized for MPEG video transmission, deploying code into routers that gives priority to packets containing I frames over those containing P and B frames. It might be equipped with congestion marking techniques that provide explicit feedback to end systems so that the video transmission can be adjusted by the end systems. Moreover, because it is designed for a specific set of participants, the HyperNet can select either a source-specific or a shared multicast tree to reach the participants, automatically deploying the appropriate code to routers to implement the selected multicast functionality. The HyperNet might also include code to track the SDN network usage for each individual participant for billing purposes.

The key point is that even ordinary users can deploy special-purpose SDN networks just by downloading an appropriate HyperNet package (i.e., an SDN application) and “running” it on the Network Hypervisor. The HyperNet, being a self-contained package with all the necessary software and logic does the rest; discovering the best SDN topology to use, reserving the necessary resources, loading the appropriate code onto routers and end systems, and handling participants as they join and leave the SDN. As a result, even non-expert users can deploy rather complicated special-purpose SDN networks.

To provide some insights into the rapidly evolving SDN landscape, we briefly highlight and discuss some of the major efforts in the SDN space. To structure our presentation, we classify SDN approaches into one of three broad categories: Programmable Devices, Programmable Testbeds, and Programmable Cloud Resources.

Programmable routers, switches, NIC cards, etc. have been in use for many years, and range from low-level hardware devices such as network processors (e.g., NetFPGAs [10] and IXP [11] processors) and programmable routers (e.g., Juniper logical routers [12] and OpenFlow switches [7]). Although each of these systems offers programmability – and in some cases virtualization support – there is no standardization across the different platforms with each device having its own APIs. NetFPGAs allow the hardware itself to be redesigned, enabling new network processing approaches to be burned into hardware on-the-fly. Network processors such as the IXP processor support a special-purpose instruction set focused on packet processing that can be programmed to process packets on the network interface cards themselves. A variety of router vendors offer programmable routers and switches. Some Juniper routers support virtualization via their “logical router” abstraction, and can also be programmed via an SDK that allows packets to undergo user-defined processing. Cisco’s One Platform Kit (OnePK) [13], while not as general, provides its own APIs for programming some CISCO routers.

A growing number of router/switch vendors now offer some level of support for OpenFlow [7]. OpenFlow separates control plane functionality from data plane functionality and requires an OpenFlow controller to set up and manage the flow tables of OpenFlow switches. Each OpenFlow switch has a flow table that defines how packets should be treated (dropped, forwarded to the next hop, forwarded to the controller, etc.) as they pass through the switch. By manipulating the flow tables on OpenFlow switches, an OpenFlow controller can control the traffic across an entire network. A variety of different OpenFlow controllers are available [14–18]. While OpenFlow attempts to define a standard, at present, vendor implementations of the standard vary widely, and the standard itself continues to evolve. Moreover, the OpenFlow standard provides a very low-level and highly restrictive form of programmability, namely control of the hardware flow tables. As a result, higher level services and abstractions must be implemented via the controllers (e.g., virtualization and slicing) where there is little standardization.

Various network testbed providers have also included support for SDN and programmability. Examples include Planetlab [19] and Emulab [20], both of which have since been incorporated into the emerging GENI network [5]. In addition to programmability, these networks support virtualization of the network hardware (routers and links) allowing users to reserve resources (“slivers”) that collectively form a “slice” across the network infrastructure. In the context of GENI, network resources are associated with (owned/managed by) an aggregate. Access to resources is achieved through the GENI Aggregate Manager (AM) API using resource specifications (RSPECs). Each aggregate can be thought of as an independent SDN provider offering SDN resources to applications. Although the GENI AM API provides a standard interface for reserving aggregate resources, it does not assist with higher-level tasks such as creating a topology or identifying the best nodes to include in a slice, and it largely ignores the problem of integrating non-GENI resources (i.e., nodes where application participants live) into a slice.

A variety of providers now offer cloud resources that one could imagine using as the basis for an SDN network. While the cloud is largely known for its compute and storage resources, some providers are beginning to offer some forms of virtualized network resources. For example, Amazon’s EC2-based [21] 
                        Amazon Virtual Private Cloud (Amazon VPC) supports an API to create virtual networks connecting Amazon Machine Instances (AMIs). Similarly, cloud providers that support OpenStack’s [22] pluggable networking APIs enable SDN applications to not only allocate computer and storage resources from the cloud, but also virtualized network channels. Both Amazon’s and OpenStack’s APIs are designed to allocate basic resources, not to provide the higher-level APIs needed to create common topologies or identify the best nodes to include in a virtualized network.

We envision a future in which providers offer their software defined networking infrastructures for a fee. We call them SDN Providers (SPs). Because software-defined networking is still a relatively young technology, there is not much standardization across emerging systems. While we fully expect that some type of standardization will occur over time—the GENI Aggregate Manager API [23] being a prime example of such an effort—we do not expect that all SPs will look the same, offer the same set of services, have the same APIs, or operate under the same set of policies. In order for all kinds of SPs to cooperate with each other, we need to define a basic set of services that one can assume is provided by all SPs (although the API associated with those services can be different). We will discuss this common set of services in just a moment, but we will begin by first defining two types of SPs. The two types of SDN providers are illustrated in Fig. 1
                     .

The first type of SDN Provider is a Hardware Infrastructure Provider (HIP) who owns and operates physical network hardware—routers, switches, PCs acting as routers, wired and wireless channels, etc.—that can be virtualized and assigned to different SDN virtual networks. To simplify this model, we define three types of nodes that a hardware infrastructure provider can make available for use:
                        
                           •
                           
                              Programmable Router (PR): PRs are routers that can be programmed via one of several standardized interfaces. In its most general form, we can think of a programmable router as a physical machine with multiple physical interfaces and a general purpose Operating System installed (e.g., Unix). Users may have administrative access to this “programmable router” and thus can install or run any program on it.


                              Way Point (WP): WPs are non-programmable routers that a user can tunnel packets through in order to define the path between PRs. An example of a way point can be an OpenFlow switch whose forwarding table can be modified to forward (or tunnel) network packets along specific paths.


                              End Systems (ES): Unlike routers, ESes represent the end systems where users, applications, and services run. They represent the source and destination of communication.

We envision different “tiers” of hardware infrastructure providers, along the lines of the current Internet structure. Some may offer long-haul backbone virtualization services while others offer regional or local services. Creating a wide-area SDN virtual network will generally involve obtaining and linking together resources from multiple hardware infrastructure providers.

The second type of SDN Provider is an Infrastructure Reseller (IR) who does not own hardware but rather purchases the virtual network resources from Hardware Infrastructure Providers (HIPs) and then resells them. Just as a travel agent can combine flights from various airlines to create a complete flight for a customer, IRs may compose HIP resources into a more comprehensive and global network virtualization platform than the platform offered by any of the HIPs alone. It should be noted that both HIPs and IRs may leverage existing IP network technology to create virtual channels between routing resources (e.g., GRE tunnels). While it is unlikely that there exists a physical channel from every virtualized router to every other virtualized router, it is possible (in fact likely) that an SP (HIP or IR) would offer full 
                        
                           N
                           
                           ∗
                           
                           N
                        
                      connectivity between virtual routers via virtual channels, thereby significantly increasing the number of potential paths and indirectly complicating the user’s task of selecting an optimal topology for its SDN virtual network.

In our model, we assume that Internet Protocol (IP) is supported by all SDN Providers and IP can be used to (1) identify any physical node, including end systems, PRs and WPs and (2) provide Internet-wide connectivity. In other words, in our architecture, we assume that IP can be used to set up the SDN network. All virtual networks, once created and deployed, do not necessarily rely on IP.

For an SP to manage its hosting resources efficiently and effectively, we assume that an SP knows and manages:
                           
                              •
                              The location of its resources. This location information can be represented by an IP address, or a geographical location (a longitude/latitude pair or zip code).

The static information about its resources, such as the resource type (hardware type and OS) and resource capability (total amount of CPU/memory/disk, interface number, type and capacity, etc.).

Dynamic information about its resources, such as link status (e.g., the available bandwidth, current delay, or loss rate of a physical link) or processing capacity of a programmable node. Such information can be monitored by substrate resources and kept in a local repository [24]. The SP may further summarize dynamic information into long term average statistics and store them in a centralized database.

The physical topology formed by its resources. This can be achieved by running, for example, a link state routing protocol.

In addition, in order to maximize its profit, an SP also monitors its resources and provides traffic engineering, e.g., by giving back partial topology information, filtering the query results to its customers, and providing QoS guarantees to reserved virtual links, etc.

Although SPs will often provide value-added services designed to give them a competitive advantage over other SPs, we assume there is a common set of basic functionality that all SPs provide. In particular, we assume all SPs provide APIs to:
                           
                              •
                              Reserve and release resources, including programmable routers, way points, end systems, etc.

Inter-connect the reserved resources to form a virtual network, i.e., reserve links.

Run customized commands or to install customized applications on reserved resources, if the resources are programmable.

Discover physical resources so that users can find out what resources are available and how they are (physically) connected.

Start, stop, renew a virtual network.

Monitor the resource usage of each virtual network.

Detect errors and failures that happen in the infrastructure.

Charge for the usage of resources.


                     Fig. 2
                      illustrates our HyperNet architecture. At the heart of the architecture is the Network Hypervisor which plays a role similar to that of a hypervisor in a virtual machine; namely, it is the platform on which HyperNets run. A HyperNet is a complete software package which includes all necessary software stacks, configuration files, scripts and programs, that can be deployed into a SDN virtual network through the Network Hypervisor. HyperNets interact with the Network Hypervisorvia the Network HypervisorAPI. The Network HypervisorAPI hides the details of the underlying SPs from the HyperNet, allowing the HyperNet to be written independent of the specifics of the SPs. The Network Hypervisor is responsible for executing HyperNets, obtaining network resources from SPs, connecting resources together to form the topology required by each HyperNet, loading the necessary software and/or configuration files on each node of the topology, and then monitoring and adapting the topology over time as network condition change and network participants come and go. In addition, the Network Hypervisormonitors the underlying SPs and provides “upcalls” to the HyperNet (akin to interrupts in an Operating System) so that the HyperNet can adapt to changes in network characteristics or network membership (participants).


                        Fig. 3
                         illustrates the API layer of the Network Hypervisor. A detailed list of the Network HypervisorAPIs can be found in [9], but in essence the Network HypervisorAPI calls are designed to extend and enhance the limited functionality provided by the underlying SPs. Generally speaking, the Network HypervisorAPI calls fall into one of the following categories:
                           
                              •
                              HyperNet management: This category contains API calls that register a HyperNet with the Network Hypervisor, deploy an SDN onto the SP infrastructure, and update/tear down an existing SDN.

Router management: This category includes API calls that find Programmable Routers (PRs) near the network locations of participants, add/remove a PR to/from an SDN network, update the configuration (CPU/memory/disk space/network interface, etc.) of a PR, and loads applications onto PRs.

Topology management: This category contains API calls that help explore the underlying physical topology of an SP. Besides the basic API calls that add/remove (virtual) links to/from an SDN network and update link configurations (bandwidth/delay/loss), this category also provides “advanced” API calls that help (1) find the shortest path (based on number of hops, or round-trip time) between two nodes on the infrastructure; (2) find the central node (based on round-trip time) among a set of given nodes in the infrastructure; (3) expose a virtual link’s underlying physical path. For advanced users who want to “manually” build the topology of their SDN, the API provides a “seeTopo ()” call that returns the entire physical topology to the caller.

End System management: This category contains API calls that help a participant (an end system user) dynamically join or leave an SDN. Because most user machines are not owned and operated by SPs, attaching a user’s machine to an SDN is not a straightforward task. In other words, existing SPs do not provide an easy interface for an Internet user to connect their end system (e.g., desktop machine) into an SDN. By leveraging current IP tunneling techniques, the Network Hypervisor provides API calls for an Internet user to join/leave a SDN even though the user’s machine is not directly attached to the SDN. Those API calls help notify a HyperNet (via an upcall) about a participant’s desire to join the SDN. The Network Hypervisor then finds a PR to act as the “entry point” for a participant. It then instructs both the joining participant and the assigned PR to cooperate with each other to create an IP tunnel connecting the participant to the SDN.

In addition to providing a basic set of Hypervisor API calls that help reserve and control SDN resources (nodes and links), the HyperNet architecture also includes a set of HyperNet libraries routines (see Fig. 3) that can be used by HyperNets. The HyperNet library routines offer advanced network-level and system-wide functionality that make it easier for a HyperNet developer to create a HyperNet. HyperNet library routines are implemented using the existing underlying Network HypervisorAPI calls (as well as other Library calls). For example, the “buildRPTree ()” library call, which helps to build a rendezvous-point based tree topology makes use of the “findCentralNode ()”, “getShortestPath ()”, “addPR ()” and “addTunnel ()” Network HypervisorAPI calls to achieve its functionality. Note that the HyperNet libraries routines are not necessarily part of the Network Hypervisor. In fact, one should think of them being part of the HyperNet, much like the C library is linked with, and becomes part of, a C program. As a result, any third-party developer can contribute to make the HyperNet architecture more powerful by extending the set of HyperNet library calls. Fig. 3 list some of the more popular library calls available. Example HyperNet library calls include: a topology library that helps building several types of topologies; a routing library that helps load dynamic routing protocols onto an SDN network and configure routing table entries of each node in an SDN network; an application library that helps easily deploy content management apps, content caching apps, and packet filtering apps onto an SDN network.

To demostrate our HyperNet architecture, we implemented and tested a Network Hypervisor capable of creating fully-functional SDN networks (slices) in GENI. Using our Network Hypervisor, we developed several example HyperNets (described in Section 7) and “ran” them to create real networks carrying actual network traffic from applications running on end systems.

Our prototype Network Hypervisor is designed to use GENI – or more specifically ProtoGENI – as the underlying VNIP provider. ProtoGENI is one of the GENI control frameworks that fully supports the GENI AM API [6]. Currently the GENI AM API provides functions to (1) discover GENI resources, (2) reserve, renew, and delete GENI “slivers” (a GENI sliver can be either a programmable router, a way point, or an end system), (3) check GENI sliver status (more tools are also available on ProtoGENI to facilitate monitoring GENI slivers [25]), and (4) create or tear down a GENI “slice”. As a result, ProtoGENI satisfies the requirements for the “basic SDN Provider Services” that we described in Section 4. Moreover, in ProtoGENI, there exist multiple “aggregates”, each with its own managed network resources and aggregate manager. Although all aggregate managers provide the same GENI AM API, in order to create an SDN across multiple aggregates, applications must talk with the aggregate managers individually to reserve resources. In order for two nodes in separate aggregates to be able to communicate with each other, a tunnel (e.g., a GRE tunnel) need to be set up between the two aggregates. Thus, in some sense, ProtoGENI aggregates act like separate SDN providers which makes ProtoGENI a good platform for our Network Hypervisor development, testing, and evaluation.

As is shown in Fig. 4
                     , the Network Hypervisorimplementation includes a set of GENI Aggregate Manager (AM) Handlers in the bottom layer. These handlers talk to different GENI AMs (via the GENI AM API [6]) to discover, manage and monitor GENI resources. The top layer of the hypervisor provides the set of Hypervisor API calls described earlier. The Network Hypervisor API calls are invoked by HyperNetsusing XML-RPC. To implement the Network HypervisorAPI calls, the Network Hypervisoris comprised of three components: the Location Manager, Topology Server/Routing Server, and Information Base.

The Location Manager determines the closest programmable router to a participant. The Location Manager fetches location information about each SP from the SPs and saves it to the Information Base. An SP’s location information includes the SP’s local view of nearby end hosts (e.g., IP prefixes). Just as today’s ISPs configure local DNS (Domain Name Service) servers for their customers and thus “know” the nearby customers, this location information can also be obtained by SPs in the HyperNet architecture. The Location Manager is also in charge of coordinating the task of discovering the network location of a participant among nearby SPs. The Location Manager may cache the results. The Location Manager then picks a programmable node closest to the participant and returns it via the findPR () API call. In our implementation, we consider each protoGENI aggregate as a distinct SP since different aggregates typically sit in different geographical locations, obtain different sets of IP addresses, and can only manage resources within their own aggregates.

To assist the Location Manager in gathering data, we created a long-lived experiment in protoGENI in which one node out of each aggregate was reserved. This long-lived slice is then used by the hypervisor as a “control plane” to facilitate the Location Manager (see Fig. 4). Since the hypervisor has control over each of the reserved nodes, we can load a probe application onto those nodes and fetch the network performance information between those nodes and any participant.

To support the seeTopo (), getShortestPath (), and findCentralNode () hypervisor API calls, we designed and implemented a Topology Server (TS)/Routing Server (RS) component. The TS/RS’s job is to obtain the topology information from the underlying SPs
                           1
                           SP topology information contains the topology that an SP exposed to the hypervisor via the SP API. It does not have to be the physical topology. In cases where the SP wants to hide part of its physical topology, virtual tunnels can be returned.
                        
                        
                           1
                        , and save it to the Information Base. The Topology Server is responsible for maintaining up-to-date topology information about an SP. The Routing Server makes use of the topology information in the Information Base and accomplishes tasks such as calculating the shortest path between two nodes, finding a central node among a set of nodes, etc.

As indicated earlier, the Information Base component contains the location information about each of the underlying SPs and the most up-to-date topology information of the SPs. In addition, for each HyperNet SDN network, the Information Base maintains a HyperNet table entry that includes: HyperNet name, participant ID list, creator information, active participants information (e.g., their IDs, IP addresses, HyperNet addresses, and PRs, etc.), SDN network topology (such as reserved nodes and links, programmable routers, etc.), credential and other monitoring information such as total active time, resources consumed, and the cost to use the resources.

When a new HyperNet participant requests to join an existing HyperNet SDN network, a tunnel needs to be created between the participant and the SDN network. This tunnel will serve as the “bridge” for the participant to communicate with the SDN network. It is the responsibility of a HyperNet package to set up such a tunnel upon receiving a join request. However, to support the joining process, the Network Hypervisorprovides three API calls for the HyperNet to use: join (), checkJoin () and addGW ().

The join () upcall to a HyperNet causes a “join request handling process” to be created. The typical join request handling process is illustrated in Algorithm 1. Upon receiving a join () call, the Network Hypervisorgathers information about the joining participant. It then chooses a programmable router based on the network location of the participant. This programmable router might be chosen from the existing programmable routers in the SDN network or as an additional node that needs to be added into the SDN network (which requires an “update” option to update the SDN network topology). Finally it sets up a tunnel between the chosen gateway and the joining participant via addGW ().
                           
                              
                                 
                                 
                                    
                                       
                                          Algorithm 1 join request handling process:
                                    
                                 
                                 
                                    
                                       
                                          while true do
                                       
                                    
                                    
                                       
                                          joinRequest participant =checkJoin ()
                                    
                                    
                                       
                                          assign gateway PR and HyperNet-specific address
                                    
                                    
                                       
                                          
                                          
                                          to the requesting participant
                                    
                                    
                                       
                                          tunnelInfo myTunnel =
                                    
                                    
                                       
                                          
                                          
                                          new tunnelInfo (gateway, HyperNet-address)
                                    
                                    
                                       
                                          addGW (myTunnel)
                                    
                                    
                                       
                                          end while
                                       
                                    
                                 
                              
                           
                        
                     

In our implementation, communication from the Network Hypervisorto the HyperNet package is achieved by an “up-call”. First of all, the hypervisor maintains a join request list and a tunnel information list for each HyperNet SDN. After a network creator deploys a HyperNet SDN, the HyperNet package enters into the “join request handling process”, in which it loops checking for join requests using the checkJoin () API call. This API call simply waits for new join upcalls that come through the Network Hypervisor.

A new participant issues a join request to the hypervisor via the join () API call (Algorithm 2) and waits for the corresponding tunnel information needed to establish a tunnel to the SDN. After checking the validity of the request, the hypervisor notifies the “join request handling process” about this new participant by returning information about the participant as a result of the checkJoin () call.
                           
                              
                                 
                                 
                                    
                                       
                                          Algorithm 2 tunnelInfo join (joinRequest request):
                                    
                                 
                                 
                                    
                                       joinRequestList.add (request)
                                    
                                    
                                       joinRequestList.notify ()//notify checkJoin ()
                                    
                                    
                                       tunnelInfoList.wait ()//wait for new tunnelInfo
                                    
                                    
                                       
                                          if new tunnelInfo is valid then
                                       
                                    
                                    
                                       
                                          joinRequestList.remove (request)
                                    
                                    
                                       
                                          return the new tunnel information
                                    
                                    
                                       
                                          else
                                       
                                    
                                    
                                       
                                          return null
                                    
                                    
                                       
                                          end if
                                       
                                    
                                 
                              
                           
                        
                     

The “join request handling process” of the HyperNet package, after being notified about the new join request, creates tunnel information (i.e., assigns a PR and a HyperNet-specific address) for the new participant, and then calls the addGW () API call (Algorithm 3) to inform the hypervisor about the tunnel information. The hypervisor then configures the assigned programmable router based on the tunnel information, and notifies the new participant by returning this new tunnel information in the join () API call. Finally, after receiving valid tunnel information, the joining participant can configure its network interface properly to connect to the assigned programmable router via an IP tunnel.
                           
                              
                                 
                                 
                                    
                                       
                                          Algorithm 3 addGW (tunnelInfo tunnel):
                                    
                                 
                                 
                                    
                                       configGW (tunnel)//configure the chosen gateway
                                    
                                    
                                       
                                          
                                          
                                          
                                          //programmable router
                                    
                                    
                                       tunnelInfoList.add (tunnel)//add the new tunnel
                                    
                                    
                                       
                                          
                                          //information onto the tunnel information list
                                    
                                    
                                       tunnelInfoList.notify ()//notify join () about this new
                                    
                                    
                                       
                                          
                                          
                                          
                                          
                                          //tunnel information
                                    
                                 
                              
                           
                        
                     

@&#EXPERIMENTAL RESULTS@&#

To evaluate our HyperNet architecture, we implemented a Network Hypervisorthat uses the GENI network [5] as the underlying SDN. We then wrote several HyperNet packages and ran them on our Network Hypervisor, including a Multicast HyperNet and a Gaming HyperNet 
                     [9,26]. The Multicast HyperNet allows a user to easily deploy a participant-specific multicast-enabled SDN network that uses PIM-based multicast trees to support IPv4 multicast communication to the participants. We have used the Multicast HyperNet to efficiently transmit video to a set of receivers. The Gaming HyperNet creates a distributed game network specifically designed to optimize the location of the game server and minimize the round trip time (RTT) to the participants in the game. We have used the Gaming HyperNet to run first person shooter interactive games and have shown that the average RTT can be reduced from roughly 200ms to 20ms.

In the following we introduce yet another type of HyperNet, called MobileNet, which is designed to help mobile devices access Internet web sites over a less-than-perfect wireless network (i.e., a lossy first hop wireless link). The key to MobileNet performance is to enhance TCP performance via “TCP Splitting” [27] in which an extra TCP connection is used across the lossy wireless link that is then spliced into a wired connection to the web site. MobileNet is an interesting example in that it does not connect participants in the MobileNet together, but rather it connects each participant to the web sites that participant needs to reach. MobileNet is also interesting in that it demonstrates a form of joining called involuntary join in which a SDN participant joins an unknowing destination (e.g., a public web site) to the SDN on the destination’s behalf. We begin by introducing the concept of an involuntary join, and then proceed to describe our MobileNet prototype and results.

Typically, HyperNet participants voluntarily join themselves to the SDN by invoking the Network Hypervisor’s join () API call. However, participants can also join unkowning sites to the SDN via a version of the join () API called an “involuntary join”. Involuntary joins are useful when a voluntary participant wants to use the SDN to reach a site that would otherwise never join the SDN. For example, an SDN participant that wants to access a commercial web site like Google, Facebook, or YouTube is unlikely to get these web sites to join their private SDN. In such cases, the participant can invoke the Network Hypervisor’s involuntary join call to “join” these unknowing (involuntary) sites to their SDN network. The involuntary join call finds the “jumping off” point on the SDN that is closest to the involuntary site, and then configures the “jumping off” point to act as a proxy/NAT box forwarding packets to (and from) the involuntary site. The involuntary site is unaware that it is using the SDN to communicate with the participant. Finally, the involuntary join API call configures the participant to route traffic destined for the involuntary site via the “jumping off” point using the HyperNet-specific address of the “jumping off” point. This can be achieved by, for example, configuring the local DNS cache so that the involuntary participant’s domain name maps to the jumping off point’s HyperNet-specific address.

Mobile devices are increasingly being used for data intensive applications such as browsing web pages, watching videos, streaming music, downloading files, installing updates, etc. Because TCP is typically used for these applications, performance can be less-than-desirable when the first hop is a lossy or intermittent wireless link – as is often the cases with mobile devices. Even a loss rate of 0.1% can significantly affect TCP’s performance. MobileNet addresses this problem by breaking the TCP connection into two parts: one TCP connection traversing the wireless link, and another TCP connection traversing the wired portion of the path (i.e., TCP splitting). Consider the network shown in 
                        Fig. 6
                        (a) in which wireless participant P wants to download data from a web server S. Because P’s first hop is a wireless link, even small data losses over the wireless link can severely degrade TCP throughput. Fig. 5(b) shows the SDN topology created by MobileNet. The MobileNet HyperNet is designed to load special TCP splitting software onto node G to intercept TCP communication between P and S in an attempt to improve TCP performance. The key point is that MobileNet needs to find the programmable router G that is close to P where the TCP splitting software will be run.

Initially P (voluntarily) joins the SDN resulting in an “‘upcall” being made to the MobileNet HyperNet. By using the “findPR ()” API call, MobileNet finds a programmable router, G, near P. In our experiment, G is found to be a PC in the (wired) Kentucky GENI Aggregate capable of acting as P’s attachment point into the SDN and also as the TCP-splitting point. P then invokes an involuntary join for server S, which causes an “upcall” to MobileNet to find a programmable router H near S and to set up H as the proxy (jumping off point) to S. Next, the HyperNet creates an inter-SP link connecting G to the jumping off point H using the “addTunnel ()” API call. The addTunnel () call automatically identifies the two aggregates containing G and H, and then talks to the aggregate managers to set up a GRE tunnel as the inter-SP link, with routing tables properly set on both ends. A TCP splitting application (we use a script that leverages “netcat” [28]) is then loaded onto G using the “loadApp ()” API call.


                        Fig. 6 shows the performance result of our MobileNet SDN network, compared with regular TCP’s performance. In all of our experiments, we use a fixed RTT of 100ms between P and S – we use the traffic control toolkit “tc” in Linux to control the delay and loss rate of each link. To evaluate normal TCP performance, traffic simply went between P and S without going through G and H. For TCP splitting tests, all traffic went through G and H. To understand how much the delay of the wireless link between P and G affected MobileNet performance, we used RTTs between P and G of 2ms, 20ms, and 50ms in our MobileNet tests.

Looking at the normal TCP curve (the lowest curve on the graph) when using a 2ms RTT between P and G, we see that TCP performance decreases rapidly, going from 90Mbps at a 0% loss rate to 4Mbps at a 0.1% loss rate and then quickly dropping toward 0Mbps.

MobileNet (with a 2ms RTT between P and G) set up a split TCP connection that was able to overcome small amounts of packet loss. The top curve shows that even at loss rates up to 1%, MobileNet is able to maintain throughputs around 85Mbps. As expected, moving G away from P (i.e., increasing the RTT between them) reduces the throughput, but is still better than normal TCP performance in all cases.

@&#RELATED WORK@&#

Many tools and applications have been invented towards easing the deployment and management of SDN networks. Some aim to help create virtualized infrastructure; some are provided to assist with resource discovery and allocation, software distribution, instrumenting and monitoring in network testbeds. While these tools are helpful in setting up a network, they still push much of the decision-making onto the users, and they require that the users become familiar with the set of available tools and manually run them (sometimes “program” them) to achieve the desired network. To the best of our knowledge, HyperNet is the first abstraction that hides these details behind a (expert-written) package that can be executed by an average user to deploy an SDN network automatically. In the following, we briefly discuss some of the existing tools and their limitations.

Both Chef [29] and Puppet [30] are designed to facilitate the deployment of virtualized compute and storage infrastructure, typically for a particular application, such as a web server. Both tools allow the user to define the desired state of the infrastructure then automatically enforce that state for the user. In Chef, the desired state is expressed in terms of a set of “Recipes” (created using a Chef-specific scripting language) stored in a “Chef-server”. A “Chef-client” on each infrastructure node is responsible for fetching “recipes” from the Chef-server and performing them on the node. Similarly, Puppet users express the desired state of their IT infrastructure using puppet commands.

One obvious shortcoming of Chef and Puppet is that setting up a virtualized slice requires that the user learn the chef/puppet tools and configuration syntax/language. The HyperNet abstraction, on the other hand, enables a non-expert user to deploy a specialized SDN network simply by “running” a package created by an expert. Typically, the only configuration that a user must supply is the name of the network and the list of participants that should be allowed to join the network. Moreover, both Chef and Puppet focus primarily on tailoring the compute/storage infrastructure and have limited support for tailoring the network(s). The HyperNet architecture, on the other hand, provides API calls for one to create an SDN network tailored for the network locations of its participants.


                        Flack 
                        [31] is a web-based GUI designed to create network experiments in GENI [32]. Through Flack, users can select and then “drag” GENI nodes onto a canvas, linking them together to form the topology for their experimental network (“slice” in GENI terminology). Flack provides a nice graphical interface to create network topologies, but it is the user’s responsibility to identify all the nodes needed in the experiment, connect them together into a topology. Moreover, after the slice has been created it is the user’s responsibility to log into each of the nodes, configure them, and start any services that need to be started.

The GENI User Shell (Gush) 
                        [33] is a follow-on to its predecessor Plush [34] previously designed for PlanetLab. Gush offers many of the same features as Plush, but is designed to support GENI resources and slices using the GENI AM APIs [6]. The goal of Gush is to provide an extensible execution management system for GENI. Just like the Chef setup, in the Gush architecture there is a Gush controller running on a user’s end system. Users describe their experiment in an application specification XML document which get interpreted by the controller. The controller then sends out commands to all Gush clients running on each of the GENI nodes to control the experimental network. Gush does not help to create the RSPEC (i.e. to define the slice or its topology), but rather assumes this has been done with some other tool. In order for a user to leverage Gush’s software loading capabilities, the user must write a Gush script and create the associated tar files using Gush’s XML-based configuration language. Finally both Gush and Flack lack support for creating connections between real world Internet users and GENI experimental networks.

@&#CONCLUSION@&#

In this paper, we described some of the emerging software-defined networking approaches and highlight some of the differences between them that make it difficult to construct end-to-end SDN paths/services. The SDN network creator not only needs to understand the (mostly low-level) APIs provided by each of the individual SDN techniques, but also needs to know how to “stitch” together SDN providers.

To address these issues, we proposed the concept of a Network Hypervisor. The Network Hypervisor provides a level of abstraction that masks the differences between SDN providers and hides the complexity of programming the networks offered by SDN providers. The Network Hypervisor provides a set of high-level API calls that (expert) HyperNet writers can use to create HyperNet Packages that can be run by non-experts. The result is a special purpose SDN network, tailored to a particular application and set of participants. Using our initial prototype, we implemented several different HyperNet Packages and demonstrated the ease with which they can be used as well as the performance benefits of using special purpose HyperNets. In particular, we showed that a (wireless) MobileNet SDN that supports TCP splitting can be created across multiple SDN providers that achieves significantly better resilience to errors (and thus performance) than conventional TCP over a wireless network.

@&#ACKNOWLEDGEMENT@&#

This work is supported in part by the National Science Foundation under Grants CNS-0834243 and CNS-1111040.

@&#REFERENCES@&#

