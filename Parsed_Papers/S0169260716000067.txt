@&#MAIN-TITLE@&#Active learning based segmentation of Crohns disease from abdominal MRI

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We propose an interactive method combining semi supervised learning (SSL) and active learning (AL) for segmenting Crohns disease affected regions in MRI.


                        
                        
                           
                           A novel query strategy for AL has been proposed that makes use of context information to identify query samples.


                        
                        
                           
                           Compared to fully supervised methods we obtain high segmentation accuracy with fewer samples and lesser computation time.


                        
                        
                           
                           Our method has the potential to be used in scenarios which pose difficulties in obtaining large numbers of accurately labeled data.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Crohns disease

Segmentation

Semi supervised classification

Active learning

Graph cuts

Label query

@&#ABSTRACT@&#


               
               
                  This paper proposes a novel active learning (AL) framework, and combines it with semi supervised learning (SSL) for segmenting Crohns disease (CD) tissues from abdominal magnetic resonance (MR) images. Robust fully supervised learning (FSL) based classifiers require lots of labeled data of different disease severities. Obtaining such data is time consuming and requires considerable expertise. SSL methods use a few labeled samples, and leverage the information from many unlabeled samples to train an accurate classifier. AL queries labels of most informative samples and maximizes gain from the labeling effort. Our primary contribution is in designing a query strategy that combines novel context information with classification uncertainty and feature similarity. Combining SSL and AL gives a robust segmentation method that: (1) optimally uses few labeled samples and many unlabeled samples; and (2) requires lower training time. Experimental results show our method achieves higher segmentation accuracy than FSL methods with fewer samples and reduced training effort.
               
            

@&#INTRODUCTION@&#

Crohns disease (CD) affects the digestive tract, and can be debilitating if not detected timely [1]. Early detection can help in rapid diagnosis, reduce time and cost for therapy planning, and improve quality of life of patients. The current reference standard for CD diagnosis is colonoscopy [2] which is invasive and poses the risk of bowel perforation, thus leading to exploration of non-invasive imaging techniques to assess CD e.g., magnetic resonance imaging (MRI) [3–5], ultrasound and computed tomography (CT) [6].

In previous work we proposed a fully automatic machine learning (ML) method for detecting and segmenting CD from MRI [7,8]. The success of a robust automated system using fully supervised learning (FSL) depends upon the availability of sufficient numbers of correctly labeled samples covering a wide range of disease severities. Obtaining such data is challenging because: (1) finding qualified experts is difficult; and (2) manual annotations are time consuming. Under the above constraints we propose an active learning (AL) framework that queries the labels of training samples that would contribute most in designing a robust classifier. An important component of active learning systems is the query strategy to select the most “informative” sample. An excellent review of different query strategies is given in [9]. Compared to fully supervised methods our work aims to obtain higher segmentation accuracy using fewer informative samples.

@&#RELATED WORK@&#

Analysis of colon and bowel MRI is challenging due to the complex structure of the bowel wall. Schunk in [10] analyzed MR images for their suitability in detecting inflammatory bowel disease (IBD), including CD. Tielbeek et al. in [11] compare the suitability of different imaging techniques such as MRI, DCE-MRI, and diffusion weighted imaging (DWI) for assessing Crohns disease activity. Although shape information has been widely employed for detecting abnormalities in medical images such as lymph node detection [12] and segmentation of skin lesions [13], it is unsuitable for CD segmentation because the bowel is composed of elastic tissues and does not have a rigid shape. Thus machine learning approaches are desirable in such problems.

Semi-supervised learning (SSL) [14] and active learning [9] methods have been used to overcome the limitations of insufficient labeled samples in medical applications such as segmenting anatomical structures [15] and detecting cancerous regions [16]. You et. al. [17] use self training and support vector machines (SVM) to design a semi-supervised approach for retinal vessel segmentation. Their results highlight the role of unlabeled samples in improving overall classification accuracy. Portela et al. [18] use a semi supervised clustering approach for MR brain segmentation by including prior knowledge into the clustering cost function. Tu et al. [19] propose Posterior Distribution Learning (PDL) to build a robust supervised model in data space. These works highlight the fact that user feedback on labels can improve segmentation accuracy in spite of fewer labeled samples. Part of our work is similar to online learning [20] where the classifiers are adapted incrementally using only the newly labeled samples without complete classifier retraining.

This work uses a combined SSL-AL framework for detection and segmentation of Crohns disease tissues from MR images. We compare our proposed method with our work in [7] and demonstrate that compared to fully supervised methods, combination of semi supervised and active learning achieves higher segmentation accuracy with fewer labeled samples and less training time.

Semi supervised methods learn a reliable classifier by exploiting hidden structural relationships in unlabeled data. Active learning queries the labels of ‘informative’ samples that would lead to maximum improvement in classifier performance. Although there are many well documented query strategies for active learning [9] they do not perform well for our specific problem. A preliminary version of this work appeared in [21,22] and this manuscript has the following novelties: (1) online learning is used to update the classifier after every new annotation; (2) detailed analysis of the effect of different components of the query strategy; (3) analysis on savings in training time; and (4) comparison with popular AL query strategies; Section 3 describes the image features, Sections 4–6 describe different parts of our method. We describe our dataset and results in Sections 7 and 8, and conclude with discussions in Section 9.

We validate our algorithm's performance on manually annotated labels. Clinical experts manually segment the diseased region in every patient, and also provide labels for selected normal regions. Training of the random forest (RF) based SSL classifier starts by taking the labeled samples of the first training patient (denoted as set L). The samples from the remaining training patients are assumed to be unlabeled (denoted as set U). Note that set U consists only of the samples that have been annotated by the experts, and does not comprise of all voxels of the image. The AL part selects the most informative sample in U whose label is obtained from the stored label set. This sample is added to set L and the classifier is updated using online learning. The query continues till the query selection algorithm cannot identify a novel informative sample, i.e., a sample whose characteristics have not been added to the training set L.

Since classifying each voxel in a 3D volume is time consuming, we make use of a volume of interest (VOI) enclosing the bowel tissues. Our algorithm analyses each VOI voxel to generate a classification map whose negative log-likelihood is the penalty cost in a second order Markov Random Field (MRF) cost function. Graph cuts (GC) is used to optimize this function and segment the diseased region. The VOIs are obtained from our previous method in [7]. An overview of our method is given in Algorithm 1.


                     
                        Algorithm 1
                        Semi supervised and active learning for segmenting Crohn's Disease tissues


                           
                              
                                 
                                    
                                    
                                       
                                          
                                             Input Image with ‘diseased’ VOI obtained from [7].
                                       
                                       
                                          
                                             Output Segmented diseased region.
                                       
                                       
                                          
                                             
                                             Sequence of steps:
                                       
                                       
                                          
                                               (1) Bias correction and intensity normalization.
                                       
                                       
                                          
                                               (2) Train RF
                                             −
                                             SSL classifier with labeled samples of the first training patient (set L) and unlabeled samples of remaining training patients set U.
                                       
                                       
                                          
                                               (3) Determine the most informative sample from U and query its label.
                                       
                                       
                                          
                                               (4) Add labeled sample to L and update RF
                                             −
                                             SSL using online learning.
                                       
                                       
                                          
                                               (5) Continue label query till no further queries are required. This completes training of RF
                                             −
                                             SSL.
                                       
                                       
                                          
                                               (6) For test patient generate classification map for the VOI patches using the final RF
                                             −
                                             SSL.
                                       
                                       
                                          
                                               (7) Segment diseased regions using final classification map and graph cuts (Eqs. (11) and (12)) to give the diseased bowel tissue.
                                       
                                    
                                 
                              
                           
                        

The images are first bias-corrected using the method in [23] to remove intensity inhomogeneities due to the magnetic field of MR machines. Other intensity inhomogeneity correction methods such as in [24,25] perform equally well for our data. Pixels with the lowest 5% intensities usually indicate noise, and are all set to zero. Similarly, higher intensity tails in the intensity histogram indicate artifacts and outliers. Hence the mean of the 5% brightest intensity values is taken to be the maximum image intensity. All intensity values are divided by this threshold and ratios exceeding one are set to one. Normalization also increases the image contrast.

We use the same set of features as in [7,26,27] for describing each voxel. The same features are used in VOI detection by taking the mean feature value of all voxels within the supervoxel. One notable exception is the context features where the difference in mean feature values are calculated only for the neighboring supervoxels and not according to the previously defined template. Since our emphasis in this paper is the combination of SSL and AL techniques, we give a brief description of each feature and refer the reader to [7] for details. For each voxel (sample) the features are extracted from its 31×31 neighborhood.

MR images commonly contain regions that do not form distinct spatial patterns but differ in their higher order statistics [28]. Therefore, in addition to the features processed by the human visual system (HVS), i.e., mean and variance, we extract skewness and kurtosis values from each voxel's 31×31 neighborhood.

Texture maps are obtained from 2-D (instead of 3D) Gabor filter banks for each slice (at orientations 0°, 45°, 90°, 135° and scale 0.5, 1). Each 31×31 image patch is partitioned into 9 equal parts corresponding to 9 sectors of a circle. Fig. 1
                         (a) shows the template for partitioning a patch into sectors and extracting entropy features. For each sector we calculate the texture entropy given by
                           
                              (1)
                              
                                 
                                    χ
                                    ani
                                    r
                                 
                                 =
                                 −
                                 
                                    ∑
                                    tex
                                 
                                 
                                    p
                                    tex
                                    r
                                 
                                 log
                                 
                                    p
                                    tex
                                    r
                                 
                                 .
                              
                           
                        
                        
                           
                              p
                              tex
                              r
                           
                         denotes the probability distribution of texture values in sector r. This procedure is repeated for all the 8 texture maps over 4 orientations and 2 scales to extract a (8×9=) 72 dimensional feature vector.

Different tissues have different curvature distributions and we exploit this characteristic for accurate disease identification. Curvature maps are obtained from the gradient maps of tangent along the 3D surface. The second fundamental form (F2) of these curvature maps is identical with the Weingarten mapping and the trace of the F2 matrix gives the mean curvature. This mean curvature map is used for calculating curvature entropy. Details on curvature calculation is given in [7,29]. Similar to texture, curvature entropy is calculated from 9 sectors of a patch and is given by
                           
                              (2)
                              
                                 
                                    Curv
                                    ani
                                    r
                                 
                                 =
                                 −
                                 
                                    ∑
                                    θ
                                 
                                 
                                    p
                                    θ
                                    r
                                 
                                 log
                                 
                                    p
                                    θ
                                    r
                                 
                                 .
                              
                           
                        
                        
                           
                              p
                              θ
                              r
                           
                         denotes the probability distribution of curvature values in sector r, and θ denotes the curvature values.

We use 2D texture and curvature maps as the 3D maps do not provide consistent features because of low resolution in the z direction (voxel resolution was 0.5mm× 0.5mm× 3.3mm). Hence the extracted 3D features are unreliable.

Context information is particularly important for medical images because of the the regular arrangement of human organs. Fig. 1 (b) shows the template for context information where the circle center is the current voxel and the sampled points are identified by a red ‘X’. At each voxel corresponding to a ‘X’ we calculate the mean intensity, texture and curvature values from a 3×3 patch. The texture values were derived from the texture maps at 90° orientation and scale 1. The ‘X's are located at distances of 1, 3, 5, 8 voxels from the center, and the angle between consecutive rays is 45°. The values from the 32 regions are concatenated into a 96 dimensional feature vector, and the final feature vector has (96+85=)181 values. The features are normalized by subtracting the mean of each dimension and dividing by its standard deviation. This ensures that all dimensions are in the same range. A recent work [30] uses a cascade of supervised RF classifiers to generate probability maps and extract context features for neo-natal brain image segmentation. However, our current work focuses on combining SSL and AL. In future work we aim to explore the usage of cascaded classifiers for improving segmentation accuracy.

Random forests (RF) [31] are increasingly popular in classification problems because of their computational efficiency for large training data, ability to handle multiclass classification, and facility to quantify importance measures of features. An excellent review of works using RFs in different applications can be found in [32]. An RF is an ensemble of decision trees, where each tree is typically trained with a different subset of the training set, thereby improving the generalization ability of the classifier.

A tree is a collection of nodes and edges organized in a hierarchical structure. Nodes are divided into internal (or split) nodes and terminal (or leaf) nodes. A decision tree can be interpreted as a technique for splitting complex problems into a hierarchy of simpler ones. Samples are processed by performing a binary test at each internal node along a path from the root to a leaf in each tree. A test compares a certain feature with a threshold. Forest training is equivalent to determining the set of tests that best separate the data into the different training classes using a measure of class impurity like class entropy. The tuning parameters are maximal tree depth, and number of trees.

The node parameters are chosen to minimize an energy function. In the case of a forest with T trees the training process is typically repeated independently for each tree. Randomness is only injected during the training process, with testing being completely deterministic once the trees are fixed. Fig. 2
                      (a) shows a number of data points on a 2D space. Different colors indicate different classes/groups of points. The distribution over classes is uniform because we have exactly the same number of points in each class. If we split the data horizontally (as shown in Fig. 2 (b)) it produces two sets of data. Each set is associated with a lower entropy (higher information, peaked histograms). The gain of information achieved by splitting the data S
                     
                        j
                      at node j is computed as
                        
                           (3)
                           
                              
                                 I
                                 j
                                 S
                              
                              =
                              H
                              (
                              
                                 S
                                 j
                              
                              )
                              −
                              
                                 ∑
                                 
                                    i
                                    ∈
                                    {
                                    L
                                    ,
                                    R
                                    }
                                 
                              
                              
                                 
                                    |
                                    
                                       S
                                       j
                                       i
                                    
                                    |
                                 
                                 
                                    |
                                    
                                       S
                                       j
                                    
                                    |
                                 
                              
                              H
                              (
                              
                                 S
                                 j
                                 i
                              
                              )
                           
                        
                     where H is the entropy of training points; 
                        
                           S
                           j
                           L
                        
                      and 
                        
                           S
                           j
                           R
                        
                      the subsets going to the left and right children A horizontal split does not separate the data well, and yields an information gain of I
                     =0.4. When using a vertical split (such as the one in Fig. 2(c)) we achieve better class separation, corresponding to lower entropy of the two resulting sets and a higher information gain (I
                     =0.69). This simple example shows how we can use information gain to select the split which produces the highest information (or confidence) in the final distributions. This concept is at the basis of the forest training algorithm.

Semi supervised learning trains classifiers with a few labeled samples [14]. They leverage the hidden structural information in many unlabeled samples and map them to the labeled data. This is a typical scenario in many medical applications where it is difficult to find qualified experts to label the large number of medical images.

For labeled samples the information gain over data splits at each node is maximized and encourages separation of the data. However for semi supervised learning the objective function encourages separation of the labeled training data and simultaneously separates different high density regions. It is achieved via the following mixed information gain:
                        
                           (4)
                           
                              
                                 I
                                 j
                              
                              =
                              
                                 I
                                 j
                                 U
                              
                              +
                              β
                              
                                 I
                                 j
                                 S
                              
                           
                        
                     where 
                        
                           I
                           j
                           S
                        
                     , is the information gain as defined in Eq. (3) and β is a user defined weight. 
                        
                           I
                           j
                           U
                        
                      depends on both labeled and unlabeled data, and is defined using differential entropies over continuous parameters as
                        
                           (5)
                           
                              
                                 I
                                 j
                                 U
                              
                              =
                              log
                              |
                              Λ
                              (
                              
                                 S
                                 j
                              
                              )
                              |
                              −
                              
                                 ∑
                                 
                                    i
                                    ∈
                                    {
                                    L
                                    ,
                                    R
                                    }
                                 
                              
                              
                                 
                                    |
                                    
                                       S
                                       j
                                       i
                                    
                                    |
                                 
                                 
                                    |
                                    
                                       S
                                       j
                                    
                                    |
                                 
                              
                              log
                              |
                              Λ
                              (
                              
                                 S
                                 j
                              
                              )
                              |
                           
                        
                     Λ is the covariance matrix of the assumed multivariate distributions at each node. For further details we refer the reader to [32]. Thus the above cost function is able to combine the information gain from labeled and unlabeled data without the need for an iterative procedure.

Our active learning strategy requires a informativeness measure for each sample such that the algorithm can select the most informative sample for label query. The selected sample should have the following characteristics: (1) high classification uncertainty to actually obtain novel information from each labeling instance; and (2) different characteristics from previously labeled samples. to minimize redundancy in labeling effort. Considering the above criteria we define a sample's informativeness as
                           
                              (6)
                              
                                 γ
                                 (
                                 x
                                 )
                                 =
                                 ϕ
                                 (
                                 x
                                 )
                                 +
                                 α
                                 +
                                 
                                    ∑
                                    
                                       u
                                       =
                                       1
                                    
                                    N
                                 
                                 β
                                 
                                    
                                       
                                          x
                                          ,
                                          
                                             x
                                             u
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        
                        γ(x) indicates the informativeness of sample x, ϕ is its uncertainty, β is the similarity between x and x
                        
                           u
                         (all other unlabeled samples in neighborhood N), and α is a parameter that incorporates contextual information into the informativeness measure. We query the label of the sample with maximum informativeness. Note that in Eq. (6) we do not normalize by the number of samples in N. In a high-density region this ensures the informativeness of x is high, while in a sparsely populated region x's informativeness is low. N
                        =35×35 was chosen as it gave the highest classification accuracy among different window sizes.


                        Sample uncertainty: The uncertainty of x is its class entropy given by
                           
                              (7)
                              
                                 ϕ
                                 (
                                 x
                                 )
                                 =
                                 −
                                 
                                    ∑
                                    
                                       
                                          y
                                          ˆ
                                       
                                    
                                 
                                 P
                                 
                                    
                                       
                                          
                                             y
                                             ˆ
                                          
                                          |
                                          x
                                       
                                    
                                 
                                 log
                                 P
                                 
                                    
                                       
                                          
                                             y
                                             ˆ
                                          
                                          |
                                          x
                                       
                                    
                                 
                                 ,
                              
                           
                        where 
                           
                              y
                              ˆ
                           
                         indicates all possible labels (in this case two) for x, and 
                           P
                           (
                           
                              
                                 
                                    
                                       y
                                       ˆ
                                    
                                    |
                                    x
                                 
                              
                           
                         is calculated by RF
                        −
                        SSL (the Random forest based semi supervised classifier). Higher entropy indicates greater uncertainty.


                        Neighborhood similarity: The similarity measure between two samples is
                           
                              (8)
                              
                                 β
                                 (
                                 x
                                 ,
                                 
                                    x
                                    u
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             e
                                             
                                                −
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                               I
                                                            
                                                            −
                                                            
                                                               x
                                                               I
                                                               u
                                                            
                                                         
                                                      
                                                   
                                                   2
                                                
                                             
                                          
                                          +
                                          
                                             e
                                             
                                                −
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                               T
                                                            
                                                            −
                                                            
                                                               x
                                                               T
                                                               u
                                                            
                                                         
                                                      
                                                   
                                                   2
                                                
                                             
                                          
                                          +
                                          
                                             e
                                             
                                                −
                                                
                                                   
                                                      
                                                         
                                                            
                                                               x
                                                               C
                                                            
                                                            −
                                                            
                                                               x
                                                               C
                                                               u
                                                            
                                                         
                                                      
                                                   
                                                   2
                                                
                                             
                                          
                                       
                                    
                                 
                                 ×
                                 
                                    e
                                    
                                       −
                                       δ
                                       (
                                       x
                                       ,
                                       
                                          x
                                          u
                                       
                                       )
                                    
                                 
                                 .
                              
                           
                        
                        x
                        
                           I
                        , x
                        
                           T
                        , x
                        
                           C
                         are, respectively, the intensity, texture and curvature components of the feature vector of sample x, and u indicates unlabeled samples in neighborhood N. 
                           
                              
                                 
                                    .
                                 
                              
                              2
                           
                         indicates the ℓ2 norm between the two vectors. The exponential assigns higher values to pairs of similar samples indicating x is more representative of the neighborhood, thus increasing the informativeness of x. δ(x, x
                        
                           u
                        ) is the Euclidean distance between samples such that spatially closer samples have greater influence than samples further apart.


                        Context information for informativeness: The similarity of an unlabeled sample to a labeled sample gives an idea of its possible label. If the radiologist were to annotate samples similar to previously labeled samples it does not lead to significant information gain. Thus unlabeled samples similar to labeled samples should be assigned lower importance to maximize the influence of each labeled sample. This also minimizes label redundancy. The parameter α in Eq. (6) considers these factors and is equal to the minimum distance (in terms of feature similarity) of labeled samples and is given by
                           
                              (9)
                              
                                 α
                                 =
                                 arg
                                 
                                    min
                                    
                                       
                                          x
                                          L
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      x
                                                      −
                                                      
                                                         x
                                                         L
                                                      
                                                   
                                                
                                             
                                             1
                                          
                                       
                                    
                                 
                                 .
                              
                           
                        where x
                        
                           L
                         denotes all the labeled samples, and 
                           
                              
                                 
                                    .
                                 
                              
                              1
                           
                         denotes the ell
                        1 norm based on feature values. Note that the most informative sample is one which scores highly on all three measures. Many samples may score highly on one or more of these criteria, but may not be considered the most informative depending upon its location and class probability values.


                        Stopping criteria: To ensure that the label query does not continue indefinitely we determine the probability values of the two classes for the most informative sample. If the probability value for any one class is less than 0.45 (or greater than 0.55 for the other class) we do not query the label for that sample. If we encounter such samples for 5 consecutive iterations then we stop the label query because this indicates that the classifier has obtained sufficient samples to have high confidence on its classification output.

@&#IMPLEMENTATION DETAILS@&#

Active learning methods give some degree of control by allowing the learner to select samples to label and add to the training set. The selected samples are expected to improve a classifier's performance and minimize redundancy. The training can be summarized in the following steps
                           
                              (1)
                              Learning starts with the labeled samples (voxels) of the first training dataset. The dataset is divided into labeled voxels (set L from first training dataset) and unlabeled patches (set U from the remaining training datasets). Features (described in Section 3) are extracted from each VOI voxel using its surrounding 31×31 neighborhood.


                                 L and U are used as inputs to an RF based SSL classifier (denoted as RF
                                 −
                                 SSL) which predicts the class labels and probabilities of the unlabeled patches. We predict two classes since the VOI consists of the intestine region, and is less likely to contain non-bowel tissues. Any background (non-intestine) regions within the VOI are always predicted to be normal.

The most informative sample is added to set L and the classifier is updated using online learning. Recall that the samples in set U are from the training images and their actual labels as provided by the expert is known.

The iterative procedure of alternating between label query and classifier update continues till no more informative samples are identified.

After training is complete the RF-SSL classifier is applied to the test images to generate probability maps. Note that for the test images we use the same VOIs from [7]. The probability maps are then used for segmenting the final diseased region using graph cuts [33]. A spatially smooth solution is obtained by formulating the segmentation as a labeling problem within a second order Markov Random Field (MRF) cost function. A second order MRF energy function is given by
                        
                           (10)
                           
                              E
                              (
                              L
                              )
                              =
                              
                                 ∑
                                 
                                    s
                                    ∈
                                    P
                                 
                              
                              
                                 D
                                 (
                                 
                                    L
                                    s
                                 
                                 )
                              
                              +
                              λ
                              
                                 ∑
                                 
                                    (
                                    s
                                    ,
                                    t
                                    )
                                    ∈
                                    N
                                 
                              
                              
                                 V
                                 (
                                 
                                    L
                                    s
                                 
                                 ,
                                 
                                    L
                                    t
                                 
                                 )
                              
                              ,
                           
                        
                     where P denotes the set of pixels and N is the set of neighbors of pixel s. λ is a weight that determines the relative contribution of penalty cost (D) and smoothness cost (V). D(L
                     
                        s
                     ) is given by
                        
                           (11)
                           
                              D
                              (
                              
                                 L
                                 s
                              
                              )
                              =
                              −
                              log
                              
                                 
                                    
                                       Pr
                                       (
                                       
                                          L
                                          s
                                       
                                       )
                                       +
                                       ϵ
                                    
                                 
                              
                              ,
                           
                        
                     where Pr is the likelihood (from probability maps) previously obtained using semi supervised random forests and ϵ
                     =0.00001 is a very small value to ensure that the cost is a real number.


                     V ensures a spatially smooth solution by penalizing discontinuities and is defined as
                        
                           (12)
                           
                              V
                              (
                              
                                 L
                                 s
                              
                              ,
                              
                                 L
                                 t
                              
                              )
                              =
                              
                                 e
                                 
                                    −
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      I
                                                      s
                                                   
                                                   −
                                                   
                                                      I
                                                      t
                                                   
                                                
                                             
                                          
                                          2
                                       
                                    
                                    /
                                    
                                       2
                                       
                                          σ
                                          2
                                       
                                    
                                 
                              
                              ·
                              
                                 1
                                 
                                    
                                       
                                          s
                                          −
                                          t
                                       
                                    
                                 
                              
                              ,
                           
                        
                     
                     I denotes the intensity. Smoothness cost is determined over a 8 neighborhood system. The labels are obtained by optimizing Eqn (10) using graph cuts [33].

We use datasets from two different sources, one from the Academic Medical Center (AMC), Amsterdam and the other from University College of London Hospital (UCL).
                        
                           •
                           
                              AMC: The data was acquired from 35 patients (mean age 38 years, range, 25.6–59.6 years, 15 females) with luminal Crohn's disease that had been approved by AMC's Medical Ethics Committee. All patients had given informed consent to the prior study. Patients fasted 4h before a scan and drank 1600ml of Mannitol (2.5%) (Baxter, Utrecht, the Netherlands) 1h before a scan. Images were acquired using a 3-T MR imaging unit (Intera, Philips Healthcare, Best, The Netherlands) with a 16-channel torso phased array body coil. The image resolution was 1.02mm ×1.02 mm×2mm/pixel, and the volume dimension was 400×400×100 pixels.


                              UCL: Data from 35 patients (mean age, 29.7 years, range, 17.4–54.3 years, 12 females) diagnosed with small bowel Crohn's disease was used. Images were acquired using a 3T MR imaging unit (Avanto; Siemens, Erlangen). The spatial resolution of the images was 1.02mm ×1.02mm×2mm per pixel. Two datasets have dimension of 512×416×48, one 512×416×64, one 512×512×56 and the rest 512×512×48. Ethical permission was given by the University College London Hospital ethics committee, and informed written consent was obtained from all participants.

Variously sized annotations identifying diseased and normal regions were made by research physicians specializing in abdominal MRI. On an average each patient had 10–20 slices of annotations and each slice had 2–4 annotations of varying size.

Our whole pipeline was implemented in MATLAB on a 2.66GHz quad core CPU running Windows 7 with 4GB RAM. The random forest code was a MATLAB interface to the code in [34] written in the R programming language. The 70 patients were divided into 5 groups of 14 patients each, and we employ a 5 fold cross validation approach where 4 groups are used for training and the remaining patient group is used for testing the algorithm. Each patient was part of the test group exactly once. The maximal tree depth was 20. The quality of our segmentations was evaluated using Dice Metric (DM) and Hausdorff Distance (HD).

@&#RESULTS@&#


                        Table 1
                         shows the segmentation results of the following methods:
                           
                              (1)
                              
                                 SSL
                                 −
                                 AL – our proposed segmentation algorithm using RF
                                 −
                                 SSL and online learning.


                                 SSL
                                 −
                                 AL
                                 
                                    AC
                                  – same as SSL
                                 −
                                 AL, but using active contours [24] instead of graph cuts to obtain the final segmentation from the probability maps.


                                 FSL – our automatic fully supervised learning based method of [7]. The classifier is trained on all the manually labeled samples given by experts and does not use active learning. A 5 fold cross validation approach is used to determine the method's performance.


                                 FSL
                                 −
                                 AL – fully supervised segmentation using active learning. The difference from SSL
                                 −
                                 AL is that instead of a semi supervised classifier we use a fully supervised classifier trained only on the labeled data. Online learning is used for updating the classifier.


                                 SSL
                                 −
                                 AL
                                 
                                    nα
                                 : SSL
                                 −
                                 AL without incorporating α in the informativeness measure in smoothness cost V.


                        SSL
                        −
                        AL gives the best segmentation results, and significantly improves over FSL's performance with p
                        <0.001. SSL
                        −
                        AL achieves higher segmentation accuracy using fewer labeled samples, which translates into less time invested for labeling. FSL
                        −
                        AL performs quite poorly because of two factors. First, it does not have the advantage of many labeled samples as in FSL and hence is unable to learn a good model from the few training samples. Secondly, it cannot exploit the structural information from unlabeled samples as in SSL classifiers. This also reinforces the importance of both SSL and AL in situations where obtaining labeled data is difficult and time consuming. SSL
                        −
                        AL
                        
                           nα
                         gives lower segmentation accuracies than SSL
                        −
                        AL because of the absence of context information for query selection. This is particularly important while segmenting diseased regions whose appearance is similar to its neighbors.


                        Figs. 4 and 5
                        
                         show segmentation results on Patient 12 (AMC) and Patient 15 (UCL) which were particularly challenging because: (1) these patients suffer from multiple diseased colon sections; (2) in both cases there is a very narrow part of diseased colon; (3) in AMC Patient 12 the diseased region has similar intensity values as the neighboring normal regions which poses challenges for conventional graph cut segmentation.

One of the primary objectives of our method is to reduce training time. By querying labels of most informative patches we also expect to reduce the redundancy of labels such that new labels provide truly novel information to the classifier. Savings in labeling effort can be estimated by the number of queried voxels during active learning and compared with the number of manually labeled voxels.

In a typical 5-fold cross validation scenario FSL uses, on an average, 80% of manual labels for training while SSL
                        −
                        AL requires labels of 48% voxels and still achieves higher segmentation accuracy. The query sample's characteristics are novel for the classifier. Hence the first few training patients provide the most queries. For the later patients, the AL algorithm does not encounter many novel patches and thus the query frequency decreases. The queried regions may belong to either the diseased or normal regions.

An advantage of online learning is the significant savings in time. Updating the classifiers with every new label requires about 0.03–0.1s. On the other hand, lots of time is required to retrain the entire classifier using the entire training set. In such a scenario, the training time for SSL classifiers increases as the number of labeled samples are accumulated over time. The retraining time can vary from 0.3s (with 4 labeled samples) to 43s (with 676 labeled samples. This gives an estimate of the time saved by using our method.

The total training time for the 56 patients in each training fold is 43min using FSL and 23min using SSL
                        −
                        AL. The lower training time for SSL
                        −
                        AL is due to the fact that less number of samples are required for each class. Although labels are queried for both diseased and normal voxels, the number of queried diseased samples (64%) is higher compared to the normal samples (36%). This is obvious since the diseased samples have different characteristics due to varying disease severities. Fewer labeled voxels lead to time savings of (1−23/43=)46% during training. However, FSL
                        −
                        AL takes a longer training time (32min) than SSL
                        −
                        AL since it does not make use of unlabeled samples. This is still less than FSL since many of the manually labeled samples are redundant. The average segmentation time for FSL, SSL
                        −
                        AL and FSL
                        −
                        AL is approximately the same, i.e., 12min. Once the classifiers are trained, the RF based classifiers take the same time to classify each voxel, generate probability maps and obtain the final segmentation using graph cuts.

To select the MRF regularization strength λ we choose a separate group of 7 patient volumes (from both hospitals), and perform segmentation using our method but with λ taking different values from 10 to 0.001. The results are summarized in Table 2
                        . The maximum average DM was obtained for λ
                        =0.02 which was fixed for subsequent experiments. Note that these 7 datasets were a mix of patients from the two hospitals and not part of the test dataset used for evaluating our algorithm.

We examine the effect of the number of trees (N
                        
                           T
                        ) in RF
                        −
                        SSL classifier on segmentation accuracy. The number of trees in RF
                        −
                        SSL is varied and the resulting DM values are summarized in Table 3
                        . Table 3 shows the training time (T
                        
                           Tr
                        ) for different N
                        
                           T
                         as a multiple of the training time for N
                        
                           T
                        
                        =50. For N
                        
                           T
                        
                        >50 there is no significant increase in DM (p
                        >0.22) but the training time increases significantly. The best trade-off between N
                        
                           T
                         and DM is achieved for 50 trees and is the reason to choose such a size of our RF ensemble. The tree depth was fixed at 20. We choose this number after results of cross validation comparing tree depth, and resulting classification accuracy.

Recall that α in Eq. (6) is the similarity of an unlabeled sample to the nearest labeled sample while other query selection strategies use a user defined parameter [9]. We varied the value of α from 0 to 10 in steps of 0.1 (denoted by α′ in Table 4
                        ) and results of cross validation showed that best segmentation accuracy was obtained for α′=2. The classification performance is lower than our adaptive α (last column in Table 4). Since a constant α discards contextual information from the nearest labeled sample, it has lower performance than an adaptive α in Eq. (6). The values in Table 4 are obtained after convergence. Generally, α and α′ converge after equal number of queries. Referring to the results of SSL
                        −
                        AL
                        
                           nα
                         in Table 1 we also conclude that without α the accuracy of the method degrades significantly and is lower than the other competing methods.

The effectiveness of our query strategy is compared with other widely used methods such as QBC [35], information density weighted (IDW) method of [9] and EGL [36]. We get the following DM values with these methods: QBC-87.3%, EGL-88.4% and IDW-86.2%. Note that the IDW method is the same as the ‘constant α method’ described in Section 8.5. These results also highlight the importance of the context parameter α which is an important contribution of this paper.

For QBC we generate two different hypotheses: Eq. (6) is calculated with ϕ determined from (1) fully supervised RFs; and (2) RF-SSL (as proposed in our method). The sample which gives maximum disagreement is chosen as the query sample. For uncertainty sampling we use only the value in Eq. (7) as a measure of a sample's informativeness. For IDW the informativeness of a sample is given by
                           
                              (13)
                              
                                 Inf
                                 (
                                 
                                    x
                                    *
                                 
                                 )
                                 =
                                 ϕ
                                 (
                                 x
                                 )
                                 +
                                 
                                    ∑
                                    N
                                 
                                 
                                    
                                       (
                                       
                                          Int
                                          ij
                                       
                                       +
                                       
                                          Tex
                                          ij
                                       
                                       +
                                       
                                          Curv
                                          ij
                                       
                                       )
                                    
                                    α
                                 
                                 .
                              
                           
                        
                     

@&#DISCUSSION AND CONCLUSION@&#

In this paper we have combined semi supervised classification and active learning for segmentation of regions affected with Crohns disease in abdominal MRI. Our method achieves higher segmentation accuracy than fully supervised methods [7] despite requiring fewer labeled samples, lower training time and less effort by experts. Semi supervised learning makes optimal use of labeled and unlabeled data while active learning makes optimal use of annotations by querying only the informative samples. The active learning query strategy is based on: (1) high classification uncertainty by the semi-supervised classifier; (2) sample's location in a dense region such that it is representative of many other samples; and (3) minimizing influence overlap of labeled samples through context information such that optimal classification is achieved using minimum number of samples.

Experimental results show better performance of our method than fully supervised methods. It also translates into less effort and time invested by the annotator. Our method's relevance for clinical usage is being explored through faster implementations of different stages. This could allow a clinician to segment diseased regions with some degree of manual intervention, and perform accurate analysis without having to wait for sufficient annotations for a fully supervised automatic algorithm.

Our method's use in actual clinical scenarios is limited by the relatively high computation time of 12min. Since our framework is based on MATLAB, it is possible to improve the computation speed by using other software packages or exploring faster ways for semi-supervised classification and active learning.

Although our algorithm results in good segmentation results with high DM and low HD values, its primary limiting factor is the quality of the labels provided by the annotator. If the annotator can correctly identify diseased and normal regions the resulting classifier and final segmentation output is accurate, and better than fully supervised methods. However, if the labels are inaccurate then the final segmentation output is also poor. Our algorithm can also be used for other relevant applications where some initial annotations are available. It may require some changes to feature extraction, but the principles of SSL and AL can be used without major changes.

@&#REFERENCES@&#

