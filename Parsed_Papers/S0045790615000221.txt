@&#MAIN-TITLE@&#An approach for prevention of privacy breach and information leakage in sensitive data mining

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           It prevents homogeneity, skewness, similarity and background knowledge attacks.


                        
                        
                           
                           The privacy is ensured while publishing sensitive data.


                        
                        
                           
                           Only fewer partitioning need to be done for a stronger privacy requirement.


                        
                        
                           
                           It gives better efficiency over the previous approaches.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Anonymization

Data mining

Privacy

Privacy preserving

Privacy preserving techniques

Sensitive data publishing

@&#ABSTRACT@&#


               
               
                  Government agencies and many non-governmental organizations often need to publish sensitive data that contain information about individuals. The sensitive data or private data is an important source of information for the agencies like government and non-governmental organization for research and allocation of public funds, medical research and trend analysis. The important problem here is publishing data without revealing the sensitive information of individuals. This sensitive or private information of any individual is essential to several data repositories like medical data, census data, voter registration data, social network data and customer data. In this paper a personalized anonymization approach is proposed which preserves the privacy while the sensitive data is published. The main contributions of this paper are three folds: (i) the definition of the data collection and publication process, (ii) the privacy framework model and (iii) personalized anonymization approach. The experimental analysis is presented at the end; it shows this approach performs better over the distinct l-diversity measure, probabilistic l-diversity measure and k-anonymity with t-closeness measure.
               
            

@&#INTRODUCTION@&#

The data collected by public organizations and private organizations are increasing every day and stored in electronic repository. The data being collected includes private or sensitive data. More and more data mining techniques are becoming now a days and this can be used for assisting decision making process. The data mining techniques are used to extract the hidden knowledge from huge data collections in the form of trends, models and patterns. While doing the data mining process the personal data of any individual need to be protected from privacy concerns [1–4]. Privacy means the individual’s personal information known as sensitive data has to be protected while publishing the data. In Schoeman [5] point of view three possible privacy definitions are proposed.
                        
                           •
                           Privacy is the right of a person. The person can decide which personal information to be communicated or published to others.

Privacy is restricted access to an individual and to any or all the features associated with the person.

Privacy is that the management over access to data or information relating to oneself.

In the above definitions the knowledge inferred is the Controlled Information Release, means that the personal data need to be hide while publishing for research and other purposes [6]. The Microdata or sensitive data is the individual’s private data like salary, age, medical information, etc. A Microdata set can be viewed as a file with n records, where each and every record contains m attributes [7]. The attributes can be classified as follows.

The attribute that unambiguously identify the individual is the identifier. Examples are name, social security number, passport number, etc.

The attributes that identify the individual with some degree of ambiguity is the quasi-identifier. Examples are age, gender, address, telephone number, etc.

The attributes that contains the sensitive information of an individual is the confidential outcome attributes. Examples are salary, health condition, religion, etc.

The attributes that do not fall in any categories above are the non confidential outcome attributes.

The paper is organized as follows: The definition of the data collection and publication process is given in Section 2. In Section 3, the privacy framework model is described and the personalized anonymization approach is discussed. Finally in Section 4 the experimental analysis is presented with example. The efficiency factor is compared with distinct l-diversity measure, probabilistic l-diversity measure and k-anonymity with t-closeness measure.

The data collection and data publishing phases are described in Fig. 1
                     . Here, in the data collection phase, the data are collected by the data publisher from the record owners. In the data publishing phase, the collected data are released by the data publisher to a researcher or to the public or to the data miner, called the data recipient. For example, a hospital collects the data from their patients and publishes the collected records to the external agency or medical centre for further research. In Fig. 1 John, Peter, Raj, Mary and Christy are the patients, who are the data owners. The hospital is the data publisher who collects the data from the data owners (patients like John, Peter) and publishes to the data recipient (medical centre). The data receiver can now apply the data mining algorithms or techniques on the received data to get the knowledge or patterns in order to do further actions.

The problem here is the data publisher (medical centre, in this example) can have the medical data of the patients which contains the sensitive data. This sensitive data need to be protected to preserve the privacy of the patient.

Numerous researches have been done to address the problems in privacy preservation data mining. In general, five phases of several approaches can be classified [8,9]. First, based on the distribution of data, it can be categorized as centralized or distributed data [10]. In a centralized database environment, all the data are stored in a single database; whereas, in a distributed database environment, data are stored in different databases. The second phase is about the modifications that are applied on data. The approaches like perturbation, swapping, aggregation, sampling, suppression, noise addition and more are the data modification approaches. The third phase is about the data mining algorithms [11]. This phase deals with the data mining algorithms such as decision tree, clustering, rough sets, association rule, and regression [12,13]. The fourth phase is concerned with data hiding; whether it is the raw or aggregate data that need to be hid. Data hiding refers to the cases where the sensitive data from original database like identity, name, and address that can be linked, directly or indirectly, to an individual person are hided [14–16]. The last phase is the privacy preservation techniques.

The techniques like generalization, data distortion, data sanitation, blocking, cryptographic [11], and anonymization [17,18], are the some of the privacy preserving techniques used to ensure the privacy of individuals while mining the sensitive data. The five phases of approaches in privacy preserving data mining are shown in Fig. 2
                      which is called privacy framework.

The k-anonymity [19] is proposed as privacy preserving data mining technique for protecting sensitive information leakage or publishing of sensitive information. In the literature it is observed that the k-anonymity technique protects against identity disclosure, but in case of attribute disclosure it does not afford adequate protection. Like k-anonymity technique many other privacy preserving data mining techniques are also proposed, few are Data Perturbation Approach, Blocking Based Technique, Cryptographic Technique, Condensation Approach, Randomized Response Technique, and Hybrid Technique.

From study [20–23] it is understand that the existing privacy preserving data mining techniques do not protect the following privacy issues:
                           
                              i.
                              Homogeneity attack.

Skewness attack.

Similarity attack.

Background knowledge attack.

To address these privacy issues a personalized anonymization approach is proposed. This approach consists of three phases:
                           
                              (1)
                              Choosing a dimension on which to partition.

Choosing a value to split.

Checking if the partitioning violates the privacy requirement.

(Top-Down Greedy algorithm for strict multidimensional partitioning)
                              
                                 
                                    
                                    
                                       
                                          Anonymize (partition)
                                       
                                       
                                          if (no allowable multidimensional cut for partition)
                                       
                                       
                                          
                                             return φ : partition→summary
                                       
                                       
                                          else
                                       
                                       
                                          
                                             dim←choose dimension()
                                       
                                       
                                          
                                             fs←frequency_set(partition, dim)
                                       
                                       
                                          
                                             splitVal←find_median(fs)
                                       
                                       
                                          
                                             lhs←{t Є partition : t.dim⩽splitV al}
                                       
                                       
                                          
                                             rhs←{t Є partition : t.dim>splitV al}
                                       
                                       
                                          
                                             return Anonymize(rhs)UAnonymize(lhs)
                                       
                                       
                                          end all
                                       
                                    
                                 
                              
                           
                        


                           
                              
                                 
                                    
                                    
                                       
                                          Let P be a set of tuples
                                       
                                       
                                          P is partitioned into r partitions{P1,P2,…,Pr}
                                       
                                       
                                          for every Pi
                                          
                                       
                                       
                                          
                                             if Pi(1⩽i⩽r)
                                       
                                       
                                          
                                             
                                             find = false
                                       
                                       
                                          for every Q Є Parent(P) and |Q|⩾n
                                       
                                       
                                          
                                             if D[Pi,Q]⩽t
                                       
                                       
                                          
                                             
                                             find = true
                                       
                                       
                                          
                                             if find==false
                                       
                                       
                                          
                                             
                                             return false
                                       
                                       
                                          return true
                                       
                                       
                                          end all
                                       
                                    
                                 
                              
                           In this approach the n and k parameters plays the crucial role. The depth of observer’s background knowledge is defined by n. Smaller the n means the observer can know the smaller group of records sensitive information. The amount of sensitive information can get by observer from the released data is defined by the parameter t. Smaller the t means a stronger privacy requirement. Therefore the privacy and utility level is affected by choosing parameters n and t. Choosing smaller t and larger n attains more privacy and less utility

@&#EXPERIMENTAL RESULTS@&#

The data set used here in the experiments is the adult data set which is freely available on the internet composed of data collected from the census of USA.

The original medical data of inpatient is shown in Table 4.1
                     . From table one can clearly and easily identify who is having a particular disease. For example who are all having Cancer or Flu or Heart Disease can be identified easily which are sensitive attribute. This Microdata is a valuable source of information for the medical research and allocation of public funds and trend analysis. Our work is publishing this data without revealing sensitive information about them.


                     Table 4.2
                      shows the patient medical data after applying it to our personalized anonymization approach. Using this table one cannot identify about any individual, at the same time this will provide the required data for research analysis.

Table does not contain tuples that are vulnerable to similarity attacks. This result shows that the personalized anonymization approach provide better privacy protection against similarity attacks. The similarity attacks are a more general form of homogeneity attack. Thus, this approach can also prevent homogeneity attacks. Here four privacy measures [24,25] are studied to understand the efficiency of the proposed approach.
                        
                           •
                           Distinct l-diversity measure with default value of l
                              =5.

Probabilistic l-diversity measure with default value of l
                              =5.


                              k-anonymity with t-closeness measure with default value of k
                              =5 and t
                              =0.15.

Proposed personalized anonymization approach with default value of k
                              =5, n
                              =1000 and t
                              =0.15.


                     Fig. 3
                      shows the running times of the distinct l-diversity, probabilistic l-diversity, k-anonymity with t-closeness and personalized anonymization approach with respect to varied quasi identifier sizes. Here k, l and t values are fixed as mentioned above and varied the quasi identifier size s where 1⩽
                     s
                     ⩽5.


                     Fig. 4
                      shows the running times of the four privacy measures with same quasi identifiers but different parameters for k, l.

The running time for personalized anonymization approach is fast enough to be used in practice, because only fewer partitioning need to be done for a stronger privacy requirement. Therefore it gives better efficiency over the distinct l-diversity measure, probabilistic l-diversity measure and k-anonymity with t-closeness measure.

@&#CONCLUSION@&#

It is observed that various techniques has been proposed earlier times as a mechanism for protecting privacy in publishing of sensitive data, but the mechanisms are insufficient to protect the privacy issues. The personalized anonymization approach is proposed to enable protection against privacy issues and the result shows that it provides better privacy protection against the attacks at the same time this will provide the required data for research analysis. The Top-Down Greedy algorithm is used in which it partitions the high-dimensional space into regions and by the region’s representation it encodes data points in one region. The efficiency is determined with default value of l
                     =5 for Distinct l-diversity and Probabilistic l-diversity, k
                     =5 and t
                     =0.15 for k-anonymity with t-closeness, k
                     =5, n
                     =1000 and t
                     =0.15 for proposed personalized anonymization approach. The efficiency with respect to varied quasi identifier sizes of personalized anonymization approach is fast enough to be used in practice. The efficiency factor is calculated by varying k and l values for the proposed approach, it also comparatively better over the distinct l-diversity measure, probabilistic l-diversity measure and k-anonymity with t-closeness measure because only fewer partitioning need to be done for a stronger privacy requirement.

@&#REFERENCES@&#

