@&#MAIN-TITLE@&#Attribute-based learning for gait recognition using spatio-temporal interest points

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A new gait feature is extracted from a raw gait video directly.


                        
                        
                           
                           The proposed gait feature extraction is performed in the spatio-temporal domain.


                        
                        
                           
                           The attribute-based learning is used to enhance the SVM-based gait classification.


                        
                        
                           
                           The proposed method has outstanding performances under changes of clothing types.


                        
                        
                           
                           The proposed method has outstanding performances under changes of carry conditions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Gait recognition

Human identification

Spatio-temporal

Attribute-based learning

@&#ABSTRACT@&#


               Graphical abstract
               
                  
                     
                        
                           
                        
                     
                  
               
            

@&#INTRODUCTION@&#

Gait recognition has been studied broadly in a surveillance perspective because it is capable of identifying humans at a distance by inspecting their walking manners. A large number of gait recognition methods have been developed and published, which can be roughly divided into two categories: 1) model-based methods [1–10]; 2) model-free methods [11–22]. These methods require a pre-processing to extract gait appearances such as shape contours, silhouettes, skeletons, and body joints for further gait analysis. This pre-processing will consume additional computing time. It may also extract incomplete gait appearances which will negatively influence performances of gait analyses in later stages. Also, these gait appearances are sensitive to variations and partial occlusions caused by several factors such as carrying a bag and varying a type of clothes [23].

@&#RELATED WORKS@&#

The model-based methods generally aim to model kinematics of human joints in order to measure physical gait parameters such as trajectories, limb lengths, and angular speeds [24]. For example, Cunado et al. [3] modeled legs as an interlinked pendulum. Then, a phase-weighted Fourier magnitude spectrum was used to recognize gait features which were derived from frequency components of changes in an inclination of human thighs. Bobick and Johnson [4,10] used activity-specific static body parameters for gait recognition without directly analyzing dynamics of gait patterns. Kovac and Peer [2] proposed a skeleton model-based gait recognition method by modeling gait dynamics and eliminating influences of subjects' appearances on recognition. Tafazzoli and Safabakhsh [8] applied active contour models and the Hough transform to model movements of articulated parts of the human body. Bouchrika and Nixon [5] used elliptic Fourier descriptors to extract crucial features from human joints. However, the methods in this category have to deal with localizations of the human joints, which are not robust on a markerless motion [7]. It is also difficult to extract underlying models from gait sequences [15].

The model-free methods typically analyze gait sequences without explicit modeling of the human body structure [24]. The different methods in this category have been developed from different perspectives. For example, BenAbdelkader et al. [21] proposed an EigenGait method using image self-similarity plots. Chai et al. [2] introduced a Perceptual Shape Descriptor technique for recognizing gaits. Tan et al. [25] used eight kinds of projective features to describe human gait and PCA was applied for gait feature dimension reduction. Han and Bhanu [11] proposed a concept of Gait Energy Image (GEI), and combined real and synthetic templates to improve the accuracy of gait recognition. Liu et al. [12] employed a population Hidden Markov Model (pHMM) to model human gaits and generated dynamics-normalized stance-frames to recognize individuals. Recently, Wang et al. [13] developed a temporal template, named Chrono-Gait Image (CGI), by encoding gait contours using a multi-channel mapping function. Roy et al. [14] modeled a gait cycle using a chain of key poses which were then averaged to generate the gait feature, called Pose Energy Image (PEI).

Without any pre-processing on a raw gait video, the proposed method is to extract and recognize gait features directly from the video on a spatio-temporal feature domain. In this paper, the pre-processing is referred only to a foreground–background segmentation and/or a shape contour/boundary/skeleton extraction. Fig. 1
                         shows the framework of the proposed method for gait recognition (the detailed frameworks of its key processes will be shown in Figs. 2, 3, and 4
                        
                        
                        ). In these figures, rectangles represent inputs/outputs, while ellipses represent processing steps.

In Fig. 1, gait recognition is to find the best matched identity of a probe gait against other gaits in a gallery dataset. First, spatio-temporal interest points (STIPs) are detected from each gait video individually. STIPs are local structures in a spatio-temporal domain where image values have significant local variations in both space and time. These variations are linked to significant movements of human body in a gait video. Therefore, STIP is an interest point of a dominant walking pattern, which can be used to represent characteristics of each individual gait.

Second, a histogram of STIP descriptors (HSD) is constructed as a gait feature. Each STIP descriptor is computed by applying a histogram of oriented gradients (HOG) and a histogram of optical flow (HOF) on a 3D video patch (i.e. width×height×time) in a neighborhood of each detected STIP. It well describes walking patterns around the interest point in space and time.

Third, a support vector machine (SVM) [26] is used as a classification method. SVM has been employed to classify gaits efficiently [17,27–30]. However, SVM has been applied based on the standard one-vs.-all strategy [31] which can be computationally infeasible at a test phase in a large scale multi-class classification (i.e. a large number of subjects). This is because its test-time complexity grows linearly with a number of subjects.

Recently, several tree-based methods [32–37] have been proposed to address sublinear testing cost for large multi-class tasks. However, they are limited to several practical constraints [38] including: 1) the tree-based methods cannot be further speeded up by using parallel computing which is a very important and useful methodology to solve a computational problem in real applications; 2) the tree-based methods consume an expensive memory for storing and loading all classifiers at all nodes of the tree in the test phase.

To avoid these limitations, this paper employs the attribute-based classification using SVM [38–40] which is shown to achieve comparable performance to the state-of-the-art [36] regarding sublinear test-time complexity and accuracy. In this paper, the attribute-based learning is to define attributes and their associations with human subjects automatically and simultaneously. Then, a classifier is learned for each attribute instead of each subject. These trained classifiers are used to predict subjects' identities based on their attribute representations. This can be seen that the test-time complexity grows linearly with a total number of attributes (M) instead of a total number of human subjects (N) in the gait database. Thus, M can be selected to be much less than N, in order to significantly reduce the complexity.

The rest of this paper is organized as follows. Section 2 explains the detection process of the spatio-temporal interest points (STIPs) in a gait video. Section 3 proposes the gait feature extraction. Section 4 discusses the attribute-based learning for gait classification. Then, experimental results are shown in Section 5, and conclusions are drawn in Section 6.

Given a raw gait video G, key spatio-temporal interest points (STIPs) {S
                     
                        n
                     }
                        n
                        =1
                     
                        N
                      are detected to represent dominant walking patterns of a person in the video, where G(x,
                     y,
                     t) is a point at the spatio-index (x,
                     y) of the image frame t in G, and S
                     
                        n
                      is a detected STIP. STIP is detected by considering large variations along both spatial and temporal directions in local spatio-temporal volumes of G 
                     [41]. Fig. 2 shows the proposed framework of the key points detection in a gait video.

First, a linear scale-space representation L of G is constructed by using an anisotropic Gaussian kernel g(x,
                     y,
                     t;
                     σ
                     2,
                     τ
                     2) with independent spatial variance σ
                     2 and temporal variance τ
                     2 as:
                        
                           (1)
                           
                              L
                              
                                 
                                    ⋅
                                    ;
                                    
                                       σ
                                       2
                                    
                                    ,
                                    
                                       τ
                                       2
                                    
                                 
                              
                              =
                              g
                              
                                 
                                    ⋅
                                    ;
                                    
                                       σ
                                       2
                                    
                                    ,
                                    
                                       τ
                                       2
                                    
                                 
                              
                              ∗
                              G
                              
                                 ⋅
                              
                           
                        
                     where ∗ is the convolution, and g(⋅;
                     σ
                     2,
                     τ
                     2) is the Gaussian kernel [41].

Second, a spatio-temporal second moment matrix μ within a Gaussian neighborhood of each point is constructed as:
                        
                           (2)
                           
                              
                                 
                                    
                                       
                                       μ
                                       
                                       
                                          ⋅
                                          
                                             σ
                                             2
                                          
                                          
                                             τ
                                             2
                                          
                                       
                                    
                                 
                                 
                                    
                                       
                                       
                                       =
                                       g
                                       
                                          
                                             ⋅
                                             ;
                                             s
                                             
                                                σ
                                                2
                                             
                                             ,
                                             s
                                             
                                                τ
                                                2
                                             
                                          
                                       
                                       ∗
                                       
                                       
                                          
                                             
                                                
                                                   
                                                   
                                                      L
                                                      x
                                                      2
                                                   
                                                   
                                                   
                                                      L
                                                      x
                                                   
                                                   
                                                      L
                                                      y
                                                   
                                                   
                                                   
                                                      L
                                                      x
                                                   
                                                   
                                                      L
                                                      t
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      L
                                                      x
                                                   
                                                   
                                                      L
                                                      y
                                                   
                                                   
                                                   
                                                      L
                                                      y
                                                      2
                                                   
                                                   
                                                   
                                                      L
                                                      y
                                                   
                                                   
                                                      L
                                                      t
                                                   
                                                
                                             
                                             
                                                
                                                   
                                                      L
                                                      x
                                                   
                                                   
                                                      L
                                                      t
                                                   
                                                   
                                                   
                                                      L
                                                      y
                                                   
                                                   
                                                      L
                                                      t
                                                   
                                                   
                                                   
                                                      L
                                                      t
                                                      2
                                                   
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     where L
                     
                        i
                      is the partial derivative of L with respect to the variable i, and i
                     =
                     x, y or t.

Third, it has been demonstrated in [41] that the interest points are located in the regions of G having significant eigenvalues λ
                     1,
                     λ
                     2,
                     λ
                     3 of μ. Thus, the spatio-temporal Harris corner function [42] is constructed by using a combination of the determinant and trace of μ as:
                        
                           (3)
                           
                              
                                 
                                    
                                       
                                       H
                                       
                                       =
                                       det
                                       
                                          μ
                                       
                                       −
                                       h
                                       
                                       trac
                                       
                                          e
                                          3
                                       
                                       
                                          μ
                                       
                                    
                                 
                                 
                                    
                                       
                                       
                                       =
                                       
                                          λ
                                          1
                                       
                                       
                                          λ
                                          2
                                       
                                       
                                          λ
                                          3
                                       
                                       −
                                       h
                                       
                                       
                                          
                                             
                                                
                                                   λ
                                                   1
                                                
                                                +
                                                
                                                   λ
                                                   2
                                                
                                                +
                                                
                                                   λ
                                                   3
                                                
                                             
                                          
                                          3
                                       
                                    
                                 
                              
                           
                        
                     
                  

Then, STIPs of G are detected at local positive maxima in H with a sufficiently large value of h. Local positive maxima of H correspond to points in G with high variations in both directions of space and time [41].

The scale parameters (σ,
                     τ) in the Gaussian kernel can be automatically adjusted to match the spatio-temporal extent of underlying image structures, as shown in [41]. Alternatively, STIPs can be extracted by using a set of multiple combinations of spatial (σ) and temporal (τ) scales. When compared to the automatic scale selection in [41], this simplification has shown to produce similar (or better) results in applications (e.g. action recognition) while resulting in a considerable speed-up and close-to-video-rate run time [43,44].


                     Fig. 3 shows the proposed framework of the gait feature extraction. Given a set of STIPs detected in Section 2 for each raw gait video, HOG [45] and HOF [46] are used to compute a point description for each STIP. By using each STIP as a center point, HOG and HOF are applied on a 3D volume of its neighborhood in the linear scale-space representation L(⋅;
                     σ
                     2,
                     τ
                     2) (see Section 2). This 3D volume is partitioned into a grid with N
                     
                        b
                      spatio-temporal blocks.

Then, a-bin HOG and b-bin HOF descriptors are computed for all blocks and concatenated into a
                     ×
                     N
                     
                        b
                     
                     − element HOG descriptor and b
                     ×
                     N
                     
                        b
                     
                     − element HOF descriptor. Finally, the HOG and HOF descriptors are concatenated to generate the point descriptor with (a
                     +
                     b)×
                     N
                     
                        b
                      elements for each STIP.

This paper proposes a histogram of STIP descriptors (HSD) as a gait feature. It contains two main steps including: 1) a histogram quantization; and 2) a histogram computation. In the histogram quantization step, the k-means clustering [47] is applied to all STIP descriptors from the training dataset. It clusters these STIP descriptors into k groups, in which the center of each group is used to define each bin of the histogram.

In the histogram computation step, for each raw gait video, its gait feature is extracted by assigning each of its STIP descriptors to the corresponding bin (i.e. by using a chosen similarity measurement function e.g. Euclidean distance [48]) and accumulating this bin's value by one. Then, L2-norm normalization is applied on the histogram to generate the k-bin HSD-based gait feature.

In this paper, support vector machine (SVM) is used as a classification method. It has been shown to be very efficient for recognizing gaits [17,27–30]. However, these methods applied SVM based on the standard one-vs.-all strategy [31] which can often be computationally infeasible at test-phase for large scale multi-class classifications i.e. gait recognitions with a large number of individual subjects. This is because its test-time complexity grows linearly with a number of subjects, by using every possible classifier trained for each individual. In this paper, the test-time complexity is a complexity for determining a subject identity of a gait sequence.

The concept of attribute-based classification [38,49,40] is adopted for this study to reduce test-time complexity and also retain or even improve the recognition accuracy. Fig. 4 shows the proposed framework of the attribute-based learning for gait classification.

The proposed framework contains three main steps in the training process, as shown in the first three rectangles of Fig. 4. The first step is to compute a similarity matrix (S) based on one-vs.-all classification using a training dataset. The matrix S represents correlations between gaits of different subject classes. Then, the second step is to create relations between subject classes and attributes which are virtually defined based on S. Subject classes are represented by different sets of attributes and each attribute may belong to several related subject classes. The third step is to learn a SVM-based classifier for each attribute instead of each subject class. In this way, the subject classes are associated with the trained classifiers by using their attribute representations.

Later at the test phase, as shown in the last rectangle of Fig. 4, the per-attribute classifiers are applied to generate predictions of attribute values for each probe gait. The final prediction score of the probe gait against each subject class is calculated from the relevant attribute values. Based on this proposed method, it can be seen that the test-time complexity grows linearly with the total number of attributes (M) instead of the total number of subject classes (C). Thus, M can be selected to be much less than C, in order to significantly reduce the computational complexity.

Assume that there are total M attributes (a
                        1,
                        a
                        2,…,
                        a
                        
                           M
                        ), each subject class c (z
                        
                           c
                        ) is represented by a set of attributes A
                        
                           c
                        
                        =[a
                        1
                        
                           c
                        ,
                        a
                        2
                        
                           c
                        ,…,
                        a
                        
                           M
                        
                        
                           c
                        ] where a
                        
                           m
                        
                        
                           c
                        (1≤
                        m
                        ≤
                        M) is a binary number. a
                        
                           m
                        
                        
                           c
                        
                        =1 means z
                        
                           c
                         is associated with a
                        
                           m
                        , otherwise z
                        
                           c
                         is not associated with a
                        
                           m
                        . Attributes are used as an in between layer to decouple images from a layer of class labels.

In this study, the probabilistic formulation of the direct attribute prediction (DAP) in [39] is adopted and revised for our attribute-based gait classification, as shown in Fig. 5
                        . SVM [26] is applied to learn a probabilistic classifier β
                        
                           m
                         for each attribute a
                        
                           m
                        . β
                        
                           m
                         is trained by using HSDs of gaits from all subject classes c for which a
                        
                           m
                        
                        
                           c
                        
                        =1 as positive training samples and the rest as negative training samples. In this way, β
                        
                           m
                         can provide a posterior probability p(a
                        
                           m
                        |HSD) of that a
                        
                           m
                         being present in HSD of a gait video.


                        β
                        
                           m
                         models the relation between a HSD and an attribute a
                        
                           m
                        . Next is to consider the relation between an attribute a
                        
                           m
                         and a subject class z
                        
                           c
                        , based on Bayes rule as:
                           
                              (4)
                              
                                 p
                                 
                                    
                                       
                                          z
                                          c
                                       
                                       |
                                       
                                          a
                                          m
                                       
                                    
                                 
                                 =
                                 
                                    
                                       p
                                       
                                          
                                             
                                                a
                                                m
                                             
                                             |
                                             
                                                z
                                                c
                                             
                                          
                                       
                                       p
                                       
                                          
                                             z
                                             c
                                          
                                       
                                    
                                    
                                       p
                                       
                                          
                                             a
                                             m
                                          
                                       
                                    
                                 
                                 =
                                 
                                    
                                       a
                                       m
                                       c
                                    
                                    
                                       p
                                       
                                          
                                             a
                                             m
                                          
                                       
                                       C
                                    
                                 
                                 .
                              
                           
                        
                        
                           Algorithm 1
                           Construction of the associations between C subject classes and M attributes
                                 
                                    
                                 
                              
                           

Thus, the best output subject class label (ĉ) for HSD of a gait video is predicted as:
                           
                              (6)
                              
                                 
                                    c
                                    ^
                                 
                                 =
                                 
                                    
                                       argmax
                                       
                                          1
                                          ≤
                                          c
                                          ≤
                                          C
                                       
                                    
                                 
                                 
                                 p
                                 
                                    
                                       
                                          z
                                          c
                                       
                                       |
                                       H
                                       S
                                       D
                                    
                                 
                              
                           
                        where p(z
                        
                           c
                        |HSD) is obtained from Eq. (5).

The remaining challenge is to construct the associations (a
                        
                           m
                        
                        
                           c
                        ) between subject classes and attributes. This will be explained in the following Section 4.2.

In this paper, attributes (a
                        
                           m
                        ) are learned based on correlations between different subject classes [38].Matrix A is defined to represent the associations (a
                        
                           m
                        
                        
                           c
                        ) between M attributes and C subject classes, as:
                           
                              (7)
                              
                                 A
                                 =
                                 
                                    
                                       
                                          
                                             A
                                             1
                                          
                                          
                                             A
                                             2
                                          
                                          …
                                          
                                             A
                                             C
                                          
                                       
                                       T
                                    
                                 
                                 =
                                 
                                    
                                       B
                                       1
                                    
                                    
                                       B
                                       2
                                    
                                    …
                                    
                                       B
                                       M
                                    
                                 
                              
                           
                        where A
                        
                           c
                         is a set of attributes representing a subject class z
                        
                           c
                         such that A
                        
                           c
                        
                        =[a
                        1
                        
                           c
                        ,
                        a
                        2
                        
                           c
                        ,…,
                        a
                        
                           M
                        
                        
                           c
                        ]∈{0,1}1×
                           M
                        , and B
                        
                           m
                         is a set of classes representing an attribute a
                        
                           m
                         such that B
                        
                           m
                        
                        =
                        t[a
                        
                           m
                        
                        1,
                        a
                        
                           m
                        
                        2,…,
                        a
                        
                           m
                        
                        
                           C
                        
                        ∈{0,1}
                           C
                           ×1.
                           Algorithm 2
                           Maxsim(…)
                                 
                                    
                                 
                              
                           

Similarity(…)
                                 
                                    
                                 
                              
                           

To reduce the test-time complexity, M must be less than C. M can be selected empirically by trading-off between the test-time complexity and the accuracy.

Basically, subject classes that have highly related walking patterns should share some common attributes. Thus, matrix A will be determined based on the similarities between subject classes using the greedy strategy [50] under certain conditions. This process will be discussed below regarding three issues: 1) measurement of such similarities in Section 4.2.1; 2) conditions for matrix A in Section 4.2.2; 3) construction of matrix A in Section 4.2.3.

Matrix S is defined to represent similarities between subject classes. S(i,
                           j),1≤
                           i,
                           j
                           ≤
                           C, is the similarity of class z
                           
                              j
                            to class z
                           
                              i
                           . It is measured from probability scores of that HSDs of subject class z
                           
                              j
                            being predicted as belonging to subject class z
                           
                              i
                           .

The matrix S will be obtained based on one-vs.-all classification in the training process, which is usually done offline beforehand. Given a training dataset, a classifier α
                           
                              c
                            is trained for each subject class z
                           
                              c
                           . That is, α
                           
                              c
                            can provide a posterior probability p(z
                           
                              c
                           |HSD) of that HSD of a gait video being predicted as belonging to subject class z
                           
                              c
                           . S(i,
                           j) is computed as:
                              
                                 (8)
                                 
                                    S
                                    
                                       i
                                       j
                                    
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   n
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                          
                                          
                                          p
                                          
                                             
                                                
                                                   z
                                                   i
                                                
                                                |
                                                H
                                                S
                                                
                                                   D
                                                   
                                                      n
                                                      ,
                                                      j
                                                   
                                                
                                             
                                          
                                       
                                       N
                                    
                                 
                              
                           where HSD
                           
                              n,j
                            is HSD of a gait video n of z
                           
                              j
                            on the training dataset.

The attribute representation must be constructed under the following two conditions.


                           C1: No identical sets of attributes representing different subject classes (i.e. A
                           
                              i
                            and A
                           
                              j
                            are not identical for any 1≤
                           i,
                           j
                           ≤
                           C and i
                           ≠
                           j). In order to differentiate all subject classes, their attribute representations must be different.


                           C2: No identical sets of classes describing different attributes (i.e. B
                           
                              m
                            and B
                           
                              n
                            are not identical for any 1≤
                           m,
                           n
                           ≤
                           M and m
                           ≠
                           n). Otherwise, it will lead to duplicated classifiers which will increase test-time complexity without additional benefit.

In this section, the associations between subject classes and attributes (a
                           
                              m
                           
                           
                              c
                           ) are constructed using the similarity matrix S based on the greedy strategy under the certain conditions (i.e. C1,
                           C2), as shown in algorithms 1, 2, and 3. Given C subject classes, the number of attributes M can be selected by trading-off between accuracy and efficiency (i.e. test-time complexity). The larger M will lead to higher accuracy but lower efficiency. This will be verified in our experiments.

Given C subject classes and M attributes, the test-time speed-up is approximately 
                              
                                 
                                    C
                                    M
                                 
                              
                           , and each subject class must be associated to at least Q attribute where 
                              
                                 M
                                 Q
                              
                              =
                              
                                 
                                    
                                       M
                                       !
                                    
                                    
                                       Q
                                       !
                                       
                                          
                                             M
                                             −
                                             Q
                                          
                                       
                                       !
                                    
                                 
                              
                              ≥
                              C
                           . For example, if C
                           =124 subjects and M
                           =42 attributes then the test-time speed-up 
                              =
                              
                                 
                                    124
                                    42
                                 
                              
                              =
                              3
                            and Q
                           ≥2.

Algorithm 1 presents the main function. It performs iteration till the maximum number of loops is reached or there is no more change in A. First, all elements in matrix A are initialized to be 0. That is, each subject class is initially not associated with any attributes. In each while loop, for each subject class z
                           
                              c
                           , the Q attributes that are most associated to z
                           
                              c
                            are determined and linked to z
                           
                              c
                           .

Algorithm 2 presents the function maxsim(…) to determine the strongest association between z
                           
                              c
                            and any attributes a
                           
                              m
                            under the conditions C1,
                           C2. In the function, max.
                           sim represents relative strength of the strongest association between attribute max.
                           attr and subject class max.
                           class. Among attribute a
                           
                              m
                            which are currently not associated with subject class z
                           
                              c
                           , the association a
                           
                              m
                           
                           
                              c
                            that has the maximum relative strength is determined.

Algorithm 3 presents the function similarity(…) to estimate the relative strength (s
                           
                              m
                           
                           
                              c
                           ) of the association between subject class z
                           
                              c
                            and attribute a
                           
                              m
                           . In this study, attribute a
                           
                              m
                            is presented by the set of subject classes. Thus, s
                           
                              m
                           
                           
                              c
                            can be equivalently calculated from correlations between class z
                           
                              c
                            and all other classes that share the same attribute a
                           
                              m
                           .

@&#EXPERIMENTS@&#

The CASIA gait database B [23] is used to evaluate the performance of the proposed method in Sections 5.1, 5.2 and 5.3. It contains 124 subjects from 11 views, namely 0∘, 18∘, 36∘, 54∘, 72∘, 90∘, 108∘, 126∘, 144∘, 162∘, and 180∘. Under each view, ten gait sequences are captured for each person including six sequences in normal walking (nm), two sequences in walking when carrying a bag (bg), and two sequences in walking when wearing a coat (cl).

In our experiments, as mentioned, STIPs are detected at multiple scales based on a regular sampling of the scale parameters [43,44]. The standard parameters are set as h
                     =0.0005, σ
                     2
                     ={4,8}, τ
                     2
                     ={2,4}. In addition, for the gait feature extraction, the number of bins is coarsely set as k
                     =100. This is because each gait contains approximately 100 STIPs, based on our investigations. Also, the 4-bin HOG and 5-bin HOF are applied to a 3D video patch (i.e. with size 18σ
                     ×18σ
                     ×18τ) in the neighborhood of each detected STIP. The 3D patch is partitioned into a grid with 3×3×2 spatio-temporal blocks.


                     Fig. 6
                      shows examples of STIPs detected on a gait video from the CASIA gait database B. These STIPs are located where there are large variations along both spatial and temporal directions. Thus, they can be referred to human walking motions particularly on lower body parts i.e. legs.

The experiments are divided into two main scenarios. The first scenario is the gait recognition with no walking variations. That is, probe and gallery gaits are recorded under the same walking conditions. The leave-one-out cross-validation is applied in this evaluation. The second scenario is the gait recognition with walking variations. That is, probe and gallery gaits are recorded under different walking conditions. For example, probe gaits are walking when carrying a bag (bg) and gallery gaits are normal walking without carrying a bag (nm). In this evaluation, gaits under one walking condition are used as probe gaits and gaits under another walking condition are used as gallery gaits.

In addition, five more gait databases (namely the CASIA gait database A, the CASIA gait database C, the CMU MoBo database, the OU-ISIR gait database: treadmill dataset A and the OU-ISIR gait database: treadmill dataset B) are used to further validate the proposed method. The corresponding experimental results and comprehensive comparisons are shown in Sections 5.4, 5.5, 5.6, 5.7 and 5.8.

The proposed method (with SVM based on the standard one-vs.-all classification) is first compared with the baseline method [23] which uses the most well known gait feature i.e. gait energy image (GEI) [11],under three variations including 1) viewing angle changes; 2) clothing changes; and 3) carrying condition changes.

As set up in [23], three sets of experiments (A, B, C) are designed to evaluate gait recognitions under three variations. Experiment Set A is for investigating the effects of views on the gait recognition performance and the algorithms' robustness to view variation. Thus, nm under one view are used as probe gaits and nm under another view are used as gallery gaits. Experiment Set B is for investigating the effects of clothes on the gait recognition performance and the algorithms' robustness to clothing change. Thus, cl under one view are used as probe gaits and nm under another view are used as gallery gaits. Experiment Set C is for investigating the effects of carrying condition on the gait recognition performance and the algorithms' robustness to carrying condition change. Thus, nm under one view are used as probe gaits and nm under another view are used as gallery gaits.

From Tables 1, 2, and 3
                        
                        
                        , many results have been observed. For the case of no variations i.e. walking under the same views without a bag and a coat, the proposed method performs comparable to the GEI-based method [23] for most views but slightly worse for frontal views. This is because the method in [23] uses the global shape information which is visible more clearly than the local motion information used in the proposed method, under frontal views. However, the proposed method is shown to be more robust to the variations on carrying conditions and clothing types because it does not rely on shape information which is significantly altered by these variations especially under side walks.

For the case of view variations, the proposed method performs better than the GEI-based method [23] for most views but comparable for frontal views. This is because view changes significantly alter available visual features for both the global shape information used in [23] and the local motion information used in the proposed method. For the cases of clothing and carrying variations, the proposed method significantly outperforms the GEI-based method [23] because GEI relies heavily on shape information which is significantly altered by changes of clothing types and carrying conditions.

In this section, the trade-off between accuracy and efficiency of the proposed method (with SVM based on attribute-based classification) is evaluated. The experiments have been carried out based on 5 different numbers of attributes (i.e. M
                        =124, 62, 42, 31, 25) with 5 different speed-up of test-time respectively (i.e. speed-up=1, 2, 3, 4, 5). Table 4
                         illustrates the trade-off between the test-time and the accuracy for six cases of (gallery, probe): (nm,
                        nm), (bg,
                        bg), (cl,
                        cl), (nm,
                        bg), (nm,
                        cl), and (cl,
                        bg), under side walk (90∘).

When M
                        =62, the highest accuracy is achieved and the test-time is reduced by approximate 2 times when compared to the standard one-vs.-all classification. The test-time can be further decreased but the accuracy will be also decreased accordingly.

The proposed method (with SVM based on attribute-based classification, M
                        =62) is further compared with other existing methods in the literature, under changes of clothing and carrying condition. Table 5
                         shows the comparison results. The last column shows the average performances of the methods which reported all six results. It can be seen that the proposed method significantly outperforms the other existing methods.

For the case of no variation (nm,
                        nm), some other shape/appearance-based methods can perform better than the proposed method. This is because the global shape/appearance information contains more discriminant features than the local motion information used in the proposed method. For the case of small variations (bg,
                        bg) and (cl,
                        cl), the proposed method significantly performs better than all other existing methods in the literature. Also, for the case of large variations (bg,
                        nm), (cl,
                        nm) and (bg,
                        cl), the proposed method can achieve the highest performance.

The CASIA gait database A [57] includes 20 subjects from three views, namely frontal (0∘), oblique (45∘) and lateral (90∘) views. Each subject walks along a straight-line path back and forth twice. In total, there are 240 gait image sequences. The experimental results and comparisons are illustrated in Table 6
                        . The last column shows the average performances of the methods which reported all three results. It can be seen that the proposed method outperforms the other existing methods in average performance.

The CASIA gait database C [68] contains 153 subjects. It was recorded under four walking conditions, namely normal walking (fn), slow walking (fs), fast walking (fq) and normal walking with a bag (fb). Ten gait videos were recorded for each subject (4 videos for fn, 2 videos for fs, 2 videos for fq, 2 videos for fb). The videos were all captured at night by infrared (thermal) cameras. The experimental results and comparisons are shown in Table 7
                        . The last column shows the average performances of the methods by using four results of fn–fn, fs–fn, fq–fn and fb–fn.

In Table 7, it can be seen that the proposed method outperforms the other existing methods in average performance. For the cases of walking with speed changes (fs–fn, fq–fn), only HOG is used to compute a point description for each STIP. HOF is not used to compute such point description because it is sensitive to speed changes. More importantly, the proposed method achieves the outstanding performance under the change of a carrying condition (fb–fn). This is because carrying a small bag can alter available visual shape appearances but cannot significantly affect human walking patterns. The proposed method does not rely on shape appearances but on motion patterns. Thus, this can be a main reason why the proposed method can perform better than the appearance-based methods as shown in Table 7.

The CMU MoBo database [81] contains 25 subjects. Three walking conditions, namely slow walking i.e. 3.3km/h (fs), fast walking i.e. 4.5km/h (fq) and slow walking with a ball (fb) are used in this experiment. The experimental results and comparisons are illustrated in Table 8
                        . In each row, the last column shows two values: 1) the average performance of the methods by using four results of fs–fs, fq–fq, fb–fb, fs–fq; 2) the average performance of the methods by using six results of fs–fs, fq–fq, fb–fb, fs–fq, fb–fq, fb–fs. As mentioned, for the cases of walking with speed changes (fs–fq, fb–fq), only HOG is used to compute a point description for each STIP. In Table 8, it can be seen that the proposed method outperforms the other existing methods in average performances. However, the proposed method is only comparable to the method in [16]. These two methods can achieve 100% accuracy for the cases of no walking variations (fs–fs, fq–fq, fb–fb). The proposed method performs better than the method in [16] for the cases of changes in carrying conditions (fb–fq, fb–fs), while the method in [16] performs better than the proposed method for the case of changes in walking speeds (fs–fq).

The OU-ISIR gait database: treadmill dataset A [82] contains six different walking speeds from 2 to 7km/h with 1km/h interval. A total of 34 subjects are used in this experiment. Two videos were recorded for each subject from each walking speed. To investigate and compare the recognition performance with other existing methods for the case of speed variations, the experimental setup as in [74] is used in this experiment. The walking with 5km/h is used as the gallery and the walking with 4km/h, 5km/h, 6km/h and 7km/h are used as the probe. The experimental results and comparisons are shown in Table 9
                        . It can be seen that the proposed method performs better than the other existing methods in the literature for the case of walking speed changes.

The OU-ISIR gait database: treadmill dataset B [82] contains 68 subjects from side view with clothes variation up to 32 combinations. The probe and gallery datasets are pre-defined in the database. The experimental results and comparisons are illustrated in Table 10
                        . It can be seen that the proposed method outperforms the other methods in the literature for the case of clothes variation.

@&#CONCLUSION@&#

This paper has proposed a new method of gait recognition. It extracts a new gait feature directly from a raw gait video without a foreground–background segmentation and/or a shape contour/boundary/skeleton extraction. The proposed gait feature extraction process is performed in the spatio-temporal domain. Then, SVM is applied for gait classification. The attribute-based learning is further applied to reduce the number of SVM models needed for recognizing each probe gait. It can significantly reduce the test-time computational complexity and also improve the recognition accuracy. When compared with other existing methods in the literature, the proposed method is shown to have the outstanding performances, especially for the cases of changes in clothing types and carrying conditions.

@&#REFERENCES@&#

