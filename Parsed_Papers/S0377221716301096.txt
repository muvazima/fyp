@&#MAIN-TITLE@&#A data analytic approach to forecasting daily stock returns in an emerging market


@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A data analytic approach is proposed for stock market prediction.


                        
                        
                           
                           The proposed decision support system effectively uses fewer variables.


                        
                        
                           
                           Higher prediction accuracy is achieved compared to the literature.


                        
                        
                           
                           Sophisticated data analytics methods present capability to capture nonlinearity.


                        
                        
                           
                           This generic methodology is applicable to the other emerging markets as well.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Prediction/forecasting

Stock market return

Business analytics

Borsa Istanbul (BIST 100)

Istanbul Stock Exchange (ISE)

@&#ABSTRACT@&#


               
               
                  Forecasting stock market returns is a challenging task due to the complex nature of the data. This study develops a generic methodology to predict daily stock price movements by deploying and integrating three data analytical prediction models: adaptive neuro-fuzzy inference systems, artificial neural networks, and support vector machines. The proposed approach is tested on the Borsa Istanbul BIST 100 Index over an 8 year period from 2007 to 2014, using accuracy, sensitivity, and specificity as metrics to evaluate each model. Using a ten-fold stratified cross-validation to minimize the bias of random sampling, this study demonstrates that the support vector machine outperforms the other models. For all three predictive models, accuracy in predicting down movements in the index outweighs accuracy in predicting the up movements. The study yields more accurate forecasts with fewer input factors compared to prior studies of forecasts for securities trading on Borsa Istanbul. This efficient yet also effective data analytic approach can easily be applied to other emerging market stock return series.
               
            

@&#INTRODUCTION@&#

It is a well-known fact that forecasting stock market return series is a difficult task in most financial markets, but because of the obvious payoff for success, both academicians and practitioners pursue this endeavor. One of the most widely accepted financial theories of the last half-century has been the efficient market hypothesis (EMH) (Fama, 1965). The EMH states that a market is efficient if prices fully reflect available information, with different forms of efficiency dependent on the relevant information set. Efficient markets “rule out the possibility of trading systems based only on (the assumed information set) that have expected profits or returns in excess of equilibrium profits or returns” (Fama, 1970, p. 385). The coexistence of forecasting and the EMH is precarious, because successful methods will lead to trades and prices which ultimately neutralize the forecasting procedure; constant-parameter models, such as regression or time series models will not work due to their assumed stationary (Timmermann & Granger, 2004).

An even more formidable assumption for the stock return generation process for forecasters is the random walk model, which assumes that successive price changes are independent and identically distributed. Several studies have shown that the serial correlation of financial time series often is economically and statistically insignificant (Hawawini & Keim, 1995), consistent with the random walk model. The random walk model is also consistent with the weakest form of the EMH, where the information set is the past return history of the financial asset.

Recently, the field of behavioral finance has presented the greatest challenge to the EMH, in which researchers document a number of departures from the rational optimizing behavior. Lo (2007) presents an excellent summary of the EMH, including numerous challenges and defenses of the theory as well as offering his vision of an evolutionary alternative which may be friendlier to prospective forecasters. Forecasters may also do better in emerging markets, with reduced trading and increased impediments to information flow. Lagoarde-Segot and Lucey (2008) mention a number of conditions proposed by various researchers that may reduce market efficiency: reduced liquidity, low degree of competition, lack of market transparency, and greater political or economic uncertainty.

In this paper, three data analytic models are used, i.e. adaptive neuro-fuzzy inference systems (ANFIS), artificial neural networks (ANNs), and support vector machines (SVMs) to forecast the daily returns of the BIST 100 Index trading on Borsa Istanbul (BIST).
                        1
                     
                     
                        1
                        The Borsa Istanbul is a result of a 2013 merger of the Istanbul Stock Exchange (ISE), the Istanbul Gold Exchange, and the Derivatives Exchange of Turkey.
                      An ANN incorporates a large number of parameters which allows learning the intrinsic non-linear relationship presented in time series, increasing their forecasting capabilities (Haykin, 2008; Specht, 1990). A fuzzy inference system (FIS) is a rule-based decision support system that uses fuzzy logic instead of Boolean logic in its knowledge base and derives conclusions from user inputs and fuzzy inference processes (Kandel, 1991); fuzzy rules and membership functions make up the knowledge base of the system. An ANFIS is a combination of both an ANN and fuzzy logic (Malhotra & Malhotra, 2002). An SVM is a supervised learning algorithm, which may perform regression or classification using a priori defined categories (Lu & Wang, 2010; Vapnik, 1998).

ANNs are popular artificial intelligence-based data mining tools due to their superior prediction performance. For example, ANNs are widely used in forecasting the following: ATM cash demand (Venkatesh, Ravi, Prinzie, & Van den Poel, 2014), wind speed (Cao, Ewing, & Thompson, 2012), foreign exchange rates (Sermpinis, Theofilatos, Karathanasopoulos, Georgopoulos, & Dunis, 2013), intraday electricity demand (Kim, 2013), and financial failure (du Jardin & Séverin, 2012). Desai, Crook, and Overstreet (1996) used ANNs in building credit scoring models on the data of credit consumers from three credit unions in Southeastern United States for the period 1988 through 1991. Their results show that among other statistical techniques, ANNs performs better, especially in predicting bad credit.

Based on the data sample of 220 manufacturing firms, Zhang, Hu, Patuwo, and Indro (1999) indicated that ANNs again perform significantly better than traditional regression methods when solving real problems such as bankruptcy prediction, while Lacher, Coats, Sharma, and Fant (1995) revealed that ANNs are able to achieve better results in estimating future financial health of a firm. Fethi and Pasiouras (2010) discussed the applications of various artificial intelligence techniques, such as ANNs, decision trees, and support vector machines, in bank failure prediction, assessment of bank creditworthiness, and underperformance. Kumar and Ravi (2007) also examined the application of the same techniques in their study of the bankruptcy prediction issues faced by banks and firms during the 1968–2005 period. Results from the study by García-Alonso, Torres-Jiménez, and Hervás-Martínez (2010) indicated that ANN models, specifically product-unit neural networks, have shown the most accurate gross margin predictions in the agrarian sector. On the other hand, Piramuthu (1999) pointed out some disadvantages of ANNs such as long training process in developing the optimal model or lack of explanation on how the solution was produced by the ANN.

Fuzzy logic has also been used in business related applications. Omero, D'Ambrosio, Pesenti, and Ukovich (2005) developed a multiple-attribute decision support system based on fuzzy logic for evaluating the performance of single economic organizations. Ordoobadi (2008) used fuzzy logic as a tool for decision makers to make more informed decisions regarding their investment in advanced technologies. Ho et al. (2008) developed fuzzy rule sets for enhancing performance in a supply chain network. Malhotra and Malhotra (2002) and Akkoc (2012) demonstrate that a three-stage hybrid ANFIS, which combines an ANN with fuzzy logic, can even eliminate the disadvantages of the pure ANN data analytic method. Overall, studies generally showed that the expert decision systems outperform the traditional forecasting techniques, such as regression, moving average, or auto-regressive models (Haefke & Helmenstein, 1996; Kuan & Liu, 1995; Refenes, Zapranis, & Francis, 1994).

Several studies applied expert decision systems to stock returns in the developed markets of the United States or Europe. The S&P 500 Index was the focus of a numerous studies (Armano, Marchesi, & Murru, 2005; Casas, 2001; Tsaih, Hsu, & Lai, 1998). The German DAX is forecasted by Rast (1999) and Siekmann, Gebhardt, and Kruse (1999). Ettes (2000) and Setnes and Van Drempt (1999) modeled the returns on the Amsterdam stock exchange. Fernandez-Rodriguez, Gonzalez-Martel, and Sosvilla-Rivero (2000) and Pérez-Rodríguez, Torra, and Andrada-Félix (2005) modeled the returns on the Madrid stock exchange. Lam (2001) forecasted stock returns on the Hong Kong stock exchange. Leigh, Paz, and Purvis (2002) used feed-forward neural network to forecast the NYSE Composite Index. Abraham, Nath, and Mahanti (2001) and Chen, Dong, and Zhao (2005) forecasted the Nasdaq 100 Index. Pai and Lin (2005) employed a hybrid model composed of ARIMA and SVM to predict the stock prices of ten leading companies in the US. In a similar fashion, Wang, Wang, Zhang, and Guo (2012) proposed a composite forecast for the DJIA Index using ARIMA, back propagation neural network, and exponential smoothing models. These stock and index returns are located in countries classified as developed markets.

Research on the value of data analytics to predict stock price movements in emerging markets is fewer in number although it is now growing. There is evidence that emerging markets behave differently than in the developed markets (Bhattacharya, Daouk, Jorgenson, & Kehr, 2000). As mentioned above, there is also the hypothesis that financial markets in emerging markets lack the conditions to fully support efficiency as defined by the EMH, which provides greater incentives to forecast returns. Some data analytic-related studies for the European emerging markets includes Walczak (1999) and Wikowska (1995) for the Polish stock exchange index and Koulouriotis, Diakoulakis, Emiris, and Zopounidis (2005) for the Athens stock exchange. Baek and Cho (2002) forecasted the Korean stock index. Chen, Leung, and Daouk (2003) predicted the Taiwan stock index. Cao, Leggio, and Schniederjans (2005) and Yiwen, Guizhong, and Zongping (2000) studied the Shanghai stock market. Wang et al. (2012) used ARIMA, back propagation neural network, and exponential smoothing models to forecast the Shenzhen Integrated Index and DJIA Index using ARIMA. Patel, Shah, Thakkar, and Kotecha (2015) compare four prediction models (ANN, SVM, random forest, and naive-Bayes) via ten technical parameters using a stock trading dataset, for indexes trading on the Bombay Stock Exchange and the National Stock Exchange of India.

Research on BIST securities based on data analytics is also limited. Some pioneer studies were conducted by Altay and Satman (2005), Diler (2003), Yümlü, Gürgen, and Okay (2005), and Kara, Boyacioglu, and Baykan (2011). Diler (2003) and Yümlü et al. (2005) deployed only ANNs as their prediction models, neither of which yielded satisfactory accuracy results. Altay and Satman (2005) also utilized ANNs and compared its results to conventional linear regression model. Kara et al. (2011) used ANN and SVM classification models and compared their performances in predicting the direction of movement in the daily BIST 100 index. Ten technical indicators were selected as input variables of the utilized models. Their results were better than that of their pioneers. To the best of our knowledge, there is no other study based on data analytical techniques to predict the BIST 100 index.

The BIST (formerly known as Istanbul Stock Exchange) was established on December 26, 1985 for the purpose of ensuring that securities are traded in a secure and stable environment, and commenced to operate on January 3, 1986. It has contributed to the development of Turkish capital markets and the Turkish economy since the date of its establishment. The BIST 100 (a.k.a. Borsa Istanbul 100 Index or BIST 100 National Index) is a market-weighted index of 100 companies selected from national market, real estate investment trusts, and venture capital investment trusts, and has been calculated on a daily basis since October, 1987. Although there are many other indices that are calculated by the BIST, the BIST 100 is the most widely used index to capture the overall BIST market performance.

The remainder of this paper is organized as follows: Section 2 describes materials and methods used in this paper. Section 3 presents the experimental results and discusses them. The study is concluded in Section 4, along with suggestions of future research directions.

@&#MATERIAL AND METHODS@&#

There are mainly two well-established and widely used data analytic methodologies in literature, i.e. CRISP-DM and SEMMA (Turban, Sharda, & Delen, 2011). CRISP-DM (Cross Industry Standard Process for Data Mining) has been consistently listed as the top methodology for analytics, data mining, or data science projects (Piatetsky, 2015). Therefore, CRISP-DM methodology is modified and enhanced in this study to develop a generic data analytic methodology to model stock exchange movements. CRISP-DM provides a comprehensive and systematic way of conducting a data analytics research, thereby improving the likelihood of obtaining accurate and reliable results. The model delivers a six-step process as summarized next (Shearer, 2000): 
                        
                           (1)
                           
                              Business understanding: Develop an understanding of the business objectives and translate this into a data mining problem to construct a plan to achieve desired goals.


                              Data understanding: Identify initial source of data and evaluate raw data for data integrity problems.


                              Data preparation: Pre-process, scrub, and construct relevant data into final dataset.


                              Modeling: Develop various models using comparable analytical techniques.


                              Evaluation: Evaluate and assess the results of each model against each other and against the goals of the study.


                              Deployment: Deploy the models for use in decision-making processes.

The first three steps (business understanding, data understanding, and data preparation) account for a big majority of the total project time. Although each stage generates inputs for the next stage, the sequence is not strict and moving back and forth is actually required to achieve optimal results. Once the initial source of data is identified for the research, the data are carefully examined for relevance and quality level. A significant amount of time is spent preprocessing the data into final dataset used for modeling. The data are cleansed to remove missing values, exclude irrelevant records, and transform variables into appropriate descriptors for predictive modeling. Next, the dataset is processed via data analytical models for prediction. For each classification model, the data are separated into 10 mutually exclusive data folds using the stratified k-fold cross-validation method. All folds except one are used to train each of the predictive classification models. In other words, model experimentation and building was conducted based on the training (in-sample) data. The final fold is reserved to test the developed model and assess the accuracy rate of its prediction ability. Therefore, model assessment was performed by using the testing (out-of-sample) data.

In this study, three different classification models are used: ANFIS, ANN, and SVM. These models are selected in this study due to their popularity in literature which stems from the fact that they have conventionally outperformed many other methods in terms of accuracy (Delen, Oztekin, & Kong, 2010; Delen, Oztekin, & Tomak, 2012; Oztekin, 2011, 2012; Oztekin & Khan, 2014; Oztekin, Delen, & Kong, 2009; Oztekin, Delen, Turkyilmaz, & Zaim, 2013; Oztekin, Kong, & Delen, 2011; Sevim et al., 2014; Turkyilmaz, Oztekin, Zaim, & Demirel, 2013). Once baseline accuracy rates are derived from these three classification models, a sensitivity analysis is conducted for each of the input variables for all three data analytic models in order to determine the importance order of variables for each of the models. The main modification on CRISP-DM as proposed in this data analytic methodology emerges particularly in steps 3, 4, and 5: Data are not randomly split into two parts for training and testing. Instead, stratified 10-fold cross-validation is utilized to minimize the bias and variation due to random splitting. Also, three model outputs are integrated to obtain summarized knowledge extracted from data via information fusion-based sensitivity analysis as introduced in Section 2.7. A flow chart of the proposed data analytic methodology described in this section is shown in Fig. 1.
                     
                  

A dataset is compiled containing daily opening and closing values of the BIST 100 index, the closing exchange rates between Turkish lira (TL) and the dollar and the TL and the euro, the closing gold prices (TL/kg), the closing values of the FTSE Istanbul Bond index (FBIST), the closing values of the Nasdaq Composite index, and an indicator that captures the trend in the BIST 100. The dataset covers a period from 2007 to 2014. After data cleansing due to missing and erroneous entries, the total number of records is 1675 trading days. All the data are obtained from the Bloomberg server. While constructing data analytic models in this research, daily or overnight percentage changes are used instead of their corresponding absolute values. That is, instead of a change in the level or a price, (Pt
                        
                        −
                        Pt−
                        
                        1), the percentage change [(Pt
                        
                        −
                        Pt
                        
                        -1)/Pt
                        
                        -1]×100 is used to measure changes in the BIST 100, the exchange rates, the price of gold, both indices, and the bond index. By converting these input and output variables to rates, normalized input variables are obtained.

A simple trading strategy is proposed here to act on the forecasts. All trades are conducted at the start of the trading day (t
                        0) at the opening value of BIST100. The models predict the change between the opening (known since the trades are done at the start of each trading day) and the closing values of the trading day. If the market is predicted to rise by any amount without any threshold, this signals a “buy”. On the other hand if it is predicted to fall, this signals a “sell”.

The output (target) variable used in the study is the percentage change in the BIST 100 index between the opening and the closing values on day t
                        0. The inputs (factors) are listed below:

                           
                              •
                              
                                 BIST 100: the percentage change in the BIST 100 index between the closing value on day t
                                 -1 and the next morning's opening value on t
                                 -
                                 0.


                                 Nasdaq composite: the percentage change in Nasdaq Composite index between the closing value on day t-
                                 
                                 2 and the closing value on t
                                 -1.


                                 Gold price: the percentage change in gold prices between the closing value on day t-2
                                  and the closing value on t-1
                                 
                              


                                 Dollar and Euro: the percentage change in the value of TL exchanged for one dollar and one euro between at the close of day t-2
                                  and on day t-1
                                 . The two exchange rates are consolidated into a single factor.


                                 FBIST bond index: the percentage change in FBIST bond index between the closing value on day t
                                 -2 and the closing value on t
                                 -1. This index tracks the government bonds of Turkey.


                                 Trend: Trend factor is used for capturing the history of the index. It tracks the up and down trends in the BIST 100. For instance, a value of −3 means there are three negative patterns in BIST 100 and a value of 0 means there is no current pattern.

In order to minimize the bias associated with the random sampling of the training and holdout data samples in comparing the predictive accuracy of two or more methods, researchers tend to use k-fold cross-validation (Desai et al., 1996). In k-fold cross-validation, also called rotation estimation, the complete dataset (D) is randomly split into k mutually exclusive subsets (the folds: D
                        1, D
                        2
                        
                           
                              ,
                              …
                              ,
                           
                         
                        Dk
                        ) of approximately equal size. The classification model is trained and tested k times. Each time (t ∈ {1, 2
                           
                              ,
                              …
                              ,
                           
                         
                        k}), it is trained on all but one fold (Dt
                        ) which is held back for testing. The cross-validation estimate of the overall performance criteria is calculated as simply the average of the k individual performance measures:

                           
                              (1)
                              
                                 
                                    
                                       
                                          
                                             C
                                             V
                                             =
                                             
                                                1
                                                k
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                k
                                             
                                             P
                                             
                                                M
                                                i
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where CV stands for cross-validation, k is the number of folds used, and PM is the performance measure for each fold (Olson & Delen, 2008).

To estimate the performance of the prediction models a 10-fold cross-validation approach was used. Empirical studies showed that 10 seem to be an optimal number of folds which optimizes the time it takes to complete the test while minimizing the bias and variance associated with the validation process (Kohavi, 1995). In 10-fold cross-validation the entire dataset is divided into 10 mutually exclusive subsets (or folds). Each fold is used once to test the performance of the prediction model that is generated from the combined data of the remaining nine folds, leading to 10 independent performance estimates as seen in Fig. 2
                        .

To compare the classification models and evaluate their performance, three different performance criteria are used: accuracy, sensitivity, and specificity. Computation of these performance metrics are adopted as follows: 
                           
                              (2)
                              
                                 
                                    
                                       
                                          
                                             Accuracy
                                             =
                                             
                                                
                                                   TP
                                                   +
                                                   TN
                                                
                                                
                                                   TP
                                                   +
                                                   TN
                                                   +
                                                   FP
                                                   +
                                                   FN
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             Sensitivity
                                             =
                                             
                                                TP
                                                
                                                   TP
                                                   +
                                                   FN
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    
                                       
                                          
                                             Specificity
                                             =
                                             
                                                TN
                                                
                                                   TN
                                                   +
                                                   FP
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where TP, TN, FP, FN denote true positive, true negative, false positive and false negative respectively and are defined as:

                           
                              
                                 True positive: number of samples classified as true while they actually were true.


                                 False positive: number of samples classified as true while they actually were false.


                                 False negative: number of samples classified as false while they actually were true.


                                 True negative: number of samples classified as false while they were actually false.

Accuracy, shown by Eq. (2), is a practical performance criterion that shows the directional success of the proposed model. It is the ratio of the correct predictions of upward and downward movements. Sensitivity and specificity, shown by Eqs. (3) and (4) respectively, measure the model's ability to recognize the positive and negative values of any nodes.

A confusion matrix is a summary of the classification model results displayed in a matrix representation. The rows represent the actual classes and the columns represent the predicted classes. For example, a two-class prediction classification problem display results are illustrated as in Table 1.
                        
                     

Since accuracy of this current data analytic methodology depends on the correct predictions of all movements of the index (up or down), it is an important metric as buy/sell decisions are based on predictions that the index would rise or fall. A simple trading strategy is proposed here to act on the forecasts: No Threshold All In/Out. All trades are conducted at the start of the trading day at the opening value of BIST 100. The models predict the closing value at the end of the trading day. If the market is predicted to rise by any amount without any threshold, this signals a “buy”. On the other hand if it is predicted to fall, this signals a “sell”. On the first day that a buy signal is received from the model, an investment is made in the BIST 100 fund using the full capital amount. Buy signals on subsequent days are treated as hold instructions until a sell signal is received. All investment units currently held are sold at the current price after a sell signal, and the capital is converted back to cash. Subsequent days’ sell signals are again treated as non-trade instructions, until another buy signal is received, and the process is repeated.

An ANFIS can construct an input–output mapping based on both human knowledge in the form of fuzzy if–then rules with appropriate membership functions and stipulated input–output data pairs. It applies a neural network in determination of the shape of membership functions and rule extraction. ANFIS architecture uses a hybrid learning procedure in the framework of adaptive networks (Jang, 1993).

Suppose that the FIS under consideration has two inputs (x and y) and one output (z) assuming that the rule base contains two fuzzy if–then rules of Takagi and Sugeno's type (Takagi & Sugeno, 1983). The fuzzy model is based on a first order Sugeno polynomial that is generally composed of rules of the form:

                           
                              Rule 1.
                              If x is A1 and y is B1, then 
                                    
                                       
                                          f
                                          1
                                       
                                       =
                                       
                                          (
                                          
                                             
                                                p
                                                1
                                             
                                             x
                                             +
                                             
                                                q
                                                1
                                             
                                             y
                                             +
                                             
                                                r
                                                1
                                             
                                          
                                          )
                                       
                                    
                                 .

If x is A2 and y is B2, then 
                                    
                                       
                                          f
                                          2
                                       
                                       =
                                       
                                          (
                                          
                                             
                                                p
                                                2
                                             
                                             x
                                             +
                                             
                                                q
                                                2
                                             
                                             y
                                             +
                                             
                                                r
                                                2
                                             
                                          
                                          )
                                       
                                    
                                 .

Each node i in this layer has a node function as
                                    
                                       (5)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         O
                                                         i
                                                         1
                                                      
                                                      =
                                                      
                                                         μ
                                                         
                                                            A
                                                            i
                                                         
                                                      
                                                      
                                                         (
                                                         x
                                                         )
                                                      
                                                      ,
                                                      i
                                                      =
                                                      1
                                                      ,
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

Here, x is the input to node i, and Ai
                                  is the linguistic label (small, large, etc.) associated with this node function. In other words, 
                                    
                                       O
                                       i
                                       l
                                    
                                  is the membership function of Ai
                                  and it specifies the degree to which the given x satisfies the quantifier Ai
                                 . Usually 
                                    
                                       
                                          μ
                                          
                                             A
                                             i
                                          
                                       
                                       
                                          (
                                          x
                                          )
                                       
                                    
                                  is chosen as bell-shaped with a maximum value of 1 and a minimum value of 0, where a generalized bell function can be expressed as:
                                    
                                       (6)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         μ
                                                         
                                                            A
                                                            i
                                                         
                                                      
                                                      
                                                         (
                                                         x
                                                         )
                                                      
                                                      =
                                                      
                                                         1
                                                         
                                                            1
                                                            +
                                                            
                                                               
                                                                  
                                                                     [
                                                                     
                                                                        
                                                                           (
                                                                           
                                                                              
                                                                                 x
                                                                                 −
                                                                                 
                                                                                    c
                                                                                    i
                                                                                 
                                                                              
                                                                              
                                                                                 a
                                                                                 i
                                                                              
                                                                           
                                                                           )
                                                                        
                                                                        
                                                                           
                                                                           2
                                                                        
                                                                     
                                                                     ]
                                                                  
                                                               
                                                               
                                                                  b
                                                                  i
                                                               
                                                            
                                                            
                                                               
                                                               
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 where {ai, bi, ci
                                 } is the membership function parameter set. As the values of these parameters change, bell-shaped functions also alter accordingly, exhibiting various forms of membership functions on linguistic label Ai. Parameters in this layer are referred to as premise parameters.

Every node in this layer multiplies the incoming signals and sends the product out
                                    
                                       (7)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         O
                                                         i
                                                         2
                                                      
                                                      =
                                                      
                                                         w
                                                         i
                                                      
                                                      =
                                                      
                                                         μ
                                                         
                                                            A
                                                            i
                                                         
                                                      
                                                      
                                                         (
                                                         x
                                                         )
                                                      
                                                      
                                                         μ
                                                         
                                                            B
                                                            i
                                                         
                                                      
                                                      
                                                         (
                                                         y
                                                         )
                                                      
                                                      ,
                                                      
                                                      i
                                                      =
                                                      1
                                                      ,
                                                      
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

Each node output represents the firing strength of a rule.

Every node in this layer calculates the ratio of the ith rule's firing strength to the sum of all rules’ firing strengths:
                                    
                                       (8)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         O
                                                         i
                                                         3
                                                      
                                                      =
                                                      
                                                         
                                                            w
                                                            ¯
                                                         
                                                         i
                                                      
                                                      =
                                                      
                                                         
                                                            w
                                                            i
                                                         
                                                         
                                                            
                                                               w
                                                               1
                                                            
                                                            +
                                                            
                                                               w
                                                               2
                                                            
                                                         
                                                      
                                                      ,
                                                      i
                                                      =
                                                      1
                                                      ,
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

For the sake of simplicity, the outputs of this layer are called normalized firing strengths.

Every node in this layer has a node function as
                                    
                                       (9)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         O
                                                         i
                                                         4
                                                      
                                                      =
                                                      
                                                         
                                                            w
                                                            ¯
                                                         
                                                         i
                                                      
                                                      
                                                         f
                                                         i
                                                      
                                                      =
                                                      
                                                         
                                                            w
                                                            ¯
                                                         
                                                         i
                                                      
                                                      
                                                         (
                                                         
                                                            
                                                               p
                                                               i
                                                            
                                                            x
                                                            +
                                                            
                                                               q
                                                               i
                                                            
                                                            y
                                                            +
                                                            
                                                               r
                                                               i
                                                            
                                                         
                                                         )
                                                      
                                                      ,
                                                      i
                                                      =
                                                      1
                                                      ,
                                                      2
                                                   
                                                
                                             
                                          
                                       
                                    
                                 where 
                                    
                                       
                                          w
                                          ¯
                                       
                                       i
                                    
                                  is the output of Layer 3, and {pi, qi, ri
                                 } is the parameter set. The parameters in this layer are referred to as consequent parameters.

The single node in this layer computes the overall output as the summation of all incoming signals:
                                    
                                       (10)
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         O
                                                         i
                                                         5
                                                      
                                                      =
                                                      
                                                         overall
                                                         
                                                         output
                                                      
                                                      =
                                                      
                                                         Σ
                                                         i
                                                      
                                                      
                                                         
                                                            w
                                                            ¯
                                                         
                                                         i
                                                      
                                                      
                                                         f
                                                         i
                                                      
                                                      =
                                                      
                                                         
                                                            
                                                               Σ
                                                               i
                                                            
                                                            
                                                               w
                                                               i
                                                            
                                                            
                                                               f
                                                               i
                                                            
                                                         
                                                         
                                                            
                                                               Σ
                                                               i
                                                            
                                                            
                                                               w
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              

The last layer has a single fixed node and its outputs have crisp characteristics.

In ANFIS structure, the premise and consequent parameters should be noted as important factors for the learning algorithm in which each parameter is utilized to calculate the output data of the training data. The premise part of a rule defines a subspace, while the consequent part specifies the output within this fuzzy subspace (Jang, 1993). It is observed that given the values of premise parameters, the overall output can be expressed as linear combinations of the consequent parameters. More precisely, the output of the ANFIS model can be written as in (Jang, 1993):

                           
                              (11)
                              
                                 
                                    
                                       
                                          
                                             f
                                             =
                                             
                                                
                                                   w
                                                   1
                                                
                                                
                                                   
                                                      w
                                                      1
                                                   
                                                   +
                                                   
                                                      w
                                                      2
                                                   
                                                
                                             
                                             
                                                f
                                                1
                                             
                                             +
                                             
                                             
                                                
                                                   w
                                                   2
                                                
                                                
                                                   
                                                      w
                                                      1
                                                   
                                                   +
                                                   
                                                      w
                                                      2
                                                   
                                                
                                             
                                             
                                                f
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Substituting Eq. (8) into Eq. (11) yields:

                           
                              (12)
                              
                                 
                                    
                                       
                                          
                                             f
                                             =
                                             
                                                
                                                   w
                                                   ¯
                                                
                                                1
                                             
                                             
                                                f
                                                1
                                             
                                             +
                                             
                                                
                                                   w
                                                   ¯
                                                
                                                2
                                             
                                             
                                                f
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Substituting the fuzzy if–then rules into Eq. (12), the equation becomes:

                           
                              (13)
                              
                                 
                                    
                                       
                                          
                                             f
                                             =
                                             
                                                
                                                   w
                                                   ¯
                                                
                                                1
                                             
                                             
                                                (
                                                
                                                   p
                                                   1
                                                
                                                x
                                                +
                                                
                                                   q
                                                   1
                                                
                                                y
                                                +
                                                
                                                   r
                                                   1
                                                
                                                )
                                             
                                             +
                                             
                                                
                                                   w
                                                   ¯
                                                
                                                2
                                             
                                             
                                                (
                                                
                                                   p
                                                   2
                                                
                                                x
                                                +
                                                
                                                   q
                                                   2
                                                
                                                y
                                                +
                                                
                                                   r
                                                   2
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

After rearrangement, the output f can be expressed as:

                           
                              (14)
                              
                                 
                                    
                                       
                                          f
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                (
                                                
                                                   
                                                      
                                                         w
                                                         ¯
                                                      
                                                      1
                                                   
                                                   x
                                                
                                                )
                                             
                                             
                                                p
                                                1
                                             
                                             +
                                             
                                                (
                                                
                                                   
                                                      
                                                         w
                                                         ¯
                                                      
                                                      1
                                                   
                                                   y
                                                
                                                )
                                             
                                             
                                                q
                                                1
                                             
                                             +
                                             
                                                (
                                                
                                                   
                                                      w
                                                      ¯
                                                   
                                                   1
                                                
                                                )
                                             
                                             
                                                r
                                                1
                                             
                                             +
                                             
                                                (
                                                x
                                                )
                                             
                                             
                                                p
                                                2
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             +
                                             
                                             
                                                (
                                                
                                                   
                                                      
                                                         w
                                                         ¯
                                                      
                                                      2
                                                   
                                                   y
                                                
                                                )
                                             
                                             
                                                q
                                                2
                                             
                                             +
                                             
                                                (
                                                
                                                   
                                                      w
                                                      ¯
                                                   
                                                   2
                                                
                                                )
                                             
                                             
                                                r
                                                2
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

A fuzzy expert system construction is comprised from four stages: fuzzification, inference, composition and defuzzification respectively. The basic design of a fuzzy inference system is illustrated in Fig. 3.
                        
                     

In this system both the input and output values are real numbers. In order to define the membership functions, input and output values were defined as the linguistic variables and rules are determined based on these linguistic variables. Details of the fuzzy inference system are summarized as follows.


                        
                           Step 1
                           —
                           Fuzzification
                        
                     

In this step, critical factors, membership functions, and fuzzy sets that affect the system are identified. Fuzzification simply refers to the process of taking a crisp input value and transforming it into the degree required by the terms. If the form of uncertainty happens to arise because of imprecision, ambiguity, or vagueness, the variable is most probably fuzzy and can be represented by a membership function. If the inputs generally originate from a piece of hardware, or drive from sensor measurement, then these crisp numerical inputs could be fuzzified in order for them to be used in a fuzzy inference system (Ross, 1976). Membership function graphics of all input variables are shown in Fig. 4(a–f).

Each input and output variables’ box-plot graphics have been drawn to determine the border of each class. Each variable's minimum to the first quartile interval is classified as “Negative”, the interval from the first to the third quartile is classified as “Stationary”, and the interval from the third quartile to the maximum is classified as “Positive”. Following this, each class of all variables’ mean and standard deviation values have been calculated and used for parameters of Gaussian membership function.


                        
                           Step 2
                           —
                           Fuzzy rule construction
                        
                     

Fuzzy systems make decisions and generate output values based on knowledge provided by the designer in the form of “if–then” rules. The rule base specifies qualitatively how the output parameter of stock exchange value is determined for various instances of the input factors (Fasanghari & Montazer, 2010).

Conventionally, domain experts are deployed to develop a fuzzy rule base system, but stock returns are affected by many different external factors and political scenarios, making it a challenge to easily evaluate and integrate expert knowledge into the data analytic modeling. All fuzzy rule base systems work with a set of rules extracted from the data. In this study, a different method was adopted to construct the fuzzy rules. By using the input and output variables’ membership functions, the number of incidences of all possible cases was determined. There were potentially 37
                        =2187 rules in the study. As a subsequent step, it is intended to seek combinations of attribute-value pairs that have high coverage (i.e. a proportion of the number of instances to which the rule applies) and high accuracy (i.e. high number of instances that are predicted correctly) (Witten, Frank, & Hall, 2011). The input and output variables are addressed with the corresponding membership functions, and then fuzzy “IF–THEN” rules linking the input and output membership functions are identified based on the generated decision tree from the data set. The fuzzy outputs of all rules are aggregated to one fuzzy set. The resulted rules are then pruned down on the basis of their coverage and their accuracy. Finally, the most important rules were extracted by deploying the Classification and Regression Tree (C&RT or CART) model as a subsequent step (Witten et al., 2011).


                        Fig. 5
                        
                         illustrates how daily BIST 100 values change based on the two independent variables, namely trend and the returns from the Nasdaq Composite. This is a clear representation of the fact that the relationships among the independent variables and dependent variable are nonlinear, one of the main reasons for the poor modeling performance of simple linear regression model and AR models as observed in our preliminary runs. Consequently, sophisticated data analytical methods as exemplified in this study are hypothesized to perform better under these circumstances.


                        
                           Step 3
                           —
                           Composition
                        
                     

The composition of input values is guided by the fuzzy rules that were formed above. The most frequently used method was also utilized here, standard max–min inference, to implement the composition step.


                        
                           Step 4
                        
                        —
                        
                           Defuzzification
                        
                     

The process of computing a single number which represents the best outcome of fuzzy set evaluation is called defuzzification (Ngai & Wat, 2003). Due to its popularity, the center of gravity method was utilized in the implementation of defuzzification in this study which is given by Eq. (15) (Bai & Wang, 2006):

                           
                              (15)
                              
                                 
                                    
                                       
                                          
                                             
                                                x
                                                ′
                                             
                                             =
                                             
                                                
                                                   
                                                      ∫
                                                      μ
                                                   
                                                   
                                                      (
                                                      x
                                                      )
                                                   
                                                   x
                                                   d
                                                   x
                                                
                                                
                                                   
                                                      ∫
                                                      μ
                                                   
                                                   
                                                      (
                                                      x
                                                      )
                                                   
                                                   d
                                                   x
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

An ANN is an interconnected assembly of simple parallel processing elements, units or nodes, whose functionality is loosely based on an animal neuron. The processing ability of the network is stored in the inter-unit connection strengths or weights, obtained by a process of adaptation to or learning from a set of training patterns (Hassoun, 1995). Artificial neurons are the processing elements of ANNs. The very first step in ANNs is the computation of the weighted sums of all input elements entering each processing element (neuron). The net input of neuron j is

                           
                              (16)
                              
                                 
                                    
                                       
                                          
                                             
                                                Y
                                                j
                                             
                                             =
                                             
                                                ∑
                                                i
                                             
                                             
                                                w
                                                
                                                   i
                                                   j
                                                
                                             
                                             
                                                x
                                                i
                                             
                                             +
                                             
                                                θ
                                                j
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where xi
                        ’s are the outputs of the neurons in the previous layer, wij
                         is the synaptic weight of neuron i to neuron j, and θj
                         is the bias which is the constant value of the sigmoid function (Turban et al., 2011). Then the weighted sum passes through a transformation (transfer) or activation function and this value becomes the output of the neuron. The Sigmoid logical activation function (or sigmoid transfer function) as given by Eq. (17)
                        
                           
                              (17)
                              
                                 
                                    
                                       
                                          
                                             f
                                             
                                                (
                                                x
                                                )
                                             
                                             =
                                             
                                                1
                                                
                                                   1
                                                   +
                                                   
                                                      e
                                                      
                                                         −
                                                         x
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        is an S-shaped transfer function in the range of 0–1. It is a popular function to use as an activation function because of its capability of capturing the nonlinearity (Turban et al., 2011).

The most critical step in ANN is the training. Back propagation is the most popular learning algorithm, a gradient descent algorithm that propagates the errors through the network and allows adaptation of the hidden neurons. It minimizes the total error via adjusting the weights along its gradient (Principe, Euliano, & Lefebvre, 1999). Root mean square error (RMSE) is the conventionally used total error value. It can be calculated as in Eq. (18)
                        
                           
                              (18)
                              
                                 
                                    
                                       
                                          
                                             RMSE
                                             =
                                             
                                                
                                                   
                                                      1
                                                      n
                                                   
                                                   
                                                      ∑
                                                      
                                                         t
                                                         =
                                                         1
                                                      
                                                      n
                                                   
                                                   
                                                      
                                                         
                                                            (
                                                            
                                                               
                                                                  Y
                                                                  t
                                                               
                                                               −
                                                               
                                                                  O
                                                                  t
                                                               
                                                            
                                                            )
                                                         
                                                      
                                                      2
                                                   
                                                
                                             
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where Ot
                         is the output vector and t is the index for output units. During back propagation learning, weights are modified according to their contribution to the error function, which is given by Eq. (19):

                           
                              (19)
                              
                                 
                                    
                                       
                                          
                                             Δ
                                             
                                                w
                                                
                                                   i
                                                   j
                                                
                                             
                                             =
                                             −
                                             η
                                             
                                                
                                                   ∂
                                                   (
                                                   
                                                      R
                                                      M
                                                      S
                                                      E
                                                   
                                                   )
                                                
                                                
                                                   ∂
                                                   
                                                      w
                                                      
                                                         i
                                                         j
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where η is the learning rate which determines the magnitude of changes to be made in the learning parameter (Haykin, 1999).

Multilayer perceptron (MLP) models are the most popular neural network models (Hornik, Stinchcombe, & White, 1990). They empirically showed that given the right size and structure, MLP is capable of learning arbitrarily complex nonlinear functions to arbitrary accuracy levels. This type of neural network consists of a feed-forward network and supervised learning that requires a desired output in order to learn. In MLP models, there is always an input layer with a number of neurons equal to the number of variables of the problem, and an output layer, where the perceptron response is made available, with a number of neurons equal to the desired number of quantities computed from the inputs (Kizilaslan & Karlik, 2009).

In this study, a three layer feed-forward neural network is employed. There are six inputs for the network which are represented by six neurons in Fig. 6
                        . The output layer has one neuron with log sigmoid as the transfer function. This transfer function results in a continuous value output between 0 and 1. For the prediction of the up or down movements, a threshold value of 0.5 is used. If the output value is greater than 0.5, the prediction is considered to be an “up” movement, else the prediction is considered a “down” movement. Tan-sigmoid function is used as a hidden layer transfer function. Gradient descent with momentum is used for setting the weight. By using the above parameter settings, different training algorithms were tried in order to find the best model under the back-propagation algorithm models.

There are many training algorithms in neural network models, but there is no exact information about the best model for defined data types. Different forecasting algorithms produce different results because the performance changes according to the data sets, network types, weight type, search type and other variations in the algorithm. We have performed comprehensive parameter settings to determine parameters for each of the 10 folds. After determining the best model by calculating the average of test performances for each of the 10 folds, parameter combinations which give the best performance were selected. The artificial neural network model parameters are the number of hidden layers, number of hidden layer neurons (n), number of epochs (iterations), the momentum rate, and the value of learning rate. 100 levels of hidden layer neuron numbers “n” were heuristically tested in order to determine the best number of neurons in the hidden layers. Learning rate and momentum constants were altered between 0.01 and 0.9. Number of the epochs was fixed at 500. Each of the parameter combination was replicated ten times for each fold and all results were saved in order to find the best models for the network. After all experiments, back-propagation algorithm with the MLP gave the most favorable results with the [6-n-m-1] architecture having the best fit, i.e. accuracy. In Fig. 6, the input, hidden, and the output layers are pictorially illustrated.

Based on Vladimir Vapnik's statistical learning theory (Cortes & Vapnik, 1995), a SVM is a supervised learning algorithm that can classify using priori defined categories or perform regression, hence it is sometimes referred to as a support vector regression (Lu & Wang, 2010; Vapnik, 1998). Due to its useful features and promising empirical performance, SVM algorithms are gaining more popularity (Cho, Asfour, Onar, & Kaundinya, 2005). Its structural risk minimization feature shows superiority to other traditional empirical risk minimization based methods. Structural risk minimization minimizes the expected risk of an upper bound while empirical risk minimization minimizes the error of the training data. Hence, SVMs provide a good generalization performance with a computational efficiency in terms of speed and complexity, and easily deal with multi-dimensional data (Cho et al., 2005). In addition, SVMs work well under many circumstances even when there is a small sample dataset (Cristianini & Shawe-Taylor, 2000).

An SVM classifier takes the inputs from different classes, and then builds input vectors into a feature space. An SVM model splits the training examples into separated categories in a mapped space as certain points (Kecman, 2005), and maps these points as vectors into a higher dimensional feature space. The vectors transform a linear or non-linear map into the feature space (Shiue, 2009). SVMs use an optimal hyperplane to separate classes in a data set, where optimality is defined as being placed at the maximum distance from the nearest points of the data set (Cortes & Vapnik, 1995). The points which determine the optimal hyperplane are called “support vectors”. These are the critical elements to train the classifying algorithm (Kecman, 2005).

To find the hyperplane in a feature space, Lagrange multipliers are introduced to solve a quadratic problem. The SVM's classification process is briefly reviewed as follows. First, the following inequalities can be written as in Eq. (20):

                           
                              (20)
                              
                                 
                                    
                                       
                                          
                                             w
                                             
                                                X
                                                i
                                             
                                             +
                                             b
                                             ≥
                                             +
                                             1
                                             
                                             f
                                             o
                                             r
                                             
                                             
                                                y
                                                i
                                             
                                             =
                                             +
                                             1
                                          
                                       
                                    
                                    
                                       
                                          
                                             w
                                             
                                                X
                                                i
                                             
                                             +
                                             b
                                             ≤
                                             −
                                             1
                                             
                                             f
                                             o
                                             r
                                             
                                             
                                                y
                                                i
                                             
                                             =
                                             −
                                             1
                                          
                                       
                                    
                                 
                              
                           
                        where xi
                         is a feature of the ith example, yi
                         denotes an output for the ith example as a binary value; w is a weight and b is a bias. When condition in Eq. (20) is considered for all pairs of (xi, yi
                        ) for i
                        =1, 2
                           …
                         
                        m, Eq. (21) is obtained:

                           
                              (21)
                              
                                 
                                    
                                       
                                          
                                             
                                                {
                                                
                                                   
                                                      (
                                                      
                                                         x
                                                         i
                                                      
                                                      ,
                                                      
                                                         y
                                                         i
                                                      
                                                      )
                                                   
                                                   
                                                      |
                                                   
                                                   
                                                      x
                                                      i
                                                   
                                                   ∈
                                                   
                                                      R
                                                      N
                                                   
                                                   ,
                                                   
                                                      y
                                                      i
                                                   
                                                   ∈
                                                   
                                                      {
                                                      
                                                         −
                                                         1
                                                         ,
                                                         1
                                                      
                                                      }
                                                   
                                                
                                                }
                                             
                                             
                                                i
                                                =
                                                1
                                             
                                             m
                                          
                                       
                                    
                                 
                              
                           
                        where m labels are the given examples in a data set.

When used for classification purposes, SVMs are extremely powerful since they can be used for linearly and non-linearly separable datasets (Han, Kamber, & Pei, 2011). Fig. 7
                         depicts the difference between both types of datasets. From Fig. 7, a linearly separable dataset can be defined to be any type of data where a straight line (or a hyperplane in higher dimensions) can separate all tuples of the negative outcome (which is recoded to a −1 class) from all the tuples of class +1. Based on Fig. 7a, there is infinite number of possibilities to draw a hyperplane to separate both sets. SVMs allow us to identify the best hyperplane, as explained next.

When the data are linearly separable, SVMs construct a hyperplane on the feature space to distinguish the training tuples in the data such that the margin between the support vectors is maximized. The optimal hyperplane is achieved by maximizing a margin between support vectors where ‖w‖ denotes the maximization of the margin by following a quadratic problem solution:

                           
                              (22)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                min
                                                
                                                   1
                                                   2
                                                
                                                
                                                   
                                                      ∥
                                                      w
                                                      ∥
                                                   
                                                   T
                                                
                                                
                                                   ∥
                                                   w
                                                   ∥
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             
                                                s
                                                
                                                   .
                                                   t
                                                
                                                .
                                                
                                                
                                                   y
                                                   i
                                                
                                                
                                                   (
                                                   
                                                      w
                                                      T
                                                   
                                                   
                                                      x
                                                      i
                                                   
                                                   +
                                                   b
                                                   )
                                                
                                                ≥
                                                1
                                                
                                                and
                                                
                                                i
                                                =
                                                1
                                                ,
                                                2
                                                ,
                                                …
                                                ,
                                                m
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Karush–Kuhn–Tucker conditions apply to this problem as shown in Eq. (23): 
                           
                              (23)
                              
                                 
                                    
                                       
                                       
                                       
                                          
                                             
                                                max
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   m
                                                
                                                
                                                   α
                                                   i
                                                
                                                −
                                                
                                                   1
                                                   2
                                                
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   m
                                                
                                                
                                                   
                                                      ∑
                                                      
                                                         i
                                                         =
                                                         1
                                                      
                                                      m
                                                   
                                                   
                                                      α
                                                      i
                                                   
                                                
                                                
                                                   α
                                                   j
                                                
                                                
                                                   y
                                                   i
                                                
                                                
                                                   y
                                                   j
                                                
                                                
                                                   
                                                      
                                                         x
                                                         i
                                                      
                                                   
                                                   T
                                                
                                                
                                                   x
                                                   j
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             
                                                s
                                                
                                                   .
                                                   t
                                                
                                                .
                                                
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   m
                                                
                                                
                                                   α
                                                   i
                                                
                                                
                                                   y
                                                   i
                                                
                                                =
                                                0
                                                ,
                                                
                                                0
                                                ≤
                                                
                                                   α
                                                   i
                                                
                                                ≤
                                                C
                                                
                                                and
                                                
                                                i
                                                =
                                                1
                                                ,
                                                2
                                                ,
                                                …
                                                ,
                                                m
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where C is a penalty parameter; Lagrange multipliers assist to solve the problem (Cristianini & Shawe-Taylor, 2000).

SVM algorithms also use kernels to reduce the complexity of problems as mapping them in a higher dimensional space (Vapnik, 1998). The kernel function allows mapping the same data in a linearly separable way on a higher dimensional space. The kernel trick is formed as follows:

                           
                              (24)
                              
                                 
                                    
                                       
                                          
                                             K
                                             
                                                (
                                                
                                                   
                                                      x
                                                      i
                                                   
                                                   ,
                                                
                                                
                                                   x
                                                   j
                                                
                                                )
                                             
                                             =
                                             ϕ
                                             
                                                
                                                   (
                                                   
                                                      x
                                                      i
                                                   
                                                   )
                                                
                                                T
                                             
                                             ϕ
                                             
                                                (
                                                
                                                   x
                                                   j
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Polynomial kernel, Gaussian radial basis function (as in Eq. (25)), and sigmoid function (as in Eq. (26)) are among the most commonly used kernel functions.

                           
                              (25)
                              
                                 
                                    
                                       
                                          
                                             K
                                             
                                                (
                                                
                                                   
                                                      x
                                                      i
                                                   
                                                   ,
                                                
                                                
                                                   y
                                                   j
                                                
                                                )
                                             
                                             =
                                             
                                                
                                                   (
                                                   
                                                      (
                                                      
                                                         x
                                                         i
                                                      
                                                      ,
                                                      
                                                         y
                                                         j
                                                      
                                                      )
                                                   
                                                   +
                                                   1
                                                   )
                                                
                                                p
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (26)
                              
                                 
                                    
                                       
                                          
                                             K
                                             
                                                (
                                                
                                                   
                                                      x
                                                      i
                                                   
                                                   ,
                                                
                                                
                                                   y
                                                   j
                                                
                                                )
                                             
                                             =
                                             exp
                                             
                                                (
                                                −
                                                
                                                   
                                                      ∥
                                                   
                                                   
                                                      (
                                                      
                                                         
                                                            x
                                                            i
                                                         
                                                         −
                                                      
                                                      
                                                         y
                                                         j
                                                      
                                                      )
                                                   
                                                   
                                                      
                                                         ∥
                                                      
                                                      2
                                                   
                                                
                                                /
                                                2
                                                
                                                   σ
                                                   2
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

As a kernel function is imposed, the problem transforms into a new quadratic model: 
                           
                              (27)
                              
                                 
                                    
                                       
                                          
                                             max
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                m
                                             
                                             
                                                α
                                                i
                                             
                                             −
                                             
                                                1
                                                2
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                m
                                             
                                             
                                                
                                                   ∑
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   m
                                                
                                                
                                                   α
                                                   i
                                                
                                             
                                             
                                                α
                                                j
                                             
                                             
                                                y
                                                i
                                             
                                             
                                                y
                                                j
                                             
                                             ϕ
                                             
                                                
                                                   (
                                                   
                                                      x
                                                      i
                                                   
                                                   )
                                                
                                                T
                                             
                                             ϕ
                                             
                                                (
                                                
                                                   x
                                                   j
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (28)
                              
                                 
                                    
                                       
                                          
                                             s
                                             
                                                .
                                                t
                                             
                                             .
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                m
                                             
                                             
                                                α
                                                i
                                             
                                             
                                                y
                                                i
                                             
                                             =
                                             0
                                             ,
                                             
                                             0
                                             ≤
                                             
                                                α
                                                i
                                             
                                             ≤
                                             C
                                             
                                             and
                                             
                                             i
                                             =
                                             1
                                             ,
                                             2
                                             ,
                                             …
                                             ,
                                             m
                                          
                                       
                                    
                                 
                              
                           
                        
                     

Finally, a decision, which is only about the sign and not the magnitude, is obtained as in Eq. (29)
                        
                           
                              (29)
                              
                                 
                                    
                                       
                                          
                                             D
                                             
                                                (
                                                x
                                                )
                                             
                                             =
                                             s
                                             i
                                             g
                                             n
                                             
                                                (
                                                
                                                   ∑
                                                   
                                                      k
                                                      =
                                                      1
                                                   
                                                   n
                                                
                                                
                                                   a
                                                   k
                                                
                                                
                                                   y
                                                   k
                                                
                                                K
                                                
                                                   (
                                                   
                                                      x
                                                      ,
                                                   
                                                   
                                                      x
                                                      k
                                                   
                                                   )
                                                
                                                +
                                                b
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

More detailed information about SVM algorithms and applications can be found in Cristianini and Shawe-Taylor (2000), Vapnik (1998), and Kecman(2005).

As was the case in ANNs, SVMs do not either have a predetermined best parameter set which would universally yield the optimum solution in every setting. Since this study adopted 10-fold cross-validated performance measurement, at each of the ten folds different parameters were adopted via an algorithmic approach by incremental changes in the parameters. To summarize, in order to determine the best hyperplane for each fold, in this study four types of Kernel functions were deployed, i.e. linear, polynomial, sigmoid, and radial basis function (RBF). Thereafter, in order to fine-tune the separated hyperplanes and to avoid over-fitting, a Kernel function (regularization parameter) was also determined for each of the ten folds, which controls the trade-off between misclassification error and the over-fitting rate. The optimum sets of results for each of these parameter combinations vary drastically from one fold to another fold. Moreover, they do not have any pictorially appealing representation other than being some numeric values therefore their presentation is purposefully omitted in this study.

After determining the performance of the different predictive models, the relative importance of each of the independent variables is measured using sensitivity analysis. This phase is indispensable to the analyses for several reasons. First, it can suggest the underlying casual factors for any of the prediction models. This is particularly important in understanding and communicating the results of ANNs (Davis, 1989) which are still considered by many to be black-box models (Molaie, Falahian, Gharibzadeh, Jafari, & Sprott, 2014). A second major reason for the importance of sensitivity analysis is it provides us with a framework to capture the importance of independent variables across different models.

The sensitivity of a specific predictor variable is calculated by taking the proportion of the error of the model that includes this variable to the error of the model when it does not include this specific variable (Principe, Euliano, & Lefebvre, 2000). The importance of a variable is in direct proportion to variance of predictive error of the classification model in the absence of that specific variable. The same method is followed for all classification models, and is used in ranking the relative importance of the variables of each classification model according to the sensitivity measure defined by Saltelli (2002). Their measure is defined as in Eq. (30)
                        
                           
                              (30)
                              
                                 
                                    
                                       
                                          
                                             
                                                S
                                                i
                                             
                                             =
                                             
                                                
                                                   V
                                                   i
                                                
                                                
                                                   V
                                                   (
                                                   y
                                                   )
                                                
                                             
                                             =
                                             
                                                
                                                   V
                                                   
                                                      (
                                                      
                                                         E
                                                         
                                                            (
                                                            
                                                               y
                                                               |
                                                               
                                                                  x
                                                                  i
                                                               
                                                            
                                                            )
                                                         
                                                      
                                                      )
                                                   
                                                
                                                
                                                   V
                                                   (
                                                   y
                                                   )
                                                
                                             
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where y is the dichotomous output, and the unconditional output variance is denoted by V(y). The expectation operator is denoted by E, which calls for an integral over all predictor variables except xi
                        . A further integral operator is implied over xi
                         by the operator Vi
                        . The importance of a specific variable is then computed as the normalized sensitivity as described by Saltelli, Tarantola, Campolongo, and Ratto (2004).

Information fusion techniques combine information obtained from multiple forecasts (from various data analytic models) in an attempt to decrease model uncertainty and increase the knowledge extracted. The motivation for such techniques also stem from observations in the literature about combining multiple forecasts. Specifically, it has been shown that robustness and accuracy of information can be increased (Cang & Yu, 2014), while the uncertainty and bias of individual models can be decreased by combining multiple forecasts (Clemen, 1989). There is an increasing deployment of information fusion techniques in data-analytic problems as opposed to the application of a single method (Graefe, Armstrong, Jones, & Cuzán, 2014). It should be noted, however, that there is no best way of combining predictions in explanatory data analysis situations, as in the case of this analysis. This is similar to the application of data mining techniques, where the best model is problem-specific and often cannot be determined prior to investigation. In such situations, the determination can only be done via “trial-and-error” experimentation (Ruiz & Nieto, 2000). In this study, the information fusion model which is presented by Sevim, Oztekin, Bali, Gumus, and Guresen (2014) is adopted since it allows decision makers to rank the input variables in terms of importance order. In other words, it would be more intuitive to explain the findings to the domain experts. The mathematical formulation of the information fusion is presented as follows.

For any of the data analytic models (SVM, ANN, and ANFIS), the formulation for the prediction model can be generalized as in Eq. (31)
                        
                           
                              (31)
                              
                                 
                                    
                                       
                                          
                                             
                                                y
                                                ^
                                             
                                             =
                                             g
                                             
                                                (
                                                
                                                   
                                                      x
                                                      1
                                                   
                                                   ,
                                                   
                                                      x
                                                      2
                                                   
                                                   ,
                                                   …
                                                   ,
                                                   
                                                      x
                                                      m
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where g represents a prediction function for the model, and the remaining variables have been explained in Section 2.1 (y is the dichotomous response variable, x is the value for a specific input variable, and m denotes the number of input variables in the model). Given r different predictor models to be combined (which for this current case is 3: SVM, ANN, and ANFIS), the information fusion model can be represented as in Eq. (32):
                        
                        
                           
                              (32)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   y
                                                   ^
                                                
                                                
                                                   f
                                                   u
                                                   s
                                                   e
                                                   d
                                                
                                             
                                             =
                                             Ψ
                                             
                                                (
                                                
                                                   
                                                      g
                                                      1
                                                   
                                                   
                                                      (
                                                      x
                                                      )
                                                   
                                                   ,
                                                   
                                                      g
                                                      2
                                                   
                                                   
                                                      (
                                                      x
                                                      )
                                                   
                                                   ,
                                                   …
                                                   ,
                                                   
                                                      g
                                                      r
                                                   
                                                   
                                                      (
                                                      x
                                                      )
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

In the Eq. (32), x denotes the vector of predictor variables. In this study, ψ is defined as a linear function, which reduces the information fusion model to

                           
                              (33)
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   y
                                                   ^
                                                
                                                
                                                   f
                                                   u
                                                   s
                                                   e
                                                   d
                                                
                                             
                                             =
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                r
                                             
                                             
                                                λ
                                                1
                                             
                                             
                                                g
                                                1
                                             
                                             
                                                (
                                                x
                                                )
                                             
                                             +
                                             
                                                λ
                                                2
                                             
                                             
                                                g
                                                2
                                             
                                             
                                                (
                                                x
                                                )
                                             
                                             +
                                             …
                                             +
                                             
                                                λ
                                                r
                                             
                                             
                                                g
                                                r
                                             
                                             
                                                (
                                                x
                                                )
                                             
                                             ,
                                             
                                             where
                                             
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                r
                                             
                                             
                                                λ
                                                i
                                             
                                             =
                                             1
                                             
                                             
                                             
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

The values for the λs are the updated classification “accuracy” measures of a single classification model. That is, the higher the accuracy metric of an individual predictor model, the more impact of it on the fused model (Sevim et al., 2014). The final sensitivity measure of a specific variable (denoted by θ) that is fused by r individual models can be obtained by combining Eqs. (30) and (33). The results is shown by Eq. (34)
                        
                           
                              (34)
                              
                                 
                                    
                                       
                                          
                                             
                                                S
                                                θ
                                             
                                             
                                                (
                                                
                                                   f
                                                   u
                                                   s
                                                   e
                                                   d
                                                
                                                )
                                             
                                             =
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                r
                                             
                                             
                                                λ
                                                i
                                             
                                             
                                                S
                                                
                                                   i
                                                   ,
                                                   θ
                                                
                                             
                                             =
                                             
                                                λ
                                                1
                                             
                                             x
                                             
                                                S
                                                
                                                   1
                                                   ,
                                                   θ
                                                
                                             
                                             +
                                             …
                                             +
                                             
                                                λ
                                                r
                                             
                                             x
                                             
                                                S
                                                
                                                   r
                                                   ,
                                                   θ
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where S
                        
                           i, θ
                         is the normalized sensitivity measure of the θth variable in the ith model. After deriving the importance of each predictor variable dictated from each classification model, as shown by Eq. (34), input variables are ranked in importance order for decision making.

@&#RESULTS AND DISCUSSION@&#

Stock market data are a good example of non-stationary data. At a given particular time, there can be trend, cycles, random walks, or combination of these (Patel et al., 2015), and for the BIST, stock values are affected by many different external factors that create trends. Turkey's dynamic political scenarios even more severely influence the results.

In this paper, as a first step, experiments were conducted to determine the influential input variables. After those experiments, parameter settings were fine-tuned for each of the data analytic models deployed. Various learning algorithms and parameters related to these algorithms were used to determine the learning algorithm that gives the best performance. In order to overcome the effect of random data splitting which gives rise to bias and variation, 10-fold stratified cross-validation was deployed. Accuracy, sensitivity, and specificity of all three data analytic models are tabulated in Table 2. As seen in Table 2, SVM outperforms the other two data analytic models, ANN and ANFIS, in terms of its accuracy results. The consistent superior performance of the SVM in all 10 folds of data can be attributed to the fact that SVMs use the marginal and not the average values in the data set to construct the classification prediction model.

An interesting result yielded in the study is that specificity values of all three models are much higher than their accuracy and sensitivity results. This signals that data analytic models perform better in predicting the negative change in the BIST 100 Index as opposed to forecasting a positive one. In other words, specificity refers to the correct predictions during a decreasing market, in which all three models perform better than predicting the increasing market. This can primarily be attributed to the fragile and, in turn, indeterminate nature of Turkish economy, which is easily affected by external factors such as political tension and changes. However, detecting a positive change is much harder; it requires a more stable and sustainable economic growth. Although the performance measures of the three techniques could be perceived to be approximately the same, in fact they are not. This is mainly because while the accuracy rates are fairly close, the sensitivity and specificity rates differ. While accuracy indicates total performance, sensitivity and specificity capture up and down movement predictions separately. Further, the loss when predicting an upward trend as a downward one is not the same with the loss when predicting a downward trend as an upward one. In the former, the investor misses out on additional gains but within the latter situation will suffer a real loss. In addition, prediction of the stock price direction does not factor in the ‘magnitude’ of the stock price movement. This means that gains from correct predictions and losses from incorrect predictions may vary (Wang et al., 2012) as observed in our case with varying values of sensitivity and specificity in Table 2.
                  

In general, forecasting in a business environment is an important technique that can help ensure the effective use of the firm's resources in order to improve the performance of different industrial and managerial operations (Klassen & Flores, 2001; Waddell & Sohal, 1994). For instance, the more accurate the forecast on the firm's sales figures, the better products distribution to retailers, the more efficient warehouse management, the better the cash management or better firm's stock policy. From the manager's point of view, it is the key aspect of managerial decision-making. There are many different forecasting tools and models that managers can utilize, but they require a number of considerations and strong assumptions which may not necessarily hold true in reality (Nikolopoulos & Assimakopoulos, 2003). With the market condition being stable and predictable, the use of statistical forecasting tools such as regression can be beneficial.

On the other hand, forecasting is more difficult for a chaotic system where the future is mostly unpredictable; in such cases, the use of sophisticated data analytics methods are required (Lines, 1996). Forecasting stock returns in an emerging market such as the BIST is probably in the middle of this continuum. In other words, it is not that easy to yield highly accurate results with conventional linear regression models or even with an ANFIS model, as demonstrated with the 52 percent accuracy obtained with that prediction model for the BIST 100 in this study. Yet, a more sophisticated nonlinear model, such as a SVM can increase the accuracy, in this case, up to 72 percent.

The Sharpe ratio is used for evaluating the performance of an investment with respect to its risk (Fernandez-Rodriguez et al., 2000). It is calculated by dividing the difference between the average investment return and the risk-free rate of return by the standard deviation of the rate of return of the investment as in Eq. (35) below (Sharpe, 1994). If the Sharpe ratio is greater than 1, the investment is considered to be “good”. If it is greater than 2 it is considered to be “very good”. Finally, if greater than 3, it is considered to be “excellent”.

The Sharpe ratio was calculated for the three different models that were used in the paper as tabulated in Table 3. Within the forecast period, the average rate of return of the risk-free government security was 9.7 percent. For the calculation of the rate of returns, a transaction cost of 0.04 percent was incorporated. Investors are assumed to short sell to take advantage of negative predictions from the models. As reported in Table 3, the MLP-based neural network model and the support vector machine model range from just below “very good” to “excellent” as an investment opportunity as measured by the risk-adjusted Sharpe ratio performance measure.

                        
                           (35)
                           
                              
                                 
                                    
                                    
                                    
                                       
                                          S
                                          h
                                          a
                                          r
                                          p
                                          e
                                          
                                          R
                                          a
                                          t
                                          i
                                          o
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          =
                                          
                                             
                                                A
                                                v
                                                e
                                                r
                                                a
                                                g
                                                e
                                                
                                                r
                                                a
                                                t
                                                e
                                                
                                                o
                                                f
                                                
                                                r
                                                e
                                                t
                                                u
                                                r
                                                n
                                                
                                                o
                                                f
                                                
                                                t
                                                h
                                                e
                                                
                                                i
                                                n
                                                v
                                                e
                                                s
                                                t
                                                m
                                                e
                                                n
                                                t
                                                −
                                                R
                                                a
                                                t
                                                e
                                                
                                                o
                                                f
                                                
                                                r
                                                e
                                                t
                                                u
                                                r
                                                n
                                                
                                                o
                                                f
                                                
                                                a
                                                
                                                r
                                                i
                                                s
                                                k
                                                
                                                f
                                                r
                                                e
                                                e
                                                
                                                s
                                                e
                                                c
                                                u
                                                r
                                                i
                                                t
                                                y
                                             
                                             
                                                S
                                                t
                                                a
                                                n
                                                d
                                                a
                                                r
                                                d
                                                
                                                d
                                                e
                                                v
                                                i
                                                a
                                                t
                                                i
                                                o
                                                n
                                                
                                                o
                                                f
                                                
                                                r
                                                a
                                                t
                                                e
                                                
                                                o
                                                f
                                                
                                                r
                                                e
                                                t
                                                u
                                                r
                                                n
                                                
                                                o
                                                f
                                                
                                                t
                                                h
                                                e
                                                
                                                i
                                                n
                                                v
                                                e
                                                s
                                                t
                                                m
                                                e
                                                n
                                                t
                                             
                                          
                                          
                                          
                                          
                                          
                                       
                                    
                                 
                              
                           
                        
                     
                  

Studies about forecasting with artificial neural networks and fuzzy logic show that the results are predicted with a very high success rate (Yildirim, Ozsahin, & Akyuz, 2011). With the advance in technology and development of various software and tools, more successful forecasts will be achievable. The use of successful prediction tools will enable investors to make better decisions when faced with uncertain future stock returns.

By adopting the procedure as explained in Section 2.7, 10-fold cross-validated information fusion-based sensitivity score of each input variable was computed. The importance rank order of predictor variables are pictorially represented as in Fig. 8
                     . Based on these results, various conclusions can be drawn as follows: It is interesting to note that the two least ranked variables with respect to their information fusion based normalized sensitivity scores are the BIST 100 percentage change and trend. Both of these variables are obtained from the historical price levels of the BIST 100 measure, and are classified as in the subset of the information set considered as “weak form”. In Fama's (1970) distinctions of various levels of market efficiency, the weak form of EMH is the lowest level of market efficiency. For example, if a stock return series follows a random walk, then historical price information will not lead to economic profit; but a stock market may be considered weak-form market efficient, yet still be inefficient with respect to other public information such as exchange rates or interest rates, which are classified as belonging to the semi-strong form. The order of the variables Fig. 8 is consistent with Fama's (1970) classification of the information set. The information based only on the historic price may already be mostly incorporated in the contemporaneous opening price, and may not be sufficient to accurately forecast the closing price. On the other hand, adding past changes of gold, currency exchange rates, movements in the U.S. markets, and particularly, bond price changes, increases the accuracy of the forecasts. The variable ranked as the highest importance in terms of its sensitivity score is FBIS (Istanbul Bond Index) Change, which is the daily percentage change of the bond index, which moves inversely with the interest rate. Bond prices reveal present value and expectations of future interest rates. Stock market assets are very sensitive to interest rate movements for a number of reasons. Interest rates will affect the cost of capital for firms which, in turn, will affect future cash flows which also affect present stock prices. Interest rates also affect the rate an investor might use to discount these future cash flows (dividends). Further, because fixed income assets are an alternative to the stock market, higher rates can draw cash out of the stock market in general, and lower rates will attract cash tied up in fixed income assets.


                     Kelly, Martins, and Carlson (1998) find that the correlation between stock and bond returns is higher in emerging countries than in developed countries. For high risk countries that have low creditworthiness, stock and bond returns should be highly correlated because bonds take on equity-like characteristics. Stock and bond markets in Turkey would exhibit a stronger relationship as Turkey's sovereign credit rating was below investment-grade until 2011. Chordia, Roll, and Subrahmanyam (2002) find that significant buying or selling pressures predict 1 day ahead market returns, demonstrating the effect of trading frictions in predicting returns. We would expect such frictions to be more pronounced for a developing economy like Turkey, where markets are less efficient, relative to developed economies. Therefore, a strong relationship between bond and stock markets and greater trading frictions in the Turkish stock market could make bond market returns quite important in predicting daily stock returns as information in the bond market gets incorporated into the stock market on the next day. Our finding of better prediction of negative returns also lends some support on the argument that a market friction like short selling constraints are producing the greater predictive ability on the downside.

@&#CONCLUSIONS@&#

It is a very complicated decision to buy or sell a stock since there are many factors that might affect the stock price values. A generic data analytic approach is developed in this study with the integration of different forecasting techniques and the study shows that the performance of the stock price forecasting can be effectively performed by using ANFIS, ANN, and SVM. This paper is unique in that only six factors have been tapped as input variables to forecast the BIST 100 index, while most of the previous studies in general used 10 or more input variables. Moreover, there are some examples like Olson and Mossman (2003) and Zorin and Borisov (2002), which use 59 and 61 input variables, respectively. Considering the fact that data collection and consolidation constitutes a significant amount of decision making via data analytic modeling, such a parsimonious model would be more attractive to the decision makers. It is a well-known fact that it is very critical to decrease the number of input variables required in forecasting in order to simplify the complexity of the model to achieve parsimony and hence decrease the computational time and in turn expedite the decision making process. Additionally, increasing the number of variables to be used in forecasting would in turn render them harder to access and/or collect. The contribution of this study can mainly be attributed to its flexible methodology (as outlined in Fig. 1) and applicability to similarly behaving emerging markets. Financial analysts can easily modify the input variables set and utilize the rest of the proposed methodology steps within their own individual setting as well. Since the data analytic models are capable of discovering non-trivial, implicit, previously unknown, but potentially useful information in the data, they would hypothetically help extract such knowledge from other emerging markets’ behavior as exemplified via Turkish economy in this case.

This paper not only provides simplicity for forecasting BIST stock exchange values with only six input variables, but it also achieves significantly better forecasting performance in terms of the accuracy rates compared to the previously published studies which used BIST stock exchange data. It has outperformed the accuracy results of Altay and Satman (2005), Diler (2003), and Yümlü et al. (2005). Moreover, it yielded comparable results to that of another more recent study conducted by Kara et al. (2011). Although all these studies, including ours, have examined the BIST stock exchange data over different time spans, they all reveal and reinforce the fact that Turkish economy is sui generis, i.e. it has its own unique characteristics, due to high volatility that stems from unstable social structure and political fracture.

The Borsa Istanbul (a.k.a. Istanbul Stock Exchange), located in Turkey, is considered an emerging (developing) market according to the list compiled by the World Bank. As stated in the paper, emerging markets are more likely to be less efficient in an efficient market hypothesis sense. In other words, it is more likely that the price of trading assets may not fully incorporate all the publicly available information, consequently making it more plausible that forecasting future stock price movements can lead to a trading advantage. As stated in Lagoarde-Segot and Lucy (2008), trading in emerging markets may be less efficient due to less liquidity, less trading competition, less transparency, greater political and/or economic uncertainty, and less “culture of equity”, meaning that not everyone is used to stock ownership, and the ones that are interested may react slowly. Occasionally markets do get reclassified. Greece has moved from emerging to developed status and then back to emerging over the last 15 years. However, in the short term Turkey is expected to maintain its “emerging market” status since our study covered an 8 year period of both up and down markets; this data analytic methodology would be expected to sustain its applicability to continue into the near and intermediate future. The main characteristics that makes the BIST an exemplar for emerging markets is the political and economic uncertainty. Although Turkey is in the world's top twenty with respect to most economic measures such as GDP, with strong growth through 2012, over 2013, both the Turkish lira as well as the BIST indices lost value, and so Turkey's relative GDP, while still in the top twenty, has fallen recently. This is mostly due to both the internal and external political uncertainty. With some corruption charges continuing in the country, these conditions transfer to the trading market and conditions which contribute toward greater market inefficiencies, which actually make conditions easier for forecasting. Although it is not intuitive, greater market efficiency makes a resulting stock price series more like a random walk, since new information gets transmitted into prices instantaneously, and new information will impact prices randomly. If the BIST is taken as an exemplar for emerging markets, the hybrid forecasting approach developed in this paper would be expected to work in other emerging markets.

The financial integration exists in across the globe is evident from both casual empiricism as well as scholarly writings (Fisher & Palasvirta, 1990). In particular, the transmission of stock market fluctuations often flows between markets in the developed countries, and from the developed markets to the emerging markets. Fisher and Palasvirta (1990) demonstrated that a high statistically significant interdependence exists between 23 stock indices representing a variety of developed and emerging markets, with U.S. stock prices leading the rest of the world. Given this evidence, it makes sense to use a U.S. index in the forecasting of the BIST 100. Directional movements in the U.S. indices, such as the DJIA, the Nasdaq Composite, or the S&P 500 mirror each other closely.

Future research might include incorporating more data analytical models, such as decision trees or Bayesian networks, to be compared against the ones used in this study. It is also essential to verify and validate the viability of the proposed method via various other stock markets in different countries. Moreover, a user-friendly graphical user interface (GUI) could be created which uses the outlined methodology of this study to render the decision making process more usable for financial experts in the field. Moreover, some other evaluation metrics of model performance, such as Pesaran and Timmermann (1992), can be considered to rank order the models in terms of their predictive performance (Pesaran & Timmermann, 1992).

@&#ACKNOWLEDGMENTS@&#

Authors are thankful to the three anonymous reviewers for their constructive comments which have helped improve this manuscript at a great deal. They are also grateful to the Editor of European Journal of Operational Research, Dr. Emanuele Borgonovo, for his very timely management of this submission.

@&#REFERENCES@&#

