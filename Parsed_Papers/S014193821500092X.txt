@&#MAIN-TITLE@&#Visual comfort enhancement in stereoscopic 3D images using saliency-adaptive nonlinear disparity mapping

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Visual comfort enhancement in stereoscopic 3D images.


                        
                        
                           
                           Saliency-adaptive disparity maps to maintain the saliency influence.


                        
                        
                           
                           Nonlinear disparity mapping to minimize disparity distortion.


                        
                        
                           
                           Virtual view generation based on DIBR.


                        
                        
                           
                           Keeping the overall viewing image quality.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Stereoscopic 3D (S3D) displays

Visual comfort enhancement

Saliency-adaptive

Salient region

Nonlinear disparity mapping

Depth-image-based-rendering (DIBR)

@&#ABSTRACT@&#


               
               
                  Perceptually salient regions have a significant effect on visual comfort in stereoscopic 3D (S3D) images. The conventional method of obtaining saliency maps is linear combination, which often weakens the saliency influence and distorts the original disparity range significantly. In this paper, we propose visual comfort enhancement in S3D images using saliency-adaptive nonlinear disparity mapping. First, we obtain saliency-adaptive disparity maps with visual sensitivity to maintain the disparity-based saliency influence. Then, we perform nonlinear disparity mapping based on a sigmoid function to minimize disparity distortions. Finally, we generate visually comfortable S3D images based on depth-image-based-rendering (DIBR). Experimental results demonstrate that the proposed method successfully improves visual comfort in S3D images by producing comfortable S3D images with high mean opinion score (MOS) while keeping the overall viewing image quality.
               
            

@&#INTRODUCTION@&#

Stereoscopic three-dimensional (S3D) media provides a more life-like and visually immersive viewing experience, and is regarded as a next generation media [1]. With the rapid advances in the multimedia technology, users can easily access S3D contents even at home. However, for their great success, the viewing safety, especially visual comfort of S3D contents should be ensured, which becomes a growing issue of concern. Visual discomfort in S3D images is often induced by several factors [2,3] such as excessive disparities, fast changes in disparity, disparity distribution, depth inconsistency, perceptual and cognitive inconsistency, the accommodation–vergence conflict, the mismatches in the left and right images, depth cue conflicts. It is necessary to improve the visual comfort in S3D images.

@&#RELATED WORK@&#

In most previous studies, the visual comfort in S3D images is quantified by global disparity statistics. It has been reported that excessive disparities cause visual discomfort in S3D images because regions with larger disparities are hard to fuse on retina [2]. That is, humans feel more visual discomfort on S3D images with the excessively pop-out objects than those with the constant depth plane. In Refs. [4,5], the statistics of disparity such as mean and standard deviation of disparity value were measured over the entire image, and applied to the visual comfort assessment (VCA). However, they were not fully correlated with human visual perception. Human tends to pay more attention to a few but salient regions than the others in an image [6–8]. That is, visual saliency has a significant influence on the perceptual quality of S3D images. Therefore, perceptually salient regions are a key factor to determine the visual comfort in S3D images including the disparity statistics [7]. In S3D displays, depth information also plays an important role in attracting human visual attention. It has been reported that the density of fixations by the human visual system (HVS) increases for objects with larger disparity magnitudes [9]. That is, humans often fixate on near objects earlier than far away ones [10]. Thus, a visual importance map is generated by the linear combination of image-based saliency and disparity-based saliency maps, then the visual importance map is used as a weight of extracted disparity features to complete the visual comfort assessment [11]. With regard to visual comfort enhancement, there are mainly two disparity mapping approaches to reduce visual discomfort caused by excessive disparities. One is the disparity scaling [12] while the other is disparity shifting [13,14]. The disparity scaling (linearly or non-linearly) reduces visual comfort in S3D images by scaling down the disparity range of an original scene into the target range. In previous studies, there are many ways to scale the disparity range of the scene into the visual comfort zone such as depth image-based rendering (DIBR) and stereoscopic warping-based rendering [15]. The disparity scaling method is a promising tool for improving visual comfort. However, it tends to produce rendering artifacts, and the image size is also reduced after the disparity scaling. Thus, we need interpolation to enlarge the scaled image to the original size, which inevitably degrades the image quality. The disparity shifting adjusts disparity by laterally shifting the left and right images which move the zero display plane (ZDP) of the original image. However, when the disparity range of the image exceeds the range of a certain comfortable viewing zone, it is hard to reduce the visual comfort. Moreover, both disparity scaling and disparity shifting are not fully correlated with human visual perception. Thus, a visual importance map is additionally considered in visual comfort enhancement in Ref. [16]. However, the linear combination of image-based saliency and disparity-based saliency weakens the disparity-based saliency effect mainly related to visual comfort, and distorts the original disparity range significantly.

In this paper, we propose visual comfort enhancement in S3D images using sensitivity-weighted saliency-adaptive nonlinear disparity mapping. Image-based saliency mainly represents visual attention where is attractive to the human eye, while disparity effectively expresses visual comfort in S3D images because excessive binocular disparity often causes visual fatigue. Thus, simple linear combination of image-based saliency and disparity-based saliency often weakens the saliency effect and distorts the original disparity range significantly. Previous work [17] has reported that there exist sensitive and insensitive regions in images to HVS [17]. HVS actively perceives the orderly contents of the input visual information and tries to avoid some uncertainties for image perception. As a result, HVS is sensitive to orderly regions which possess regular structures, but insensitive to disorderly regions which possess uncertain structures. HVS differently perceives sensitive and insensitive regions according to structural uncertainty. The saliency detection should be based on the human visual perception of image content. Thus, we adopt a sensitivity-weighted saliency-adaptive disparity map instead of the linear combination of image-based saliency and disparity-based saliency to maintain the effects of sensitivity, disparity- and image-based saliency and preserve the original visual comfort disparities. The sensitivity-weighted saliency-adaptive disparity map is obtained by using sensitivity and image-based saliency as a weight for disparity map. Moreover, we perform nonlinear disparity mapping based on a sigmoid function to minimize disparity distortions and produce perceptually comfortable S3D images.


                        Fig. 1
                         illustrates the overall framework of the proposed method. First, we obtain sensitivity-weighted saliency-adaptive disparity maps to maintain the effects of sensitivity and saliency as well as preserve the original visual comfort disparities. Then, we perform nonlinear disparity mapping based on a sigmoid function to minimize disparity distortions in S3D images. Finally, we produce the visually comfortable S3D images using depth-image-based-rendering (DIBR).

The remainder of this paper is organized as follows. In Section 2, we explain the proposed method for visual comfort enhancement in detail. In Section 3, we provide experimental results and their corresponding analysis. Conclusions are made in Section 4.

@&#PROPOSED METHOD@&#

The proposed framework consists of six main steps: (1) Disparity estimation using the depth estimation reference software (DERS), (2) sensitivity estimation using pattern masking, (3) saliency estimation by graph-based visual saliency (GBVS), (4) sensitivity-weighted saliency-adaptive disparity generation, (5) nonlinear disparity mapping, and (6) depth-image-based rendering (DIBR).

In S3D displays, disparity is a key factor that induces visual discomfort. It has been reported that excessive disparities cause visual discomfort in S3D images because regions with larger disparities are difficult to fuse on retina [2]. That is, humans feel more visual discomfort on S3D images with the excessively pop-out objects than those with the constant depth plane. Previous studies have reported that visual discomfort increases as disparity increases [18]. Thus, we estimate a disparity map D for the right view using DERS in Ref. [19]. Fig. 2
                        (c) shows the disparity maps of Fig. 2(a) and (b) obtained by DERS.

Previous work has reported that there exist sensitive and insensitive regions in images to HVS [17]. HVS actively perceives orderly regions in input images and tries to avoid some uncertainties for visual perception. As a result, HVS is sensitive to orderly regions which possess regular structures, and insensitive to disorderly regions which possess uncertain image structures. HVS differently perceives sensitive and insensitive regions according to structural uncertainty [17]. Thus, we first segment an image into the sensitive and insensitive regions to indicate the sensitivity of HVS to different regions. To achieve this, we use pattern masking [17] which is consistent with HVS to get a sensitivity map Ss
                         of the input image. Fig. 2(d) shows sensitivity maps. In sensitivity maps, the sensitivity range is from 0 to 1: 0 represents the most insensitive and 1 represents the most sensitive.

In Refs. [20,21], individual differences in human depth perception have been explored. HVS generally focuses on salient regions in an image. Therefore, it is necessary to produce an image-based saliency map. We adopt graph-based visual saliency (GBVS) in Ref. [22] to obtain an image-based saliency map SI
                        . GBVS is a bottom-up approach to visual saliency detection based on low-level features such as intensity, orientation, and color. Thus, GBVS correlates with human fixations well [23]. Fig. 2(e) shows the image-based saliency maps obtained by GBVS. In the image-based saliency maps, if the saliency value is closer to 0, it becomes less salient, while if the saliency is closer to 1, it becomes more salient.

We get disparity maps by disparity estimation while getting sensitivity maps of HVS through sensitivity map estimation. In saliency estimation, we obtain the image-based saliency maps in a S3D image. In Ref. [16], Jung et al. have first generated a disparity-based saliency map which assigns the maximum disparity to 1 and the minimum disparity to 0 as shown in Fig. 3
                        (b), and then combined image-based saliency and disparity-based saliency by linear combination with the same weight as shown in Fig. 3(d). However, the simple linear combination weakens the disparity-based saliency effect, and changes the original disparity range significantly. As can be observed, the original disparity range of the disparity map is largely changed by linear combination compared with Fig. 3(a). Thus, we need to consider sensitivity in saliency estimation. In this work, we propose elaborate combination of sensitivity, image-based saliency, and disparity-based saliency to maintain the sensitivity, saliency, and disparity effects as well as avoid large change of the original disparity range. Image-based saliency mainly represents visual attention where HVS focuses, but has little to do with visual comfort. Sensitivity also has little to do with visual comfort, but it is also concerned with human attention. However, disparity-based saliency effectively represents visual comfort in S3D images, and indicates their visual comfort degree. Thus, we obtain a weight map Wt
                         for generating a sensitivity-weighted saliency-adaptive disparity map as follows:
                           
                              (1)
                              
                                 
                                    
                                       W
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    
                                       S
                                    
                                    
                                       I
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 ∗
                                 
                                    
                                       S
                                    
                                    
                                       S
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                              
                           
                        where SS
                        (x, y) is a sensitivity map at a pixel (x, y) in the sensitivity map; and SI
                         is an image-based saliency map. Then, we obtain a sensitivity-weighted saliency-adaptive disparity map Dsad
                         as follows:
                           
                              (2)
                              
                                 
                                    
                                       D
                                    
                                    
                                       sad
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    
                                       W
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 ·
                                 D
                                 (
                                 x
                                 ,
                                 y
                                 )
                              
                           
                        where D is a disparity map. We use Wt
                         as a weight for D. As Wt
                         is close to 0, small weights are assigned to D. Thus, excessive disparities become smaller than the original ones in non-salient discomfort regions, and small disparities become even smaller in non-salient comfort regions. In contrast, as Wt
                         is close to 1, large weights are assigned to D. Thus, the disparity changes are little in these regions, and the comfortable disparity is maintained. Dsad
                         is shown in Fig. 3(e). Thus, Dsad
                         effectively considers sensitivity, image-based saliency, and disparity-based saliency. Moreover, it successfully preserves disparities in visually comfortable regions of D.


                        Dsad
                         contains visually comfortable and uncomfortable regions. Thus, it is required to minimize the excessive disparity of discomfort regions for improving visual comfort in S3D images. In contrast, we need to preserve the original disparity in comfortable regions for 3D effects. It is proven that nonlinear disparity mapping satisfies the two requirements [15]. Lang et al. have recommended using a log function to perform nonlinear disparity mapping. To keep the overall image quality, we selectively perform nonlinear disparity mapping based on a sigmoid function (we only use the positive part in Fig. 4
                        ) as follows:
                           
                              (3)
                              
                                 
                                    
                                       D
                                    
                                    
                                       nm
                                    
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             D
                                          
                                          
                                             r
                                          
                                       
                                    
                                    
                                       M
                                    
                                 
                                 ∗
                                 
                                    
                                       1
                                    
                                    
                                       1
                                       +
                                       
                                          
                                             e
                                          
                                          
                                             -
                                             t
                                             ∗
                                             
                                                
                                                   D
                                                
                                                
                                                   sad
                                                
                                             
                                             (
                                             x
                                             ,
                                             y
                                             )
                                          
                                       
                                    
                                 
                                 -
                                 
                                    
                                       
                                          
                                             D
                                          
                                          
                                             r
                                          
                                       
                                    
                                    
                                       2
                                       ∗
                                       M
                                    
                                 
                              
                           
                        where Dr
                         is the disparity range of the mapped disparity map Dnm
                        ; t and M are constants. Based on this nonlinear disparity mapping, we compress sensitive salient discomfort regions (high values in Dsad
                        ) more; both insensitive non-salient discomfort regions and sensitive salient comfort regions (middle values in Dsad
                        ) less; and insensitive non-salient comfort regions (middle values in Dsad
                        ) little. Thus, we compress excessive disparities more while preserving comfortable disparities well. Fig. 5
                         shows nonlinear disparity mapping results by (3): (a) Sensitivity-weighted saliency-adaptive disparity map and (b) its nonlinearly mapped disparity map. It is obvious that excessive disparities are successfully compressed while small disparities are preserved as much as possible by nonlinear disparity mapping.

Finally, we conduct depth-image-based-rendering (DIBR) for visually comfortable view synthesis [24]. DIBR is an auto-stereo content generation tool, and generates a virtual view from a reference color image and its corresponding depth map. It mainly contains four steps: stereo matching, image shifting, hole-filling and smoothing, and virtual view rendering. The stereo matching step is the same as disparity estimation by DERS in Section 2.1. The image shifting step shifts the input color image according to the value of the input depth map. The hole-filling and smoothing step deals with dis-occluded regions caused by image shifting using simple nearest neighbor interpolation. The virtual view rendering step synthesizes the virtual right view using the nonlinearly mapped disparity map and the original color right image. Consequently, we obtain a visually comfortable S3D image from the original left image and the virtual right view.

@&#EXPERIMENTAL RESULTS@&#

To verify the superiority of the proposed method, we use the IVY lab’s stereoscopic database which consists of various types of S3D images with resolution of 1920×1080 pixels captured by a 3D digital camera with dual lenses (Fujifilm FinePix 3D W3) [25]. About the parameters, after a few experiments with different parameters, we finally use t
                     =0.02 and M
                     =2. To evaluate the performance of the proposed method, we choose a set of 10 uncomfortable S3D images (MOS<3.0) and 10 comfortable S3D images (MOS⩾3.0) for tests. We provide original S3D images and their synthesized S3D images in Figs. 6 and 7
                     
                     . As we can see, the original images in Fig. 6(a) all have excessive disparities. However, the processed images in Fig. 6(b) become more comfortable by the proposed method because the original uncomfortable image with larger excessive disparity is mapped by nonlinear disparity mapping. That is, the sensitive-salient discomfort regions in an image are compressed more without loss of image quality. Fig. 7 shows original S3D images and their synthesized S3D images in 5 comfortable S3D images. Because the original S3D images are relatively comfortable, their subjective comfort score is higher than 3.5. They have a small amount of excessive disparities in salient regions, and HVS easily focuses on the salient regions. Thus, HVS feels comfortable while seeing comfortable S3D images. The original images in Fig. 7(a) have a large screen disparity in the vertical boundary of the image. However, the processed images in Fig. 7(b) have completely eliminated the large screen disparity by the proposed method.

To provide more quantitative evaluations, we perform subjective evaluations on the results. In the evaluations, we choose 12 experts who have research background in the computer vision and image processing areas. We use a LG 3DTV with a 55-in. 240-HZ stereoscopic display and polarized glasses which offer high definition (HD) resolution of 1920×1080. In the design of the experimental environments, we follow the guideline in ITU-R BT.500-11 [26]. We use the double stimulus continuous quality scale (DSCQS) method in ITU-R BT.500-11 to measure the degree of visual comfort enhancement. We randomly display 20 test images which consist of 10 uncomfortable S3D images and 10 comfortable S3D images for the observers. Each test image is comprised of the original S3D image and their synthesized S3D images by the proposed method. We set the visual comfort degrees, i.e. MOS, from 1 to 5 for the evaluations: 1 is very poor; 2 is poor; 3 is medium; 4 is good; and 5 is very good.

To verify the superiority of the proposed method, we compare the performance of the proposed method with that of disparity scaling in Ref. [12]. Fig. 8
                      shows the subjective evaluation results in terms of the mean opinion score (MOS). Fig. 8 (a) shows MOS results in 10 uncomfortable S3D images. Their original visual comfort scores are smaller than 3.0. The proposed method achieves a significant improvement in the visual comfort score than [12]. Fig. 8(b) shows MOS results in 10 comfortable S3D images. Their original visual comfort scores are greater than 3.0. As can be observed, the visual comfort improvements are not very large by both methods because original test images are visually comfortable. However, MOS gains by the proposed method are a bit higher than those by Ref. [12]. Specifically, the proposed method achieves the average improvement of 2.40±0.53 in visual comfort score for uncomfortable S3D images. Moreover, the proposed method achieves the average improvement of 0.61±0.52 in visual comfort even for comfortable S3D images. Consequently, we can safely conclude that the proposed method successfully improves visual comfort in S3D images without loss of the overall quality.

@&#CONCLUSIONS@&#

Visual discomfort has serious impact on the viewer’s experience in S3D images. To address the visual discomfort problem, we have proposed a visual comfort enhancement method using sensitivity-weighted saliency-adaptive nonlinear disparity mapping. Based on human visual perception, salient regions are more attractive than non-salient ones for viewers. Moreover, there exist sensitive and insensitive regions to HVS. Thus, we take both visual saliency and sensitivity into consideration as an indicator for nonlinear disparity mapping. Through the sensitivity-weighted saliency-adaptive nonlinear disparity mapping based on a sigmoid function, sensitive-salient discomfort regions are more compressed, while the other regions are less compressed. Therefore, the proposed method successfully compresses the excessive disparities while maintaining the comfortable disparities as much as possible. The experimental results demonstrate that the proposed method effectively improves visual comfort in S3D images.

@&#REFERENCES@&#

