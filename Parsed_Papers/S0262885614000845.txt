@&#MAIN-TITLE@&#Real-time fingertip localization conditioned on hand gesture classification

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A novel use of the ORD feature, to both globally and locally characterize hands


                        
                        
                           
                           A dataset for hand gesture classification and fingertip localization is proposed.


                        
                        
                           
                           We reduce the search space of fingertip locations conditioned to a hand gesture.


                        
                        
                           
                           A new cost function to solve the graph matching problem of fingertip localization


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Hand gesture recognition

Fingertip classification

Range camera

Interactivity

Dataset

@&#ABSTRACT@&#


               
               
                  A method to obtain accurate hand gesture classification and fingertip localization from depth images is proposed. The Oriented Radial Distribution feature is utilized, exploiting its ability to globally describe hand poses, but also to locally detect fingertip positions. Hence, hand gesture and fingertip locations are characterized with a single feature calculation. We propose to divide the difficult problem of locating fingertips into two more tractable problems, by taking advantage of hand gesture as an auxiliary variable. Along with the method we present the ColorTip dataset, a dataset for hand gesture recognition and fingertip classification using depth data. ColorTip contains sequences where actors wear a glove with colored fingertips, allowing automatic annotation. The proposed method is evaluated against recent works in several datasets, achieving promising results in both gesture classification and fingertip localization.
               
            

@&#INTRODUCTION@&#

Until recent years, interaction between humans and computer systems has been typically performed with peripherals (e.g. mouse, keyboard). Current trends focus on improving the user experience when interacting with computers and systems. Apple's trackpad [1] or multi-touch devices are commercially successful examples of new interaction paradigms; they have created a habit in users thanks to their easy-to-learn and intuitive gestures combining simple movements and finger configurations. However, these methods are limited to systems where the user is required to physically touch the device.

Touch-less interaction is an emerging alternative providing a more immersive and intuitive experience. In this paper, we propose a touch-less interaction paradigm that aims at extending multi-touch gestural interaction to spatial processing. Consequently, hand gestures and finger configurations are combined with simple movements in order to deliver a large number of interaction options with a rather low number of one-hand gestures. In order to put this strategy into operation, accurate real-time fingertip detection is required.

Self-occlusions, limited resolution, the degrees of freedom of a hand and the intra-class variability of gestures render hand gesture recognition and fingertip detection as challenging problems. Fortunately, a number of traditional vision challenges involving scale and illumination changes have been overcome thanks to the irruption of consumer depth cameras (e.g. Kinect). Such depth cameras have paved the way towards touch-less interactivity; as a remarkable example, it has taken a few years for full-body pose interaction [2] to achieve commercial success in gaming. However, few works tackle fingertip detection from depth cameras [3–6] (see Section 2).

The objective of this work is to locate fingertips in real-time. More precisely, to know where fingers are placed (detection), and which finger is each (classification). Instead of facing the problem from raw data, as in [2], we propose an intermediate step to restrict the search space. Here, the intuition that fingertip locations are conditioned by hand gestures is exploited, and formulated such that the statistical correlation between gestures and fingertip locations restricts the search space of fingertip configurations, rendering faster and more accurate inference. In a first step, the most probable hand gesture labels are inferred from data. Next, fingertip locations are inferred from depth data and most probable hand gesture label, which is used as a discriminative auxiliary variable. For this second step, a graph matching approach exploiting fingertip structure is proposed. Instead of hand gesture labels, other variables, such as hand orientation, could have been chosen to restrict the search space. However, gestures have a semantic purpose on their own, and are easier to annotate.

In this work, we also propose a novel usage of the Oriented Radial Distribution (ORD) feature, presented in [7]. The ORD feature characterizes a point cloud such that end-effectors have high responses, whereas flat parts have low responses. Therefore, ORD is suitable to both globally characterize the structure of a hand gesture and to locally locate its end-effectors. Such property nicely fits in the above mentioned two-step method: globally for the hand gesture classification task and locally for the fingertip detection step. That is, a single ORD calculation suffices for both tasks.

Available datasets for gesture recognition with depth data have been proposed to typically respond to specific needs of the aimed interaction paradigms. Ganapathi et al. [8] provide a body pose estimation dataset using a Time-of-Flight (TOF) camera. Pugeault and Bowden [9] propose a hand gesture dataset using Kinect, which is intended for American Sign Language (ASL) purposes. Although useful for the evaluation of some tasks, such as hand gesture recognition, none of the available datasets is suitable to test the interaction paradigm proposed in this paper. For that matter, we present ColorTip [10], a depth-based dataset consisting of 7 subjects performing 9 different hand gestures (Figs. 1 and 2
                     
                     ). Ground-truth annotations for hand positions, hand gestures, fingertip locations and finger labels are also provided. Finger positions are obtained using a colored glove during capture, enabling a non-costly color-wise segmentation. Furthermore, each subject performs two sequences (Set A and Set B), with increased intra-gesture variability in the latter.

Summarizing, in this work we propose the following main contributions:
                        
                           •
                           A practical touch-less interaction concept, combining finger configurations, hand gesture and simple movements.

A real-time method to obtain fingertip locations and labels, as well as hand gestures, using Kinect. We propose to exploit the statistical correlation between hand gestures and fingertip locations.

A novel use of the Oriented Radial Distribution feature, exploiting its global structure for hand gesture characterization and its local values for fingertip detection.

ColorTip, a public dataset intended for hand gesture classification and fingertip localization.

The effectiveness of the proposed method is experimentally evaluated in different datasets. Furthermore, we conduct experiments assessing the performance of each aspect of our approach. At the feature level, we show the validity of the ORD feature by means of a 3D feature benchmark. Next, hand gesture classification accuracy is evaluated in the ASL database provided by [9]. Finally, fingertip localization results are compared to a state-of-the-art Random Forest (RF) approach using the ColorTip dataset.

The remainder of the paper is organized as follows. A summary of related work is provided in Section 2. In Section 3 we present the ColorTip dataset. The Oriented Radial Distribution feature is described in Section 4. Section 5 contains the theoretical description of the proposed method, followed by experimental results in Section 6. Finally, conclusions are drawn and discussed in Section 7.

@&#RELATED WORK@&#

An early hand gesture recognition from depth data [11] tackles hand gesture recognition using a laser-based camera to produce low-resolution depth images. They interpolate hand pose using sets of finger poses and inter-relations. Liu and Fujimura [12] recognize dynamic hand gestures using Time-of-Flight depth images. Hands are detected measuring shape similarity by means of the Chamfer distance. They analyze the trajectory of the hand and classify gestures using shape, location, trajectory, orientation and speed features. As range sensors become progressively cheaper, the number of approaches towards touch-less interactivity grows, most remarkably full body pose estimation [2,13,8,14] and hand gesture recognition [15–17].

Many authors have explored how to control a virtual environment with hands (e.g. PC desktop, 3D model). Such applications generally involve dynamic hand gesturing. In this regard, Soutschek et al. [15] propose a user interface for the navigation through 3D datasets using a Time-of-Flight (TOF) camera. They perform a polar crop of the hand over a distance threshold to the centroid, and a subsequent NN classification into five hand gestures. With a similar objective, Van den Berg and Van Gool [18] improve their work in [17] by combining RGB and depth to construct classification vectors. Their alphabet consists of four gestures that enable selecting, rotating, panning and zooming of a 3D model on a screen. Hackenberg et al. [3] estimate hand pose by identifying palm and finger candidates, after a pixel-wise classification into tips and pipes. The final hand structure is obtained with optical flow techniques. Ren et al. [19] segment the hand under some restrictive assumptions and adapt the earth mover's distance to a finger signature, finding the NN according to this metric. Malassiotis and Strintzis [16] extract PCA features from depth images of synthetic 3D hand models for training.

Obtaining hand gestures with the Nearest Neighbor (NN) classification has proven to be a promising approach when dealing with depth data [15,19,20]. However, most recent works use features that are not specifically designed for depth data.

Other works have focused on finger-spelling using the American Sign Language (ASL). While still being an alphabet, the ASL contains 26 hand poses and their accurate classification becomes a challenging task. We remark that 24 of the 26 hand poses are static gestures and 2 of them are dynamic (involve trajectory). Most of the related works are focused on the static subset. Keskin et al. [21] take advantage of Randomized Decision Trees to classify hand shapes. Zhang et al. [22] recently propose a descriptor for depth data which encodes 3D facets into a histogram. They prove the suitability of this descriptor for hand gesture recognition on ASL datasets. Zhu and Wong [23] propose to fuse common color and depth descriptors and use linear SVMs to predict the hand gesture. Kollorz et al. [20] obtain a fast NN classification using simple feature projection on two axes, which they apply to the first 12 letters of the ASL (static gestures). Uebersax et al. [24] perform an iterative hand segmentation by optimizing the center, orientation and size of the hand. They aggregate three classifiers that take shape and orientation into account. Pugeault and Bowden [9] propose a multi-resolution Gabor filtering of the hand patch to train a Random Forest classifier. In their work, they provide a complete dataset of the 24 American Sign Language (ASL) static gestures captured with the Kinect sensor, with both color and depth information available. Their dataset contains patches roughly centered at the hand centroid.

Fewer works tackle fingertip localization. In [3], fingertips are detected but not labeled, as well as in [4] where also the palm and fingers orientations are estimated. Both approaches exploit geometric features to detect fingertips on the hand point cloud. The body part classification approach proposed by Shotton et al. in [2] is applied to hand parts by Keskin et al. [5], obtaining full hand poses at the expense of a costly training. Recently, Oikonomidis et al. [6] formulated hand pose estimation as an optimization problem, where the target is to minimize a cost function expressing the discrepancy between an articulated hand model and the observed hand. Their method estimates full hand poses but requires pose initialization. And, since the cost function relies on color information, the validity of the approach is limited to controlled lighting conditions.

ColorTip [10] is a public dataset for hand gesture recognition and fingertip localization captured with the Kinect sensor, which consists of a set of recordings and annotations with a two-fold objective: First, to provide a benchmark with challenges such as high intra-class variability; and second, to enable novel interactive applications involving hand gesturing and fingertip localization.

The ColorTip dataset is divided into the following folders:
                           
                              •
                              Subject: N subjects performing gestures like those shown in Fig. 1, ensuring intra-user variability. Four of them are untrained users, which learned how to perform the gestures with a single and short explanation.

Challenge: We consider that a given gesture may vary in orientation and translation. Therefore, raising 4 fingers is assumed as gesture number 4, but also moving these 4 fingers towards the camera, side views and hand rotations (Fig. 1). The amount of intra-gesture variability determines how challenging a given sequence is. The Set A sequences contain limited intra-gesture variation, which mainly consists in hand rotations on the vertical plane. On the other hand, the Set B sequences contain a higher intra-gesture variability, with free rotations and finger movement (as shown in Fig. 1).

In total, ColorTip contains a set of (7 subjects×2 challenges)=14 sequences of between 600 and 2000 frames each.

Inspired by the work of Wang and Popović [25], a black glove with colored fingertips is used to capture the training sequences (see Fig. 2). In this way, fingertip annotations are obtained without expensive motion capture systems, like those used in [2,5]. Furthermore, one can easily record additional data to update the dataset. Fingertip location annotations are obtained by first segmenting the Kinect color images with a color-based Binary Partition Tree [26] (see Fig. 2) and then computing the region centroids. Color labels have an associated numerical label l.

Hand gestures are manually annotated among the 1–9 gestures, plus an extra label 0 for those frames with an unknown gesture. Also, hand location annotation in image coordinates is provided.

We characterize hands using the Oriented Radial Distribution feature, extending our previous work [7]. ORD is a feature for detecting end-effectors on depth images. One may obtain 3D point clouds Ω from depth images by means of, for example, the Point Cloud Library (PCL) [27], which is built on OpenNI [28]. These 3D point clouds represent a sampling of the 3D surfaces in a scene.

ORD characterizes point clouds with an oriented 2D disk which is divided into S sectors. The orientation of the disk is given by the surface normals on the point cloud. The average radius of the inlying points in each sector is used as a measure of the curvature and extremeness of the different parts of the point cloud. In Fig. 3
                     , the bold crosses indicate the average radius of a sector's points 
                        
                           
                              
                                 δ
                                 ¯
                              
                              j
                           
                        
                     , while the 
                        
                           
                              
                                 1
                                 
                                    2
                                 
                              
                           
                           ρ
                        
                      central circle indicates the value of such radius for a completely flat zone. Thus, the farther away the crosses are located from this circle, the more extremal the point is. In the example of Fig. 3, point (a) belongs to a flat zone, while (b) belongs to an extremal zone. Eq. (1) shows how ORD values are computed for a given point p
                     ∈
                     Ω (S are those sectors that are not empty).
                        
                           (1)
                           
                              
                                 ORD
                                 
                                    p
                                    Ω
                                    ξ
                                 
                                 =
                                 
                                    1
                                    
                                       
                                          
                                             1
                                             
                                                2
                                             
                                          
                                       
                                       ρ
                                       
                                          S
                                          f
                                       
                                    
                                 
                                 
                                    
                                       ∑
                                       
                                          j
                                          =
                                          0
                                       
                                       
                                          S
                                          f
                                       
                                    
                                    
                                 
                                 |
                                 
                                    
                                       
                                          δ
                                          ¯
                                       
                                       j
                                    
                                 
                                 −
                                 
                                    1
                                    
                                       2
                                    
                                 
                                 ρ
                                 |
                              
                           
                        
                     
                     
                        
                           
                              
                                 with
                                 
                                 parameterization
                                 
                                 ξ
                                 =
                                 
                                    ρ
                                    S
                                 
                                 .
                              
                           
                        
                     
                  

Parameter ρ is called ORD scale, and allows selecting the size of the extrema to be found. Selecting a scale of about the hand size (ρ
                     ≈12cm) will result in high ORD values at extrema of a similar scale. We propose to exploit this multi-resolution feature of ORD for the characterization of hands and fingers, by selecting the appropriate scales. In [7], the possibility of parameterizing ORD with a given radius was mentioned. However, since all the experiments were focused on detecting extremities, only a single scale was used.

The second parameter in the calculation of ORD is the number of zones S into which the oriented disk is divided. Low values of S (i.e. S
                     =2) lead to noisy and non-robust detection of end-effectors, the spatial resolution of the ORD feature being too poor. On the other hand, high values of S (i.e. S
                     =128) provide a too smooth transition between end-effectors and the non-desired zones. Reasonable trade-off values are between 8 and 16 divisions.

ORD gives a representation of a depth-captured object, highlighting its end-effectors and curved parts (Fig. 4c) and characterizes flat zones with low values. We propose to use the global description ability of ORD to represent hand gestures for further classification, and to exploit the ORD multi-resolution extrema detection to locally detect hands and fingers, by choosing appropriate ρ scales (Fig. 4b, c).

Contrarily to other features or descriptors that are defined on the image domain, the ORD scale is defined in the 3D space and thus ORD responses at different scales correspond to the actual size of objects, and not to the apparent size. Since body parts' sizes can be known a priori (using anthropometrics studies, for instance) one can use such prior information to set ORD scales. Our approach efficiently exploits these ORD descriptor properties to jointly address three different tasks (Fig. 5
                     ):
                        
                           •
                           Hand detection and segmentation, with a simple threshold on the hand scale ORD as done in [7].

Characterization of global hand pose for hand gesture recognition (Section 5.2)

Characterization of local hand parts for fingertip localization (Section 5.3).

The scheme in Fig. 5 summarizes the main blocks involved in the proposed method. In a preliminary step, we perform body segmentation by means of background subtraction with depth data. Then, we detect and segment the hand by using the ORD at hand scale (in this work ρ
                        =12cm).

One could use other hand detection methods, like Kinect's skeleton or very fast methods like those in [29]. However, a nice advantage of using ORD is that it provides flexibility of using any viewport that contains the hand, which is not the case with skeleton-based methods, as they require the whole body. In addition, ORD may be used to detect other body parts by tuning the scale.

ORD is very sensitive to depth gaps, so hands will still return high ORD values when placed few centimeters away from another body part. Hands touching other limbs will cause segmentation problems. At that precision level, both ORD and the depth camera resolution are pushed to the limit. In general, two hands placed at a distance which is greater than ρ will be properly detected.

Next, we compute the ORD at finger scale (ρ
                        =3cm) on a small patch containing the segmented hand, thus obtaining high ORD responses at fingertips and eventually at knuckles (see Fig. 4). The objective of this finger scale ORD is two-fold:
                           
                              1.
                              Gesture recognition: On the one hand, we use the ORD values to select the most likely hand poses (gestures) by computing distances between feature vectors, obtaining a subspace of likely hands from the ColorTip (Section 5.2).

Fingertip localization: On the other hand, higher ORD responses are used as sparse fingertip candidates, and serve to infer fingertip locations conditioned on the previously selected subspace. A structured inference framework is proposed, formulated as a graph matching problem (Section 5.3).

The whole framework is based on the ORD, which is a good feature representation for the task of hand gesture recognition and fingertip localization with depth data. In addition, this feature can be efficiently computed, making it convenient for real-time applications.

Formally, let Z
                        ={z
                        1,…,z
                        
                           i
                        ,…,z
                        
                           N
                        } be squared training patches of different sizes containing depth data of the segmented hand. We start by computing the ORD(z
                        
                           i
                        ) at finger scale on each training patch. Then, we resample into a regular grid of m
                        ×
                        m blocks to characterize the training patches (Fig. 6
                        ). Each block is represented by the mean ORD value of the pixels inside it, obtaining a set of m
                        2-dimensional feature vectors X
                        ={x
                        1,…,x
                        
                           i
                        ,…,x
                        
                           N
                        }. Besides, let r
                        
                           i
                        
                        ∈
                        ℝ
                        2×5 denote the ground truth fingertip locations (in pixel coordinates) corresponding to the i-th training sample, being r
                        
                           i
                        [m]∈
                        ℝ
                        2 with m
                        =1,…,5 each fingertip location (used in Section 5.3). Additionally, let y
                        
                           i
                         be gesture labels. Then, training templates are defined as h
                        
                           i
                        
                        ={x
                        
                           i
                        ,r
                        
                           i
                        ,y
                        
                           i
                        }, and the complete training dataset as H.

Given a test patch z, the objective is to locate fingertip positions in it, that is to maximize p(r|z). We propose to divide the problem of obtaining fingertips from data p(r|z) into two more tractable problems that can be efficiently solved. For that matter, the hand gesture y is introduced as an auxiliary variable, such that the posterior of fingertip locations given data can be posed as:
                           
                              (2)
                              
                                 
                                    p
                                    
                                       
                                          r
                                          |
                                          z
                                       
                                    
                                    =
                                    
                                       
                                          ∑
                                          y
                                       
                                       
                                          p
                                          
                                             
                                                r
                                                |
                                                y
                                                ,
                                                z
                                             
                                          
                                          ⋅
                                          p
                                          
                                             
                                                y
                                                |
                                                z
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

However, the marginalization of gestures implies a time consuming summation. Since real-time is a requirement, we approximate the maximization of Eq. (2) by firstly maximizing p(y|z), obtaining the best candidate 
                           
                              
                                 h
                                 ^
                              
                              ∈
                              H
                           
                         for the subsequent fingertip localization:
                           
                              (3)
                              
                                 
                                    p
                                    
                                       
                                          r
                                          |
                                          z
                                       
                                    
                                    ≈
                                    p
                                    
                                       
                                          r
                                          |
                                          
                                             h
                                             ^
                                          
                                          ,
                                          z
                                       
                                    
                                    
                                    with
                                    
                                    
                                       
                                          
                                             h
                                             ^
                                          
                                          ∈
                                          H
                                          |
                                          
                                             y
                                             ^
                                          
                                          =
                                          
                                             argmax
                                             y
                                          
                                          
                                             
                                                p
                                                
                                                   
                                                      y
                                                      |
                                                      z
                                                   
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Gesture recognition (maximization of p(y|z)) is solved using a k-Nearest Neighbor (k-NN) classifier. k-NN techniques are strongly sensitive to the data nature. Thus, an inappropriate feature selection could lead to a bad k-NN classification. Choosing a k-NN classifier helps in the testing of the suitability of the ORD feature, as well as providing a fast classification taking advantage of a kd-tree 
                        [30] structure.

We remark that the term hand gesture refers to specific hand poses with a given meaning. Such poses may be static (i.e. ASL dataset) or dynamic (i.e. ColorTip dataset). The posed gesture recognition problem is solved no matter the movement of the hand.

Regarding the fingertip localization problem p(r|y,z), we propose to solve it using a graph matching algorithm with a structure-based cost on edges. Such step is conditioned on the search space obtained from the p(y|z) problem.

In pattern recognition problems, the accuracy of a method ultimately depends on the distance metrics on the feature space, i.e., whether classes in the feature space appear separate enough to learn a robust classification rule. In this work, we propose a feature space based on the ORD descriptor. Feature vectors obtained by computing the ORD descriptor on the input data provide a representation of salient regions of the hand. In other words, ORD-feature vectors of hand poses can be seen as the distribution of important parts of the hand and even interpreted as where the knuckles and fingers lay within the patch. For that reason, an ORD-based feature space is a suitable space for matching hand poses.

We choose a k-NN classifier for pose and gesture recognition. In this way, we show that even by simple matching techniques, the ORD feature space is adequate for hand analysis.

To use a k-NN classifier on a large set of instances, we use a m
                        2-dimensional kd-tree 
                        [31] that efficiently organizes feature vectors, allowing fast NN queries. The L
                        2 norm is used in this work, since it offers a good trade-off between speed and performance.

For a test patch at a given time instant (we omit the temporal subscript t in this section for readability reasons), the k-NN search returns a set of k training templates H
                        
                           k
                        
                        ={h
                        1,…,h
                        
                           j
                        ,…,h
                        
                           k
                        } with associated distances to the test patch δ
                        :
                        H
                        
                           k
                        
                        ↦
                        ℝ. Let Φ
                        
                           y
                        (H
                        
                           k
                        ) be the distribution of gestures obtained from H
                        
                           k
                        .

Let 
                           
                              h
                              ^
                           
                         denote the k-NN best match by majority, as specified in Eq. (4). The estimation of 
                           
                              h
                              ^
                           
                         is conditioned on the gesture that maximizes Φ
                        
                           y
                        (H
                        
                           k
                        ). Therefore, maximizing Φ
                        
                           y
                        (H
                        
                           k
                        ) is equivalent to the maximization of Eq. (3). Recall that one may refer to 1-NN best match, which is the first nearest neighbor in the training dataset.
                           
                              (4)
                              
                                 
                                    
                                       h
                                       ^
                                    
                                    =
                                    
                                       h
                                       j
                                    
                                    ∈
                                    
                                       H
                                       k
                                    
                                    
                                    |
                                    
                                    j
                                    =
                                    
                                       argmin
                                       j
                                    
                                    
                                       
                                          δ
                                          
                                             
                                                h
                                                j
                                             
                                          
                                          
                                          |
                                          
                                          
                                             y
                                             j
                                          
                                          =
                                          
                                             argmax
                                             y
                                          
                                          
                                             
                                                
                                                   Φ
                                                   y
                                                
                                                
                                                   
                                                      H
                                                      k
                                                   
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

k-NN search may deliver false detections, resulting in a noisy gesture recognition. We propose to smooth the classification in Eq. (4) by exploiting human motion smoothness.

In many cases, we are subject to analyze video sequences, which intrinsically have a temporal consistency over consecutive frames. Hand dynamics are smooth, hence we assume that hand gestures are not instantly changing, but are maintained during a minimal number of frames.

In order to exploit such video consistency, we propose to keep a trace of the last Q predicted gestures 
                              
                                 
                                    
                                       
                                          Y
                                          ^
                                       
                                       Q
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             y
                                             ^
                                          
                                          
                                             t
                                             −
                                             Q
                                          
                                       
                                    
                                    …
                                    
                                       
                                          
                                             y
                                             ^
                                          
                                          
                                             t
                                             −
                                             1
                                          
                                       
                                    
                                 
                              
                           , obtained from the gesture labels of 
                              
                                 
                                    H
                                    Q
                                 
                                 =
                                 
                                    
                                       
                                          
                                             h
                                             ^
                                          
                                          
                                             t
                                             −
                                             Q
                                          
                                       
                                    
                                    …
                                    
                                       
                                          
                                             h
                                             ^
                                          
                                          
                                             t
                                             −
                                             1
                                          
                                       
                                    
                                 
                              
                           . Let 
                              
                                 
                                    
                                       
                                          y
                                          ˜
                                       
                                       Q
                                    
                                 
                                 =
                                 
                                    argmax
                                    y
                                 
                                 
                                    
                                       
                                          Φ
                                          y
                                       
                                       
                                          
                                             H
                                             Q
                                          
                                       
                                    
                                 
                              
                            be the statistical mode of the last gestures 
                              
                                 
                                    
                                       Y
                                       ^
                                    
                                    Q
                                 
                              
                           , and let 
                              
                                 
                                    
                                       y
                                       ^
                                    
                                    t
                                 
                              
                            be the predicted gesture at time instant t, obtained as detailed in Eq. (4).

If the predicted gesture differs from the statistical mode of the last Q gestures, we select the closest template among the k-NN set with a gesture label equal to 
                              
                                 
                                    
                                       Y
                                       ^
                                    
                                    Q
                                 
                              
                           . In this way, transitions between gestures are smoothed, avoiding gesture flickering, as well as de-noising intra-gesture false detections.

In order to respond to gesture changes, when no gestures in the k-NN set equal the statistical mode, the 1-NN template is selected. The Dynamically-Constrained (DC) k-NN is detailed in Algorithm 1.

During the first frames (i.e. start of a live demo, begin of a video sequence, etc.) a simpler k-NN by majority is used, until the number of frames is greater than Q. Thus, during these first Q frames, the performance of the gesture recognition part corresponds to that shown in Fig. 9, with label ORD; and that of label ORD-DC after the first Q frames. By adopting this strategy, no gesture initialization is required.
                              Algorithm 1
                              Dynamically Constrained k-NN search


                              
                                 
                                    
                                       
                                    
                                 
                              

We address the problem of fingertip localisation by making use of the ORD descriptor in a structured inference framework. Maxima of the ORD of the input patch are likely to represent fingertip locations. However, as mentioned before, for some hand poses these maxima may correspond to other salient points of the hand. But, even if all the maxima correspond to finger locations, one should be able to classify which finger belongs to each maximum. Consequently, there is a need to exploit the global hand structure to overcome these issues.

Fingertip localization on test patches takes advantage of the gesture recognition scheme presented in Section 5.2. Let us recall that, in the training phase, we define templates h
                        
                           i
                        
                        ={x
                        
                           i
                        ,r
                        
                           i
                        ,y
                        
                           i
                        } comprising the feature vectors, ground truth fingertip locations and gesture labels, respectively. Our method exploits the geometric structure of the ground truth fingertip locations 
                           
                              r
                              ^
                           
                         of the best template match 
                           
                              h
                              ^
                           
                         provided by the k-NN pose recognition block. The objective is to infer correspondences between ORD maxima of the test patch and fingertip locations and labels in the template 
                           
                              h
                              ^
                           
                        . Let G
                        
                           h
                        
                        =(V
                        
                           h
                        ,E
                        
                           h
                        ) be a fully connected graph where vertices v
                        
                           h
                        
                        ∈
                        V
                        
                           h
                         correspond to the available fingertip coordinates in 
                           
                              h
                              ^
                           
                        , which we denote by r[v
                        
                           h
                        ]∈
                        ℝ
                        2 (if a fingertip is not visible, such vertex is not considered). Additionally, let G
                        
                           z
                        
                        =(V
                        
                           z
                        ,E
                        
                           z
                        ) be the fully connected graph where vertices v
                        
                           z
                        
                        ∈
                        V
                        
                           z
                         correspond to the ORD maxima s of the test patch z
                        
                           t
                        , namely s[v
                        
                           z
                        ]∈
                        ℝ
                        2. We obtain a correspondence between vertices in G
                        
                           h
                         and vertices in G
                        
                           z
                         by computing the maximum common subgraph [32] (Fig. 7
                        ). This process consists in obtaining the graph G with the maximum number of vertices such that there exist subgraph isomorphisms
                           1
                        
                        
                           1
                           A graph isomorphism of graphs G and H is a bijection f between the vertex sets of G and H such that any two vertices u and v of G are adjacent in G if and only if f(u) and f(v) are adjacent in H.
                         from G to G
                        
                           h
                         and from G to G
                        
                           z
                        . Note that in general there exists more than one maximum common subgraph. From the set of maximum common subgraphs we choose the one that best satisfies a geometric constraint defined on its edges. Let us denote by G
                        
                           mcs
                        
                        =(V′,E′) a graph from the set of maximum common subgraphs of G
                        
                           h
                         and G
                        
                           z
                        , which involves the mappings f
                        
                           h
                        
                        :
                        V′↦
                        V
                        
                           h
                         and f
                        
                           z
                        
                        :
                        V′↦
                        V
                        
                           z
                        . Then, for each edge (u,v)∈
                        E′ we can obtain the vectors e
                        
                           h
                        
                        =
                        r[f
                        
                           h
                        (u)]−
                        r[f
                        
                           h
                        (v)] and e
                        
                           z
                        
                        =
                        s[f
                        
                           z
                        (u)]−
                        s[f
                        
                           z
                        (v)] that geometrically characterize the graphs G
                        
                           h
                         and G
                        
                           z
                        . We select the maximum common subgraph that minimizes the cost:
                           
                              (5)
                              
                                 
                                    C
                                    =
                                    
                                       
                                          ∑
                                          
                                             
                                                u
                                                v
                                             
                                             ∈
                                             
                                                E
                                                ′
                                             
                                          
                                       
                                       
                                          
                                             
                                                
                                                   e
                                                   h
                                                
                                                −
                                                
                                                   e
                                                   z
                                                
                                             
                                          
                                          +
                                          1
                                          −
                                          
                                             
                                                
                                                   e
                                                   h
                                                
                                                ⋅
                                                
                                                   e
                                                   z
                                                
                                             
                                             
                                                
                                                   
                                                      e
                                                      h
                                                   
                                                
                                                
                                                   
                                                      e
                                                      z
                                                   
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           
                        
                     

Eq. (5) combines a cost proportional to the difference of relative distances between fingertips with a cost that penalizes matches with distinct relative orientation between fingertips. But taking into account the geometric structure of the fingertips' set, of both test and template match, we allow matches despite of missing or false fingertip detections.

ORD maxima are found by clustering pixels with a minimum ORD value (>
                        t
                        
                           f
                        ) into, at most, 5 clusters of a given minimal size s
                        
                           f
                        . 2.5D connectivity is used, i.e. neighborhoods are defined with adjacent pixels and depth distances, and clustering is solved with the 3D Euclidean method proposed by Rusu in [33] Recall that t
                        
                           f
                         and s
                        
                           f
                         are parameters of the finger localization method. Summarizing, the method proceeds as follows, also described in Algorithm 2:
                           
                              1.
                              The test feature vector x
                                 
                                    j
                                  is processed by the k-NN pose gesture recognition block. As a result, we match a template 
                                    
                                       h
                                       ^
                                    
                                  and build the graph G
                                 
                                    h
                                  using the ground truth finger coordinates r.

Coordinates of ORD maxima s are computed using the clustering method and the graph G
                                 
                                    z
                                  is built.

Fingertips' correspondence between the test patch and the template match is solved by finding the maximum common subgraph G that minimizes the cost C in Eq. (5)
                              

Fingertips in the template match not found in the test patch are copied according to the average displacement between both sets of fingertip coordinates.


                              
                                 
                                    
                                 
                              
                           

@&#EXPERIMENTAL RESULTS@&#

The ASL dataset provided by [9] in [34] is used in the following experiment. Such dataset contains annotated hand patches of 5 subjects recorded with the Kinect camera, performing 24 ASL alphabet gestures. Accuracy is used as a measure to be able to compare with other reference methods.

As in [9], the dataset is randomly split into equally sized training and test subsets. Doing so is advantageous for a k-NN strategy, since the probability of having a consecutive frame (very similar) in the training subset is very high. Such effect is reflected in the random column of Table 1
                        , achieving an accuracy of 98% against a 73% of [9] on the same dataset. Other classification methods achieve similar performance [22,21].

A leave-one-subject-out-cross-validation (LOSOCV) is also carried out. In that case, we achieve an accuracy of 76.1%, still about 25 points higher than [9] (49%, results provided in [34]). Our method achieves state-of-the-art performance compared to that of [22,24], although results in [24] are obtained in a slightly different dataset. Only [21] obtains significantly better results, by taking advantage of a large training dataset with synthetic data.

It should be noted that the reference methods [9,23,24,21,22] are mainly gesture recognition methods, none of them provides fingertip localization. However, fingertip positions could be extracted from the pixel-wise classification proposed in [21] by means of a mode finding approach as in [5,2]. The proposed method uses the same ORD feature calculation to additionally provide such fingertip locations.

The ColorTip dataset, consisting of a total of 14 sequences (Section 3), is used for evaluation in the following experiments. We distinguish between Set A and Set B sequences in the results, given the considerable difference of intra-gesture variation.

Results are obtained considering a LOSOCV strategy. Thus, results for subject-i are obtained using as training dataset the remaining subjects' sequences. Recall that if we consider subject-i in Set A, the sequence subject-i Set B is not used for training, and vice versa.

The suitability of the ORD feature for gesture recognition is evaluated. With this purpose, we compare the results obtained with ORD against a benchmark of 3D features. Also, we provide a comparison with state-of-the-art gesture recognition methods using the challenging ASL alphabet.

The performance of our method under several parameterizations is presented in this section. First, suitable values for parameters (k, Q and m) are found by grid search on the ColorTip dataset. This search shows that the best results can be achieved using about k
                           =15 nearest neighbors, Q
                           =5 and m
                           =12. We illustrate the performance as a function of number of k-NN and Q to be used (Fig. 8
                           ). Details on the selection of m are provided in Section 6.3.3.

A benchmark consisting of various 3D features is considered to evaluate the performance of ORD for the classification task. We denote by P as the 3D point cloud of the segmented hand, and by ρ(p) as the neighborhood of radius ρ of a point p. The proposed benchmark consists of:
                              
                                 •
                                 Depth computed with respect to the average depth of P.

Curvature computed as 
                                       
                                          
                                             
                                                λ
                                                0
                                             
                                             
                                                
                                                   λ
                                                   0
                                                
                                                +
                                                
                                                   λ
                                                   1
                                                
                                                +
                                                
                                                   λ
                                                   2
                                                
                                             
                                          
                                       
                                    , where λ
                                    
                                       i
                                     are the eigenvalues of the eigendecomposition of ρ(p).

3DSC 3D shape context, Frome et al. in [35].

VFH viewpoint feature histogram, Rusu et al. in [36].

SHOT Signature of Histograms of OrienTations, Tombari et al. in [37].

The 3DSC and SHOT features provide pixel-wise histograms. As proposed in [38], we compute the Kullback–Leibler divergence between each histogram and the average histogram to obtain scalar values per pixel. Then, the same m
                           ×
                           m subsampling as in Section 5.2 is performed, obtaining m
                           2 sized feature vectors. The depth and curvature features provide scalar values per pixel, and the VFH feature delivers a feature vector of size 308 that is directly used for our experiments.

In order to compare ORD against the benchmark, a k-NN by majority classification is performed with every feature (see Eq. (4)). For this experiment, LOSOCV is performed with the 14 available training sequences. We present in Fig. 9
                            the average F-Measure of the 14 tested sequences (about 18,000 frames). The F-Measure is calculated as 
                              
                                 
                                    
                                       2
                                       ⋅
                                       P
                                       ⋅
                                       R
                                    
                                    
                                       P
                                       +
                                       R
                                    
                                 
                              
                           , where P
                           =precision and R
                           =recall. Results obtained with Dynamically Constrained (DC) k-NN classification using ORD (Section 5.2.1) are also included.

The ORD feature outperforms state-of-the-art features, with an average F-Measure of 0.75. The fact that ORD is focused on characterizing 3D surfaces (by adapting orientation locally) helps in achieving better results, since P is indeed a 3D surface. Surprisingly, ORD is followed by normalized depth, indicating that more sophisticated features such as VFH or SHOT do not perform well for the classification task. The reason for such a poor performance is that these features do not take into account the 3D surface nature of P, and analyze it as it was a 3D point cloud.

Note that the DC k-NN search proposed in Section 5.2.1 helps in increasing the F-Measure from 0.75 to 0.86 by exploiting video temporal consistency of gestures.

The dimension m
                           ×
                           m of the feature vectors {x
                           
                              i
                           } has a noticeable impact on the hand gesture recognition results. In order to assess such effect, and with the objective of selecting an optimal value for m, we compute hand gesture recognition accuracy for various values of m (Fig. 10
                           ). We recall that a feature vector consists of the re-sampling of an ORD patch to an m
                           ×
                           m grid.

Experiments show that low m
                           ≈4 values lead to feature vectors which are not representative enough to distinguish between gestures. On the other hand, large m
                           >14 values lead to an overfitting problem, since feature vectors become too related to data (usually noisy). In such case, the predictive performance degrades. Thus, values of m
                           ≈12 provide the best results in terms of hand gesture recognition.

The size of training datasets may suppose the bottleneck of a classification system. Designing scalable methods is crucial, allowing further incorporation of new training data if required. Furthermore, memory access and capacity problems may also occur due to large datasets.

We analyze in this experiment how the proposed method behaves with small training datasets. A basic clustering by Euclidean distance is performed to reduce the original training dataset H, taking advantage of the already built kd-tree. More precisely, a template h
                              j
                           
                           ∈
                           H is randomly selected, grouping all those templates h
                           
                              i
                            at a certain distance ∥
                           x
                           
                              j
                           
                           −
                           x
                           
                              i
                           
                           ∥<
                           D into a new average training template 
                              
                                 
                                    
                                       h
                                       ¯
                                    
                                    j
                                 
                              
                           . Such step is repeated until all the original templates are checked, obtaining the reduced dataset 
                              
                                 
                                    H
                                    ¯
                                 
                                 =
                                 
                                    
                                       
                                          
                                             h
                                             ¯
                                          
                                          j
                                       
                                    
                                 
                              
                           . We denote by 
                              
                                 
                                    F
                                    %
                                 
                                 =
                                 
                                    
                                       
                                          H
                                          ¯
                                       
                                       H
                                    
                                 
                              
                            the reduction factor.

In Fig. 11
                            we present the F-Measure degradation as H is reduced, using a LOSOCV strategy. The original experiment (at F
                           %
                           =100%) consists of an average of 15,200 training templates.

The proposed method successfully tolerates drastic reductions of the training dataset. Such scalable behavior allows reducing the training dataset down to F
                           %
                           =20% (3040 templates) with an accuracy degradation of less than 5%.

The k-NN DC search performs better with the complete dataset, even if in the case of Set A sequences such effect is barely visible given the good performance of the stand-alone k-NN. We remark that, in the case of Set A, the performance without DC is already close to the annotation error due to transitions between gestures. However, we note that k-NN DC degrades faster.

In the Set B case, at F
                           %
                           ≈35% the stand-alone k-NN search already outperforms the DC version, since the number of erroneous gestures being smoothed grows.

In our case, a training template h
                           
                              i
                           
                           ={x
                           
                              i
                           ,r
                           
                              i
                           ,y
                           
                              i
                           } occupies 12⋅12⋅4+10⋅4+1=587b. Thus, at F
                           %
                           =20%, the reduced dataset only occupies about 3040⋅587≈1.78Mb. Scalability is achieved by taking advantage of the robustness against drastic reductions of the dataset, allowing the incorporation of new training sequences at low memory cost.

We conduct several experiments to evaluate the ORD and the proposed framework in the fingertip localization task. First, we compare the proposed fingertip inference method (Section 5), with a state-of-the-art fingertip detector based on Random Forests (RFs). The RF method is also used to demonstrate the suitability of the ORD feature for hand analysis tasks. Then, we show the computational performance of the proposed method.

The fingertip evaluation protocol consists in a LOSOCV. We consider that a finger has been correctly localized if the estimated location and the ground-truth location are within a distance of 10pixels. We make use of a 2D criterion since the ColorTip dataset contains hands placed at a fixed distance.

The proposed algorithm is compared with two baselines. The first baseline consists in a fingertip localization method using Random Forests (RFs) [39]. The RF localization method is based on the successful system for detecting body parts from range data proposed by Shotton et al. [2]. We use very similar depth-invariant features, but in addition to depth data, we include the ORD feature.

We employ RFs comprising 10 trees of maximum depth 15. Three baselines are trained: one using depth (D) information exclusively, another using ORD exclusively and a baseline combining both features. The precision and recall performance of the RF approach are evaluated with 50 different detection thresholds (Fig. 12
                           ). The experiments reveal that RF trained with the stand-alone ORD values provides the best results (in red in Fig. 12), showing that ORD is also a suitable feature to locally describe parts of an object.

The second baseline, denoted by GM, is a method directly inferring fingertip positions and labels from the ORD data, using the graph matching approach proposed in this paper. Specifically, we perform graph matching with the whole set of training templates, keeping the one with minimum cost C (see Eq. (5)). The results show that, depending on the complexity of the test graph (G
                           
                              z
                           ), doing graph matching with the complete dataset takes between 0.1 and 50s per frame (ColorTip provides about 15,000 templates). Also, this approach strongly degrades the performance of the fingertip localization task, obtaining an average precision/recall of 0.317/0.125, which means an F-Measure of 0.180. Such results are far worse (both in computational time and performance) than those obtained with the proposed combined approach.

Our approach is evaluated with 3 different t
                           
                              f
                            and 8 different s
                           
                              f
                            parameters, obtaining the best results with t
                           
                              f
                           
                           =0.3 and s
                           
                              f
                           
                           =0.8cm2. Some visual results are provided in Fig. 13
                           .

Comparative results between our approach and the best RF baseline are presented in Table 2(Set A) and Table 3 (Set B). The proposed method consistently outperforms all the RF baseline configurations. The main reason is the ability of our method to infer fingertip locations using structured inference given a template pose. First, hand pose matching allows one to robustly locate fingertips under several hand rotations, which is the main limitation of the RF approach. Second, the global structure of the hand pose helps one to robustly detect fingertips even when there is weak evidence of a finger location. In contrast, the RF approach requires each finger to have strong evidence (votes) in order to be robustly detected.

The RF baseline results also show the suitability of the ORD for the fingertip localization task. The Best RF performance is achieved when binary tests exclusively use the ORD descriptor. Interestingly, the ORD contributes to a significant increase in the index finger localization (finger 2).

The above experiments are carried out on an Intel Core2 Duo CPU E7400 @ 2.80GHz. To calculate the ORD feature, we have coded a parallel implementation on a NVIDIA GeForce GTX 295 GPU, performing about 70−140× faster than the implementation in [7]. The overall approach (gesture recognition+fingertip localization) performs in real-time, at a frame-rate of about 15−17fps. A frame-rate of 16fps is achieved by [24]. Remark that our proposal delivers fingertip positions in addition to hand gestures. Moreover, a 176×144 camera is used in [24], with a smaller resolution than Kinect. Real-time is also attained by [9] for gesture recognition, using a state-of-the-art body tracker to detect hands. Public real-time demonstrations have been recently carried out at ECCV'12 [40], and also within the FascinatE Project [41].

In this paper, we have presented a joint method for gesture recognition and fingertip localization. To do so, we have proposed a two-fold exploitation of the ORD feature. Firstly, we utilize ORD to globally describe hand gestures for further classification purposes. Secondly, we take advantage of the multi-scale local description ability of ORD to robustly detect hands and fingertip locations.

More precisely, we have proposed a method to infer fingertip locations by including hand gesture as an auxiliary variable in the problem. Doing so, fingertips are obtained on a search space that is conditioned on the obtained hand gesture. Experiments showed that our method is robust, scalable and runs in real-time.

For gesture recognition, we conducted experiments on a publicly available ASL dataset and on our own hand gesture dataset. In both datasets, our method shows state-of-the-art performance, with the added value of providing fingertip positions. Furthermore, ORD has proven to be more effective than other 3D features for classification tasks.

For fingertip localization, we compared our method with a state-of-the-art approach based on Random Forests. Our experiments show the superior performance of the proposed approach due to the ability to perform structured inference on robustly recognized hand poses.

The ColorTip dataset for hand gesture recognition and fingertip localization on depth data has been proposed. In the near future, more sequences at various depth levels will be publicly available within the ColorTip dataset.

@&#ACKNOWLEDGMENT@&#

The research leading to these results has received funding from the European Union's Seventh Framework Programme (FP7/2007–2013) under grant agreement no. 248138.

This work has been partially supported by the Spanish Ministerio de Ciencia e Innovación, under project TEC2010-18094.

@&#REFERENCES@&#

