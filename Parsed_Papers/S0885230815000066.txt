@&#MAIN-TITLE@&#Evaluation of speech-based HMI concepts for information exchange tasks: A driving simulator study

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           We compare different speech-based in-car HMI concepts in a driving simulator study.


                        
                        
                           
                           The HMI concepts are evaluated in terms of usability and driver distraction.


                        
                        
                           
                           The comparison of speech dialog strategies revealed only differences in usability.


                        
                        
                           
                           The use of a GUI impaired the driving performance and raised gaze-based distraction.


                        
                        
                           
                           An avatar does not additionally raise driver distraction but is not accepted by users.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Speech dialog system

Driving simulation study

Usability

Driver distraction

@&#ABSTRACT@&#


               
               
                  Due to the mobile Internet revolution, people tend to browse the Web while driving their car which puts the driver's safety at risk. Therefore, an intuitive and non-distractive in-car speech interface to the Web needs to be developed. Before developing a new speech dialog system (SDS) in a new domain developers have to examine the user's preferred interaction style and its influence on driving safety.
                  This paper reports a driving simulation study, which was conducted to compare different speech-based in-car human–machine interface concepts concerning usability and driver distraction. The applied SDS prototypes were developed to perform an online hotel booking by speech while driving. The speech dialog prototypes were based on different speech dialog strategies: a command-based and a conversational dialog. Different graphical user interface (GUI) concepts (one including a human-like avatar) were designed in order to support the respective dialog strategy the most and to evaluate the effect of the GUI on usability and driver distraction.
                  The results show that only few differences concerning speech dialog quality were found when comparing the speech dialog strategies. The command-based dialog was slightly better accepted than the conversational dialog, which seems to be due to the high concept error rate of the conversational dialog. A SDS without GUI also seems to be feasible for the driving environment and was accepted by the users. The comparison of speech dialog strategies did not reveal differences in driver distraction. However, the use of a GUI impaired the driving performance and increased gaze-based distraction. The presence of an avatar was not appreciated by participants and did not affect the dialog performance. Concerning driver distraction, the virtual agent did neither negatively affect the driving performance nor increase visual distraction.
                  The results implicate that in-car SDS developers should take both speaking styles into consideration when designing an SDS for information exchange tasks. Furthermore, developers have to consider reducing the content presented on the screen in order to reduce driver distraction. A human-like avatar was not appreciated by users while driving. Research should further investigate if other kinds of avatars might achieve different results.
               
            

@&#INTRODUCTION@&#

Thanks to the mobile Internet revolution people can access the Internet via their smartphone anywhere at anytime. Smartphones are considered as people's companion and support the user to make life easier in various daily situations. The pervasive use of smartphones also significantly impacts the automotive environment. In order to stay “always connected” people do not even refrain from using their smartphone's Internet functions manually while driving. However, the manual use of smartphone distracts from driving and endangers the driver's safety. According to the Governors Highway Safety Association 25% of U.S. car crashes are related to drivers using their cellphones while driving (Governors, 2011).

In order to control the in-car infotainment system in a safe manner speech technology has been applied for in-vehicle use since many years. Previous research proves that controlling an in-car infotainment system by speech distracts less than manual control (Peissner et al., 2011; Barón and Green, 2006). By using speech instead of manually controlling the smartphone, drivers could keep their hands on the wheel and their eyes on the road. In order to increase driver safety it is essential to develop an intuitive and non-distractive in-car speech interface to the Web.

Before developing a new speech dialog system (SDS) in a new domain developers have to examine how users would interact with such a system. An Internet user study by Hofmann et al. (2012) in which the subjects had to solve Internet tasks orally, revealed that concerning information exchange tasks (e.g. booking a hotel) conversational and command-based speaking styles were used with equal frequency of occurrence. Command-based utterances consist of short impersonal phrases whereas conversational utterances resemble human–human communication. Because of the equal distribution it is valuable to examine which speech dialog style and strategy is the most suitable for these tasks. In the driving scenario, the most user-friendly and the least distractive speech dialog needs to be found.

When designing SDSs several dialog strategies have to be taken into consideration: system-directed, user-directed and a mixed-initiative dialogs (McTear, 2004). For applications in which multiple parameters have to be collected, such as information exchange tasks, López-Cózar Delgado and Araki (2005) suggest to use a mixed-initiative or a system-directed dialog strategy. Mixed-initiative dialogs are the basis for conversational speech dialogs and resemble human-human communication. As this dialog strategy allows users greater input variability, SDS developers attempt to model the automatic speech recognition (ASR) and natural language understanding (NLU) components to cope with conversational utterances. In contrast, system-directed dialogs are less flexible and limit the input possibilities to certain commands and other elliptical utterances, which is why they are also called command-based dialogs. At a first glance due to the flexibility conversational speech interfaces seem to provide greater usability and should be the mean of choice when it comes to the dialog strategy decision. However, Thomson and Wisowaty (1999) argue that conversational speech interfaces lead to user confusion, as users do not know how to talk to such a SDS. Often, users are unfamiliar with the system capabilities and do not exactly know what spoken format is expected by the speech recognizer. In addition, research found out that allowing users to speak freely, leads to inaccuracies in the ASR and NLU components (Aust et al., 1995), which might negatively affect the usability (Berg, 2013). In contrast, system-directed dialogs guide the user and limit the user input to certain commands, which reduces the risks of recognition errors. Sturm et al. (1999) investigate a mixed-initiative SDS for access to train timetable information, which allows users to take the initiative but also guides novice users if the dialog initiative is not taken. They made the experience that the mixed initiative capabilities were used only occasionally by the users. Due to the directive questions the system asks users did not spontaneously make use of the flexibility in the dialog and followed the system-directed questions. As both strategies obviously provide advantages and disadvantages research needs to focus on the direct comparison of these two speech dialog strategies.

Studies on the direct comparison of dialog strategies have been conducted by Walker et al. (1997) and Devillers and Bonneau-Maynard (1998). Walker et al. compare a mixed-initiative dialog agent to a system-directed dialog agent. They conclude that the system-initiative dialog is better for inexperienced users. Mixed-initiative is preferred by users as they gain experience using the system successfully. As for the overall dialog quality the mixed-initiative dialog strategy did not surpass the system-directed strategy concerning the overall dialog performance across several tasks. Deviller et al. compare two SDSs allowing the user to retrieve touristic information. One dialog strategy guides the user via system suggestions, the other does not. The authors conclude that user guidance is suitable for novices and appreciated by all kinds of users.

However, all the presented research work did not address a dual-task scenario such as accessing the Internet by speech while driving. When performing two tasks in parallel the user's behavior and preferences might be different. When users have to perform a task by speech while driving their car they might prefer a simple and clearly directed dialog with low mental demands, which would speak for a system-directed dialog strategy. Nevertheless, this dialog strategy requires many dialog steps and demands the user's attention for a long time. Using a mixed-initiative dialog, the user is able input multiple parameters at once and thereby, could shorten the speech dialog. However, this dialog strategy is more complex and mentally demands the user, which might have a negative effect on the primary task driving. Depending on the environmental context one or the other strategy might be preferred. Therefore, it is important to find out, which is the most appropriate strategy for information exchange task in the driving scenario.

Until today only few research has examined different speech dialog strategies while driving. Research work by Mutschler et al. (2007) investigated speech dialog strategies as part of a multimodal research question. In the EU funding project TALK,
                        2
                     
                     
                        2
                        
                           http://talk-project.eurice.eu.
                      Mutschler et al. compared two multimodal systems, one based on a command-based speech dialog, the other based on a conversational speech dialog in a driving scenario. In the experiment, participants had to control the in-car mp3-player by speech or haptic input while driving. Each speech dialog strategy was supported by the same graphical user interface (GUI). The main research goal was to investigate multimodal interaction with the focus on modality selection. Although the conversational dialog was more efficient the command-based dialog was more appreciated by the participants. According to Mutschler et al. a high error rate of the conversational strategy was the reason for the higher acceptance of the command-based dialog. There were no significant differences in the driving performance revealed when using the different SDS. However, the comparison of speech dialog strategies is only achieved on the basis of the available speech turns and is rather a side product of this experiment and therefore, the results have to be handled with care. As the speech recognizer quality has improved enormously within the last five years, the influence of the weak speech recognition performance of Mutschler et al.'s conversational dialog may be less significant today. Furthermore, the use of the same GUI for different dialog strategies could have additionally influenced the result. The GUI should be adapted to the particular dialog strategy in order to benefit from the advantages of the respective strategy the most and to allow for a comparison of optimal systems.

This paper reports experimental results from the development and the evaluation of various in-car SDS prototypes and presents follow-on work to the research reported in Hofmann et al. (2014). Besides the comparison of different speech-based human–machine interface (HMI) concepts the capabilities of today's speech recognition and understanding technology are of interest. Therefore, instead of evaluating the different concepts in a Wizard-of-Oz setup real system prototypes are used. The SDS prototypes are based on different speech dialog strategies, a command-based and a conversational dialog, which were evaluated on usability and driver distraction. The systems have been developed for German users and allow to perform a hotel booking by speech. Since it is common in today's in-car infotainment systems the developed SDS prototypes were supported by a GUI, which gives visual feedback to the user to support the speech interaction. In this research, different GUIs (one including a human-like avatar) were designed in order to support the respective dialog strategy the most and to evaluate the effect of the GUI on usability and driver distraction. Objective and subjective dialog measures are applied to assess the dialog quality and user acceptance. In order to assess the level of driver distraction, objective driving performance and subjective workload measures are applied. Additionally, visual demand is assessed by recording the participants’ glances on the screen using an eye tracker. The experiments have been conducted at DFKI, Saarbrücken using the OpenDS
                        3
                     
                     
                        3
                        
                           http://www.opends.eu/.
                      driving simulation. The research work is performed within the scope of the EU FP7 funding project GetHomeSafe.
                        4
                     
                     
                        4
                        
                           http://www.gethomesafe-fp7.eu.
                     
                  

The remainder of the paper is structured as follows: In Section 2, the developed SDS prototypes are described. In this section, first the different speech dialog strategies are described, followed by the description of the GUI concepts. Section 3 presents the experimental procedure. In Section 4 the results are presented and discussed and finally, conclusions are drawn.

Booking a hotel requires to input several parameters, which makes it a good use case to highlight the strengths and weaknesses of the command and conversational dialog strategy. Another advantage of the hotel booking use case is that every user has a clear conception, of what is needed to book a hotel. This distinct mental model makes it easier for the user to understand the voice-control concept, simplifies explanations about the functionality of the system and thereby, reduces the risk of misunderstandings in the experiment. In order to access hotel data the online booking service HRS
                        5
                     
                     
                        5
                        
                           http://www.hrs.com.
                      has been used as data provider for the SDS. The selected use case covers many different subdialog types (parameter input, list presentation and browsing, etc.). Each SDS prototype concept offers the same functionality: First, the user has to input several search parameters to retrieve a list of hotels. The user can browse the list and ask for detailed information about a certain hotel. If the hotel matches his needs he is able to book the hotel. In addition, the user can change search parameters and thereby recover from errors if they occur.

In the following, the developed speech dialog strategies and the different GUI concepts are described.

SDS prototypes for German language have been developed including the following SDS features: In order to speak to the system the driver has to press a Push-To-Activate (PTA) button. Furthermore, the driver is able to interrupt the system while prompting the user (“barge-in”). Previous research work (Walker et al., 1997; Devillers and Bonneau-Maynard, 1998; Mutschler et al., 2007) has shown that a command-based dialog strategy based on a system-directed initiative and a conversational dialog strategy based on a mixed initiative should be investigated. When designing the different dialog strategies we particularly focused our attention on the dialog initiative, the possibility to enter multiple input parameters and the acoustic feedback.

The dialog behavior of the command-based dialog strategy corresponds to the voice control which can be found in current state-of-the-art in-car SDS. By calling predefined explicit or implicit speech commands, the speech dialog is initiated. There are several synonyms available for each command. System feedback of what was understood is given via the system reaction (e.g. execution of what was demanded) or by spoken (implicit or explicit) feedback in the system's voice prompt. If more information is needed from the user, in order to fulfill the user's demands the system guides the user. This system-directed dialog strategy is adopted to the command-based dialog strategy of a hotel booking. The GUI supports the speech dialog by showing the “speakable” commands as widgets on the screen (see Section 2.2). A sample dialog is illustrated in the following:


                           
                              
                                 
                                    
                                    
                                    
                                       
                                          Driver:
                                          
                                             Book a hotel.
                                          
                                       
                                       
                                          System:
                                          
                                             Where would you like to book a hotel?
                                          
                                       
                                       
                                          Driver:
                                          
                                             In Stuttgart.
                                          
                                       
                                       
                                          System:
                                          
                                             When do you want to arrive in Stuttgart?
                                          
                                       
                                       
                                          Driver:
                                          
                                             Tomorrow.
                                          
                                       
                                       
                                          System:
                                          
                                             How long would you like to stay in Stuttgart?
                                          
                                       
                                       
                                          Driver:
                                          
                                             Until the day after tomorrow.
                                          
                                       
                                    
                                 
                              
                           
                        

After the first speech command the driver is guided by the system and executes the next steps which are suggested and displayed by the system. The dialog is mainly system-driven and the input possibilities are restricted to direct answers (no over-answering). Furthermore, the user is only able to set one input parameter within an utterance to keep the dialog simple and lower the mental demand. When the parameters have been input HRS is called to retrieve the list of hotels. The HRS service returns a result list of hotels, which the SDS presents itself one-by-one. For each hotel the most important facts are read out. The user can interrupt the process of reading out by speaking commands for e.g. selecting a presented hotel.

A conversation between an agent and a client over the phone is taken as an example for the conversational dialog design. In the conversational dialog strategy, the dialog initiative switches during the speech interaction. The driver is able to speak whole sentences where multiple parameters can be set within one single utterance. Thereby, the dialog can run more natural, flexible and efficient. The driver is informed about what the system has understood by using explicit or implicit feedback and via the system reaction. If the driver has set multiple parameters in his utterance, the system does not repeat all parameters as the system response would be too long. Therefore, the system repeats only the contextually most important parameter. The GUI does not present the “speakable” utterances on the screen. In order to indicate the possible functions, icons are used (see Section 2.2). A sample dialog is illustrated below:


                           
                              
                                 
                                    
                                    
                                    
                                       
                                          Driver:
                                          
                                             I would like to book a hotel in Stuttgart.
                                          
                                       
                                       
                                          System:
                                          
                                             When do you arrive in Stuttgart?
                                          
                                       
                                       
                                          Driver:
                                          
                                             I arrive tomorrow and leave the day after tomorrow.
                                          
                                       
                                    
                                 
                              
                           
                        

The user starts the speech interaction by speaking to the system in whole sentences. He can already mention some input parameters when addressing the system for the first time. The system verifies which input parameter are missing in order to send a request to the HRS service. The system prompts the user and collects the missing information. Although the system asks for only one parameter, the user is able to give more or other information than requested. When the parameters have been input HRS is called to retrieve the list of hotels. Equivalent to the command-based dialog, the HRS service returns a result list of hotels, which the SDS presents itself one-by-one. Now, the user can continue the interaction by speaking freely and without having to speak certain commands.

The original TRINDI ticklist from (Bohlin et al., 1999), which characterizes the dialog behavior of an SDS with the help of 12 Yes-No-questions, gives a good overview of the implemented dialog features. Both of the SDS prototypes have been developed and differentiated corresponding to this list. The filled out TRINDI ticklist for both dialog strategies is illustrated in Table 1
                           .

In this research work, the most important dialog features, which allow to distinguish both dialog strategies have been realized so far. Concerning the dialog design of the conversational dialog, we set a high value on the flexibility to input parameters by speech (e.g. Q2, Q3). In order to achieve a successful human-machine speech dialog an SDS has to be able to interpret utterances dependent on the dialog context (Q1). Dialog features which are no beneficial characteristic of one of the dialog strategies and which do not reveal differences in the evaluation are left out to lower the development effort (e.g. Q5, Q6). Impact of the environment on the speech interaction is not in focus of this research (Q8). The dialog flow of a hotel booking dialog is linear and does not allow for context-relevant branches whereby Q11 becomes superfluous.

The speech dialog strategies have been implemented in the Daimler Speech Dialog Framework (SDF) which allows to quickly realize multimodal SDS prototypes.

In the Daimler SDF the dialog is modeled as a hierarchy of sub-tasks including roles which can trigger a system reaction if the according user input is given (Ehrlich, 1999). The ASR and NLU module of the SDS prototypes were based on grammars. In order to cover the large variety of possible utterances of conversational speech a linguistic grammar approach (Hofmann et al., 2013) was applied. The grammar-based ASR engine is Nuance's VoCon® 32005 embedded speech recognizer. The embedded TTS engine integrated in the prototypes is Nuance's Vocalizer for Automotive.
                              6
                           
                           
                              6
                              
                                 http://www.nuance.com.
                           
                        

In the next section, the different GUI concepts, which have been designed to support the speech dialog are described.

As common in today's in-car infotainment systems a GUI gives visual feedback in order to support the speech dialog strategies the most. Different GUI concepts have been designed and customized corresponding to the dialog strategies only as much as necessary since an objective comparison is targeted. When designing the screens the international standardized AAM-Guidelines (Driver, 2002) were adhered to, which determine the minimum font sizes, the maximum numbers of widgets, etc. in order to minimize distraction.

Several GUI concepts have been designed. One concept, which is especially adapted to the command-based dialog and one concept which is adapted to the conversational dialog. Furthermore, the conversational dialog is supported by a human-like avatar to raise the level of naturalness in the interaction. Finally, a concept which gives almost no visual feedback is designed in order to investigate the necessity of a GUI for a successful speech dialog and its influence on driver distraction. The different GUI concepts are described in the following with the aid of screenshots.

In the command-based dialog strategy, as common in state-of-the art in-car SDS the “speakable” input is presented on the GUI supporting thus the speech dialog. The most relevant speech commands are displayed on the screen at all times, which may lead to a high visual distraction. In automotive terms, the command-based speech dialog strategy is also called “speak-what-you-see” strategy (Niemann, 2013).

When the user has initiated the speech dialog the parameter input process begins and the screen illustrated in Fig. 1
                            appears. Here, the first input parameter “destination” (“Ziel” in German) has to be set by the user after being requested by the system. Afterwards the user is guided step-by-step by the system. When the driver has given the requested information, a new widget appears on the screen and the system asks the driver for the corresponding input (see Fig. 2
                           ).

When all the parameters are elicited by the system and the hotel service has returned the list of hotels, the possible commands for changing the input parameters (“Suche ändern”), presenting the result list (“Liste”) and starting a new search (“Neue Suche”) become visible in the sub-function line, which is located on the bottom of the screen (see Fig. 3
                           ). For instance, by calling the command “Liste” (or synonyms of the command) the list browsing sub-dialog is triggered and the hotel list is displayed (see Fig. 4
                           ). Further screens were designed for the remaining sub-dialogs.

In the conversational dialog strategy, the driver can speak freely and does not have to call certain commands. There is no need to give the driver a visual feedback of the currently “speakable” input whereby the visual distraction may be lowered. For that reason, the content on the car's head unit screen does not have to indicate the possible options to proceed with the speech dialog. The sub-function line, which was used to indicate the available commands is replaced by only few symbols, which resemble the current GUI state.

When the user asks to book a hotel the form filling screen illustrated in Fig. 5
                            appears. This screen represents the main screen at the beginning of the parameter input dialog where the user is already able to input several parameters at once (see Fig. 6
                           ). The respective fields of the form are filled in the course of the parameter input as illustrated in Fig. 7
                           .

After having input all required (and optional) parameters the system calls the HRS service and retrieves a list of hotels (see Fig. 8
                           ). The symbols on the bottom of the screen resemble the GUI states for parameter input/changes and the result list. Depending on the current GUI state the respective symbol is highlighted. Visual feedback is given by updating the fields of the form. For the remaining sub-dialogs further screens were designed.

The goal of using a human-like avatar is to raise the naturalness of the human-machine interaction. By expressing gestures and mimics, the avatar contributes to a more human-like interaction. When seeing a human character on the screen, the driver might tend to speak more naturally, as if he would talk to a human being. Previous research by Nicolescu (2009) investigated different in-car user interface concepts for virtual instruction manuals, one including an avatar. The results showed that the avatar-based UI concept was appreciated by the participants of the user study. Therefore, in our research the presence of an avatar might also have a positive effect on the user acceptance. However, the user might be more distracted by a human character on the screen. So far, those positive and negative effects of an SDS with avatar while driving have not been examined.

The avatar is only used in combination with the conversational dialog strategy. Therefore, the GUI concept with avatar is based on the conversational dialog GUI. A virtual character designed and developed by Charamel
                              7
                           
                           
                              7
                              
                                 http://www.charamel.de.
                            is integrated. The avatar overlays the background illustrated in Figs. 5 and 8 but does not cover the widgets, which are currently important for the speech dialog (see Figs. 9 and 10
                           
                           ).

The human agent is already visible at the very beginning of the speech dialog. The avatar makes certain gestures to give the SDS some human character. For example, when the system asks for inputting the arrival date, the avatar points toward the arrival date widget on the screen. When the user browses the hotel result list, the avatar makes a swipe gesture to support the scrolling in the list as illustrated in Figs. 9 and 10.

Another goal of this research was to investigate the need for a visual feedback during the speech interaction. Can a speech dialog without a GUI still be performed effectively and efficient and will users accept such a kind of speech interaction? How strong are the influences of the GUI on driver distraction? In order to answer these questions the two speech dialog strategies are also evaluated “without GUI”. In this case, “without GUI” means that no content information is displayed on the screen. However, the visual feedback, which indicates if the user is allowed to talk is still presented in the top bar of the screen (see Fig. 11
                           ).

The different GUI concepts have been specified using the Daimler SDF. The GUI of the Daimler SDF is implemented as finite state machine, which can present different background images, image-based widgets or text fields. In order to present the avatar on the screen the avatar engine had to be integrated in the Daimler SDF and the lip movements had to be synchronized with the TTS engine.

In this section, the developed speech dialog strategies and GUI concepts have been described. These concepts have been implemented as SDS prototypes and evaluated in a driving simulator study. The driving simulation study setup is described in the following.

In this section, the method of the driving simulator experiment is described in detail. First, the target group and the experimental design is described, followed by the description of the used materials and the experimental procedure. Finally, the dependent variables and the derived hypotheses are presented.

A study conducted by BITKOM (2011) revealed that especially young German adults (18–35 years) are quite interested in Internet access in the car. The elders’ interest in a connected car is much lower. Therefore, corresponding to the BITKOM study, this driving simulation study was targeted at young German adults at the age of 18–35.

In total, 25 German participants (mainly students) participated in the experiment. All participants possessed a valid driver's license. The participants comprised 11 male and 14 female subjects and the average age was 26.0 years (SD
                        =6.0). 52% of the participants were driving their car at least once a week. 68% had little to no experience with speech-controlled devices.

@&#EXPERIMENTAL DESIGN@&#

Several speech-based HMI concepts, which consisted of combinations of different speech dialog strategies and GUI concepts had to be evaluated: the command-based dialog was combined with 2 GUI variants (without GUI and command-based GUI) and the conversational dialog was combined with 3 GUI variants (without GUI, conversational GUI, and conversational GUI with avatar). Comparing all 5 speech-based HMI concepts within one experiment would lead to an unbalanced experimental design, which again would lead to learning and order effects. Thus, a fair comparison of the two speech dialog strategies could not be guaranteed anymore. Therefore, the experiment was split into two sessions, which were conducted on different days with the same participants. A participant had to wait at least 3 days and maximum 5 days before returning to the lab to perform Session 2. Except for one, all participants returned to the lab. Therefore, in the end, 25 participants took part in Session 1 and 24 participants took part in Session 2.

The experimental design of Session 1 and Session 2 are presented in the following. The two sessions are considered as separate experiments. In each session we investigated the participants’ speech dialog performance and influences on driver distraction while using the SDS.


                        Design of Session 1. In the experiment, 4 different HMI concept variants were tested in a 2 (speech dialog strategy: command-based vs. conversational)×2 (GUI: with GUI vs. without GUI) design. The command-based and conversational GUI were only used with the corresponding dialog strategy. The 4 HMI concepts were the following:
                           
                              •
                              Command-based speech dialog (“Comm”)
                                    
                                       –
                                       command-based GUI (“CommGUI”)

without GUI (“CommNoGUI”)

Conversational speech dialog (“Conv”)
                                    
                                       –
                                       conversational GUI (“ConvGUI”)

without GUI (“ConvNoGUI”)

Each participant encountered all four conditions (“within-design”).


                        Design of Session 2. In the second session the utility of an avatar was tested. As the effect of an avatar has not been evaluated in a driving simulator study yet, this experiment underlied rather an explorative character. Here, we compared the basic conversational dialog GUI with the conversational dialog GUI including the avatar:
                           
                              •
                              Conversational speech dialog with conversational GUI (“noAvatar”)

Conversational speech dialog with conversational GUI and avatar (“Avatar”)

Each participant encountered both conditions (“within-design”).

The focus of Session 1 was on the investigation of usability and driver distraction of the different speech dialog strategies and GUI conditions. Since learning effects would have strongly influenced the dialog performances of the respective speech dialog strategy each participant first performed Session 1. As the focus of Session 2 was only on the evaluation of the use of an avatar (both conditions are based on the conversational dialog) counterbalancing the two sessions across the experiment was not necessary.

This section describes the materials, which were applied in the experiment.


                        Speech dialog prototypes. The described speech dialog prototypes based on the SDS concepts have been developed and used in the experiment. For all five conditions the barge-in function was activated.

In order to explain the functionality and the control of the SDS prototypes to the user, instruction videos for each speech dialog strategy were presented. By presenting tutorial videos, we ensured that each participant was given identical instructions.


                        Tasks. During the experiment participants had to solve several tasks: They had to book a certain hotel according to given search parameters. The tasks were verbalized as little stories, which contained the necessary parameters in a memorable manner. A sample task translated into English is presented below:
                           “Imagine, you and one of your colleagues are currently on the way to Cologne for a two-day meeting. You need two single rooms for these two nights which you have not booked, yet. Your appointment takes place in the city center of Cologne, where you would like to spend the two nights. Please look for a matching hotel for those nights.”
                        
                     

In total, 16 tasks were created. 4 different explorative tasks were used to familiarize the participants with the task setup and the voice control of the SDS prototypes. These explorative tasks were reused among the experiment. The 12 remaining tasks were used for the data collection.


                        Questionnaires. During the experiment different questionnaires were used:
                           
                              •
                              Preliminary interview: In a preliminary questionnaire we collected demographical information (age, gender, etc.) about the participants.

SASSI questionnaire (Hone and Graham, 2001): The SASSI questionnaire is widely used to measure subjective usability evaluation of SDS.

DALI questionnaire (Pauzie, 2008): The DALI questionnaire is applied to evaluate the user's cognitive load.

Final interview: This questionnaire was designed to allow for a direct comparison of the respective SDS prototypes at the end of the experiment. Each participant had to rate the different SDS on a scale from 1 to 10 regarding several subjective measures.


                        Driving simulator setup. The experiment was conducted in the driving simulator at DFKI's “future lab” (see Fig. 12
                        ) in Saarbrücken. The participants were sitting on the driver's seat in a car, which was placed in front of a canvas onto which the driving simulation was projected. The participants controlled the driving simulation by the car steering wheel and pedals. During the experiment the examiner was sitting on the passenger seat.

Previous driving simulation studies employ the standard Lane Change Test (LCT) by Mattes (2003). However, the LCT only includes foreseeable events and driving in the periods between events does almost not comprise any demands in terms of steering and no braking at all. This might lead to strategic task completion and to an underestimation of driver distraction. Furthermore, LCT is based on fixed tracks, which limits continuous recordings to three minutes. We employed the ConTRe (continuous tracking and reaction) task as part of the OpenDS
                           8
                        
                        
                           8
                           
                              http://www.opends.eu/.
                         driving simulation software which complements the de-facto standard LCT including higher sensitivity and a more flexible driving task without restart interruptions. The steering task for lateral control resembles a continuous follow drive, which will help to receive more detailed results about effects of the two diverse dialog strategies. Furthermore, mental demand can be assessed by an additional reaction task implemented as longitudinal control. A detailed description of the ConTRe task can be found in Mahr et al. (2012). Visual demand off the road reduces driver situation awareness (Rogers et al., 2011) and negatively influences driving performance (Horrey et al., 2006), as the time spent looking inside the vehicle is not spent looking at the road for potential crash-inducing hazards. Therefore, we were interested in investigating the effects of the GUI in a speech dialog on visual distraction. In order to assess visual demand an eye tracker, which records glances on the screen, is used for gaze-based distraction evaluation. The driving performance was recorded during the baseline drives and the drives, when an SDS was used. The eye tracker data were only recorded when participants interacted with an SDS.

@&#PROCEDURE@&#

This section describes the experimental procedure of experiment sessions.


                        Procedure of Session 1. In the experiment, four different conditions were evaluated. We did not randomize all four conditions, because the participants might have been confused if the speech dialog styles vary too often. Therefore, we decided to separate the experiment into two blocks as illustrated in Fig. 13
                        . In each block, only one speech dialog variant with the two GUI variants was tested. The order of the two blocks was counterbalanced between participants to control for learning and order effects. Furthermore, the order of GUI variants within one block was balanced between participants. In each of the four conditions, the participants had to perform two tasks. The order of the hotel booking tasks was the same for all participants regardless of the system condition. When the second task was finished, participants had to fill out the SASSI and the DALI questionnaire for each condition.

The overall procedure of the experiment is illustrated in Fig. 14
                        . At the beginning of the experiment, participants had to fill out the preliminary questionnaire. Afterwards they had the possibility to get to know the driving simulation in a test drive lasting at least four minutes. After the test drive, in order to assess driver distraction without secondary task the participants completed a four minutes baseline drive and had to fill out the DALI questionnaire afterwards. Next, the participants were shown the video of their first SDS type and became familiar with the SDS by performing four explorative tasks. Subsequently, participants performed two tasks for the first SDS type with or without GUI. Then, they performed two explorative tasks within the first SDS type and the remaining GUI condition in order to get familiar with the new GUI condition. Afterwards, the data collection of SDS type 1 with the remaining GUI condition was conducted by performing again a simple and a difficult task and the SASSI and the DALI questionnaires had to be filled out. After the first block, the second SDS type was tested. Analogically to the first block, participants had to perform the different explorative tasks and data collection tasks for the second SDS type and the two GUI conditions and had to fill out the questionnaires. Finally, the participants completed a second baseline drive and filled out the final questionnaire.


                        Procedure of Session 2. In Session 2, the conversational speech dialog is tested with the two avatar conditions (with avatar and without avatar). The experimental procedure was analogical to Session 1 and also consisted of the preliminary interview, the test drive, two baseline drives, the introduction of the SDS via a video and trial bookings, the data collection and the final interview. However, as only one SDS type was tested compared to Session 1 only one data collection block was needed. Within this block, the avatar conditions were counterbalanced across participants. As both of the avatar conditions were tested within the conversational SDS type, 4 explorative tasks had to be conducted at the beginning of Session 2 and only 2 explorative tasks in between the two avatar conditions.

Several types of data were collected in order to evaluate the speech dialog and the driver distraction. The SDS, the driving simulation software and the eye tracker produce logfiles at runtime. In the course of the experiment, the examiner was observing the test procedure in order to take notes on hotel booking task success. Based on the collected data, different evaluation measures were computed. The evaluation measures, their descriptions and their data source are illustrated in Table 2
                            and explained in the following.

Based on the observations the task success (TS) of each speech dialog is assessed. The speech dialog logs are used to compute the Number of Turns (NoT) and the dialog duration (DD) of each dialog. In order to evaluate the system's understanding qualities, we assess the concept error rate (CER), which indicates, if the system has correctly interpreted the user's utterance. A detailed description and definition of the dialog measures can be found in ITU-T (2005). A subjective usability assessment was achieved by employing the SASSI questionnaire. Based on the OpenDS logs we compute the mean lateral deviation (MDev) from the ideal lane (lane of the moving object the participant has to follow) and the response time on the reaction task (RT). Subjective cognitive load assessment is achieved by applying the DALI questionnaire (DALI). Visual demand relates closely to driver distraction, as the time spent looking inside the vehicle is not spent looking at the road for potential crash-inducing hazards. Based on the eye tracker data the number of glances (NoG), the mean glance duration (MGD) and the percent dwell time (PDT) on the display is assessed. The PDT is the percentage of time that the participants spend looking at the GUI during a hotel booking task. The AAM guidelines suggest that single glance durations should generally not exceed two seconds and that the total glance time should not exceed 20 seconds.
                              9
                           
                           
                              9
                              The NHTSA provided new guidelines very recently (National Highway Traffic Safety Administration, 2013) However, these guidelines have not been established in the automotive community, yet. Nevertheless, in future work these guidelines will be explored.
                            Here, the 85th percentile of the distribution is relevant for the evaluation, which represents a common design standard in traffic engineering (Driver, 2002). In order to verify if our 5 SDS prototypes are compliant to the AAM guidelines we additionally assess the distribution of single glance durations (SGD) and the distribution of total glance times (TGT).

Overall, in Session 1 we expect better usability results for the conversational dialog compared with the command-based. The participants will accept the conversational dialog better than the command-based dialog because it reflects the human-human communication. Furthermore, we expect the conversational dialog to distract less than the command-based dialog as the user is able to communicate with the system as if he would talk to a human being. Thereby, users are less mentally demanded and can concentrate on the driving task. Generally, a visual feedback makes it more comfortable to interact with an SDS and more effective in terms of disambiguation. Therefore, we expect the participants to accept the SDS with GUI better than without GUI. However, concerning the influence of the GUI on the driving performance, we expect the GUI to cause more driver distraction due to the glances off the road.

Session 2 underlied rather an explorative character. We expected differences in user acceptance when comparing the condition with avatar with the condition without avatar. However, since user acceptance is a very subjective measure it is hard to predict, which condition will be more preferred. As the user is visually more distracted when interacting with the avatar, there should be higher driver distraction in the avatar condition in terms of driving performance, visual distraction and subjective workload assessment.

@&#RESULTS@&#

This section presents the results of Session 1 and Session 2. When analyzing the data of the experiments a large number of tests were performed, which lead to a large result data set. In this paper, only the most relevant and significant results are illustrated and discussed in the following. Less relevant and insignificant results are not illustrated since this would go beyond the scope of this paper.

In total, 200 dialogs were recorded as sound files (100 command-based and 100 conversational dialogs). Due to measurement errors and system crashes, eight command-based dialogs and one conversational dialog were lost. In total, 191 dialogs (92 command-based dialogs and 99 conversational dialogs) were transcribed. The driving data set comprised 100 drives, in which the command-based SDS was applied, 100 drives with the conversational SDS and additional 50 baseline drives. We only recorded eye gazes, when the participants had to use an SDS prototype. Due to calibration problems, only 82 data sets when using the command-based SDS and 84 data sets when using the conversational SDS could be recorded. The missing speech dialog and eye tracking data were estimated using the expectation maximization method. The objective and subjective data altogether comprised of 10,000 values including 3.6% of missings. Analyzes were performed conducting a mixed factorial ANOVA. In order to assess the magnitude of the analyzed phenomena the effect size η
                        2 is additionally presented in the results (an effect size of 0.2–0.5 is considered as small, 0.5–0.8 as medium and greater than 0.8 as large).

In the course of a pretest, which was conducted to verify if the speech dialogs matched the actual user expectancies and to reveal potential system shortcomings, and in the course of this experiment, we observed that a high number of understanding errors led to multiple correction dialogs. A high number of correction dialogs sometimes frustrated the users. Furthermore, we observed that participants tended to look on the screen in order to get visual help when an error occurred and when additional correction dialogs had to be performed. Therefore, concerning the two speech dialog strategies we analyzed the relationship of the CER and the NoT on other speech dialog measures and on driving distraction measures by computing Pearson's product-moment correlation coefficient r
                        
                           p
                        .

In the following only the most relevant results of Session 1 are illustrated. Further information and more detailed results can be retrieved from Hofmann et al. (2014).

This section presents the results of the objective speech dialog performance measures (TS, NoT, DD, CER) followed by the results of the questionnaires (SASSI, final interview).


                           Task success. Using the command-based SDS prototype, participants were able to solve 78% of the tasks. 71% of the tasks could be solved when using the conversational prototype. There was no significant difference revealed between the two dialog strategies. The comparison of the conditions with GUI with the conditions without GUI did not show any significant results neither.


                           Dialog duration. The command-based dialogs took 93.1s and the conversational dialogs took 94.3s in average. There was no significant difference revealed. Fig. 15
                            illustrates the average DD per GUI condition. When comparing the GUI conditions within one speech dialog strategy, the dialogs using the command-based SDS without GUI took significantly longer than with GUI (F(1, 24)=6.20, p
                           <0.05, η
                           2
                           =0.21). No significant differences were found when comparing the GUI conditions of the conversational dialog.


                           Number of turns. The command-based dialogs needed 30.1 turns and the conversational dialogs needed 27.3 turns in average. The difference in dialog turns was not significant. The comparison of GUI conditions within one dialog strategy did not show any significant results neither.


                           Concept error rate. The average CER per dialog is significantly smaller in the command-based dialogs compared to the conversational dialogs (F(1, 23)=17.05, p
                           <0.001, η
                           2
                           =0.43) (see Fig. 16
                           ). The analysis of the data revealed that the main reason for the increased CER of the conversational dialogs was a higher number of out-of-grammar statements by the users due to the flexibility of conversational speech. When comparing the GUI conditions within one speech dialog strategy no significant differences were found.


                           Subjective usability assessment. All SDS achieve a positive usability assessment. As illustrated in Fig. 17
                            the command-based dialog is significantly better accepted by the user than the conversational dialog (F(1, 23)=10.91, p
                           <0.01, η
                           2
                           =0.32). The comparison of GUI conditions within one dialog strategy did not reveal any significant differences.


                           Correlation of measures. As mentioned before observations during the experiment give reason to assume that the CER and the NoT might have an influence on other dialog quality measures. The CER correlates positively with the NoT (r
                           
                              p
                           
                           =0.65, p
                           <0.001). There is a strong negative relationship between the NoT and the SASSI result of the conversational dialog (r
                           
                              p
                           
                           =−0.53, p
                           <0.01). Furthermore, there is a strong positive relationship between the NoT and the DD (r
                           
                              p
                           
                           =0.736, p
                           <0.001).

This section presents the results of the objective driving performance (MDev, RT), the subjective workload assessment (DALI, final questionnaire), and visual distraction measures (MGD, NoG, PDT, TGT, SGD).


                           Mean deviation, response time. The results of the MDev were similar to the RT results. Figs. 18 and 19
                           
                            show the MDev and the RT of the baseline drive and the GUI conditions. No significant differences were found when comparing the two speech dialog strategies which is why the results are not presented in these Figures. The MDev of the baseline drive was significantly smaller than when using an SDS prototype while driving (F(1, 24)=15.08, p
                           <0.01, η
                           2
                           =0.39). The sub-analyses revealed a significant difference between CommGUI and CommNoGUI (F(1, 24)=14.38, p
                           <0.001, η
                           2
                           =0.38) and also between ConvGUI and ConvNoGUI (F(1, 24)=30.33;
                           p
                           <0.001;
                           η
                           2
                           =0.56). The RT was significantly smaller when the participants were not using an SDS prototype while driving (F(1, 24)=25.41, p
                           <0.001, η
                           2
                           =0.51). Sub-analyses revealed a significant difference between CommGUI and CommNoGUI (F(1, 24)=4.60, p
                           <0.05, η
                           2
                           =0.16) and also between ConvGUI and ConvNoGUI (F(1, 24)=5.14, p
                           <0.05, η
                           2
                           =0.18).


                           Subjective workload assessment. Fig. 20
                            shows the DALI overall result of the baseline drive and the GUI conditions. When driving without using any of the four SDS prototypes the driver workload was perceived to be significantly lower (F(1, 24)=111.39, p
                           <0.001, η
                           2
                           =0.82). The driver workload for the different SDS prototype was generally perceived as neutral. No significant differences were found, neither between the speech dialog strategies, nor between the GUI conditions.


                           Percent dwell time, number of glances, mean glance duration. As the results of the MGD, the NoG and the PDT are similar and redundant only the results of the PDT are presented. No significant differences between the two speech dialog strategies were revealed. In Fig. 21
                           , the PDT for each GUI condition is presented.
                              10
                           
                           
                              10
                              Concerning the conditions without GUI the reader might wonder why people look on the screen although no content is displayed. However, the GUI of these conditions still presents a visual feedback in the top bar of the screen, which indicates if the user is allowed to talk. If participants were not sure if the speech dialog is still active they glanced at the screen.
                            The ratio of glances on the screen was significantly higher when the command-based dialog with GUI was used compared to the command-based dialog without GUI (F(1, 24)=50.98, p
                           <0.001;
                           η
                           2
                           =0.68). The same applies for the conversational dialog and its GUI conditions (F(1, 24)=64.98, p
                           <0.001;
                           η
                           2
                           =0.73).


                           Total glance time, single glance duration. In the course of only few hotel booking tasks the sum of participant's glances on the screen exceeded the TGT limit of 20s: three tasks when using the command-based dialog with GUI and two tasks when using the conversational dialog with GUI. For each SDS prototype the 85th percentile of the distribution of the TGT is below twenty seconds (see Table 3
                           ).

Concerning single glance durations in the experiment only few glances exceeded the maximum glance duration limit of two seconds: one glance when using the command-based dialog with GUI and one glance when using the conversational dialog with GUI. As can be obtained from Table 3, for each SDS prototype the 85th percentile of the distribution of the SGD is below two seconds.


                           Correlations of measures. The CER and the NoT correlate with the MGD (CER
                           :
                           r
                           
                              p
                           
                           =0.62, p
                           <0.01, NoT
                           :
                           r
                           
                              p
                           
                           =0.54, p
                           <0.01).

This section first discusses the results of the speech dialog measures, followed by the discussion of the driver distraction measures. It is important to note that the results can only be applied for dialogs, which stay within one application. Dialogs, which switch across applications have not been investigated.

The results show that the participants were able to successfully finish the tasks with each prototype. However, the flexibility and variety of conversational language poses great challenges to the speech recognition and the natural understanding module of an SDS. The understanding of conversational language is more difficult than understanding simple commands. Therefore, the CER of the conversational dialog is higher than the CER of the command-based dialog. The CER of 4.0% of the command-based dialog is an adequate result. 9.6% of CER when using the conversational dialog still leaves room for improvements. The CER results indicate that a grammar-based ASR language model does not satisfy the requirements of conversational speech. Using statistical language models instead might help to reduce the number of understanding errors.

We expected a lower NoT and a shorter DD when using the conversational dialog compared to the command-based dialog due to the multiple parameter input capabilities of the conversational dialog. However, the higher CER led to additional correction dialogs the user had to go through if an error occurred. The positive correlation between the CER and the NoT and the NoT and the DD confirms this explanation. The DD of the command-based dialog with GUI was significantly shorter than without GUI. When the SDS was supported by a GUI the users were able to gather information quicker from the screen and used the barge-in function to speed up the dialog. Similar behavior was observed during the experiment regarding the conversational dialogs. However, these results could not be confirmed.

The command-based dialog was better accepted by the user than the conversational dialog. As observed in the experiment correction dialogs frustrated the users. Concerning the conversational dialog, the unexpectedly high NoT might have impaired the SASSI result. The strong negative correlation between the NoT and the SASSI result of the conversational dialog would confirm this hypothesis. We expected participants to accept an SDS supported by a GUI better than without a GUI. This assumption could not be confirmed. It seems that the support of an SDS by a visual feedback does not raise the user acceptance in a driving scenario.

The results of the objective driving performance data and the DALI questionnaire show that performing a secondary task by speech while driving impairs the steering performance and the RT and increases the driver workload. The use of the SDS mentally demands the user and thereby decreases the driving performance. However, previous studies show that voice interaction is still superior in terms of driving performance when compared to manual control (Peissner et al., 2011; Barón and Green, 2006). The results of the MDev, the RT and the DALI questionnaire did not reveal differences between the two dialog strategies although we expected the conversational dialog to distract less. The high CER of the conversational dialog might have increased the driver workload and thereby, degraded the driving performance. However, it turned out that the steering performance and the reaction time are impaired if the SDS is supported by a GUI. The visual feedback on the screen has a negative influence on the primary steering task performance. Furthermore, drivers cannot react on an event appearing in the driving environment if they look on the GUI screen while the stimuli happens. Thus, the RT is increased. Concerning visual distraction, our four SDS prototypes are compliant to the rules the AAM guidelines prescribe. Furthermore, the analysis of the eye tracker data mirrors similar results as the objective driving performance results. The NoG, the MGD, and the PDT did not reveal differences in the comparison of the speech dialog strategies. During the experiment we observed that people looked on the screen in order to get visual help when an error occurred. It seems that the CER has a negative influence on visual distraction. The positive correlation between the CER and the MGD strengthens this assumption. If less understanding errors occurred, the visual demand could maybe be reduced. Especially the visual distraction results of the conversational dialog could be improved as the CER of this dialog strategy leaves more room for improvements. The use of a GUI had significant impact on visual distraction. When information is presented on the screen people tend to glance more frequently and longer at the display to get visual help. Thus, the NoG, the MGD, and the PDT increase. These findings are consistent with previous research (Kun et al., 2009). Concluding one can say that in the driving scenario, the choice of speech dialog strategies does not have a strong influence on usability and driver distraction. The command-based dialog was slightly better accepted than the conversational dialog. However, as no strong differences were revealed, we cannot claim that one of the dialog strategies is more preferred by users. If the CER of the conversational SDS would be lowered, this dialog strategy might outperform the command-based dialog in terms of dialog performance and user acceptance. The current results implicate that in-car SDS developers should take both speaking styles into consideration when designing an SDS for information exchange tasks. Especially, when the user has the initiative (e.g. at the beginning of the dialog) conversational utterances containing several input parameters and commands should be covered. However, when the system starts to guide the user rather short elliptical utterances are used. Therefore, developers should take a hybrid solution into consideration, which covers only one or both strategies dependent on the context. The effects of a hybrid concept on dialog performance and user acceptance and the comparison to the other speech dialog strategies should be investigated in the future.

The comparison of speech dialog strategies did not reveal differences in driver distraction measures. However, as expected, the GUI caused more driver distraction due to the glances onto the screen. Therefore, the conditions without GUI achieved better results in steering performance, reaction time and visual distraction. In the driving scenario, the choice of the speech dialog strategy does not have a strong influence on the driving performance. Instead, the content presentation of the GUI is important. Therefore, when designing in-car SDSs, which are supported by a GUI, developers have to consider reducing the content presented on the screen in order to reduce driver distraction.

As one participant did not return to the lab for the second experiment, session 2 comprises of only 24 participants. In total, 96 dialogs (48 conversational dialogs with avatar and 48 conversational dialogs without avatar) were recorded and transcribed and no dialog data was lost in this experiment. The driving data set comprised of 48 drives, in which the conversational dialog with avatar was applied, 48 drives with the conversational SDS without avatar and additional 48 baseline drives. Due to calibration problems, 4 eye tracker data sets were lost. Again, the missing data were estimated using the expectation maximization method. The subjective and objective data comprised altogether 4872 values including 4,4% of missings. Analyzes were performed conducting a repeated measures ANOVA. Similar to session 1 the effect size η
                        2 is presented in the results to assess the magnitude of the analyzed phenomena (0.2<
                        η
                        2
                        <0.5: small, 0.5<
                        η
                        2
                        <0.8: medium, 0.8<
                        η
                        2: large).

This section presents the results of the objective speech dialog performance measures (TS, NoT, DD, CER) followed by the results of the questionnaires (SASSI, final interview).


                           Task success. Using the SDS prototype without avatar, participants were able to solve 85.4% of the tasks. 83.3% of the tasks could be solved when using the SDS prototype with avatar. There was no significant difference revealed between the two GUI conditions.


                           Dialog duration, number of turns, concept error rate. Regarding the remaining objective usability measures DD, NoT and CER, no significant differences were found.


                           Subjective usability assessment. The SASSI revealed to be marginally significant (p
                           =.08), the avatar condition tending to be less accepted. However, the SASSI part of the final interview showed that the avatar condition is significantly less accepted than the condition without the avatar (F(1, 23)=13.58, p
                           <0.01, η
                           2
                           =0.37), see Fig. 22
                           . Furthermore, the final interview contained questions about the usefulness of the avatar on a scale from 1 (=rejection) to 10 (=acceptance). The results showed that the avatar did not support the speech dialog (MV
                           =1.67, SD
                           =0.9).

Analogically to Session 1 this section presents the results of the objective driving performance (MDev, RT), the subjective workload assessment (DALI, final interview), and visual distraction measures (MGD, NoG, PDT, TGT, SGD).


                           Mean deviation, response time. The results of the MDev and the RT are illustrated in Table 4
                           . The observed MDev revealed that there was significantly more deviation within the two conditions compared to the baseline drives (F(1, 23)=31.22, p
                           <0.001;
                           η
                           2
                           =0.58). No significant differences between the two GUI conditions were revealed.

The analysis revealed that the RT is significantly lower in the baseline drives compared to the GUI conditions (F(1, 23)=21.81, p
                           <0.001;
                           η
                           2
                           =0.49). However, the RT did not significantly differ when comparing the two GUI conditions.


                           Subjective workload assessment. The analysis of the DALI questionnaire, which was asked after each condition, revealed that the driver workload of the baseline drive was perceived to be significantly lower compared to the two GUI conditions (F(1, 23)=80.02, p
                           <0.001;
                           η
                           2
                           =0.78). No significant difference between the two avatar conditions was found. However, the results of the DALI part of the final interview showed that the perceived workload was significantly higher when using the condition with avatar (F(1, 23)=14.56, p
                           <0.01, η
                           2
                           =0.39), as illustrated in Fig. 23
                           . Furthermore, the final interview contained questions about the general subjective driver distraction assessment on a scale from 1 (=rejection) to 10 (=acceptance). The results show that the avatar is perceived as disturbing (MV
                           =7.88, SD
                           =2.1) and distractive (MV
                           =7.17, SD
                           =2.9).


                           Mean glance duration, number of glances, percent dwell time. Regarding the eye tracking measures, no significant differences could be revealed for the two avatar conditions.


                           Total glance time, single glance duration. In the course of only few hotel booking tasks the sum of participant's glances on the screen exceeded the TGT limit of 20s: four tasks when using the conversational dialog without avatar and 6 tasks when using the conversational dialog with avatar. For each SDS prototype the 85th percentile of the distribution of the total glance durations is below twenty seconds (see Table 5
                           ).

Concerning single glance durations in the experiment only few glances exceeded the maximum glance duration limit of two seconds: two glances when using the conversational dialog without avatar and two glances when using the conversational dialog with avatar. As can be obtained from Table 5, for each SDS prototype the 85th percentile of the distribution of the SGD is below two seconds.

This section discusses the results of Session 2, beginning with the usability results, followed by the results of the driver distraction measures. Equivalent to Session 1 the results are only valid for dialogs, which stay within one application.

The results show that the participants were able to successfully finish the tasks. We did not reveal differences concerning the speech dialog performance measures when comparing the condition with avatar with the condition without avatar. It seems that the presence of a virtual character on the screen does not influence people's way of interacting with an SDS by speech. The SASSI questionnaire used after each condition only showed marginally significant differences between the two avatar conditions. The SASSI questionnaire rather focuses on the quality of the speech interaction (e.g. dimensions: accuracy, speed) and how people accept the speech interface (e.g. dimensions: likeability, habitability). The authors admit that it would have been better to employ a less speech-focused questionnaire, such as Attrakdiff by Hassenzahl et al. (2003) or SUMI by Kirakowski (1996) for this comparison. Nevertheless, the result of the SASSI part of the final questionnaire revealed that the SDS with avatar was significantly less accepted than an SDS without avatar. After having experienced both conditions participants did not seem to appreciate the presence of the human-like virtual agent anymore. Contrary to our expectations the driving performance measures and the eye tracker data did not reveal differences in driver distraction. However, in the subjective workload assessment participants indicated that they perceived the presence of the avatar as distractive. Although the presence of the avatar did not impair the driving performance or raise visual distraction significantly the participants felt subjectively more distracted. The animation of the avatar seems to make the people feel attracted to the head unit screen, which is why they ranked this condition as more distractive. Both SDS prototypes are compliant to the rules the AAM guidelines prescribe. Although the avatar was animated and showed mimics and made gestures, the maximum limits were not exceeded.

Concluding one can say that the avatar did not improve the objective speech dialog performance measures and was not accepted by participants. The avatar raised the perceived driver workload but did not negatively affect the objective driver distraction measures. However, this study did not answer the question if people generally do not like avatars in the speech dialog in the driving environment or if they just did not like the selected appearance of the virtual character. Other kinds of virtual characters (male human agent, comic figure, etc.) might be accepted by the users.

@&#CONCLUSIONS@&#

This paper reports a driving simulation study in which different in-car speech-based HMI concepts for performing information exchange tasks were compared. The paper describes the different concepts, which have been designed for a hotel booking by speech, the driving simulation study, and the results of the experiment.

The described HMI concepts are based on different dialog strategies which include speech as main input and output modality. The speech dialog is supported by a GUI which is adapted to the respective speech dialog strategy. The first HMI concept is based on a command-based dialog strategy where the driver is able to start the speech dialog by single commands and is led step-by-step by the system afterwards. The available commands are displayed on the GUI screen. The second dialog strategy, the conversational dialog, allows the driver speaking entire sentences as if he would talk to a human being. Thereby, multiple parameters can be input at once and the dialog initiative may switch frequently. Two different GUI design concepts were targeted to support the conversational dialog. The first concept does not display the commands anymore but uses icons to suggest possible functions of the system to the driver. Based on the first conversational GUI concept, the second concept contributes to a more conversational interaction by displaying additionally a human-like character on the screen. In order to investigate the need for a visual feedback during the speech interaction the two speech dialog strategies were also evaluated without a GUI.

The different speech-based HMI concepts were evaluated in a driving simulator study concerning usability and driver distraction. The results show that only few differences concerning speech dialog quality were found when comparing the speech dialog strategies. The command-based dialog was slightly better accepted than the conversational dialog, which seems to be due to the high CER of the conversational dialog. SDSs without GUI also seem to be feasible for the driving environment and are accepted by the users. The comparison of speech dialog strategies did not reveal differences in driver distraction. However, the use of a GUI impaired the driving performance and increased gaze-based distraction. The presence of an avatar was not appreciated by participants and did not affect the dialog performance. Concerning driver distraction, the virtual agent did not negatively affect the driving performance or increase visual distraction.

The current results implicate that in-car SDS developers should take both speaking styles into consideration when designing an SDS for information exchange tasks. Furthermore, developers have to consider reducing the content presented on the screen in order to reduce driver distraction.

In the future, we will develop a hybrid SDS concept and compare its performance with the SDS prototypes described in this paper. As the results of this research work only allow to make implications for speech dialogs within one application further investigations are needed to examine the most appropriate speech dialog strategy for speech dialogs across applications. Therefore, further experiments on the comparison of speech dialog strategies in the driving environment for cross-application tasks or task switches will be conducted.

@&#ACKNOWLEDGMENTS@&#

The research work described in this paper is performed in the context of the project GetHomeSafe which is conducted within the scope of the Seventh Framework Program of the European Commission (Grant Number 288667). We would like to thank the European Commission for funding the project GetHomeSafe.

@&#REFERENCES@&#

