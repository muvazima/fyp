@&#MAIN-TITLE@&#Estimating translation probabilities for social tag suggestion

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We present a new perspective to tag suggestion and treat it as a translation process.


                        
                        
                           
                           We propose two methods to estimate the translation probabilities.


                        
                        
                           
                           Our methods can solve the problem of vocabulary gap.


                        
                        
                           
                           Our methods are effective and robust compared with other methods.


                        
                        
                           
                           Our methods are relatively simple and efficient, which makes them practical.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Natural language processing

Tag suggestion

Translation model

Word alignment model

Pointwise mutual information

@&#ABSTRACT@&#


               
               
                  The task of social tag suggestion is to recommend tags automatically for a user when he or she wants to annotate an online resource. In this study, we focus on how to make use of the text description of a resource to suggest tags. It is intuitive to select significant words from the text description of a source as the suggested tags. However, since users can arbitrarily annotate any tags to a resource, tag suggestion suffers from the vocabulary gap issue — the appropriate tags of a resource may be statistically insignificant or even do not appear in the corresponding description. In order to solve the vocabulary gap issue, in this paper we present a new perspective on social tag suggestion. By considering both a description and tags as summaries of a given resource composed in two languages, tag suggestion can be regarded as a translation from description to tags. We propose two methods to estimate the translation probabilities between words in descriptions and tags. Based on the translation probabilities between words and tags estimated for a large collection of description-tags pairs, we can suggest tags according to the words in a resource description. Experiments on real-world datasets indicate that our methods outperform other methods in precision, recall and F-measure. Moreover, our methods are relatively simple and efficient, which makes them practical for Web applications.
               
            

@&#INTRODUCTION@&#

In Web 2.0, Web users often use tags to collect and share online resources such as Web pages, photos, videos, movies and books. As an example, we consider a social tagging system for books. Table 1
                      presents a book entry annotated with several tags by multiple users.
                        1
                        The original record was obtained from the book review website Douban (www.douban.com) in Chinese. Here we translate it into English for comprehension.
                     
                     
                        1
                      On the top of Table 1 we list the title and a short introduction of the book “The Count of Monte Cristo”. The bottom of Table 1 shows the annotated tags, each of which is followed by a number in brackets, which is the total number of users who used the tag to annotate the book. As the tags of online resources are annotated collaboratively by multiple users, we also refer to these tags as social tags. For a resource, we refer to the additional information, such as the title and the introduction of a book, as description, and the user-annotated social tags as annotation.

The task of social tag suggestion is to automatically recommend tags for a user when he or she wants to annotate a resource. Social tag suggestion, as a crucial component for social tagging systems, can help users annotate resources. Moreover, social tag suggestion is usually considered as an equivalent problem to modeling social tagging behaviors, which is playing an increasingly important role in social computing and information retrieval.

Most online resources have descriptions, usually containing abundant information about resources (Liu, Chen, & Sun, 2011). For example, on a book review website, each book entry contains a title, the author(s) and an introduction of the book. Thus, a number of researchers (Liu et al., 2011; Katakis, Tsoumakas, & Vlahavas, 2008; Mishne, 2006; Xu, Fu, Mao, & Su, 2006) propose to automatically suggest tags based on resource descriptions, which is collectively known as the content-based approach (Xu et al., 2006). In this study, we focus on how to make use of the text description of a resource to suggest tags. Note that besides descriptions, online resources may also have multimedia data (e.g., images, videos and audio files) and a survey of multimedia tagging can be found in Wang, Ni, Hua, and Chua (2012).

One may think to suggest tags by selecting important words from descriptions. This approach is far from sufficient because descriptions and annotations use diverse vocabularies, which is typically referred to as the vocabulary gap problem (Liu et al., 2011). The vocabulary gap is usually reflected in two primary issues:
                        
                           1.
                           A portion of tags in the annotation do appear in the corresponding description, but they may not be statistically significant.

A portion of tags may even not appear in the description.

Taking the book entry in Table 1 as example, the tag “classic” had been annotated by 1062 users but it did not appear in the description; another appropriate tag “famous book” also did not appear in the description.

Many approaches have been proposed to reduce the vocabulary gap and find the semantic correspondence between descriptions and annotations. Several researchers regard social tag suggestion as a classification problem by considering each tag as a category label (Fujimura, Fujimura, & Okuda, 2008; Heymann, Ramage, & Garcia-Molina, 2008; Katakis et al., 2008; Lee & Chun, 2007; Mishne, 2006; Ohkura, Kiyota, & Nakagawa, 2006). Various classifiers such as Naive Bayes, kNN and SVM have been explored and words are used as features. Some researchers propose to use the topic information between words and tags to suggest tags (Iwata, Yamada, & Ueda, 2009; Si, Liu, & Sun, 2010).

In this paper, we propose a new perspective on social tag suggestion to solve the vocabulary gap problem. By regarding both the description and the annotation as parallel summaries of a resource, we want to build a translation model to estimate the translation probabilities between the words in descriptions and tags in annotations. The translation probabilities are able to capture the semantic relation between words and tags. After obtaining the translation probabilities, the tagging behavior associated with a resource can then be regarded as a word translation process:
                        
                           1.
                           A user reads the resource description and understands its substance according to the important words in the description.

Triggered by the important words in the description, the user translate these words into corresponding tags and annotate the resource with these tags.

In Fig. 1
                     , we provide a simple example to demonstrate the basic idea of using word translation for tag suggestion. In this figure, some words in the first sentence of book description are translated to tags in the annotation. The translation is denoted with various arrows from words or phrases in the description to tags in the annotation. For example, the phrase Count of Monte Cristo in the description is translated to two tags, including Dumas and Count of Monte Cristo, and the word fictions is translated to novel.

In this paper, we propose two methods to estimate translation probabilities between words in descriptions and tags in annotations. One method is the word alignment model (WAM) in statistical machine translation (SMT) and the other method is mutual information (MI) (Lin, 1998). It is straightforward to use WAM since it is the basic model in SMT to estimate the translation probabilities. For training, WAM requires a collection of parallel documents, where each document pair should have the comparable length. In this paper we propose a sampling method to prepare length-balanced description–annotation pairs for WAM. Moreover, we propose the second method, MI, to estimate the translation probabilities. Mutual information is a popular measure that can utilize co-occurrence information to measure semantic similarities between two words (Lin, 1998).

Our model can solve the vocabulary gap problem because the translation probabilities estimated by WAM and MI are able to capture the semantic relation between words and tags. Thus we can suggest tags that are not statistically significant or even not appear in the descriptions based on the translation probabilities. We hypothesize that our approach is better than the methods mentioned above and conduct experiments to investigate the performance of our model in the task of tag suggestion. Experiments on real-world datasets indicate that our method outperforms other methods in precision, recall and F-measure. Moreover, our method is relatively simple and efficient, as proven with the computational complexity, which makes it practical for Web applications.

The remainder of this paper is organized as follows: In Section 2 we briefly introduce some of the most commonly used methods for tag suggestion. Sections 3 and 4 introduce the details of our approach. Section 5 presents the experimental evaluation of our approach compared to other existing techniques. Finally Section 6 concludes the paper.

@&#RELATED WORK@&#

Many researchers have built social tag suggestion systems based on collaborative filtering (CF) (Herlocker, Konstan, Borchers, & Riedl, 1999; Herlocker, Konstan, Terveen, & Riedl, 2004). CF is a widely used technique in recommender systems (Resnick & Varian, 1997). The collaboration-based methods typically base their suggestions on the tagging history of the given resource and user, without considering resource description. Matrix Factorization (Rendle, Balby Marinho, Nanopoulos, & Schmidt-Thieme, 2009) and FolkRank (Jaschke, Marinho, Hotho, Schmidt-Thieme, & Stumme, 2008) are representative CF methods for social tag suggestion. Most of these methods suffer from the cold-start problem (Lam, Vu, Le, & Duong, 2008), i.e., they are not able to provide effective suggestions for resources that no one has annotated yet. The content-based approach for social tag suggestion ameliorates the cold-start problem of the collaboration-based approach by suggesting tags according to resource descriptions. Therefore, the content-based approach plays an important role in social tag suggestion, especially for new resources and new tagging systems without tagging history.

Several researchers regard social tag suggestion as a classification problem by considering each tag as a category label (Fujimura et al., 2008; Heymann et al., 2008; Katakis et al., 2008; Lee & Chun, 2007; Mishne, 2006; Ohkura et al., 2006). Various classifiers such as Naive Bayes, kNN, SVM and neural networks have been explored to solve the social tag suggestion problem.

There are two issues emerging from the classification-based methods: (1) the annotations provided by users are noisy, and the classification-based methods cannot handle the issue well (Liu et al., 2011); (2) the training cost and classification cost of many classification-based methods are usually proportional to the number of classification labels (Si et al., 2010). Thus, these methods may be inefficient for a real-world social tagging system, where hundreds of thousands of unique tags should be considered as classification labels.

Inspired by the emerging popularity of latent topic models such as Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003), various methods have been proposed to model tags using generative latent topic models. One intuitive approach is assuming that both tags and words are generated from the same set of latent topics. By representing both tags and descriptions as the distributions of latent topics, this approach suggests tags according to their likelihood given the description (Krestel, Fankhauser, & Nejdl, 2009; Si & Sun, 2009). Bundschus et al. (2009) proposed a joint latent topic model of users, words and tags. Iwata et al. (2009) proposed an LDA-based topic model, Content Relevance Model (CRM), which aims to identify the content-related tags for suggestion. Empirical experiments revealed that CRM outperformed both classification methods and Corr-LDA (Blei & Jordan, 2003), a generative topic model for contents and annotations.

Most latent topic models have to pre-specify the number of topics before training. We can either use cross-validation to determine the optimal number of topics or employ the infinite models for automatically adjusting the number of topics during training. Both of the solutions are usually computationally complicated. More importantly, topic-based methods suggest tags by measuring the topical relevance of tags and resource descriptions. The latent topics are at the concept level (Liu, Huang, Zheng, & Sun, 2010), which are usually too coarse-grained to precisely suggest fine-grained tags such as named entities, e.g., the tags “Dumas” and “Count of Monte Cristo” in Table 1. To remedy the problem, Si et al. (2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags.

Our model is also a content-based approach so we compare some of the content-based approach mentioned above (kNN, Naive Bayes, CRM and TAM) to investigate the performance of our approach.

Given a resource description, the ranking score of a tag can be calculated from two probabilities: (1) the translation probabilities between words and tags, (2) the probabilities of a word given the description. In Section 3, we will present how to use two different methods, word alignment model and mutual information, to estimate the translation probabilities between words in description and tags in annotations. We will introduce how to calculate the probabilities of a word given the description and perform tag suggestion in Section 4.

First we give formal definitions to description and tags. In this paper, the description of a resource is the textual information of the resource, including the title and the short introduction. The description can be treated as a bag of words. Here we do not use stemming techniques or lemmatization techniques to preprocess the description. We just removed the stop-words in the description. Tags of a resource are labels with count information to describe the resource.

Before introducing the methods in details, we introduce the notation. A resource is denoted as 
                        
                           r
                           ∈
                           R
                        
                     , where R is the set of resources. Each resource in the training set contains a description and an annotation containing a set of tags. The description 
                        
                           
                              
                                 d
                              
                              
                                 r
                              
                           
                        
                      of resource r can be regarded as a bag of words 
                        
                           
                              
                                 w
                              
                              
                                 r
                              
                           
                           =
                           
                              
                                 {
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       c
                                    
                                    
                                       
                                          
                                             w
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                                 )
                                 }
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 
                                    
                                       N
                                    
                                    
                                       r
                                    
                                 
                              
                           
                        
                     , where 
                        
                           
                              
                                 c
                              
                              
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                              
                           
                        
                      is the count of word 
                        
                           
                              
                                 w
                              
                              
                                 i
                              
                           
                        
                     , and 
                        
                           
                              
                                 N
                              
                              
                                 r
                              
                           
                        
                      is the number of unique words in r. The annotated tags 
                        
                           
                              
                                 a
                              
                              
                                 r
                              
                           
                        
                      of the resource r is represented as 
                        
                           
                              
                                 t
                              
                              
                                 r
                              
                           
                           =
                           
                              
                                 {
                                 (
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       c
                                    
                                    
                                       
                                          
                                             t
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                                 )
                                 }
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 
                                    
                                       M
                                    
                                    
                                       r
                                    
                                 
                              
                           
                        
                     , where 
                        
                           
                              
                                 c
                              
                              
                                 
                                    
                                       t
                                    
                                    
                                       i
                                    
                                 
                              
                           
                        
                      is the count of the tag 
                        
                           
                              
                                 t
                              
                              
                                 i
                              
                           
                        
                     , and 
                        
                           
                              
                                 M
                              
                              
                                 r
                              
                           
                        
                      is the number of unique tags for r.

WAM, as a traditional machine translation method, requires a parallel training dataset consisting of a number of aligned sentence pairs. We assume the description and the annotation of a resource are written in two distinct languages. Thus, we prepare our parallel training dataset by pairing descriptions and annotations. Accordingly, the WAM-based approach contains two steps. First, given a collection of annotated resources, we prepare description–annotation pairs for using the word alignment model. Second, given a collection of description–annotation pairs, we adopt IBM Model-1, a widely used word alignment model, to learn the translation probabilities between words in descriptions and tags in annotations. We will introduce the two steps separately.

In a typical tag suggestion system, the length of a resource description is usually limited to hundreds of words. In addition, it is common for some popular resources to be annotated by multiple users with thousands of tags. For example, the tag Dumas is annotated by 2,748 users for the book in Table 1. In another extreme, a resource may be annotated with only several tags. We have to address the length-unbalance between a resource description and its corresponding annotation for two reasons. (1) When the number of annotated tags is large, it is impossible to list all annotated tags on the annotation side of a description–annotation pair. The performance of word alignment models will also suffer from the unbalanced length of aligned pairs in the parallel training data set (Och & Ney, 2003). (2) Moreover, the annotated tags may have different importance for the resource. It would be unfair to treat these tags without distinction.

In this study, we propose a sampling method to prepare length-balanced description–annotation pairs for word alignment. The basic idea is to sample a bag of tags from the annotation according to tag weights and make the generated bag of tags have comparable length with the words in description. For example, the length of the description in Table 1 is 54 and the number of unique tags is 21. If we list 54 words in one side and 21 tags in another side, we will get a sentence pair with unbalanced length. Thus we propose to sample a bag of tags with comparable length with the words, for example, 54 tags.

We consider two parameters when sampling tags. First, we have to select a tag weighting type for sampling. In this paper, we investigate two straightforward weighting types, including tag frequency (TF
                              t
                           ) within the annotation, which considers the local importance, and tag-frequency inverse-document-frequency (TF-IDF
                              t
                           ), which also considers global specification (IDF
                              w
                           ) besides TF
                              t
                           . Given a resource r, TF
                              t
                            and TF-IRF
                              t
                            of the tag t are defined as
                              
                                 (1)
                                 
                                    
                                       
                                          TF
                                       
                                       
                                          t
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                c
                                             
                                             
                                                t
                                             
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                t
                                             
                                          
                                          
                                             
                                                c
                                             
                                             
                                                t
                                             
                                          
                                       
                                    
                                    ,
                                    
                                    TF-
                                    
                                       
                                          IDF
                                       
                                       
                                          t
                                       
                                    
                                    =
                                    
                                       
                                          
                                             
                                                c
                                             
                                             
                                                t
                                             
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                t
                                             
                                          
                                          
                                             
                                                c
                                             
                                             
                                                t
                                             
                                          
                                       
                                    
                                    ×
                                    log
                                    
                                       
                                          
                                             
                                                
                                                   |
                                                   R
                                                   |
                                                
                                                
                                                   |
                                                   {
                                                   r
                                                   ∈
                                                   R
                                                   :
                                                   
                                                      
                                                         c
                                                      
                                                      
                                                         t
                                                      
                                                   
                                                   >
                                                   0
                                                   }
                                                   |
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           where 
                              
                                 |
                                 r
                                 ∈
                                 R
                                 :
                                 
                                    
                                       c
                                    
                                    
                                       t
                                    
                                 
                                 >
                                 0
                                 |
                              
                            indicates the number of resources that have been annotated with the tag t.

Another parameter is the length ratio between the description and the sampled annotation. We denote the ratio as 
                              
                                 δ
                                 =
                                 
                                    
                                       |
                                       
                                          
                                             w
                                          
                                          
                                             r
                                          
                                       
                                       |
                                    
                                    
                                       |
                                       
                                          
                                             t
                                          
                                          
                                             r
                                          
                                       
                                       |
                                    
                                 
                              
                           , where 
                              
                                 |
                                 
                                    
                                       w
                                    
                                    
                                       r
                                    
                                 
                                 |
                              
                            is the number of words in the description and 
                              
                                 |
                                 
                                    
                                       t
                                    
                                    
                                       r
                                    
                                 
                                 |
                              
                            is the number of tags in the annotation. Still take the book in Table 1 for example, if the length of the description is 54 and the length ratio is 
                              
                                 10
                                 /
                                 5
                              
                           , then we will sample 27 tags from the annotations.

After preparing aligned description–annotation pairs for WAM, next we will choose an appropriate WAM model to obtain the translation probabilities between words in description and tags in annotations. Note that the annotated tags (
                              
                                 
                                    
                                       t
                                    
                                    
                                       r
                                    
                                 
                              
                           ) form a bag of labels with no position information, thus, we select IBM Model-1 (Brown, Pietra, Pietra, & Mercer, 1993) for training, which does not take word position information into account on both sides for each aligned pair.

Suppose the source language is the description, and the target language is the annotation. We use word alignment models to learn the translation probabilities between words in descriptions and labels in annotations. In IBM Model-1, the relationship of the source language 
                              
                                 w
                                 =
                                 
                                    
                                       w
                                    
                                    
                                       1
                                    
                                    
                                       J
                                    
                                 
                              
                            and the target language 
                              
                                 t
                                 =
                                 
                                    
                                       t
                                    
                                    
                                       1
                                    
                                    
                                       I
                                    
                                 
                              
                            is connected via a hidden variable describing an alignment mapping from source position j to target position 
                              
                                 
                                    
                                       a
                                    
                                    
                                       j
                                    
                                 
                              
                           : 
                              
                                 (2)
                                 
                                    Pr
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   1
                                                
                                                
                                                   J
                                                
                                             
                                             |
                                             
                                                
                                                   t
                                                
                                                
                                                   1
                                                
                                                
                                                   I
                                                
                                             
                                          
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   a
                                                
                                                
                                                   1
                                                
                                                
                                                   J
                                                
                                             
                                          
                                       
                                    
                                    Pr
                                    
                                       
                                          
                                             
                                                
                                                   w
                                                
                                                
                                                   1
                                                
                                                
                                                   J
                                                
                                             
                                             ,
                                             
                                                
                                                   a
                                                
                                                
                                                   1
                                                
                                                
                                                   J
                                                
                                             
                                             |
                                             
                                                
                                                   t
                                                
                                                
                                                   1
                                                
                                                
                                                   I
                                                
                                             
                                          
                                       
                                    
                                    .
                                 
                              
                           The alignment 
                              
                                 
                                    
                                       a
                                    
                                    
                                       1
                                    
                                    
                                       J
                                    
                                 
                              
                            also contains empty-word alignments 
                              
                                 
                                    
                                       a
                                    
                                    
                                       j
                                    
                                 
                                 =
                                 0
                              
                            which align source words to the empty word. IBM Model-1 can be trained using an Expectation–Maximization (EM) algorithm in an unsupervised fashion, and obtains the translation probabilities of two vocabularies, i.e., 
                              
                                 Pr
                                 (
                                 w
                                 |
                                 t
                                 )
                              
                           , where t is a tag and w is a word.

IBM Model-1 only produces one-to-many alignments from source language to target language. The learned model is thus asymmetric, i.e., the model learned from description–annotation pairs is different from the model learned from annotation–description pairs. So we establish learned translation models in two directions: one regards description as the source language and annotations as the target language, and the other is in the reverse direction of the pairs. We denote the first model as 
                              
                                 
                                    
                                       Pr
                                    
                                    
                                       d
                                       2
                                       a
                                    
                                 
                              
                            and the latter as 
                              
                                 
                                    
                                       Pr
                                    
                                    
                                       a
                                       2
                                       d
                                    
                                 
                              
                           . We further define 
                              
                                 Pr
                                 (
                                 t
                                 |
                                 w
                                 )
                              
                            as the harmonic mean of the two models:
                              
                                 (3)
                                 
                                    Pr
                                    (
                                    t
                                    |
                                    w
                                    )
                                    ∝
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      λ
                                                   
                                                   
                                                      
                                                         
                                                            Pr
                                                         
                                                         
                                                            d
                                                            2
                                                            a
                                                         
                                                      
                                                      (
                                                      t
                                                      |
                                                      w
                                                      )
                                                   
                                                
                                                +
                                                
                                                   
                                                      1
                                                      -
                                                      λ
                                                   
                                                   
                                                      
                                                         
                                                            Pr
                                                         
                                                         
                                                            a
                                                            2
                                                            d
                                                         
                                                      
                                                      (
                                                      t
                                                      |
                                                      w
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                       
                                          -
                                          1
                                       
                                    
                                    ,
                                 
                              
                           where λ is the harmonic factor for combining the two models. When 
                              
                                 λ
                                 =
                                 1
                              
                            or 
                              
                                 λ
                                 =
                                 0
                              
                           , it simply uses model 
                              
                                 
                                    
                                       Pr
                                    
                                    
                                       d
                                       2
                                       a
                                    
                                 
                              
                            or 
                              
                                 
                                    
                                       Pr
                                    
                                    
                                       a
                                       2
                                       d
                                    
                                 
                              
                            correspondingly.

Finally we get the translation probabilities 
                              
                                 Pr
                                 (
                                 t
                                 |
                                 w
                                 )
                              
                            using the WAM model, which can be regarded as the sematic relatedness between words and tags.

From Section 3.1.1, we can see that WAM has to use a sampling technique to prepare description–annotation pairs. Unlike WAM, MI only needs the co-occurrence information between words in description and tags in annotations, which does not require sampling. We obtain translation probabilities using MI as follows.

First, for each pair of a word w in the descriptions and a tag t in the annotations, we compute their mutual information score. Informally, mutual information divides the probability of observing w and t together in the same resource by the probabilities of observing w and t independently. The mutual information between a word w and a tag t is calculated as follows:
                           
                              (4)
                              
                                 I
                                 (
                                 w
                                 ;
                                 t
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                X
                                             
                                             
                                                w
                                             
                                          
                                          =
                                          0
                                          ,
                                          1
                                       
                                    
                                 
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          
                                             
                                                X
                                             
                                             
                                                t
                                             
                                          
                                          =
                                          0
                                          ,
                                          1
                                       
                                    
                                 
                                 p
                                 (
                                 
                                    
                                       X
                                    
                                    
                                       w
                                    
                                 
                                 ,
                                 
                                    
                                       X
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 ·
                                 log
                                 
                                    
                                       Pr
                                       (
                                       
                                          
                                             X
                                          
                                          
                                             w
                                          
                                       
                                       ,
                                       
                                          
                                             X
                                          
                                          
                                             t
                                          
                                       
                                       )
                                    
                                    
                                       Pr
                                       (
                                       
                                          
                                             X
                                          
                                          
                                             w
                                          
                                       
                                       )
                                       Pr
                                       (
                                       
                                          
                                             X
                                          
                                          
                                             t
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    X
                                 
                                 
                                    w
                                 
                              
                           
                         and 
                           
                              
                                 
                                    X
                                 
                                 
                                    t
                                 
                              
                           
                         are binary variables indicating whether w or t is present or absent, respectively. The estimation of the probabilities 
                           
                              Pr
                              (
                              
                                 
                                    X
                                 
                                 
                                    w
                                 
                              
                              )
                              ,
                              Pr
                              (
                              
                                 
                                    X
                                 
                                 
                                    t
                                 
                              
                              )
                           
                         and 
                           
                              Pr
                              (
                              
                                 
                                    X
                                 
                                 
                                    w
                                 
                              
                              ,
                              
                                 
                                    X
                                 
                                 
                                    t
                                 
                              
                              )
                           
                         can be found in Karimzadehgan and Zhai (2010).

For a word w, we set a tag co-occurrence threshold γ and will remove the mutual information between word w and tag t if 
                           
                              c
                              (
                              
                                 
                                    X
                                 
                                 
                                    w
                                 
                              
                              =
                              1
                              ,
                              
                                 
                                    X
                                 
                                 
                                    t
                                 
                              
                              =
                              1
                              )
                              ⩽
                              γ
                           
                        , where 
                           
                              c
                              (
                              
                                 
                                    X
                                 
                                 
                                    w
                                 
                              
                              =
                              1
                              ,
                              
                                 
                                    X
                                 
                                 
                                    t
                                 
                              
                              =
                              1
                              )
                           
                         is the number of resources that contain both w and t. We set the co-occurrence threshold γ for two reasons. (1) We are usually less confident with the translation probabilities estimated using infrequent word-tag pairs, which are usually noisy and unimportant. With the threshold, we can filter out a lot of noisy information. (2) Moreover, we can largely reduce the computation cost of estimating mutual information scores. Of course, when 
                           
                              γ
                              =
                              0
                           
                        , we will not remove any word-tag pairs.

Thus, we normalize the mutual information score to obtain a translation probability 
                           
                              Pr
                              (
                              t
                              |
                              w
                              )
                           
                         between a word u and a tag t:
                           
                              (5)
                              
                                 Pr
                                 (
                                 t
                                 |
                                 w
                                 )
                                 =
                                 
                                    
                                       I
                                       (
                                       w
                                       ;
                                       t
                                       )
                                    
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             
                                                
                                                   t
                                                
                                                
                                                   ′
                                                
                                             
                                          
                                       
                                       I
                                       (
                                       w
                                       ;
                                       
                                          
                                             t
                                          
                                          
                                             ′
                                          
                                       
                                       )
                                    
                                 
                              
                           
                        Intuitively, the probability is higher if the word u and the tag t are more likely to co-occur. For example, we get the probabilities 
                           
                              Pr
                              {
                              revenge
                              |
                              revenge
                              }
                              =
                              0.127
                           
                         and 
                           
                              Pr
                              {
                              martialarts
                              |
                              revenge
                              }
                              =
                              0.088
                           
                         from MI model, standing for the word “revenge” is more likely to co-occur with the tag “revenge” than the “martial arts”.

As a word may not always appear as a tag in annotation, the approaches described in Section 3.1 and Section 3.2 may under-estimate the self-translation probabilities, i.e., it is possible that 
                           
                              Pr
                              (
                              t
                              
                              ≠
                              
                              w
                              |
                              w
                              )
                              >
                              Pr
                              (
                              t
                              =
                              w
                              |
                              w
                              )
                           
                        . Here we propose to emphasize the self-translation probability for two reasons. (1) Under estimation of self-translation probabilities may lead to a situation where a proper tag t that also appears frequently in the description may receive a less recommendation score (i.e., 
                           
                              Pr
                              (
                              t
                              |
                              w
                              =
                              t
                              )
                           
                        ) compared to other tags 
                           
                              
                                 
                                    t
                                 
                                 
                                    ′
                                 
                              
                           
                         (i.e., 
                           
                              Pr
                              (
                              
                                 
                                    t
                                 
                                 
                                    ′
                                 
                              
                              |
                              w
                              =
                              t
                              )
                           
                        ). (2) In some real tag suggest systems, the tags that appear in the resource description are more likely to be selected by users for annotation. Hence, we introduce a parameter α to emphasize self-translation probabilities. This idea can be applied to adjust the translation probabilities from any translation models.
                           
                              (6)
                              
                                 Pr
                                 (
                                 t
                                 |
                                 w
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   α
                                                   +
                                                   (
                                                   1
                                                   -
                                                   α
                                                   )
                                                   Pr
                                                   (
                                                   t
                                                   =
                                                   w
                                                   |
                                                   w
                                                   )
                                                
                                                
                                                   t
                                                   =
                                                   w
                                                
                                             
                                             
                                                
                                                   (
                                                   1
                                                   -
                                                   α
                                                   )
                                                   Pr
                                                   (
                                                   t
                                                   |
                                                   w
                                                   )
                                                
                                                
                                                   t
                                                   
                                                   ≠
                                                   
                                                   
                                                   w
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        When 
                           
                              α
                              =
                              1.0
                           
                        , the method will suggest tags simply according to their importance scores in the description, whereas when 
                           
                              α
                              =
                              0
                           
                        , it will not emphasize the tags that appear in the description and suggest tags only according to translation probabilities. For example, if 
                           
                              α
                              =
                              0.5
                           
                         and 
                           
                              Pr
                              {
                              revenge
                              |
                              revenge
                              }
                              =
                              0.127
                           
                        , then we will get a emphasized probability 
                           
                              Pr
                              {
                              revenge
                              |
                              revenge
                              }
                              =
                              0.5635
                           
                        .

After estimating translation probabilities between words and tags 
                        
                           Pr
                           (
                           t
                           |
                           w
                           )
                        
                      in Section 3, we will show how to suggest tags in this section.

Given a resource description 
                        
                           
                              
                                 d
                              
                              
                                 r
                              
                           
                        
                     , our model for tag suggestion is a 3-step process:
                        
                           1.
                           Measure the importance score 
                                 
                                    Pr
                                    (
                                    w
                                    |
                                    
                                       
                                          d
                                       
                                       
                                          r
                                       
                                    
                                    )
                                 
                               of each word w in description 
                                 
                                    
                                       
                                          d
                                       
                                       
                                          r
                                       
                                    
                                 
                              .

Compute the ranking score of tag t by
                                 
                                    (7)
                                    
                                       Pr
                                       (
                                       t
                                       |
                                       
                                          
                                             d
                                          
                                          
                                             r
                                          
                                       
                                       =
                                       
                                          
                                             w
                                          
                                          
                                             r
                                          
                                       
                                       )
                                       =
                                       
                                          
                                             
                                                ∑
                                             
                                             
                                                w
                                                ∈
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      r
                                                   
                                                
                                             
                                          
                                       
                                       Pr
                                       (
                                       t
                                       |
                                       w
                                       )
                                       Pr
                                       (
                                       w
                                       |
                                       
                                          
                                             d
                                          
                                          
                                             r
                                          
                                       
                                       )
                                    
                                 
                              
                           

According to 
                                 
                                    Pr
                                    (
                                    t
                                    |
                                    
                                       
                                          d
                                       
                                       
                                          r
                                       
                                    
                                    )
                                 
                              , suggest top-ranked tags to users.

Since we get 
                        
                           Pr
                           (
                           t
                           |
                           w
                           )
                        
                      in Section 3, we focus on how to measure 
                        
                           Pr
                           (
                           w
                           |
                           
                              
                                 d
                              
                              
                                 r
                              
                           
                           )
                        
                      in this section. Here we investigate three methods to compute the importance score of a word in a resource description: TF-IDF
                        w
                     , TextRank and their product. TF-IDF
                        w
                      and TextRank are the two most widely adopted methods to weight words in a document.

Similar to TF-IDF
                        t
                      mentioned in Section 3.1.1, TF-IDF
                        w
                      (Salton & Buckley, 1988) considers both the local importance (TF
                        w
                     ) and global specification (IDF
                        w
                     ). TextRank (Mihalcea & Tarau, 2004) is proposed to compute term importance using graph-based algorithms such as PageRank (Page, Brin, Motwani, & Winograd, 1998), which considers the semantic relations between terms as a term graph of the given resource description. We also use the product of TF-IDF
                        w
                      and TextRank to weight terms, which potentially takes both global information and term relations into account.

Finally we get the ranking of tags according to Eq. (7). Because WAM and MI can estimate the translation probabilities between words and tags, which can be regarded as the semantic relatedness between words and tags, we hypothesize that our methods are better than the baseline methods mentioned in Section 2 (kNN, Naive Bayes, CRM and TAM). In next section we run experiments to validate this hypothesis.

@&#EXPERIMENTS@&#


                        Datasets In our experiments, we select two real-world datasets of social tagging systems that have diverse properties to evaluate our methods. In Table 2
                         we show the detailed statistical information of the two datasets.

The first dataset, denoted as BOOK, was obtained from a popular Chinese book review website, www.douban.com, which contains the descriptions of books and the tags collaboratively annotated by users.

The second dataset, denoted as BIBTEX, was obtained from an English online bibliography website, www.bibsonomy.org.
                           2
                           The dataset can be obtained from http://www.kde.cs.uni-kassel.de/bibsonomy/dumps.
                        
                        
                           2
                         The dataset contains the descriptions for academic papers and the tags annotated by users.

As shown in Table 2, the average length of descriptions in the BIBTEX dataset is much shorter than the BOOK dataset. Moreover, the BIBTEX dataset does not provide how many times each label is used in a resource annotation.

We use precision that measures exactness, recall that measures completeness and F-measure, which is the harmonic mean of precision and recall, to evaluate the performance of tag suggestion methods. For a resource, we denote the original tags (gold standard) as 
                              
                                 
                                    
                                       T
                                    
                                    
                                       a
                                    
                                 
                              
                           , the suggested tags as 
                              
                                 
                                    
                                       T
                                    
                                    
                                       s
                                    
                                 
                              
                           , and the correctly suggested tags as 
                              
                                 
                                    
                                       T
                                    
                                    
                                       s
                                    
                                 
                                 ∩
                                 
                                    
                                       T
                                    
                                    
                                       a
                                    
                                 
                              
                           . Precision, recall and F-measure are defined as follows:
                              
                                 (8)
                                 
                                    p
                                    =
                                    
                                       
                                          |
                                          
                                             
                                                T
                                             
                                             
                                                s
                                             
                                          
                                          ∩
                                          
                                             
                                                T
                                             
                                             
                                                a
                                             
                                          
                                          |
                                       
                                       
                                          |
                                          
                                             
                                                T
                                             
                                             
                                                s
                                             
                                          
                                          |
                                       
                                    
                                    ,
                                    
                                    r
                                    =
                                    
                                       
                                          |
                                          
                                             
                                                T
                                             
                                             
                                                s
                                             
                                          
                                          ∩
                                          
                                             
                                                T
                                             
                                             
                                                a
                                             
                                          
                                          |
                                       
                                       
                                          |
                                          
                                             
                                                T
                                             
                                             
                                                a
                                             
                                          
                                          |
                                       
                                    
                                    ,
                                    
                                    F
                                    =
                                    
                                       
                                          2
                                          pr
                                       
                                       
                                          p
                                          +
                                          r
                                       
                                    
                                    .
                                 
                              
                           The final evaluation scores are computed by micro-averaging (i.e., averaging on resources of test set). We performed 5-fold cross-validation for each method for both datasets. In the experiments, the number of suggested tags 
                              
                                 
                                    
                                       M
                                    
                                    
                                       r
                                    
                                 
                              
                            ranges from 1 to 10.

We select four algorithms as the baselines for comparison: Naive Bayes (NB) (Manning, Raghavan, & Schtze, 2008), k nearest neighborhood (kNN) (Manning et al., 2008), Content Relevance (CRM) Model (Iwata et al., 2009) and Tag Allocation Model (TAM) (Si et al., 2010). Our methods are denoted as WAM and MI.

NB and kNN are two representative classification methods. NB is a simple generative model, which models the probability of each tag t given a description d as
                              
                                 (9)
                                 
                                    Pr
                                    (
                                    t
                                    |
                                    d
                                    )
                                    ∝
                                    Pr
                                    (
                                    t
                                    )
                                    
                                       
                                          
                                             ∏
                                          
                                          
                                             w
                                             ∈
                                             d
                                          
                                       
                                    
                                    Pr
                                    (
                                    w
                                    |
                                    t
                                    )
                                    .
                                 
                              
                           
                           
                              
                                 Pr
                                 (
                                 t
                                 )
                              
                            is estimated by the frequency of the documents annotated with the tag t. 
                              
                                 Pr
                                 (
                                 w
                                 |
                                 t
                                 )
                              
                            is estimated by the frequency of the word w in the resource descriptions annotated with the tag t. kNN is a widely used classification method for tag suggestion, which annotates tags to a resource according to the annotated tags of similar resources measured using vector space models (Manning et al., 2008).

CRM and TAM are selected to represent topic-based methods for tag suggestion. CRM is an LDA-based generative model. The number of latent topics K is the key parameter for CRM. In the experiments, we evaluated the performance of CRM with different K values, and here, we only present the best one obtained by setting 
                              
                                 K
                                 =
                                 1
                                 ,
                                 024
                              
                           . TAM is also a generative model that considers the words in descriptions as the topics to further generate tags for the resource. We set parameters for TAM as in Si et al. (2010).

We compare the complexity of these methods. We denote the number of training iterations in CRM, TAM and WAM as I, and the number of topics in CRM as K. For the training phase, the complexity of NB is 
                              
                                 O
                                 (
                                 D
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       d
                                    
                                 
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       a
                                    
                                 
                                 )
                                 ;
                                 k
                              
                           NN is 
                              
                                 O
                                 (
                                 1
                                 )
                              
                           ; TAM is 
                              
                                 O
                                 (
                                 ID
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       d
                                    
                                 
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       a
                                    
                                 
                                 )
                              
                           ; CRM is 
                              
                                 O
                                 (
                                 IKD
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       d
                                    
                                 
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       a
                                    
                                 
                                 )
                              
                           ; WAM is 
                              
                                 O
                                 (
                                 ID
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       d
                                    
                                 
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       a
                                    
                                 
                                 )
                              
                           ,
                              3
                              In more detail, the training phase of WAM contains preparing parallel training dataset with 
                                    
                                       O
                                       (
                                       D
                                       
                                          
                                             
                                                
                                                   N
                                                
                                                
                                                   ¯
                                                
                                             
                                          
                                          
                                             a
                                          
                                       
                                       )
                                    
                                  and learning translation probabilities using word alignment models with 
                                    
                                       O
                                       (
                                       ID
                                       
                                          
                                             
                                                
                                                   N
                                                
                                                
                                                   ¯
                                                
                                             
                                          
                                          
                                             d
                                          
                                       
                                       
                                          
                                             
                                                
                                                   N
                                                
                                                
                                                   ¯
                                                
                                             
                                          
                                          
                                             a
                                          
                                       
                                       )
                                    
                                 , where I is the number of iterations for learning trigger probabilities, and 
                                    
                                       
                                          
                                             
                                                
                                                   N
                                                
                                                
                                                   ¯
                                                
                                             
                                          
                                          
                                             a
                                          
                                       
                                    
                                  is the average number of tags for each description after sampling.
                           
                           
                              3
                            and MI is 
                              
                                 O
                                 (
                                 D
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       d
                                    
                                 
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       a
                                    
                                 
                                 )
                              
                           . When suggesting for a given resource description with length 
                              
                                 
                                    
                                       N
                                    
                                    
                                       d
                                    
                                 
                              
                           , the complexity of NB is 
                              
                                 O
                                 (
                                 
                                    
                                       N
                                    
                                    
                                       d
                                    
                                 
                                 T
                                 )
                                 ;
                                 k
                              
                           NN is 
                              
                                 O
                                 (
                                 D
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       d
                                    
                                 
                                 
                                    
                                       
                                          
                                             N
                                          
                                          
                                             ¯
                                          
                                       
                                    
                                    
                                       a
                                    
                                 
                                 )
                              
                           ; CRM is 
                              
                                 O
                                 (
                                 
                                    
                                       IKN
                                    
                                    
                                       d
                                    
                                 
                                 T
                                 )
                              
                           ; TAM is 
                              
                                 O
                                 (
                                 
                                    
                                       IN
                                    
                                    
                                       d
                                    
                                 
                                 T
                                 )
                              
                           ; WAM is 
                              
                                 O
                                 (
                                 
                                    
                                       N
                                    
                                    
                                       d
                                    
                                 
                                 T
                                 )
                              
                           , and MI is 
                              
                                 O
                                 (
                                 
                                    
                                       N
                                    
                                    
                                       d
                                    
                                 
                                 T
                                 )
                              
                           . From the analysis, we can see that WAM and MI are relatively simple methods for both training and suggestion. This is especially valuable because WAM and MI also present good effectiveness for tag suggestion compared with other methods as we will shown later.

For WAM, we use GIZA++ (Och & Ney, 2003)
                              4
                              GIZA++ is freely available from code.google.com/p/giza-pp. The toolkit is widely used for word alignment in SMT. In this paper, we use the default setting of parameters for training.
                           
                           
                              4
                            with the IBM Model-1 to determine translation probabilities using description–annotation pairs for WAM. The experimental results of WAM are obtained by setting parameters as follows: label weighting type as TF-IDF
                              t
                           , length ratio 
                              
                                 δ
                                 =
                                 1
                              
                           , harmonic factor 
                              
                                 λ
                                 =
                                 0.5
                              
                            and the type of word importance scores as TF-IDF
                              w
                           . For MI, we set the tag co-occurrence threshold 
                              
                                 γ
                                 =
                                 0
                              
                            and the type of word importance scores as TF-IDF
                              w
                           . The values are used as default values by maximizing the F-measure on a development set of 1000 instances from the website where BOOK dataset are obtained (not included int the BOOK dataset). The influence of parameters to WAM and MI can be found in Section 5.3.

In Fig. 2
                            we present the precision–recall curves of NB, kNN, CRM, TAM, WAM and MI for the two datasets. Each point of a precision–recall curve represents different numbers of tags from 
                              
                                 M
                                 =
                                 1
                              
                            (bottom right, with higher precision and lower recall) to 
                              
                                 M
                                 =
                                 10
                              
                            (upper left, with higher recall but lower precision). The closer the curve to the upper right is, the better the overall performance of the method is.

From Fig. 2, we observe the following:
                              
                                 1.
                                 The method based on MI consistently performs the best for both datasets. The method based on WAM achieves the second best performance for both datasets. These results indicate that our method is robust and effective for social tag suggestion.

The advantage of our method based on WAM is more significant on the BOOK dataset than the baseline method. The reason is that WAM has a good advantage in count information of tags compared with other baseline methods.

Although WAM has a good count information advantage for the BOOK dataset, MI is still better than WAM. The reason is that MI tries to translate a word to more tags, whereas the translation probabilities of a word in WAM always focus on only one or two tags. This leads to the result that the tags suggested by MI have a better coverage than WAM. We will present the translation probability table of words in the next subsection.

The average length of resource description is short in BIBTEX, which makes it difficult to determine the importance score of words, but even for the BIBTEX dataset with no count information of tags, our method still outperforms other methods.

To further demonstrate the performance of our word translation method and other baseline methods, in Table 3
                            we show the precision, recall and F-measure of NB, kNN, CRM, TAM,WAM and MI applied to the BOOK dataset when suggesting 
                              
                                 M
                                 =
                                 3
                              
                            labels.
                              5
                              We elected to show this number because it is near the average number of labels for the BOOK dataset.
                           
                           
                              5
                            Here we also show the variance of F-measure. In fact, MI achieves the best performance when 
                              
                                 M
                                 =
                                 2
                              
                           , where the F-measure of MI is 0.399, indicating an outperforming of both CRM (
                              
                                 F
                                 =
                                 0.263
                              
                           ) and TAM (
                              
                                 F
                                 =
                                 0.277
                              
                           ) by more than 10%.

In Table 4
                           , we show the top 10 tags suggested by NB, CRM, TAM, WTM and MI applied to the book in Table 1. The number in brackets after the name of each method is the count of correctly suggested labels. The correctly suggested tags are marked in bold face. We elected not to show the kNN results because the tags suggested by kNN are totally unrelated to the book due to the insufficient finding of nearest neighbors.

From Table 4, we observe that NB, CRM and TAM, as generative models, tend to suggest coarse-grained tags, such as “novel”, “literature”, “classic” and “France”, and fail in suggesting fine-grained tags such as “Alexandre Dumas”, “Count of Monte Cristo”, “revenge” and “suspense”. On the contrary, WAM and MI succeed in suggesting both the coarse-grained and the fine-grained tags related to the book.

To find out how our model can suggest these fine-grained tags, we list four important words (using TF-IDF
                              w
                            as weighting metric) of the description and their corresponding tags with the highest translation probabilities in Tables 5 and 6
                           
                           . The values in brackets are the probability of tag t given word 
                              
                                 w
                                 ,
                                 Pr
                                 (
                                 t
                                 |
                                 w
                                 )
                              
                           . For each word, we eliminated the tags with a probability less than 0.05. We can see that the translation probabilities can map the words in descriptions to their semantically corresponding tags in annotations. Take the word “Count of Monte Cristo” in Table 5 for example, besides the tag identical to itself, it has a high probability to the tag “Alexander Dumas”, which indicates the tag “Alexandre Dumas” is highly related to the word “Count of Monte Cristo”. In fact, the word “Count of Monte Cristo” appears in 19 books (12 of them are the different versions of “Count of Monte Cristo” and other are the novels written by “Alexander Dumas”) and 16 of them are labeled with the tag “Alexander Dumas”. This proves that our model can capture the semantic relation between words and tags.

Note that “Count of Monte Cristo” and “Alexander Dumas” correspond to the title and the author of the book in Table 1, they may be easily derived from other metadata of the book (although it is not the case of our dataset). So it is more interesting to see that our model can suggest the fine-grained tags like “revenge”. From Table 6, we can see that each of the words “Count of Monte Cristo”, “Alexander Dumas” and “revenge” has a probability to the tag “revenge”. The tag “revenge” is suggested jointly by combining the scores from these important words in the description. The ability to suggest a tag jointly enables our model to suggest the tags that are not statistically significant or even not appear in the descriptions. Thus our model can solve the vocabulary gap problem.

We explore the parameter influences on WAM for tag suggestion. The parameters include harmonic factor, length ratio, tag weighting types, and types of word translation power. When investigating one parameter, we set the other parameters to the values inducing the best performance as mentioned in Section 5.2. Finally, we also investigated the influence of training data size for classification performance. In the experiments we found that WAM reveals similar trends for both the BOOK dataset and the BIBTEX dataset. Thus, we only show the experimental results for the BOOK dataset for analysis.

In Fig. 3
                               we investigate the influence of harmonic factor via the curves of F-measure of WAM versus the number of suggested tags on the BOOK dataset when harmonic factor λ ranges from 0.0 to 1.0. As shown in Section 3.1.2, the harmonic factor λ controls the proportion between model 
                                 
                                    
                                       
                                          Pr
                                       
                                       
                                          d
                                          2
                                          a
                                       
                                    
                                 
                               and 
                                 
                                    
                                       
                                          Pr
                                       
                                       
                                          a
                                          2
                                          d
                                       
                                    
                                 
                              .

From Fig. 3, we observe that neither single model 
                                 
                                    
                                       
                                          Pr
                                       
                                       
                                          d
                                          2
                                          a
                                       
                                    
                                 
                               (
                                 
                                    λ
                                    =
                                    1.0
                                 
                              ) nor 
                                 
                                    
                                       
                                          Pr
                                       
                                       
                                          a
                                          2
                                          d
                                       
                                    
                                 
                               (
                                 
                                    λ
                                    =
                                    0.0
                                 
                              ) achieves the best performance. When the two models are combined by the harmonic mean, the performance is consistently better, especially when λ ranges from 0.2 to 0.6. This is reasonable because IBM Model-1 constrains that only the term in the source language that can be aligned to multiple terms in target language, which makes the translation probability learned by a single model asymmetric.


                              Fig. 4
                               shows the influence of length ratios for WAM on the BOOK dataset. From the figure, we observe that the performance for tag suggestion is robust as the length ratio varies, except when the ratio breaks the default restriction of GIZA++ (i.e., 
                                 
                                    δ
                                    =
                                    10
                                 
                              ).
                                 6
                                 GIZA++ restricts the values of length ratio within 
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         1
                                                      
                                                      
                                                         9
                                                      
                                                   
                                                   ,
                                                   9
                                                
                                             
                                          
                                       
                                     by setting the parameter maxfertility
                                    
                                    =
                                    
                                    10. From Fig. 4, we can see when 
                                       
                                          δ
                                          =
                                          10
                                       
                                    , the performance becomes much worse as GIZA++ will cut off the sentences out of range.
                              
                              
                                 6
                              
                           

The influence of two weighting types, TF
                                 t
                               and TF-IDF
                                 t
                              , on tag suggestion when 
                                 
                                    M
                                    =
                                    3
                                 
                               on the BOOK dataset is shown in Table 7
                              . TF-IDF
                                 t
                               tends to select the tags more specific to the resource, whereas TF
                                 t
                               tends to select the most popular tags, because the latter does not consider global information (the IDF
                                 t
                               part). Table 7 verifies the analysis, where TF-IDF
                                 t
                               is slightly better than TF
                                 t
                              .

In Table 8
                              , we show the performance of WAM applied to the BOOK dataset with different methods for computing word importance scores. From the table, we can see that there is no significant difference between TF-IDF
                                 w
                               and the product of TF-IDF
                                 w
                               and TextRank, and TextRank performs the worst. This performance indicates that TextRank is less competitive when measuring word importance scores, as it does not take global information into consideration.

We also investigated the influence of training data size for WAM. As shown in Fig. 5
                              , we increased the training data size from 8000 to 56,000 step by 8000, and performed evaluation on 4000 resources. The figure shows that: (1) when the training data size is small (e.g., 8000), WAM can still achieve good performance, and (2) when the training data size increases, the performance will be improved, but the improvement speed will be slowed down as the training data size increases. This indicates that WAM does not require a huge size dataset to achieve good performance.

The parameters of the MI-based method include tag co-occurrence threshold and types of word importance scores. When investigating one parameter, we set the other parameters to be the values inducing the best performance as mentioned in Section 5.2. Similar to WAM, we only present the experimental results for the BOOK dataset for analysis.


                              Fig. 6
                               shows the influence of the tag co-occurrence threshold for MI on the BOOK dataset. We set the tag co-occurrence threshold γ to different values. The figure shows that the MI-based method achieves the best performance with the BOOK dataset when 
                                 
                                    γ
                                    =
                                    0
                                 
                              . This indicates that the set of tags that have low co-occurrences with a word not only contains noisy tags, but also contains proper tags that need to be suggested by the word.

In Table 9
                              , we show the performance of MI applied to the BOOK dataset with different methods for computing word importance scores. From the table, we can see that for MI, TF-IDF
                                 w
                               performs the best and TextRank performs the worst. It is similar to WAM, and these results indicate that TextRank is less competitive for measuring word importance scores, as it does not take global information into consideration.

By analyzing the influences of parameters on WAM and MI, we find that the word translation model is robust to parameter variations.

In Fig. 7
                         we investigate the influence of self-translation parameter via the curves of F-measure of MI versus the number of suggested tags on the two datasets when the self-translation parameter α ranges from 0.0 to 0.9. As shown in Section 3.3, the parameter α controls the self-translation probabilities.

From Fig. 7, we observe that MI achieve the best performance when 
                           
                              alpha
                              =
                              0.2
                           
                         for the BOOK dataset, whereas 
                           
                              alpha
                              =
                              0.4
                           
                         on BIBTEX dataset. These results indicate that for both dataset, for one word, we need to emphasize the translation probability to itself. This is reasonable because without self-translation, a important tag may not be suggested as it does not suggest itself using the translation probabilities. We also see that for different dataset, the self-translation parameter is not a constant and varies from the need of emphasizing the words in the current document.

Finally, we tested the performance of emphasizing self-translation probability for WAM with different word translation methods for the BOOK dataset. As shown in Table 10
                        , emphasizing the self-translation probability improves the performance of WAM (in Table 8) as applied to the BOOK dataset when using TF-IDF
                           w
                         and the product as the methods for computing the word trigger powers, but decays when using TextRank. This result verifies that TF-IDF
                           w
                         is the best method to measure word importance scores for WAM, which indicates that emphasizing the tags appearing in the descriptions may enhance the classification performance of the word translation method.

However, the performance when emphasizing the self-translation probability on the BIBTEX dataset decays much compared with WAM. The F-measure of emphasizing the self-translation probability is only 
                           
                              F
                              =
                              0.229
                           
                         compared with WAM 
                           
                              F
                              =
                              0.267
                           
                        . The main reason for the decay is that the average length of descriptions in the BIBTEX dataset is too short to provide sufficient information to precisely emphasize tags, and usually emphasize wrong tags and drop correct tags.

The experimental results for emphasizing the self-translation probability suggest that, we have to analyze the characteristics of the tag suggestion systems to decide whether to emphasize the tags that appear in the corresponding descriptions. It is also worth investigating the problem when combining with collaboration-based methods for social tag suggestion.

@&#CONCLUSIONS@&#

In this paper, we present a new perspective on social tag suggestion and propose two methods to estimate translation probabilities between words in descriptions and tags. One method is the word alignment model in statistical machine translation and the other method is mutual information. Based on the translation probabilities between words and tags, we propose the word translation method. The experiments revealed that our method is effective and efficient for social tag suggestion compared with other baseline methods.

There are several open issues for further investigation:
                        
                           1.
                           Our model focuses on suggesting social tags according to the resource descriptions. We will take advantages of more social information such as user information to improve the performance of social tag suggestion.

Other metadata of resources (author, title, images and videos) can be also taken into account to improve the performance of social tag suggestion.

Our model is a supervised model which requires a large collection of annotated resources. We will explore to use large-scale unlabeled text corpora to estimate the translation probabilities between words, which could be used to enhance the estimation of translation probabilities between words in descriptions and tags in annotations.

In this paper we suggest each tag in isolation, without considering the correlations between tags. We will investigate the hieratical structure and the semantic relatedness between tags to regularize the granularity of tags.

@&#ACKNOWLEDGMENTS@&#

This work is supported by the National Natural Science Foundation of China (NSFC) under Grant Nos. 61170196 and 61202140. The authors would like to thank Peng Li for his insightful suggestions.

@&#REFERENCES@&#

