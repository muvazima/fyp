@&#MAIN-TITLE@&#Unsupervised language identification based on Latent Dirichlet Allocation

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           An unsupervised language identification approach based on Latent Dirichlet Allocation with high precisions, recalls and F scores.


                        
                        
                           
                           The raw n-gram count as features without any smoothing, pruning or interpolation.


                        
                        
                           
                           Purifies main language with unknown number of other languages in high precision.


                        
                        
                           
                           Find out the nearest measure related to the minimum of topic number.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Language filtering

Language purifying

Language identification

@&#ABSTRACT@&#


               
               
                  To automatically build, from scratch, the language processing component for a speech synthesis system in a new language, a purified text corpora is needed where any words and phrases from other languages are clearly identified or excluded. When using found data and where there is no inherent linguistic knowledge of the language/languages contained in the data, identifying the pure data is a difficult problem. We propose an unsupervised language identification approach based on Latent Dirichlet Allocation where we take the raw n-gram count as features without any smoothing, pruning or interpolation. The Latent Dirichlet Allocation topic model is reformulated for the language identification task and Collapsed Gibbs Sampling is used to train an unsupervised language identification model. In order to find the number of languages present, we compared four kinds of measure and also the Hierarchical Dirichlet process on several configurations of the ECI/UCI benchmark. Experiments on the ECI/MCI data and a Wikipedia based Swahili corpus shows this LDA method, without any annotation, has comparable precisions, recalls and F-scores to state of the art supervised language identification techniques.
               
            

@&#INTRODUCTION@&#

The motivation for our approach to language identification comes from the specific application of building speech synthesis systems in under-resourced languages. In the field of speech synthesis we have developed techniques to automatically prepare recording scripts and derive front-ends—the front end of a speech synthesis system is the language processing part of the system and generally consists of a series of modules that convert the input text to a phonemic representation including linguistic context that can then be synthesised by the acoustic back-end—from found data with little or no expert linguistic knowledge of the language. The general scenario is that we would take a text corpora in the new language in question and from it derive a front-end to phonetically process input text in this language, additionally we would create a recording script of up to a few thousand sentences to maximise phonetic coverage of the language which would be then recorded from a native speaker. Watts et al. (2013) show that we can efficiently use vector-space models to automatically build the language processing front-end of a speech synthesis system, and the recording script can trivially be created from a larger corpus using a greedy algorithm to optimise coverage. However, each of these steps is adversely affected by the purity of the larger corpus—the recording script is difficult to read if it contains other languages and the front-end models become polluted with characteristics of the other languages.

As our found data generally comes from sources such as Wikipedia or web-orientated news material and is in minority languages, it is often quite impure, either as the result of code switching, the inclusion of foreign names, or of being a partial translation from another language. We require a language purification tool to identify which sentences in a found corpus are suitable for inclusion in the sub-corpus used for speech synthesis. Existing language identification tools are generally unsuited to this task, primarily because the languages in question would not necessarily be supported by them. This paper discusses the development of a text purification tool along with its evaluation as both a tool for text purification and for the more general case of language identification.

Language identification is generally viewed as a form of text categorization, many classification approaches have been used to identify the language of a document: Markov models combined with Bayesian classification (Dunning, 1994), discrete hidden Markov models (Xafopoulos et al., 2004), Kullback–Leibler divergence–namely relative entropy (Sibun and Reynar, 1996), minimum cross-entropy (Teahan, 2000), decision trees (Hakkinen and Tian, 2001), neural networks (Tian and Suontausta, 2003), support vector machines (Zhai et al., 2006), multiple linear regression (Murthy and Kumar, 2006), centroid-based classifications (Kruengkrai et al., 2005) and improvements to the previous method (Takçıand Güngör, 2012), conditional random fields (King and Abney, 2013), minimum description length with dynamic programming (Yamaguchi and Tanaka-Ishii, 2012) and bootstrapped methods such as Mayer (2012), Goldszmidt et al. (2013). These methods are all supervised and require clean editorially managed corpora for training. They are appropriate only for a limited number of languages, and require relatively large-sized documents. Lui and Baldwin (2012) do however provide a pre-trained off the shelf model for language identification. These tools and approaches perform well when there is sufficient data available, but there are problems when the text to be identified is out of domain, out of style, or includes language not found in the training corpora.

In the task we are addressing, other than assuming that the majority of the text is from the language we are interested in, we can make no assumptions about the number or quantity of other languages present. We require individual sentences to be classified as being purely from the language in question or containing other languages, this is slightly different task than determining the primary language of a larger document. For our requirement to purify a text where we have little linguistic knowledge of the language or languages present, this presents a problem and raises the key question: can the language of a sentence be automatically identified using unsupervised methods or can we at least identify sentences as being of different languages. Amine et al. (2010) demonstrate an approach using similarity measures, but performance is greatly reduced when compared to supervised methods. Biemann and Teresniak (2005) present a promising co-occurrence words graph approach, namely Chinese Whispers (Biemann, 2006), claiming an F1 score of 99%, but their work focuses on long documents (each language present must have a minimum of 100 sentences). Shiells and Pham (2010) incorporate what they call the “purity” and “authority” into Chinese Whispers to identify the language of one million short Tweets. They find that the algorithm does not seem to converge when using Twitter data as opposed to when using much longer documents (Biemann and Teresniak, 2005; Biemann, 2006) due to many short Tweets mixing words from more than one language. That is there are many more edges between language clusters. Another issue is that using words directly as features means that this kind of algorithm is affected by the tokenization performance for languages that do not use white space to mark word boundaries.

Collectively this means that although the Chinese Whispers approach and its improvements are effective for some scenarios, it is unsuitable for our particular task. Furthermore, experimentation with existing techniques highlights three main requirements we need to fulfill. We require a technique that: (1) equally applicable to long and short documents; (2) that when used in filtering, can filter those documents mixed with words from other languages; and (3) that can deal with all languages including those that cannot be tokenised using white-space.

Note that (1) and (3) concern the representation of our data, namely what feature space can express the documents for training and identifying the language present. The word-level features, used by, for example, Grefenstette (1995) and Rehurek and Kolkus (2009), have drawbacks in this respect. The more appropriate alternatively is to create a language n-gram model based on characters (Cavnar and Trenkle, 1994) or encoded bytes (Dunning, 1994). With n-grams where n
                     ≤3, linguistic phenomenon with granularity smaller than words can usually be characterized and a relatively small training corpus can be used and training converges faster (Souter et al., 1994). According to Chen and Goodman (1996), Zhai and Lafferty (2001), and Zhai and Lafferty (2004) (as many others), there are empirical results that using smoothing or interpolation will promise in better results on Natural Language Processing. The smoothing of language model here is to better estimate real probabilities and avoid the bias, because there is always insufficient data in training set to estimate probabilities accurately in the languages. However, due to the need for smoothing, pruning or interpolation, it can be time-consuming to generate n-gram features. In this paper we will see that even the raw n-gram counts can effectively be used as features for effective language identification as long as an appropriate learning algorithm is employed to provide effective smoothing (with alpha and beta of the Dirichlet parameter in our paper).

Conversely (2) means that the local linkage information of words (or n-grams) may lead Chinese Whispers based algorithms to fail to filter out the mixed language short sentences, since the words from other languages will add incorrect linkages between language clusters. We show that an appropriately structured probabilistic model, which utilizes global statistical information is able to avoid these local incorrect interventions.

This paper presents an unsupervised language identification method using the raw n-gram count to characterize features and a reformulated Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). This approach is tested on the ECI/MCI benchmark in the context of being a language identification tool, and on other data as a filtering tool. Additionally, we compared four kinds of measure and also the Hierarchical Dirichlet process (HDP) on several configurations of ECI/UCI benchmark, to determine the number of languages present.

The outline of the rest of this paper is as follows: in Section 3, we conduct a detailed analysis of the problem we are addressing and propose the Language Identification model based on Latent Dirichlet Allocation (LDA-LI). In Section 4, we introduce the feature space, training and inference algorithms of the LDA-LI. In Section 4, we also briefly evaluate four kinds of measure and the HDP, compared their differences on finding the suitable topic number(number of clustering). In Section 5, we present experimental results including analyses using the ECI/MCI benchmark and a Wikipedia based Swahili corpus. Finally, Section 6 draws conclusions and proposes future directions.

To be able to identify language in an unsupervised fashion we adopt and adapt a model from the field of topic modelling. Here, a topic model is a statistical model for discovering the abstract topics that occur in a collection of documents. Papadimitriou et al. (1998) give a probabilistic analysis of Latent Semantic Indexing which is seen as an early topic model. Almost at the same time, Hofmann (1999) proposed the probabilistic Latent Semantic Indexing-pLSI with a suitable EM learning algorithm. The most common topic model currently in use, is the generalization of pLSI into Latent Dirichlet Allocation, which allows documents to contain a mixture of topics, developed by Blei et al. (2003). In LDA, each document is modelled as a mixture of K latent topics, where each topic k is a multinomial distribution 
                           
                              
                                 ϕ
                                 →
                              
                              k
                           
                         over a W-word vocabulary. For any document j, its topic mixture 
                           
                              
                                 
                                    θ
                                    →
                                 
                              
                              j
                           
                         is a probability distribution drawn from a Dirichlet prior with parameter α. For each ith word x
                        
                           ij
                         in j, a topic z
                        
                           ij
                        
                        =
                        k is drawn from 
                           
                              
                                 
                                    θ
                                    →
                                 
                              
                              j
                           
                        , and x
                        
                           ij
                         is drawn from 
                           
                              
                                 ϕ
                                 →
                              
                              k
                           
                        . The generative process for LDA is thus given by
                           
                              
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      θ
                                                      →
                                                   
                                                   j
                                                
                                                ∼
                                                Dir
                                                (
                                                α
                                                )
                                                ,
                                                
                                                
                                                   
                                                      ϕ
                                                      →
                                                   
                                                   k
                                                
                                                ∼
                                                Dir
                                                (
                                                β
                                                )
                                                ,
                                             
                                          
                                          
                                             
                                                
                                                   z
                                                   ij
                                                
                                                (
                                                =
                                                k
                                                )
                                                |
                                                j
                                                ∼
                                                Mult
                                                (
                                                
                                                   
                                                      θ
                                                      →
                                                   
                                                   j
                                                
                                                )
                                                ,
                                             
                                          
                                          
                                             
                                                
                                                   x
                                                   ij
                                                
                                                |
                                                
                                                   z
                                                   ij
                                                
                                                ∼
                                                Muti
                                                (
                                                
                                                   
                                                      ϕ
                                                      →
                                                   
                                                   k
                                                
                                                )
                                                ,
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where Dir(*) denotes the Dirichlet distribution and Mult(*) is the multinomial distribution. LDA is a kind of Bayesian hierarchical model, the graphical model for it is illustrated in Fig. 1
                        , where the observed variables, that is, words x
                        
                           ij
                         and hyper-parameters α and β, are shaded. For more detailed description of LDA, see Blei et al. (2003). The LDA model has three merits. The first is exchangeability, according to Blei et al. (2003), topics are conditionally independent and identically distributed in a fixed document which means the topics are infinitely exchangeable within the document. The second is that the conjugate distribution of the Dirichlet is a multinomial distribution. The exchangeability and conjugation between of the Dirichlet and Multinomial distribution makes the learning algorithm relative simple as a pseudo-count can be used directly by Expectation Maximization (Blei et al., 2003) or Gibbs Sampling. In Section 4.1, our presented implementation uses Collapsed Gibbs Sampling (Griffiths and Steyvers, 2004). The third merit of LDA is that it inherently provides some degree of the automatic smoothing. Asuncion et al. (2009) point out the LDA is a flexible latent variable framework for modelling sparse data in extremely high dimensional spaces. Even with the default hyper-parameter settings of those learning algorithms, LDA can smooth the sparse count data and infer on unseen data (Blei et al., 2003). Thus in Section 4.2 we have just used the raw n-gram counts as the features. These counts are from unigram to 5-g, so include both lower-order and high-order n-gram.

The basic idea behind traditional LDA is that documents are represented as random mixtures over latent topics, where each topic is characterised by a distribution over words (Blei et al., 2003). To adapt this to language identification we consider documents represented as random mixtures over latent languages, where each language is characterised as a distribution over letter n-grams counts.

In such way, the document∼Language and Language∼N-gram hierarchies can similarly be modelled by the LDA for language identification, We call this approach LDA-LI for short. Fig. 2
                         gives the pseudo code of generative LDA-LI model.

During the inference phase of LDA-LI, to classify a document as given language either the most probably language can be chosen, or a threshold can be set and multiple languages can be assigned to individual documents. Additionally, the formulation of the model places no restrictions on the length of the document and is able classify very short documents, i.e., individual short sentences. As our speech synthesis work currently builds systems in a single language at a time, this paper explores the result of assuming that we are only interested in sentences that are purely of one language, and investigates whether we can identify these sentences appropriately. However there is plenty of scope for using this model in scenarios that do not make this assumption.

The learning algorithm in this paper is based on the Collapsed Gibbs Sampling (CGS) Griffiths and Steyvers (2004), a Markov-chain Monte Carlo method. The model parameter 
                           ϕ
                           =
                           {
                           
                              
                                 ϕ
                                 →
                              
                              k
                           
                           |
                           
                              
                                 
                                    ϕ
                                    →
                                 
                                 k
                              
                           
                           ∼
                           Dir
                           (
                           β
                           )
                           }
                        , the set of topic distributions, can be integrated using the Dirichlet-multinomial conjugacy. The posterior distribution P(Z|W) can then be estimated using the Collapsed Gibbs Sampling algorithm, which, in each iteration, updates each topic assignment z
                        
                           ij
                        
                        ∈
                        Z by sampling the full conditional posterior distribution:
                           
                              (1)
                              
                                 p
                                 (
                                 
                                    
                                       z
                                       ij
                                    
                                 
                                 =
                                 k
                                 |
                                 
                                    
                                       Z
                                       
                                          
                                             ij
                                             ¯
                                          
                                       
                                    
                                 
                                 ,
                                 
                                    
                                       x
                                       ij
                                    
                                 
                                 =
                                 w
                                 ,
                                 
                                    
                                       W
                                       
                                          
                                             ij
                                             ¯
                                          
                                       
                                    
                                 
                                 )
                                 ∝
                                 
                                    
                                       
                                          
                                             C
                                             kj
                                             doc
                                          
                                          +
                                          α
                                       
                                    
                                 
                                 
                                    
                                       
                                          C
                                          kw
                                          word
                                       
                                       +
                                       β
                                    
                                    
                                       
                                          ∑
                                          
                                             
                                                v
                                                ′
                                             
                                          
                                       
                                       
                                          
                                             C
                                             
                                                
                                                   kv
                                                   ′
                                                
                                             
                                             word
                                          
                                          +
                                          W
                                          β
                                       
                                    
                                 
                              
                           
                        where k
                        ∈[1, K] is a topic, 
                           w
                           ∈
                           [
                           1
                           ,
                           W
                           ]
                         is a word in the vocabulary, x
                        
                           ij
                         denotes the ith word in document j and z
                        
                           ij
                         the topic assigned to x
                        
                           ij
                        . 
                           
                              W
                              
                                 
                                    ij
                                    ¯
                                 
                              
                           
                         denotes the words in the corpus with x
                        
                           ij
                         excluded, and 
                           
                              Z
                              
                                 
                                    ij
                                    ¯
                                 
                              
                           
                         are the corresponding topic assignments of 
                           
                              W
                              
                                 
                                    ij
                                    ¯
                                 
                              
                           
                        . In addition, 
                           
                              C
                              kw
                              word
                           
                         denotes the number of times that word 
                           w
                         is assigned to topic k not including the current instance x
                        
                           ij
                         and z
                        
                           ij
                        , and 
                           
                              C
                              kj
                              doc
                           
                         the number of times that topic k has occurred in document j not including x
                        
                           ij
                         and z
                        
                           ij
                        . Whenever z
                        
                           ij
                         is assigned to a sample drawn from (1), matrices C
                        
                           word
                         and C
                        
                           doc
                         are updated. After enough sampling iterations to burn in the Markov chain, 
                           θ
                           =
                           
                              
                                 {
                                 
                                    
                                       
                                          θ
                                          →
                                       
                                       j
                                    
                                 
                                 }
                              
                              
                                 j
                                 =
                                 1
                              
                              D
                           
                         and 
                           ϕ
                           =
                           
                              
                                 {
                                 
                                    
                                       
                                          ϕ
                                          →
                                       
                                       k
                                    
                                 
                                 }
                              
                              
                                 k
                                 =
                                 1
                              
                              K
                           
                         can be estimated by
                           
                              (2)
                              
                                 
                                    
                                       θ
                                       kj
                                    
                                 
                                 =
                                 
                                    
                                       
                                          C
                                          kj
                                          doc
                                       
                                       +
                                       α
                                    
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          K
                                       
                                       
                                          
                                             C
                                             ij
                                             doc
                                          
                                       
                                       +
                                       K
                                       α
                                    
                                 
                                 ,
                              
                           
                        
                        
                           
                              (3)
                              
                                 
                                    
                                       ϕ
                                       kv
                                    
                                 
                                 =
                                 
                                    
                                       
                                          C
                                          kv
                                          word
                                       
                                       +
                                       β
                                    
                                    
                                       
                                          ∑
                                          
                                             j
                                             =
                                             1
                                          
                                          W
                                       
                                       
                                          
                                             C
                                             kj
                                             word
                                          
                                       
                                       +
                                       W
                                       β
                                    
                                 
                              
                           
                        
                     

From Eqs. (2) and (3), we see that the CGS learning and inference are some kinds of pseudo counts of original corpus. The implementation of CGS used in this paper is based upon the implementation of Wang et al. (2009) using a Map-Reduce parallel framework with efficiency improvements by Liu et al. (2011) using a Message Passing Interface (MPI).
                           1
                        
                        
                           1
                           
                              https://code.google.com/p/plda/.
                        
                     

Sometimes we are required to actually identify the individual languages present (for example, when evaluating the model with a test set and calculating precision and recall), for each language (topic) cluster we examine the sentence assigned to that language with the highest probability, and manually determine which language it is and then label all the sentences assigned to this language cluster as being of that language. Merging languages is also performed manually if there are more language clusters than actual languages known to be present. This strategy is used throughout our experiment which means use of LDA-LI reduces the need for annotation of hundreds or thousands of sentences to a few representative ones for given languages. This classifies each sentence as being of an actual language, which can then be evaluated correct or not in experiments where ground-truth is known.

A corpus is first converted into samples by considering each individual sentence a document. These documents are then converted into character based n-gram counts (tokens for spaces, and beginning and end of sentence markers are included for each document). Cavnar and Trenkle (1994) show that for supervised learning n
                        ≤3 is sufficient n-gram length. We however found improved performance with our unsupervised method when we included n-grams with n in the range 1–5. which we believe allows us to capture more information across both short and long contexts. We did attempt to build models using n
                        >5 but they proved computationally impractical to train. Due to the smoothing ability of LDA with large sparse data as discussed in Section 3.1, we are able to use the raw n-gram counts. In practice, the smoothing and pruning is actually realised by the hyper parameters α and β, which are configured with their default small values (<1) suggested by Liu et al. (2011).

An important issue with LDA topic modelling is how to determine that an adequate number of individual topics are being modelled. In most cases (Blei et al., 2003; Griffiths and Steyvers, 2004; Newman et al., 2009; Wallach et al., 2009; Grün and Hornik, 2011), perplexity is used to evaluate the resulting model on held-out data.

In our experiment (see Section 5), we found that the perplexity always reduces as the number of languages is increased, as shown in Fig. 9 of Blei et al. (2003) and Fig. 10 of Newman et al. (2009), this continues beyond the point where the number of languages in the model is larger than the actual number of different languages in the data.

In fact, the perplexity 2
                           H(p,q) is another form of cross-entropy over the test set
                           
                              (4)
                              
                                 H
                                 (
                                 p
                                 ,
                                 q
                                 )
                                 =
                                 −
                                 
                                    ∑
                                    
                                       v
                                       =
                                       1
                                    
                                    W
                                 
                                 
                                    
                                       
                                          p
                                          v
                                       
                                    
                                    log
                                    
                                       
                                          q
                                          v
                                       
                                    
                                 
                                 =
                                 H
                                 (
                                 p
                                 )
                                 +
                                 KL
                                 (
                                 P
                                 |
                                 |
                                 Q
                                 )
                              
                           
                        
                        
                           
                              p
                              v
                           
                         is the probability of each word v, estimated by 
                           
                              
                                 p
                                 v
                              
                           
                           =
                           
                              
                                 n
                                 jv
                              
                           
                           /
                           
                              
                                 ∑
                                 
                                    j
                                    ,
                                    v
                                 
                              
                              
                                 
                                    n
                                    jv
                                 
                              
                           
                         in test set, 
                           
                              q
                              v
                           
                         is the probability of each word v computed by the LDA model with
                           
                              
                                 
                                    
                                       q
                                       v
                                    
                                 
                                 =
                                 
                                    ∑
                                    j
                                 
                                 
                                    
                                       ∑
                                       
                                          k
                                          =
                                          1
                                       
                                       K
                                    
                                    
                                       
                                          
                                             θ
                                             kj
                                          
                                       
                                       
                                          
                                             ϕ
                                             kv
                                          
                                       
                                    
                                 
                              
                           
                        
                        H(p) denotes the entropy of p and KL(P||Q) is the Kullback–Leibler divergence (the KL divergence or relative entropy) of q from p. From analyses, it can be seen that there is no explicit penalty term on the language number in Eq. (4) which means perplexity is not biased towards minimising the number of languages. Further more, even if a model makes 
                           
                              
                                 p
                                 v
                              
                           
                           =
                           
                              
                                 q
                                 v
                              
                           
                         on every word such that KL(P||Q)=0. it is by no means that the model is the best model, because H(p, q)=
                        H(p) is not the minimum of real underlying probability distribution 
                           
                              p
                              ˜
                           
                        , instead it is just a bias estimation of 
                           H
                           (
                           
                              p
                              ˜
                           
                           )
                         on limited test set. This implies that perplexity may not be the best measure to find the smallest topic number without significantly degrading performance.

Another way to find the correct number of languages is to use the Hierarchical Dirichlet process (HDP) (Teh et al., 2006), instead of LDA to hierarchically cluster the document into languages and thus automatically find the number of languages. However, here we find that HDP tends to choose far too many languages than the measure in Arun et al. (2010) of LDA (see Table 2 of Section 5.3). The HDP behaviour is more like a language hashing process than one that can find the minimal language number appropriate for a given dataset. This means HDP is not necessary to find the minimum of topic number since the split-merge process are recursively run on sub topics.

In Cao et al. (2009), standard cosine distance (similarity) is used to measure the correlation between topics:
                           
                              (5)
                              
                                 corre
                                 (
                                 
                                    
                                       ϕ
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       ϕ
                                       j
                                    
                                 
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                          v
                                       
                                       
                                          
                                             
                                                ϕ
                                                iv
                                             
                                          
                                          
                                             
                                                ϕ
                                                jv
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             
                                                ∑
                                                i
                                             
                                             
                                                
                                                   
                                                      ϕ
                                                      iv
                                                   
                                                
                                             
                                          
                                       
                                       
                                          
                                             
                                                ∑
                                                j
                                             
                                             
                                                
                                                   
                                                      ϕ
                                                      jv
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where i, j
                        ∈[1, K], 
                           v
                           ∈
                           [
                           1
                           ,
                           W
                           ]
                        . When corre(ϕ
                        
                           i
                        , ϕ
                        
                           j
                        ) is smaller, the topics are more independent. the average cosine distance between every pair of topics is used to measure the stability of a topic structure:
                           
                              (6)
                              
                                 ave
                                 _
                                 dis
                                 (
                                 structure
                                 )
                                 =
                                 
                                    
                                       
                                          ∑
                                          i
                                          K
                                       
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                i
                                                +
                                                1
                                             
                                             K
                                          
                                          
                                             corre
                                             (
                                             
                                                
                                                   ϕ
                                                   i
                                                
                                             
                                             ,
                                             
                                                
                                                   ϕ
                                                   j
                                                
                                             
                                             )
                                          
                                       
                                    
                                    
                                       K
                                       (
                                       K
                                       −
                                       1
                                       )
                                       /
                                       2
                                    
                                 
                              
                           
                        
                     

A smaller ave
                        _
                        dis shows that the structure is more stable, so ave
                        _
                        dis is minimised. However, in some situations, more topics always reduce ave
                        _
                        dis even though they are unnecessary, as we saw with perplexity. This happens in our experiments, see Section 5.3 and Fig. 8(a) for detail. Alternatively, Zavitsanos et al. (2008) use KL divergence instead of cosine similarity. Here, we revise this notion by replacing the term from Eq. (5) in Eq. (6) with symmetric KL-divergence, since it is more reasonable that Eq. (6) should average correlation between pairs of topics. Now we can use this to find the language number with the maximal average KL-divergence, since the distributions of word-vectors of different languages should have maximal average divergence when the correct number of languages is selected. Thus it will reduce the difference (in divergence) to either decrease or increase the number of languages. However, we find this measure is not sufficiently sensitive to language number and its value almost does not change with language number in some situations which are confirmed by Fig. 8(b). The disadvantage with both Cao et al. (2009) and Zavitsanos et al. (2008) is that they only consider the information in the stochastic language–letter n-gram matrix and ignore the sentence-language matrix.


                        Arun et al. (2010) view LDA as a matrix factorization mechanism, where a given corpus 
                           
                              ℂ
                           
                         is split into two matrix factors and given by:
                           
                              
                                 
                                    
                                       
                                          
                                             ℂ
                                          
                                       
                                       
                                          D
                                          ×
                                          W
                                       
                                    
                                 
                                 =
                                 
                                    
                                       Θ
                                       
                                          D
                                          ×
                                          K
                                       
                                    
                                 
                                 ·
                                 
                                    
                                       Φ
                                       
                                          K
                                          ×
                                          W
                                       
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       Θ
                                       
                                          D
                                          ×
                                          K
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          [
                                          
                                             
                                                
                                                   
                                                      θ
                                                      →
                                                   
                                                
                                                1
                                             
                                          
                                          ,
                                          
                                             
                                                
                                                   
                                                      θ
                                                      →
                                                   
                                                
                                                2
                                             
                                          
                                          ,
                                          …
                                          ,
                                          
                                             
                                                
                                                   
                                                      θ
                                                      →
                                                   
                                                
                                                D
                                             
                                          
                                          ]
                                       
                                       T
                                    
                                 
                                 ,
                                 
                                 
                                    
                                       Φ
                                       
                                          K
                                          ×
                                          W
                                       
                                    
                                 
                                 =
                                 
                                    
                                       
                                          [
                                          
                                             
                                                
                                                   
                                                      ϕ
                                                      →
                                                   
                                                
                                                1
                                             
                                          
                                          ,
                                          
                                             
                                                
                                                   
                                                      ϕ
                                                      →
                                                   
                                                
                                                2
                                             
                                          
                                          ,
                                          …
                                          ,
                                          
                                             
                                                
                                                   
                                                      ϕ
                                                      →
                                                   
                                                
                                                K
                                             
                                          
                                          ]
                                       
                                       T
                                    
                                 
                              
                           
                        where D is the number of documents present in the corpus and W is the size of the vocabulary as mentioned in 4.1. The quality of the split depends on K, the right number of topics chosen. This measure is computed in terms of symmetric KL-divergence of salient distributions that are derived from these matrix factors. Θ
                        
                           D×K
                         is the document-topic(sentence-language) matrix, while Φ
                        
                           K×W
                         is the topic-word (language–n-gram) matrix. Note that here 
                           θ
                           =
                           
                              
                                 {
                                 
                                    
                                       
                                          θ
                                          →
                                       
                                       j
                                    
                                 
                                 }
                              
                              
                                 j
                                 =
                                 1
                              
                              D
                           
                         and 
                           ϕ
                           =
                           
                              
                                 {
                                 
                                    
                                       
                                          ϕ
                                          →
                                       
                                       k
                                    
                                 
                                 }
                              
                              
                                 k
                                 =
                                 1
                              
                              K
                           
                         are different from Section 4.1, and are just the numerators of Eqs. (2) and (3). So it is clear that:
                           
                              (7)
                              
                                 
                                    ∑
                                    
                                       v
                                       =
                                       1
                                    
                                    W
                                 
                                 
                                    Φ
                                    (
                                    k
                                    ,
                                    v
                                    )
                                    =
                                    
                                       ∑
                                       
                                          d
                                          =
                                          1
                                       
                                       D
                                    
                                    
                                       Θ
                                       (
                                       d
                                       ,
                                       k
                                       )
                                       
                                       
                                       ∀
                                       k
                                       ∈
                                       [
                                       1
                                       ,
                                       K
                                       ]
                                    
                                 
                                 ,
                              
                           
                        where 
                           Θ
                           (
                           k
                           ,
                           v
                           )
                         is the kth row vth column element of matrix Θ, the same goes with Φ(d, k). Eq. (7) is the number of words assigned to each topic looked in two different ways – one as row sum over words and other as column-sum over documents. However, when both these matrices are row normalized (as done by LDA), this equality will not hold any more. This is the reason only the numerator of Eqs. (2) and (3) are used.

They proposed the Symmetric KL divergence of C
                        
                           Θ
                         and C
                        
                           Φ
                        
                        
                           
                              (8)
                              
                                 symKL
                                 =
                                 KL
                                 (
                                 
                                    
                                       C
                                       Θ
                                    
                                 
                                 |
                                 |
                                 
                                    
                                       C
                                       Φ
                                    
                                 
                                 )
                                 +
                                 KL
                                 (
                                 
                                    
                                       C
                                       Θ
                                    
                                 
                                 |
                                 |
                                 
                                    
                                       C
                                       Φ
                                    
                                 
                                 )
                              
                           
                        where, C
                        
                           Φ
                         is the distribution of singular values of topic-word matrix Φ, C
                        
                           Θ
                         is the distribution obtained by normalizing the vector L
                        *
                        Θ(L is 1*
                        D vector of lengths of each document in the corpus and Θ is the document-topic matrix). Both the distributions C
                        
                           Θ
                         and C
                        
                           Φ
                         are in sorted order so that the corresponding topic components are expected to match. With the measure in Eq. (8), they find that this divergence between the C
                        
                           Φ
                         distribution and the C
                        
                           Θ
                         distribution initially degrades then starts to increase once the right number of topics is reached. In Section 5.4, we will see the effectiveness of symKL and also compared with the others measures mentioned above.

It is difficult to fairly evaluate the LDI-LI model directly against other supervised language identification models partly due to the bias of the tasks that they are designed to perform and partly due to the supervised-unsupervised difference. Therefore results comparing systems, should not be treated judgementally to say one system is better than the other, but rather they should be used to understand how the models behave differently.

Experiments 1 and 2 evaluated the LDA-LI in this way as a general language identification model. Here we performed experiments either using LDA-LI as unsupervised learner (Experiment 1) or as unsupervised clusterer (Experiment 2), and compared our model to other approaches using the ECI/MCI,
                        2
                     
                     
                        2
                        
                           http://www.elsnet.org/eci.html.
                      a benchmark corpora for language identification studies (Armstrong-Warwick et al., 1994).

We then perform a number of experiments to further evaluate the LDA-LI in the way we intended to use it. In Experiment 3, we compared the measures related to the topic number mentioned in Section 4.3. We also filtered either on ECI-MCI dataset (Experiment 4) or a corpus of Swahili from Wikipedia (Experiment 5) as tests of found data where a language was mixed with unknown other languages.

Finally, in Experiment 6, we compared the performance of LDA-LI to that of a more traditional LDA word-feature model to show that the letter n-gram counts a much better representation than words for this task.

As mentioned in the Section 4.1, to actually calculate precision and recall, for each topic modelled by LDA-LI, we examined the first sentence (that with the highest probability which is computed and output by LDA approach as Eq. (2) in 4.1) assigned to that topic, manually identified its language, and then labelled all the other sentences assigned to that topic as being of that same language. We then, where necessary manually merged topics assigned the same language. This strategy was used throughout our experiments.

Experiments 1 and 2 used nine languages
                           3
                        
                        
                           3
                           Dutch-291K, English-108K, French-108K, German-171K, Italian-99K, Portuguese-107K, Spanish-107K, Swedish-91K, and Turkish-109K.
                         which had the same configurations as Takçıand Güngör (2012). First in Experiment 1, we tested the hypothesis that our system was equal to supervised systems trained with the same limited data (actually, for LDA-LI we consider this pseudo-training, as no use was made of the predetermined language classes). We performed 10-fold Cross Validation (CV) training and testing with the same configuration used by Takçıand Güngör (2012) and calculated precision, recall and F-score.

We compared our LDA-LI model to 3 available existing methods of Language identification: the langID
                        
                           4
                        
                        
                           4
                           
                              https://github.com/saffsd/langID.
                         tool of Lui and Baldwin (2012), the Guess_language
                        
                           5
                        
                        
                           5
                           
                              https://bitbucket.org/spirit/guess_language/overview.
                         tool—the current version of TextCat (Cavnar and Trenkle, 1994)—and the ICF of Takçıand Güngör (2012).

We firstly prepared the ECI/MCI data for 10-fold Cross Validation with the same configuration with Takçıand Güngör (2012). With this we then trained and tested langID, Guess_language and our LDA-LI models. Note that for the LDA-LI model this is pseudo-training as we did not use the language classes present. The average precision, recall and F-scores were then compared (here the precision, recall and F-score were firstly averaged across the 10-CV, then averaged according to the language ratio configuration of Takçıand Güngör (2012)). We also include results reported by Takçıand Güngör (2012) for their ICF model that used the same configuration. Fig. 3
                        (a), (b) and Table 1
                         show that our LDA-LI system though is unsupervised generally compared favourably in performance with the supervised systems.

Note that we present the LDA-LI model with 16 topic classes where symmetric KL-divergence reaches the minimum. This is discussed further in the Experiment 3.

One criticism of Experiment 1 is that it is not fair on the supervised systems, as they are designed to be used trained on larger data sets. We address this with Experiment 2. In Experiment 2 we tested the hypothesis that our system, as a unsupervised algorithm, would perform as well as supervised systems in the more real-word situation where the supervised systems are were fully trained on additional data. We used our LDA-LI system to cluster the ECI/MCI data in an unsupervised fashion and compared to the other systems fully-trained on their wider variety of training data.

We evaluated on the full ECI/MCI subset used above. Default versions of langID and Guess_language were taken, pre-trained on their very large corpora, while the LDA-LI ran solely on the ECI/MCI as a standard clustering algorithm without additional language knowledge. The trade off here was that our LDA-LI system was solely pseudo-trained on in-domain data, where as the other systems are trained on substantially more (including in-domain) data. We justify this comparisons as it is how each of these systems would be used in practice. Here the LDA-LI model was used with 16 topic classes, and performed using 10-CV as described in Experiment 1.


                        Fig. 4(a) and (b) shows that our LDA-LI system generally outperformed the other systems. In a sense this was not surprising in terms of the data used, as the LDA-LI clusters all the data, and hence was in-domain. The more general training regimes of the other systems was to their disadvantage here. Domain and style differences are the likely explanation for langID outperforming Guess_language as the former is more up to date in terms of content of the training corpora and closer to the data being classified. The general conclusion here is training the supervised systems only on the in-domain here data does not harm them, this may not however be the case for smaller data sets.

In Experiment 3, we investigated measures for finding the correct number of languages present in a corpus. We implemented the perplexity and HDP measures in conjunction with the nine languages used in Experiments 1 and 2. We also used an additional nine languages, now 18 languages in total.
                           6
                        
                        
                           6
                           Dutch, English, French, German, Italian, Spanish, Swedish, Turkish, Portuguese, Albanian, Bulgarian, Czech, Estonian, Latin, Lithuanian, Modern Greek, Norwegian and Russian.
                         For each N of (N
                        =3, 6, 9, 12, 15, 18) we randomly select N languages and spit the data from these languages to perform a 10-fold Cross Validation for a given N. We then averaged the cosine distance (Cao et al., 2009), word KL-divergence (Zavitsanos et al., 2008) and symmetric KL divergence (Arun et al., 2010). To test for consistency in our choice of languages selected, we repeated this set of Experiments 5 times, randomly choosing different subsets of languages each time for each N. Results were found to be consistent irrespective of the languages chosen.

Experiments 1 and 2, showed that we can use the LDA-LI as a language identification tool with the ability to generalize well if we could find the appropriate number if topics present. As a general language identification tool, there are two requirements on the language number. Firstly, the language number must be large enough to account for all the languages present. Then it is better for the language number to be as close to actual number of languages present in order to avoid unnecessary examination and merging of the language clusters actually of the same language.

To evaluate the ability to find the correct number of languages present, we calculated the perplexity, HDP and symKL (all averaged across 10-CV) on the nine languages used in Experiments 1 and 2, and compared with the precision, recall and F-scores. We used 18 languages4 to compare the cosine distance, word KL-divergence and symmetric KL divergence (abbreviated as: cosim, wordkl and symKL, respectively). The five measures are described in detail in Section 4.3.

In Fig. 5
                         we can see that perplexity always reduced with an increase in language number even when the number of languages was greater than 50 and the precision and recall were degraded (see Fig. 6
                        , each time the topic number increases, a new LDA is trained-subsequently the F score might drop), therefore we discard perplexity. Table 2
                         shows the language number chosen by HDP
                           7
                        
                        
                           7
                           HDP-faster http://www.cs.cmu.edu/~chongw/software/hdp.tar.gz.
                         where we see many more languages predicted that actually present. Because it is not feasible to find the optimized hyper-parameters of HDP in every different unknown language combination (the hyper-parameter configuration of HDP for 3 languages is not good when 18 languages are actually present and vice versa), we just used the default hyper-parameters of HDP and LDA. In contrast, the symKL (Table 2), reached its minimum with the language number of 16 when the correct number of languages was 9, which means the symKL measure is closer to the actual number of languages present. This also held when there were 3 or 18 languages present (see Table 2).

Examples of topics/clusters (from Table 2 row 2 for symKL) are shown in Fig. 9, these examples come from cross-validation subset 9. Fig. 9 shows the first three sentences of 5 language clusters (only the first three clusters and the two most mixed clusters 3, 5 are shown for brevity), the clusters assign to the same language are in same background color. In cluster 3, LDA-LI mistakes all the following 11 sentences as French as the first sentence is French. This degrades the precision of French, and the recall of German and Dutch. In cluster 5, LDA-LI mistakes a few Portuguese sentences as Spanish sentences. This degrades the recall of Portuguese and the precision of Spanish. The overall precisions and recalls on the test set of this cross-validation subset are as Table 3
                        .

Furthermore, as we see in Fig. 6, the F-score, precision and recall reach maximum values at 16 languages which exactly coincides with the symKL minimum. Closer analysis of the categorization performed suggested that the LDA-LI tended to classify accurately into more categories than there ever actually were. We often observed a language split into two or more languages, a typical case we often saw was where a high proportion of sentences in a language contain numbers as digits. If the number of languages was set to the actual number of languages, the sentences with and without digits would often be classified as individual languages in their own right at the expense of other language distinctions present. This also happened even if we simply deleted the digits as part of the tokenisation. For this reason almost all the measures choose a language number greater than the actual number of languages present, but the language number which the symKL choose is the nearest to the actual language number. This was also found for the following experiment on 18 languages.

Next we used 18 languages, for each N in (N
                        =3, 6, 9, 12, 15, 18) we randomly chose N languages and spit the data to perform the 10-fold Cross Validation for that N, we then averaged the cosim, wordKL and symKL measures as mentioned above. We repeated this Experiments 5 times with different language sub-sets and found results were very similar each time. One of the results is shown in Fig. 7
                         and Table 4
                        . In Fig. 7 almost all the language numbers where symKL reaches minimum fulfil the two requirements mentioned: were large enough to result in high precision, recall and F-score (as in Table 4) and were the nearest to language numbers compared with cosim and wordKL in Fig. 8
                        , thus required minimal annotating and merging of language clusters. Although in Fig. 7(e) the symKL with 2 languages was occasionally smaller than that with 18 languages, we still see in the trend of Fig. 7(e), the symKL with 18 languages was the minimum. In the Fig. 8(a), we see that cosim always reduced with the language number increasing. On the other hand, when there were 12, 15 or 18 languages in Fig. 8(b), the wordkl was not sensitive when the language number changed from 10 to 30, while the corresponding F-score, precision and recall in Fig. 7 increased rapidly. Since each time the topic number increase, the LDA reconstruct an actual different LDA. It empirically leads the symKL looks bumpy. If just split the original topic into subtopics similar with HDP, the symKL may be smooth but HDP tends too many topics.

Note that the language numbers of minimum symKL with the same number of languages as Table 4 and as Table 2 are different from each other. This is because the languages chosen randomly in Table 4 are different from those in Table 2. Also in Table 4, we see that the chosen language number for 15 or 18 languages is smaller than that for 12 languages, this is also because the languages were chosen randomly with each language number N, namely, the languages we chose for 12, 15 or 18 combinations were completely different from each other. However, in Table 2 the language number increased with language numbers from 3 to 18, since the 3 languages are included in the 9 languages (This also holds for the 9 languages and being included in the 18 languages in Table 2.)

In Experiment 4 we moved towards trying to simulate the typical scenario of unsupervised language filtering using the ECI/MCI. A majority language was chosen and mixed with an unknown number of other languages (both closely related languages and more distantly related languages are used) in difference ratios. We focused on determining the precision and recall for the majority language present using our LDA-LI system with the language number fixed to 2.

In Fig. 10
                        
                        (a), German was the considered the primary language and mixed with Dutch, English and Turkish. We increasingly added equal amounts of Dutch English and Turkish to lower the overall proportion of the primary language, German, present.

We evaluated the results for proportions of the other languages present between 1% and 50% so that there was always more German than the other languages combined so that German remained the majority language. In Fig. 10(b), Dutch was the chosen as the primary language and mixed with German, English and Turkish to provide a comparison.

For this experiment we fixed the language number of LDA-LI to 2, as in practice we would not know the number of other languages present and wish to apply unsupervised language filtering and would not wish to explicitly identify the other languages present. Fig. 10(a) and (b) shows the results for German and Dutch respectively. Where German was the primary language we see a very high precision whilst the proportion of the other languages was less than 0.25 after which point the precision drops or becomes a little erratic. However, the recall is lower and the model is quite conservative in identifying all of the German sentences.

Where Dutch was the primary language (Fig. 10(b)) we see a similar picture with precision, but here the recall is generally much higher, except when the mix of other languages present is less than 0.1, where we hypothesise that there is not sufficient data to learn a good model for the non-Dutch language class.

In general we see that our LDA-LI system can purify the main language with high precision when it has ≤30% other languages which is the typical scenario of language purifying. The results were better when the main language is English (Fig. 10(c)) or Turkish (Fig. 10(d)).

We can get alleviate the low recall by executing the LDA-LI one more time for each cluster of to improve the recall almost without degrading the precision. This kind of iterative use can also be used to improve the precision when mixed ratio is above 30%. Since the LDA-LI is conservative in identifying the main language, we can use LDA-Li twice, first filtering into two topics, one cluster is mostly the main language, the other is mixed. We then repeat the process on the mixed cluster to obtain more main language sentences. This process will increase the recall empirically.

In Experiment 5 we investigated the ability of our model to filter a real Swahili corpora crawled from Wikipedia, where Swahili was the majority language in the content, but was mixed with additional material from an unknown number of other languages. Here we investigated the ability to filter this data and find pure Swahili sentences with LDA-LI again with the number of languages set to 2.

In Experiment 5, we used a Swahili corpus crawled from Wikipedia. This nominally consisted of 172,724 Swahili sentences but actually contained a mix of Swahili and other languages. Here, we evaluated the LDA-LI model using 2 languages (i.e. Swahili and Other) as a filtering procedure to find pure Swahili sentences. Because it was unreasonable to determine the language or languages present in every sentences of this corpus manually, we used the langID with its pre-trained model to evaluate the performance of LDA-LI. That is, we took the inferences of langID as the underline correct language in the corpus to compute pseudo-precision and pseudo-recall values (pPrecision and pRecall, respectively in Fig. 11
                         for the LDA-LI model with the probabilities greater than thresholds between 50% and 90%. This probability measure is the confidence score from the LDA-LI, e.g., if the required probability is 90%, the LDA-LI only outputs all those sentences which are confirmed as Swahili with the probability≥90%. Because we set the topic number fixed to 2, the smallest confidence is 50%.

It is easy to understand that changing the required probability of the LDA-LI from 50% to 90% will degrades the recall of LDA-LI, since higher confidence probability leads to a more strict filtering condition thus leading to lower recall. However, it has a negligible affect on the precision of LDA-LI to increase the confidence probability. The reason for this is that in addition to any miss-inferences of LDA-LI, there are also many very short sentences, which are Swahili and correctly identified by LDA-LI but cannot be recognized by langID. This means the actual precision will be higher than that shown in Fig. 11.

For the case where a sentences comprises multiple languages, we find in practice, that the presence of one or two other-language words is sufficient to classify the sentence as not being Swahili. This is the behaviour we desire for the task we are investigating, but may not always be appropriate.

In Experiment 6, we firstly compare the LDA for language identification with two different feature configurations. We compare our model as described above to a similar model where we replace our letter n-gram features with the more traditional word based features of the standard LDA model. We then compared the language filtering performance of these models.

The experiment of language identification with word and n-gram features are carried out on the same ECI-MCI dataset and the same configuration with Experiment 1. In Fig. 12
                        , the wPrecision means precision of word feature(the same with wRecall and wF-sorce), while the gPrecision is the precision of n-gram feature(the same with gRecall and gFsorce). See Fig. 12, the precision, recall and F-score of word feature are too much lower than those of n-gram feature, and also when using word feature, it has less effect on the performance(precision, recall or F-score) to increase language number. The result empirically show that word feature is more about the content of the document than its language, and further considering that here the document is a sentence (potentially a short sentence), the word has less chance to appear in any two different sentences, while the n-gram (especially the low order n grams) have much more of a chance to exist in different sentences. This make the feature space of words sparser than that of n-gram. The lower order n-gram maybe appear in two different languages but often with different frequency, while the high order gram maybe exists in only one of the language. Because we use either lower order n-gram or high order n-gram, the n-gram feature space is compact (but not sparse) and representative for language identification.

The similar result happens in language filtering with word features. Comparing the result in Fig. 13
                         with Fig. 10, we can see that any precision of word feature is much lower than that of n-gram feature (considering the situation of English as main language, the LDA-LI even refuse to construct the filtering model when all the others language is less than 30).

In this paper, we presented LDA-LI, an approach to unsupervised language identification which takes raw 1–5g counts as features and allows us to both classify sentences by language or filter sentences not of a specific language from a corpora. We can get some way towards identifying the number of languages present by using the symmetric KL-divergence measure of Arun et al. (2010). Our experiments have shown that the LDA-LI is robust both for initial annotation of unknown languages and for further inferring and filtering.

However we achieve better results classifying slightly more languages than actually present according to the symmetric KL-divergence of Arun et al. (2010) due to some intra-language differences being greater than some inter-language differences. Further work is needed to determine if the excess topics can be merged automatically, or to determine whether this issue could be resolved by text normalisation.

For the language purification task we have shown that the LDA-LI system purifies with a high precision for mixes of languages similar to those we would require the task for. This makes it a useful tool for preparing found language corpora for building speech synthesis front ends, and for recording-script production in these languages, as a pure script is easier for a subject to record and provides a better training set for acoustic models.

As the primary language becomes less the majority language present precision begins to suffer. It is possible in this case that it would be better to use the system as a more general language identification tool and allow it to classify the individual non-majority languages present into their own categories as we have shown precision to remain high here. The current system would reject sentences containing the primary language of interest mixed with another language (in the same sentence) but the LDA framework allows for multiple topics to be assigned, so there is scope for further investigation here.

In general we found that the LDA-LI model proved to meet our languages filtering needs but turned out to be a flexible more generally applicable language identification tool.

@&#ACKNOWLEDGMENTS@&#

This work is supported by the Simple4All: funded by the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287678, the Natural Science Foundation of Shandong Province of China (ZR2012FM016) and the China Scholarship Council.

@&#REFERENCES@&#

