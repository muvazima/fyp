@&#MAIN-TITLE@&#Amplifying scientific paper’s abstract by leveraging data-weighted reconstruction

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           This paper explores the impact of heterogeneous bibliographic network for generating scientific paper’s amplified abstract.


                        
                        
                           
                           The amplified abstract is generated by leveraging target scientific paper’s abstract and citation sentence’s content and structure, which is addressed through document summarization manner.


                        
                        
                           
                           Sentence’s weight is learned by exploiting regularization for ranking on heterogeneous bibliographic network.


                        
                        
                           
                           Data-weighted reconstruction is proposed to assign different priority to sentences when reconstructing the original document.


                        
                        
                           
                           Various evaluation metrics are designed to validate the effectiveness of our approach.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Document summarization

Citation analysis

Scientific literature

Data-weighted reconstruction

@&#ABSTRACT@&#


               
               
                  In this paper, we focus on the problem of automatically generating amplified scientific paper’s abstract
which represents the most influential aspects of scientific paper. The influential aspects can be illustrated by the target scientific paper’s abstract and citation sentences discussing the target paper, which are provided in papers citing the target paper. In this paper, we extract representative sentences through data-weighted reconstruction approach(DWR) by jointly leveraging target scientific paper’s abstract and citation sentences’ content and structure. In our study, we make two-folded contributions.
                  Firstly, sentence’s weight was learned by exploiting regularization for ranking on heterogeneous bibliographic network. Specially, Sentences-similar-Sentences
relationship was identified by language modeling-based approach and added to the bibliographic network. Secondly, a data-weighted reconstruction objective function is optimized to select the most representative sentences which reconstructs the original sentence set with minimum error. In this process, sentences’ weight plays a critical role. Experimental evaluation over real dataset confirms the effectiveness of our approach.
               
            

@&#INTRODUCTION@&#

With the rapidly expanding scientific literature, identifying and digesting valuable knowledge is a challenging task. The explosive growth of the publications makes it rather difficult to quickly understand large amounts of scientific papers. Given a query, there are too many scientific papers for researcher to understand. The best approach to this problem is to select the most influential and representative papers which meanwhile are close to the researcher’s research interests.

Although most researchers grasp a scientific paper’s general outline through its abstract, the aspects described by abstract are frequently biased and incomplete. The abstract reflects the author’s viewpoint about its key characteristic, which is subjective. Intuitively, the influential aspects or contributions of papers should be identified and evaluated by researchers in the same field, especially the authors who cited the target paper.

Based on the above analysis, in this paper, our goal is to generate amplified scientific paper’s abstract, which can illustrate the most influential aspects of paper. In this paper, we achieve this goal through data-weighted reconstruction approach which consists of weight learning and salient sentence selection. Citation sentences’ semantic information and social structure are taken into consideration in the process of sentences’ weight learning.

In our study, the sentences of amplified scientific paper’s abstract is extracted from the target scientific paper’s abstract and citation sentences provided in papers which cite the target paper. So document summarization technique is a natural choice to work out this problem.

Inspired by document summarization based on data reconstruction (DSDR) (He et al., 2012), which selects a subset of sentences to best reconstruct the original document, we optimize a data-weighted reconstruction objective function for salient sentence selection. DSDR tends to select sentences that span the intrinsic subspace of candidate sentence space so that it is able to cover the core information of the document. The drawback of DSDR is that it treats all sentences equally important, which violates realistic cases obviously. Some sentences appear as the decorated or transitionary role in document, especially in scientific literature, the citation sentences are inherently informal, noisy and not well structured. Many citation sentences may contain information irrelevant to the target scientific paper. For these noisy sentences, we should not reconstruct them or reconstruct them at a little cost.

Especially to generate amplified scientific paper’s abstract, each citation sentence’s authority is another consideration other than the summarization’s coverage. In other words, citation sentences from influential papers are probably more important than others. Intuitively, the reconstruction of important sentences should be assigned to high priority.

Based on the above analysis, we make such assumption:

                        
                           •
                           Different sentences should be reconstructed with different priority.

So this paper proposes a data-weighted reconstruction approach(DWR) to generate amplified scientific paper’s abstract, which is designed to take the assumption into consideration. DWR first learns sentence’s weight and then selects salient sentences from data-weighted reconstruction perspective.

For data-weighted reconstruction objective function, sentences’ weight need to be first learned. In our study, sentences’ weight are learned based two factors: sentence’s semantic information and scientific paper’s social structure (Table 1
                     ) .

Those two factors are embodied in the heterogeneous bibliographic network. In the field of scientific literature, there are various kinds of social media information, including different types of objects and relations among these objects. For example, a typical bibliographic information network contains objects in four types of entities: paper(P), venue(i.e., conference or journal)(V), author(A), and sentence(S). For each paper, it has links to a set of authors, a venue, and a set of sentences, belonging to a set of link types. Intuitively these abundant heterogeneous information should be leveraged to measure sentence’s importance. In the process of evaluating sentence’s importance, we suppose close ranking scores should be assigned to similar sentences. Specifically, hypergraph is utilized to model the various objects and relations, and then regularization cost function is introduced to evaluate sentence’s importance considering the social media information and sentence’s semantic similarity.

After sentence’s weight learning process, the data-weighted reconstruction objective function can be solved to select salient sentences.

The contributions of this paper are summarized as follows: Firstly, we exploit semi-supervised PLSA and regularization for ranking on heterogeneous bibliographic network to compute sentence’s weight. Secondly, a data-weighted reconstruction objective function is proposed to generate amplified scientific paper’s abstract.

@&#RELATED WORKS@&#

Citation information is a valuable resource in the study of scientific literature (Galgani, Compton, & Hoffmann, 2015; Mei & Zhai, 2008; Nakov, Schwartz, & Hearst, 2004). The content of papers that cite an author’s papers provides supplementary evidence used in modeling the author’s research interests and has been used to generate relevant suggestions for researchers in recommendation system (Sugiyama & Kan, 2013). Plagiarism Detection System (Gipp, Meuschke, & Beel, 2011) is based on citation information by comparing the occurrence of citations in order to identify similarities. Citation information has been used to browse the literature (Wan, Paris, & Dale, 2009) and improve information retrieval of scientific paper (Ritchie, Robertson, & Teufel, 2008).

Citation contexts tend to have further focused information that is not present in the target scientific paper’s abstract (Elkiss et al., 2008). Provided accumulated enough citations, the set of citation sentences for a target scientific paper covers all information found in its abstract and provides about 20% more concepts (Divoli, Nakov, & Hearst, 2012). So citation sentences are attracting more attentions for summary (Abu-Jbara & Radev, 2011; Qazvinian & Radev, 2008; Qazvinian, Radev, & Özgür, 2010), which can be utilized to illustrate the target paper’s impact or attributions (Siddharthan & Teufel, 2007). Citation information has been utilized not only to generate single-document summarization (Mei & Zhai, 2008), but also to obtain a survey on a given topic (Mohammad et al., 2009; Qazvinian et al., 2013a).

In fact, citation information is noisy and informal. Some citations are topically similar, others are used as survey articles to provide background information to the reader (Caragea, Silvescu, Mitra, & Giles, 2013).

Bibliographic network is a heterogeneous information network (Sun & Han, 2013; Wang et al., 2013a), which consists of varied objects and relationships. Objects in this network are not isolated tuples, they contain rich, inter-related semantic information that can and should be systematically explored (Sun, Han, Yan, & Yu, 2012). In this paper, Hypergraph is utilized to model bibliographic network that contain rich, inter-related, multi-typed data and information.

Hypergraph is a typical representation for complex information network, by which multiple kinds of relationships can be taken into consideration not just pairwise relationship among objects. Hypergraph learning has been applied in many applications, such as classifying gene expression (Hwang, Tian, Kuang, & Kocher, 2008; Tian, Hwang, & Kuang, 2009), image retrieval (Huang, Liu, Zhang, & Metaxas, 2010; Liu, Huang, & Metaxas, 2011), recommendation (Bu et al., 2010; Li & Li, 2013) and video summarization (Wang, Chen, & Zhu, 2011).

Graph-based regularization ranking (Pan, You, Chen, Tao, & Pang, 2013; Wang, Li, Li, Li, & Wei, 2013b; Zhao, Guan, & Liu, 2015) is to estimate a function on the graph, which can then be used to predict node’s ranking score. The function consists of two constraints: fitting constraint and smoothness constraint, which are usually formulated by fitness regularizer and smoothness regularizer. Hu, Guo, Ji, and He (2013) proposed a unified regularization framework over heterogeneous bibliographic network to infer the impact of hybrid citation context and rank citation sentences. Deng, Han, Lyu, and King (2012) proposed a joint regularization framework to enhance expertise retrieval by modeling heterogeneous bibliographic network as regularization constraints on top of document-centric model. Graph-based regularization ranking has also taken over hypergraph in order to integrate both group relationship and pairwise relationship into a unified framework, such as music recommendation (Bu et al., 2010), sentence ranking (Wang et al., 2013b).

Document summarization aims to generate a compressed summary by extracting the major information, which can help users to understand the main contents of multiple related text documents rapidly. Given a collection of documents, a variety of summarization methods based on different strategies (Alguliev, Aliguliyev, & Isazade, 2013; Song, Cheon Choi, Cheol Park, & Feng Ding, 2011; Wang & Li, 2012; Yang, Wen, Chen, Sutinen et al., 2014a; Yeh, Ke, & Yang, 2008) have been proposed to extract the most important sentences from the original documents.

From the perspective of text representation, there are two approaches of automatic summarization: extraction and abstraction (Sarkar, 2009). Extractive methods select a subset of sentences from the original documents based on their salience scores and abstractive methods reformulate and compress the salient units selected from the original document. Although abstractive methods could be more reasonable, they require deep natural language processing techniques. In contrast, extractive methods are more feasible and has attracted more attention.

Feature-based model selects sentences based on their ranking score calculated by a variety of features, such as the number of keywords (Yih, Goodman, Vanderwende, & Suzuki, 2007), TF-IDF (Lin & Hovy, 2002), and sentence or term position (Lin & Hovy, 2002; Ouyang, Li, & Li, 2007). For example, MEAD (Radev, Jing, Styś, & Tam, 2004) is a typical feature-based algorithm which computes sentence’s ranking score according to sentence-level and inter-sentence features.

Graph-based ranking model (Canhasi & Kononenko, 2014; Shen & Li, 2010; Wan & Yang, 2008; You, Li, Zhang, Li, & Lu, 2013; Zhao, Wu, & Huang, 2009) usually works by performing over an affinity graph to detect the most salient sentences for summarization. Given an affinity graph where vertices represent sentences and edges indicate the semantic relevance between vertices, the extracted summary can be described using the idea of graph domain, i.e., LexRank (Erkan & Radev, 2004), TextRank (Mihalcea & Tarau, 2004), and DivRank (Mei, Guo, & Radev, 2010). Although such ranking methods capture dependency between sentences, they cannot well model the objective of maximizing the coverage on the content with a small number of sentences. Even worse, redundancy removal (Goldstein, Mittal, Carbonell, & Kantrowitz, 2000) is not inherently captured by importance ranking because the top ranked sentences would probably have overlapped meaning.

Topic models (Celikyilmaz & Hakkani-Tür, 2011; Eisenstein & Barzilay, 2008; Gong & Liu, 2001; Luo, Zhuang, He, & Shi, 2013; Titov & McDonald, 2008; Yang et al., 2014a) are often utilized to generate opinion-based summarization from semantic perspective. For example, the query Latent Dirichlet Allocation (qLDA) (Tang, Yao, & Chen, 2009) is proposed to take into account the query information for query-oriented summary. Contextual topic model (Yang et al., 2014a) is used to determine the relevance of sentences by leveraging hierarchical topics and their correlations with lexical co-occurrence of words.

Integer linear programming (ILP) based framework was introduced as a global inference algorithm for multi-document summarization (McDonald, 2007). ILP framework can maximize the coverage of concept (Gillick & Favre, 2009), information and entity (Yang, Jiang, Huang, Qiu, & Liao, 2014b) from the original corpus, which can ensure redundancy removal and global decision in the meanwhile. For example, ILP was used to automatically generate comprehensive textual overview (Sauper & Barzilay, 2009) by optimizing both local fit of information into each topic and global coherence across the entire overview. Yang et al. (2014b) modified ILP framework to select sentences from online forums to generate entity-oriented travel guides, which emphasizes the inclusion of potential points of interest. Kikuchi, Hirao, Takamura, Okumura, and Nagata (2014) first coded both dependency between words and dependency between sentences into a nested tree, and then formulated the dependency in the nested tree as an ILP optimization problem to improve the quality of summarization.

Other methods include non-negative matrix factorization (NMF) based summarization (Ding, He, & Simon, 2005), CRF-based summarization (Shen, Sun, Li, Yang, & Chen, 2007), hidden Markov model (HMM) based summarization (Conroy & O’leary, 2001), ensemble methods (Pei, Yin, Fan, & Huang, 2012; Wang & Li, 2012) combining results from different summarizers.

Document summarization based on data reconstruction (He et al., 2012) selects sentences to reconstruct the original document with minimum error. The generated summary is a compression of original document, which strikes the balance of coverage and redundancy. The only weak point is that all sentences need to be reconstructed equally. This violates the reality obviously for various noise scattered in document.

The proposed approach incorporates scattered citation sentences to amplify target scientific paper’s abstract. The amplified scientific paper’s abstract regarded as summary, essentially selects representative, authoritative sentences from the target paper’s abstract and citation sentences. A good summary should strike a balance among coverage, redundancy and authority.

The main focus is sentence salience, which is difficult to evaluate. Those citation sentences scattered in different scientific papers are inherently informal and noisy. Many citation sentences may contain information irrelevant to the target paper’s content. In this case, we may have to fall back to identify important sentences.

Our approach consists of two main steps which can be illustrated from Fig. 1
                     :

                        
                           •
                           Sentence’s weight learning.

Salient sentence selection.

The weight learning step serves as the input for the salient sentence selection. In the weight learning step, the heterogeneous bibliographic network is first built through step(a1) and step(a2). The social relations, such as paper-coauthor-paper, paper-cite-paper are identified in step(a1). On the other hand, semantic similar relations among sentences are identified in step(a2) by leveraging semi-supervised PLSA (Lu & Zhai, 2008). Specifically we first learn the topic’s word distribution and cluster semantic similar sentences under the same topic. The well structured target paper’s abstract contains the main aspects, while the noisy citation sentences provide complementary aspects. So the abstract’s sentences can be modeled as prior in semi-supervised PLSA, which can be deemed as semi-supervised learning.

In step(b), in order to explore the impact of bibliographic network to sentence’s weight, the hypergraph is utilized to model heterogeneous objects and relations. Then a ranking regularization framework is operated over the hypergraph of bibliographic network.

Based on the learned sentence’s weight, in salient sentence selection step(c), a data-weighted objective function is proposed, which can detect salient sentences from the data reconstruction perspective. In this step, our approach uses weighted reconstruction error as the objective function, obviously the sentence’s ranking score can be utilized as the sentence’s reconstruction weight. The implicit assumption is that the sentences are not equally important, and the reconstruction process should focus on important sentences.

In order to identify semantic relationship from original abstract and citation sentences, the characteristics of these two sentence collections should be exploited. The well-structured original abstract contains the main aspects, while the noisy citation sentences provide complementary aspects. So the high readability of abstract can be leveraged to structure the unorganized citation sentences.

In this paper, Semi-supervised probabilistic latent semantic analysis (Semi-PLSA)(Lu & Zhai, 2008) is utilized to use the abstract as a “template” to mine citation sentences for ordinary aspects. The abstract can be casted as prior in the Semi-PLSA model and fit the model to citation sentences with maximum a posterior (MAP) estimation.

Semi-PLSA was first proposed to find, integrate and digest ordinary opinions and expert opinions for products. In this paper, the relationship between scientific paper’s abstract and its citation sentences is similar to the relationship between product’s official introduction and related reviews. So the scientific paper’s abstract can be utilized as the prior topics. With this model, we can automatically identify supplementary or extra topics and cluster similar sentences with respect to different topics.

Given a scientific paper Pt
                        ’s abstract T consisting of a set of sentences 
                           
                              T
                              =
                              {
                              
                                 t
                                 1
                              
                              ,
                              
                                 t
                                 2
                              
                              ,
                              …
                              ,
                              
                                 t
                                 K
                              
                              }
                              ,
                           
                         and a set of citation sentences 
                           
                              C
                              =
                              {
                              
                                 c
                                 1
                              
                              ,
                              
                                 c
                                 2
                              
                              ,
                              …
                              ,
                              
                                 c
                                 L
                              
                              }
                           
                         where ci
                         is a citation sentence associated with T, the task is to identify different topics about the interested scientific paper and instantiate Sentences-similar-Sentences relationship with respect to those identified topics.

In the above definition, T expresses author’s opinion in well-written text, while C conveys research community’s opinions generally scattering in different short textual snippets. We argue that the topics expressed by scientific paper’s abstract are more common than those expressed in citation sentences. The citation sentences not only argue topics expressed in the scientific paper’s abstract, but also contain additional topics.

We first define a unigram language model for each sentence of the target scientific paper’s abstract, which would capture the similar topics scattered around citation sentences. Furthermore, we define a certain number of unigram language models to capture the extra topics which are complementary to the target paper’s abstract. This is achieved by extending the basic PLSA algorithm to incorporate a conjugate prior defined based on the topics expressed in the target paper’s abstract. Algorithm 1
                         illustrates the pseudocode for this task.

The unigram language model p(w|ri
                        ) is defined for each sentence of target scientific paper’s abstract, which can be estimated by Maximum Likelihood: 
                           
                              (1)
                              
                                 
                                    p
                                    
                                       (
                                       w
                                       |
                                       
                                          r
                                          i
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          c
                                          (
                                          w
                                          ,
                                          
                                             r
                                             i
                                          
                                          )
                                       
                                       
                                          
                                             ∑
                                             
                                                
                                                   w
                                                   
                                                      
                                                      ′
                                                   
                                                
                                                ∈
                                                V
                                             
                                          
                                          c
                                          
                                             (
                                             
                                                w
                                                
                                                   
                                                   ′
                                                
                                             
                                             ,
                                             
                                                r
                                                i
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        where w is the word, ri
                         ∈ T, c(w, ri
                        ) is the count of word w in sentence ri
                        . A conjugate Dirichlet prior for each multinomial distribution topic model is defined as Dir({σjp(w|rj
                        )}), where σj
                         is confidence parameter required to set empirically.

The prior for all parameters is calculated by

                           
                              (2)
                              
                                 
                                    p
                                    
                                       (
                                       Λ
                                       )
                                    
                                    ∝
                                    
                                       ∏
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          K
                                          +
                                          m
                                       
                                    
                                    
                                       ∏
                                       
                                          w
                                          ∈
                                          V
                                       
                                    
                                    p
                                    
                                       
                                          (
                                          w
                                          |
                                          
                                             θ
                                             j
                                          
                                          )
                                       
                                       
                                          
                                             σ
                                             j
                                          
                                          p
                                          
                                             (
                                             w
                                             |
                                             
                                                r
                                                j
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        where K is the number of sentences in T, which is also assumed to be the number of topics expressed in the target paper’s abstract. p(w|θj
                        ) is topic-word distribution under the j-th topic, Λ is the set of model parameters and m > 0, represents the number of additional topics other than the corresponding sentences in T. So we set 
                           
                              
                                 σ
                                 j
                              
                              =
                              0
                           
                         for 
                           
                              K
                              <
                              j
                              <
                              =
                              K
                              +
                              m
                           
                        .

The log-likelihood of the whole sentences sets 
                           
                              X
                              =
                              
                                 T
                                 ∪
                                 C
                              
                           
                         is:

                           
                              (3)
                              
                                 
                                    log
                                    
                                       p
                                       (
                                       X
                                       |
                                       Λ
                                       )
                                    
                                    =
                                    
                                       ∑
                                       
                                          x
                                          ∈
                                          X
                                       
                                    
                                    
                                       ∑
                                       
                                          w
                                          ∈
                                          V
                                       
                                    
                                    
                                       {
                                       c
                                       
                                          (
                                          w
                                          ,
                                          x
                                          )
                                       
                                       *
                                       log
                                       
                                          
                                             ∑
                                             
                                                j
                                                =
                                                1
                                             
                                             
                                                K
                                                +
                                                m
                                             
                                          
                                          
                                             [
                                             
                                                π
                                                
                                                   x
                                                   ,
                                                   j
                                                
                                             
                                             p
                                             
                                                (
                                                w
                                                |
                                                
                                                   θ
                                                   j
                                                
                                                )
                                             
                                             ]
                                          
                                       
                                       }
                                    
                                 
                              
                           
                        where π
                        
                           x, j
                         is the proportion of topic j in sentence x.

We then use Maximum A Posterior(MAP) estimator to estimate all the parameters as follows:

                           
                              (4)
                              
                                 
                                    
                                       Λ
                                       ^
                                    
                                    =
                                    arg
                                    
                                       max
                                       Λ
                                    
                                    
                                       p
                                       (
                                       X
                                       |
                                       Λ
                                       )
                                       p
                                       (
                                       Λ
                                       )
                                    
                                 
                              
                           
                        The MAP estimate can be easily computed using EM algorithm, and finally the updating formula is:

                           
                              (5)
                              
                                 
                                    p
                                    
                                       (
                                       
                                          z
                                          
                                             x
                                             ,
                                             w
                                             ,
                                             j
                                          
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             π
                                             
                                                x
                                                ,
                                                j
                                             
                                             
                                                (
                                                n
                                                )
                                             
                                          
                                          
                                             p
                                             
                                                (
                                                n
                                                )
                                             
                                          
                                          
                                             (
                                             w
                                             |
                                             
                                                θ
                                                j
                                             
                                             )
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                
                                                   j
                                                   
                                                      
                                                      ′
                                                   
                                                
                                                =
                                                1
                                             
                                             
                                                K
                                                +
                                                m
                                             
                                          
                                          
                                             
                                                π
                                                
                                                   x
                                                   ,
                                                   
                                                      j
                                                      
                                                         
                                                         ′
                                                      
                                                   
                                                
                                                
                                                   (
                                                   n
                                                   )
                                                
                                             
                                             
                                                p
                                                
                                                   (
                                                   n
                                                   )
                                                
                                             
                                             
                                                (
                                                w
                                                |
                                                
                                                   θ
                                                   
                                                      j
                                                      
                                                         
                                                         ′
                                                      
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (6)
                              
                                 
                                    
                                       π
                                       
                                          x
                                          ,
                                          j
                                       
                                       
                                          (
                                          n
                                          +
                                          1
                                          )
                                       
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                w
                                                ∈
                                                V
                                             
                                          
                                          
                                             c
                                             
                                                (
                                                w
                                                ,
                                                x
                                                )
                                             
                                             p
                                             
                                                (
                                                
                                                   z
                                                   
                                                      x
                                                      ,
                                                      w
                                                      ,
                                                      j
                                                   
                                                
                                                )
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                
                                                   j
                                                   
                                                      
                                                      ′
                                                   
                                                
                                                =
                                                1
                                             
                                             
                                                K
                                                +
                                                m
                                             
                                          
                                          
                                             ∑
                                             
                                                w
                                                ∈
                                                V
                                             
                                          
                                          
                                             c
                                             
                                                (
                                                w
                                                ,
                                                x
                                                )
                                             
                                             p
                                             
                                                (
                                                
                                                   z
                                                   
                                                      x
                                                      ,
                                                      w
                                                      ,
                                                      
                                                         j
                                                         
                                                            
                                                            ′
                                                         
                                                      
                                                   
                                                
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                        
                           
                              (7)
                              
                                 
                                    
                                       p
                                       
                                          (
                                          n
                                          +
                                          1
                                          )
                                       
                                    
                                    
                                       (
                                       w
                                       |
                                       
                                          θ
                                          j
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                x
                                                ∈
                                                X
                                             
                                          
                                          
                                             c
                                             
                                                (
                                                w
                                                ,
                                                x
                                                )
                                             
                                             p
                                             
                                                (
                                                
                                                   z
                                                   
                                                      x
                                                      ,
                                                      w
                                                      ,
                                                      j
                                                   
                                                
                                                )
                                             
                                          
                                          +
                                          
                                             
                                                σ
                                                j
                                             
                                             
                                                p
                                                (
                                                w
                                                |
                                                
                                                   r
                                                   j
                                                
                                                )
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                
                                                   w
                                                   
                                                      
                                                      ′
                                                   
                                                
                                                ∈
                                                V
                                             
                                          
                                          
                                             
                                                ∑
                                                
                                                   x
                                                   ∈
                                                   X
                                                
                                             
                                             
                                                c
                                                
                                                   (
                                                   w
                                                   ,
                                                   x
                                                   )
                                                
                                                p
                                                
                                                   (
                                                   
                                                      z
                                                      
                                                         x
                                                         ,
                                                         
                                                            w
                                                            
                                                               
                                                               ′
                                                            
                                                         
                                                         ,
                                                         j
                                                      
                                                   
                                                   )
                                                
                                             
                                          
                                          +
                                          
                                             σ
                                             j
                                          
                                       
                                    
                                 
                              
                           
                        where p(z
                        
                           x, w, j
                        ) is the probability of assigning topic j to word w in sentence x, and n denotes the current iteration number. As prior, σjp(w|rj
                        ) is topic-word distribution in scientific paper’s abstract to affect the probability of word w from topic j. The implicit assumption is that the abstract of scientific paper can be deemed as some training data or priori knowledge for the corresponding topic.

The convergence of the EM algorithm can be illustrated experimentally from Fig. 5. From this figure, we can see the log-likelihood is close to stable after 20 steps. And the log-likelihood is approximately 
                           
                              −
                              3
                           
                        .

After we obtain every topic’s word distribution, all sentences in the collection can be clustered into one of the 
                           
                              K
                              +
                              m
                           
                         semantic group relations {Gi
                        } by choosing the topic with the largest probability of generating sentence x ∈ X:

                           
                              (8)
                              
                                 
                                    arg
                                    
                                       max
                                       j
                                    
                                    
                                       p
                                       (
                                       x
                                       |
                                       
                                          θ
                                          j
                                       
                                       )
                                    
                                    =
                                    arg
                                    
                                       max
                                       j
                                    
                                    
                                       
                                          ∑
                                          
                                             w
                                             ∈
                                             V
                                          
                                       
                                       
                                          c
                                          
                                             (
                                             w
                                             ,
                                             x
                                             )
                                          
                                          p
                                          
                                             (
                                             w
                                             |
                                             
                                                θ
                                                j
                                             
                                             )
                                          
                                       
                                    
                                 
                              
                           
                        Finally, Sentences-similar-Sentences relationship can be instantiated by each semantic group relations {Gi
                        }, that is sentences in each relation are semantically similar to each other respectively.

Huge amount of scientific papers form a large scale bibliographic network through various kinds of objects and relations. Conventionally, a typical bibliographic information network contains objects in three types: paper(Vp
                        ), venue(i.e., conference or journal)(Vc
                        ), author(Va
                        ). For the sake of evaluating sentence’s weight, we add sentence(Vs
                        ) from scientific paper’s abstract and citation sentences into this bibliographic information network.

For this complex bibliographic network, hypergraph is utilized to model various kinds of objects and relations. In our study, we defined four types of vertices and eight types of hyperedges. The statistics about the vertices and hyperedges is described in Tables 2
                         and 3
                        .

In hypergraph, hyperedge contains two or more vertices, which is usually exploited to model high-order relation. Although hyperedges usually consist of two types of objects, they contain more than two objects. For example, Paper-contain-Sentences is defined as the combination of the paper and contained abstract sentences and citation sentences, which obviously contains more than two objects. Sentences-similar-Sentences relations are instantiated by each semantic cluster identified by Algorithm 1, which represent the semantic relevance within these clusters respectively.


                        Paper-publish-Conference, Authors-write-Paper, and Paper-contain-Sentences take the paper as the center. For example, Authors-write-Paper is defined as the combination of the paper and all attached authors. So the number of these relations equals to the number of papers respectively. Although the number of sentences is more than the number of papers, the number of multiple-sentence Sentences-similar-Sentences hyperedge is less than the number of papers. The reason is that, as a hyperedge, a Sentences-similar-Sentences relation contains several sentences.

As illustrated in Fig. 1, different hyperedges are denoted by different color edges and maybe connected by common vertices. For example, Sentences-similar-Sentences and Paper-contain-Sentences might be connected by common sentences.

Let G(V, E) denote the hypergraph, where V is the set of vertices, and E is the set of hyperedges. 
                           
                              V
                              =
                              {
                              
                                 V
                                 s
                              
                              ,
                              
                                 V
                                 p
                              
                              ,
                              
                                 V
                                 a
                              
                              ,
                              
                                 V
                                 c
                              
                              }
                           
                         denotes the four types of vertices, 
                           
                              E
                              =
                              {
                              
                                 E
                                 i
                              
                              }
                              ,
                              i
                              =
                              1
                              ,
                              …
                              ,
                              8
                           
                         denotes the eight types of hyperedges. Each hyperedge e ∈ E is a subset of V. 
                           H
                         ∈ R
                        |V| × |E| is the incidence matrix whose entry h(v, e) is 1 if v ∈ e and 0 otherwise. 
                           
                              ϕ
                              
                                 (
                                 e
                                 )
                              
                              =
                              |
                              e
                              |
                           
                         is defined as the degree of a hyperedge e, which is computed by 
                           
                              ϕ
                              
                                 (
                                 e
                                 )
                              
                              =
                              
                                 ∑
                                 
                                    v
                                    ∈
                                    V
                                 
                              
                              h
                              
                                 (
                                 v
                                 ,
                                 e
                                 )
                              
                           
                        . A vertex v’s degree is defined by 
                           
                              d
                              
                                 (
                                 v
                                 )
                              
                              =
                              
                                 ∑
                                 
                                    e
                                    ∈
                                    E
                                 
                              
                              
                                 h
                                 (
                                 v
                                 ,
                                 e
                                 )
                              
                           
                        . We denote 
                           D
                        
                        
                           e
                         and 
                           D
                        
                        
                           v
                         as two diagonal matrices whose diagonal elements are degree of hyperedge and vertex respectively.

Let f: x → R denote a ranking function which assigns each vertice Vi
                         a ranking value fi
                        . We can view 
                           f
                         as a vector 
                           
                              f
                              =
                              
                                 
                                    [
                                    
                                       f
                                       1
                                    
                                    ,
                                    …
                                    ,
                                    
                                       f
                                       
                                          |
                                          V
                                          |
                                       
                                    
                                    ]
                                 
                                 T
                              
                           
                        . The query of interest is vertices associated with the target paper and we will rank the other vertices on the hypergraph with respect to their relevance to the query. Then we define a vector 
                           
                              y
                              =
                              
                                 
                                    [
                                    
                                       y
                                       1
                                    
                                    ,
                                    …
                                    ,
                                    
                                       y
                                       
                                          |
                                          V
                                          |
                                       
                                    
                                    ]
                                 
                                 T
                              
                           
                         as the initial score of vertices. In our experiment, we set 
                           
                              
                                 y
                                 i
                              
                              =
                              1
                           
                         if vertice Vi
                         is from the target scientific paper and 0 otherwise.

A regularization framework is developed by regularizing the smoothness of relevance over the hypergraph and the cost function is defined as follows:

                           
                              (9)
                              
                                 
                                    Q
                                    
                                       (
                                       f
                                       )
                                    
                                    =
                                    μ
                                    
                                       ∑
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          |
                                          V
                                          |
                                       
                                    
                                    ∥
                                    
                                       
                                          f
                                          i
                                       
                                       −
                                       
                                          y
                                          i
                                       
                                    
                                    
                                       ∥
                                       2
                                    
                                    +
                                    
                                       1
                                       2
                                    
                                    
                                       ∑
                                       
                                          i
                                          ,
                                          j
                                          =
                                          1
                                       
                                       
                                          |
                                          V
                                          |
                                       
                                    
                                    
                                       ∑
                                       
                                          e
                                          ∈
                                          E
                                       
                                    
                                    
                                       1
                                       
                                          ϕ
                                          
                                             (
                                             e
                                             )
                                          
                                       
                                    
                                    
                                       ∑
                                       
                                          {
                                          
                                             v
                                             i
                                          
                                          ,
                                          
                                             v
                                             j
                                          
                                          }
                                          ⊂
                                          e
                                       
                                    
                                    ∥
                                    
                                       
                                          
                                             f
                                             i
                                          
                                          
                                             
                                                d
                                                (
                                                
                                                   v
                                                   i
                                                
                                                )
                                             
                                          
                                       
                                       −
                                       
                                          
                                             f
                                             j
                                          
                                          
                                             
                                                d
                                                (
                                                
                                                   v
                                                   j
                                                
                                                )
                                             
                                          
                                       
                                    
                                    
                                       ∥
                                       2
                                    
                                 
                              
                           
                        where the first term defines the difference between the obtained ranking score and the initial score, the second term guarantees vertices should have similar ranking scores if they are contained in many common hyperedges. The parameter μ > 0 controls the relative importance of these two terms. In this paper, all hyperedges’ weights are supposed to be same for simplicity.

For simplicity, we rewrite this cost function into matrix form as follows:

                           
                              (10)
                              
                                 
                                    
                                       
                                          
                                             Q
                                             (
                                             f
                                             )
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   |
                                                   V
                                                   |
                                                
                                             
                                             
                                                f
                                                i
                                                2
                                             
                                             −
                                             
                                                ∑
                                                
                                                   i
                                                   ,
                                                   j
                                                   =
                                                   1
                                                
                                                
                                                   |
                                                   V
                                                   |
                                                
                                             
                                             
                                                ∑
                                                
                                                   e
                                                   ∈
                                                   E
                                                
                                             
                                             
                                                
                                                   
                                                      f
                                                      i
                                                   
                                                   h
                                                   
                                                      (
                                                      
                                                         v
                                                         i
                                                      
                                                      ,
                                                      e
                                                      )
                                                   
                                                   h
                                                   
                                                      (
                                                      
                                                         v
                                                         j
                                                      
                                                      ,
                                                      e
                                                      )
                                                   
                                                   
                                                      f
                                                      j
                                                   
                                                
                                                
                                                   
                                                      
                                                         d
                                                         
                                                            (
                                                            
                                                               v
                                                               i
                                                            
                                                            )
                                                         
                                                         d
                                                         
                                                            (
                                                            
                                                               v
                                                               j
                                                            
                                                            )
                                                         
                                                      
                                                   
                                                   ϕ
                                                   
                                                      (
                                                      e
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             +
                                             μ
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                
                                                   |
                                                   V
                                                   |
                                                
                                             
                                             ∥
                                             
                                                
                                                   f
                                                   i
                                                
                                                −
                                                
                                                   y
                                                   i
                                                
                                             
                                             
                                                ∥
                                                2
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   f
                                                
                                                T
                                             
                                             f
                                             −
                                             
                                                
                                                   f
                                                
                                                T
                                             
                                             M
                                             f
                                             +
                                             μ
                                             
                                                
                                                   (
                                                   f
                                                   −
                                                   y
                                                   )
                                                
                                                T
                                             
                                             
                                                (
                                                f
                                                −
                                                y
                                                )
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              M
                              =
                              
                                 D
                                 v
                                 
                                    −
                                    1
                                    /
                                    2
                                 
                              
                              H
                              
                                 D
                                 e
                                 
                                    −
                                    1
                                 
                              
                              
                                 
                                    H
                                 
                                 T
                              
                              
                                 D
                                 v
                                 
                                    −
                                    1
                                    /
                                    2
                                 
                              
                           
                        .

To minimise the cost function Q(f), we make the derivative of function Q(f) with respect to f equals to zero:

                           
                              (11)
                              
                                 
                                    
                                       
                                          ∂
                                          Q
                                       
                                       
                                          ∂
                                          f
                                       
                                    
                                    =
                                    
                                       (
                                       I
                                       −
                                       M
                                       )
                                    
                                    
                                       
                                          f
                                       
                                       *
                                    
                                    −
                                    μ
                                    
                                       (
                                       
                                          
                                             
                                                f
                                             
                                             *
                                          
                                          −
                                          y
                                       
                                       )
                                    
                                    =
                                    0
                                 
                              
                           
                        Following some simple algebraic steps, we can derive a closed-form optimal solution:

                           
                              (12)
                              
                                 
                                    
                                       
                                          f
                                       
                                       *
                                    
                                    =
                                    
                                       μ
                                       
                                          1
                                          +
                                          μ
                                       
                                    
                                    
                                       
                                          (
                                          I
                                          −
                                          
                                             1
                                             
                                                1
                                                +
                                                μ
                                             
                                          
                                          M
                                          )
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    y
                                 
                              
                           
                        
                     

The vector 
                           f
                        
                        * is the final ranking score of vertices in bibliographic network. The ranking score represents the affinity between vertices and the target paper through considering the impact of bibliographic network, which is propagated through hyperedges. Then sentence’s ranking score can be extracted from f
                        * and formed sentence weight’s diagonal matrix 
                           U
                         after normalization. Pseudocode for this algorithm is provided in Algorithm 2
                        . In next section, the learned sentence’s ranking score is utilized as weight to influence the generation of amplified scientific paper’s abstract.

Conventionally, document summarization is generated by selecting salient sentences that covers the main topics of a document with a minimum redundancy. The candidate sentence set is 
                           
                              X
                              =
                              T
                              ∪
                              C
                              =
                              
                                 
                                    [
                                    
                                       x
                                       1
                                    
                                    ,
                                    
                                       x
                                       2
                                    
                                    ,
                                    …
                                    ,
                                    
                                       x
                                       N
                                    
                                    ]
                                 
                                 T
                              
                           
                         where xi
                         ∈ RD
                         is a weighted term frequency vector for sentence i. In this paper, 
                           X
                         is collected from the target paper’s abstract and corresponding citation sentences. Assume the sentence set has D terms and N sentences, then 
                           X
                         ∈ R
                        
                           N × D
                        . 
                           X
                         is used to represent both the matrix and candidate set. 
                           
                              S
                              =
                              {
                              
                                 s
                                 1
                              
                              ,
                              
                                 s
                                 2
                              
                              ,
                              …
                              ,
                              
                                 s
                                 m
                              
                              }
                           
                         denotes the summarization with m < N and S ⊂ X.

There are many existing algorithms for document summarization (Ma, Sun, Yuan, & Cong, 2012; Qazvinian et al., 2013b; Sarkar, 2009; Wan & Yang, 2008). However, most of them focus on selecting salient sentences through greedy algorithm, which cannot ensure global optimization. Even worse, similar salient sentences with high ranking score introduce redundancy. Inspired by He et al. (2012), which selects a subset of sentences to reconstruct the original document, we present data-weighted reconstruction for sentence selection. Pseudocode for this algorithm is provided in Algorithm 3
                        . The drawback of He et al. (2012) is that it treats every sentence equally important, which violates most realistic cases obviously. Some sentences appearing as the decorated or transitionary role in document are very common. Especially in scientific literature, the citation sentences are inherently informal, noisy and not well structured. Many citation sentences may contain information irrelevant to the target scientific paper. For these noisy sentences, we should not reconstruct them or reconstruct them at a little cost.

Based on the above analysis, we define the objective function of nonnegative linear reconstruction as follows:

                           
                              (13)
                              
                                 
                                    
                                       
                                          
                                             arg
                                             
                                                min
                                                
                                                   
                                                      a
                                                      i
                                                   
                                                   ,
                                                   
                                                      β
                                                   
                                                
                                             
                                             J
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                {
                                                
                                                   u
                                                   i
                                                
                                                ∥
                                                
                                                   x
                                                   i
                                                
                                                −
                                                
                                                   
                                                      X
                                                   
                                                   T
                                                
                                                
                                                   a
                                                   i
                                                
                                                
                                                   ∥
                                                   2
                                                
                                                +
                                                
                                                   ∑
                                                   
                                                      j
                                                      =
                                                      1
                                                   
                                                   N
                                                
                                                
                                                   
                                                      a
                                                      
                                                         i
                                                         j
                                                      
                                                      2
                                                   
                                                   
                                                      β
                                                      j
                                                   
                                                
                                                }
                                             
                                          
                                       
                                    
                                    
                                       
                                       
                                       
                                          
                                             +
                                             γ
                                             ∥
                                             
                                                β
                                             
                                             
                                                ∥
                                                1
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             s
                                             .
                                             t
                                             .
                                          
                                       
                                       
                                       
                                          
                                             
                                                β
                                                j
                                             
                                             ≥
                                             0
                                             ,
                                             
                                             
                                                a
                                                
                                                   i
                                                   j
                                                
                                             
                                             ≥
                                             0
                                             
                                             a
                                             n
                                             d
                                             
                                             
                                                a
                                                i
                                             
                                             ∈
                                             
                                                R
                                                N
                                             
                                             .
                                          
                                       
                                    
                                 
                              
                           
                        where ai
                         is reconstruction coefficient, ai
                         ≥ 0 means nonnegative linear reconstruction which allows only additive, not subtractive, combinations of the sentences. 
                           
                              
                                 β
                              
                              =
                              
                                 
                                    [
                                    
                                       β
                                       1
                                    
                                    ,
                                    …
                                    ,
                                    
                                       β
                                       N
                                    
                                    ]
                                 
                                 T
                              
                           
                         is an auxiliary variable controlling the candidate sentences selection. If 
                           
                              
                                 β
                                 j
                              
                              =
                              0
                              ,
                           
                         then all 
                           
                              
                                 a
                                 
                                    1
                                    j
                                 
                              
                              ,
                              …
                              ,
                              
                                 a
                                 
                                    N
                                    j
                                 
                              
                           
                         must be 0 which means the j-th candidate sentence is not selected. ui
                         is the weight of sentence i, which is the output of Algorithm 2.

This objective function implies that the optimal document summarization should reconstruct the original document with least error. On the meanwhile, each sentence’s weight ui
                         is considered as the degree of focus on sentences. The reconstruction error of sentences with high weight will be magnified, and by this means, important sentences will be priority.

By fixing ai
                         and setting the derivative of J with respect to 
                           β
                         to be zero, we can obtain the minimum solution of 
                           β
                        :

                           
                              (14)
                              
                                 
                                    
                                       β
                                       j
                                    
                                    =
                                    
                                       
                                          
                                             
                                                ∑
                                                
                                                   i
                                                   =
                                                   1
                                                
                                                N
                                             
                                             
                                                a
                                                
                                                   i
                                                   j
                                                
                                                2
                                             
                                          
                                          γ
                                       
                                    
                                 
                              
                           
                        Then let αij
                         be the Lagrange multiplier for constraint aij
                         ≥ 0 and 
                           
                              A
                              =
                              [
                              
                                 a
                                 
                                    i
                                    j
                                 
                              
                              ]
                              ,
                           
                         the Lagrange L is:

                           
                              (15)
                              
                                 
                                    
                                       
                                          L
                                       
                                       
                                          =
                                       
                                       
                                          
                                             J
                                             +
                                             T
                                             r
                                             [
                                             α
                                             
                                                
                                                   A
                                                
                                                T
                                             
                                             ]
                                          
                                       
                                    
                                    
                                       
                                       
                                          =
                                       
                                       
                                          
                                             T
                                             r
                                             
                                                [
                                                U
                                                
                                                   (
                                                   X
                                                   −
                                                   AX
                                                   )
                                                
                                                
                                                   
                                                      (
                                                      X
                                                      −
                                                      AX
                                                      )
                                                   
                                                   T
                                                
                                                +
                                                d
                                                i
                                                a
                                                g
                                                
                                                   
                                                      (
                                                      
                                                         β
                                                      
                                                      )
                                                   
                                                   
                                                      −
                                                      1
                                                   
                                                
                                                
                                                   
                                                      A
                                                   
                                                   T
                                                
                                                A
                                                ]
                                             
                                             +
                                             γ
                                             
                                                ∥
                                                
                                                   β
                                                
                                                
                                                   ∥
                                                   1
                                                
                                             
                                             +
                                             T
                                             r
                                             
                                                [
                                                α
                                                
                                                   
                                                      A
                                                   
                                                   T
                                                
                                                ]
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           U
                         is sentences’ weight matrix with diagonal entries of 
                           
                              
                                 u
                                 1
                              
                              ,
                              …
                              ,
                              
                                 u
                                 N
                              
                              ,
                           
                         and diag(
                           β
                        ) is a matrix with diagonal entries of 
                           
                              
                                 β
                                 1
                              
                              ,
                              …
                              ,
                              
                                 β
                                 N
                              
                           
                        . The derivative of L with respect of A is:

                           
                              (16)
                              
                                 
                                    
                                       
                                          ∂
                                          L
                                       
                                       
                                          ∂
                                          A
                                       
                                    
                                    =
                                    −
                                    2
                                    UX
                                    
                                       
                                          X
                                       
                                       T
                                    
                                    +
                                    2
                                    UAX
                                    
                                       
                                          X
                                       
                                       T
                                    
                                    +
                                    2
                                    A
                                    d
                                    i
                                    a
                                    g
                                    
                                       
                                          (
                                          
                                             β
                                          
                                          )
                                       
                                       
                                          −
                                          1
                                       
                                    
                                    +
                                    
                                       α
                                    
                                 
                              
                           
                        
                     

Setting the above derivative to be zero, 
                           α
                         can be represented as:

                           
                              (17)
                              
                                 
                                    
                                       α
                                    
                                    =
                                    2
                                    UX
                                    
                                       
                                          X
                                       
                                       T
                                    
                                    −
                                    2
                                    UAX
                                    
                                       
                                          X
                                       
                                       T
                                    
                                    −
                                    2
                                    A
                                    d
                                    i
                                    a
                                    g
                                    
                                       
                                          (
                                          
                                             β
                                          
                                          )
                                       
                                       
                                          −
                                          1
                                       
                                    
                                 
                              
                           
                        Using Karush–Kuhn–Tucker condition 
                           
                              
                                 α
                                 
                                    i
                                    j
                                 
                              
                              
                                 a
                                 
                                    i
                                    j
                                 
                              
                              =
                              0
                              ,
                           
                         we can iteratively obtain the aij
                        :

                           
                              (18)
                              
                                 
                                    
                                       a
                                       
                                          i
                                          j
                                       
                                    
                                    ←
                                    
                                       
                                          
                                             a
                                             
                                                i
                                                j
                                             
                                          
                                          
                                             
                                                (
                                                UX
                                                
                                                   
                                                      X
                                                   
                                                   T
                                                
                                                )
                                             
                                             
                                                i
                                                j
                                             
                                          
                                       
                                       
                                          
                                             [
                                             UAX
                                             
                                                
                                                   X
                                                
                                                T
                                             
                                             +
                                             A
                                             d
                                             i
                                             a
                                             g
                                             
                                                
                                                   (
                                                   
                                                      β
                                                   
                                                   )
                                                
                                                
                                                   −
                                                   1
                                                
                                             
                                             ]
                                          
                                          
                                             i
                                             j
                                          
                                       
                                    
                                 
                              
                           
                        
                     


                        Eqs. (14) and (18) are iteratively performed until convergence. The proof of convergence is illustrated in (3) based on He et al. (2012) and Sha, Lin, Saul, and Lee (2007). Furthermore, the convergence of parameter 
                           β
                         and a is illustrated experimentally from Figs. 6 and 7.

Finally amplified scientific paper’s abstract is generated through selecting sentences according to the vector 
                           
                              
                                 β
                              
                              =
                              
                                 
                                    [
                                    
                                       β
                                       1
                                    
                                    ,
                                    …
                                    ,
                                    
                                       β
                                       N
                                    
                                    ]
                                 
                                 T
                              
                           
                         in descending order. Through incorporating sentence’s weight into objective function, the amplified scientific paper’s abstract can strike a balance among coverage, redundancy and authority.

We present experiments to explore two main questions: What is the quality of amplified abstract generated using the proposed approach DWR, and how influencing factors affect the performance of DWR. To explore the first question, we conduct experiments to compare summarization performance with general document summarization baselines and similar-setting approaches. Recall of key-phrase is also applied to evaluate the performance between DWR and baselines. In addition, user study is provided to compare the quality between the amplified abstract and the original abstract. Moreover, we conduct case study, which shows three examples, to assess the quality of amplified abstracts. To explore the second question, influencing factors, including sub-component of DWR, topic models for semantic relationship identification, social information contribution and parameter sensitivity, are taken into consideration.

To evaluate our approach, we utilized two datasets: AAN dataset (Radev, Muthukrishnan, & Qazvinian, 2009) and Microsoft dataset.

                           
                              
                                 AAN data-set: It is a manually curated networked database of citations, collaborations, and summaries in the field of Computational Linguistics. Each scientific paper’s abstract and citation sentences can be crawled from AAN data-set site.
                                    1
                                 
                                 
                                    1
                                    
                                       http://clair.eecs.umich.edu/aan/index.php.
                                 
                              


                                 Microsoft data-set: It is collected from Microsoft Academic Search.
                                    2
                                 
                                 
                                    2
                                    
                                       http://academic.research.microsoft.com/.
                                 Each scientific paper’s abstract sentences, authors, venue and citation sentences were extracted, furthermore citation sentence’s attached paper, authors and venue were also collected.

For the above two datasets, we respectively selected 25 highly cited papers to form our evaluation dataset. To generate the gold standard summarization, five workers with background in Natural Language Processing were hired to read scientific paper’s abstract and corresponding citation sentences and then select fifteen salient sentences for each paper.

For AAN dataset, each scientific paper has 116 citation sentences on average, while in Microsoft dataset, each scientific paper has 296 citation sentences on average. For all data collections, Porter stemmer (Porter, 1980) is used to stem the text and stop words
                           3
                        
                        
                           3
                           
                              http://www.lextek.com/manuals/onix/stopwords1.html .
                         in general English are removed. The statistics about those two datasets is described in Tables 2 and 3. We have released the AAN dataset, Micro dataset, standard human generated summaries, source-code of DWR for publicly available.
                           4
                        
                        
                           4
                           
                              https://github.com/yss-miawptgm/DataWeightedReconstruction .
                        
                     

To evaluate our system, the ROUGE toolkit (Lin & Hovy, 2003) is adopted. ROUGE is the most commonly used metric that evaluates automatic summarization’s quality by counting overlapping units such as n-gram, word sequence, and word pairs between automatically generated summary and the gold standard summary. We choose ROUGE-N and ROUGE-L in our experiments. ROUGE-N is an n-gram recall between candidate summary and gold standard summary and ROUGE-L uses the longest common subsequence metric. ROUGE-N is computed as follows:

                           
                              (19)
                              
                                 
                                    R
                                    O
                                    U
                                    G
                                    E
                                    −
                                    N
                                    =
                                    
                                       
                                          
                                             ∑
                                             
                                                
                                                   
                                                      
                                                         S
                                                         ∈
                                                         {
                                                         R
                                                         }
                                                      
                                                   
                                                
                                             
                                          
                                          
                                             ∑
                                             
                                                g
                                                r
                                                a
                                                
                                                   m
                                                   n
                                                
                                                ∈
                                                S
                                             
                                          
                                          
                                             C
                                             o
                                             u
                                             n
                                             
                                                t
                                                
                                                   m
                                                   a
                                                   t
                                                   c
                                                   h
                                                
                                             
                                             
                                                (
                                                g
                                                r
                                                a
                                                
                                                   m
                                                   n
                                                
                                                )
                                             
                                          
                                       
                                       
                                          
                                             ∑
                                             
                                                S
                                                ∈
                                                
                                                   {
                                                   R
                                                   }
                                                
                                             
                                          
                                          
                                             
                                                ∑
                                                
                                                   g
                                                   r
                                                   a
                                                   
                                                      m
                                                      n
                                                   
                                                   ∈
                                                   S
                                                
                                             
                                             
                                                C
                                                o
                                                u
                                                n
                                                t
                                                
                                                   (
                                                   g
                                                   r
                                                   a
                                                   
                                                      m
                                                      n
                                                   
                                                   )
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where R denotes the gold standard summaries, n stands for the length of the n-gram, gramn
                        , and Countmatch
                        (gramn
                        ) is the maximum number of n-grams co-occurring in candidate summarization and gold standard summarization. Lin and Hovy (2003) show that the unigram-based ROUGE score(ROUGE-1) agrees with human judgement most.

The Student T-test is utilized to measure the significance of improvement. In our experiment, the null hypothesis is that the results generated by two systems are equivalent. The approaches are evaluated by using p < 0.05 and p < 0.01 as criterion for statistical significance.

We compare our approach with six state-of-the-art baselines described briefly as follows:

                           
                              
                                 Random: This method selects sentences randomly from the set of sentences and adds to the summarization.


                                 Centrality (Ribeiro & de Matos, 2011): The basic idea of Centrality summarizer is to select the most similar sentences to the original document. Sentence’s centrality is computed by comparing each candidate sentence to every other sentences. Here we define centrality as:

                                    
                                       (20)
                                       
                                          
                                             C
                                             e
                                             n
                                             t
                                             r
                                             a
                                             l
                                             i
                                             t
                                             y
                                             
                                                (
                                                x
                                                )
                                             
                                             =
                                             
                                                1
                                                N
                                             
                                             
                                                ∑
                                                
                                                   y
                                                   ≠
                                                   x
                                                
                                             
                                             
                                                s
                                                i
                                                m
                                                i
                                                l
                                                a
                                                r
                                                i
                                                t
                                                y
                                                (
                                                x
                                                ,
                                                y
                                                )
                                             
                                          
                                       
                                    
                                 where sentence similarity can be assessed by cosine similarity. Those scores are then used to create a ranked sentence list: sentences with highest scores are selected to compose the summarization.


                                 LexRank (Erkan & Radev, 2004): LexRank computes sentence’s importance based on the concept of eigenvector centrality in a graph representation of sentences and extracts the most important ones to include in the summarization. In this model, a connectivity matrix based on inter-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences.


                                 DivRank (Mei et al., 2010): It introduces the rich-gets-richer mechanism to PageRank style random walks with reinforcements on transition probabilities. After each step, the transition probabilities will be reinforced by the number of visits to each vertex. Let pT(u, v) be the transition probability from any sentence u to any sentence v at time T which satisfies

                                    
                                       (21)
                                       
                                          
                                             p
                                             T
                                             
                                                (
                                                u
                                                ,
                                                v
                                                )
                                             
                                             =
                                             
                                                (
                                                1
                                                −
                                                λ
                                                )
                                             
                                             
                                                p
                                                *
                                             
                                             
                                                (
                                                v
                                                )
                                             
                                             +
                                             λ
                                             
                                                
                                                   
                                                      p
                                                      0
                                                   
                                                   
                                                      (
                                                      u
                                                      ,
                                                      v
                                                      )
                                                   
                                                   
                                                      N
                                                      T
                                                   
                                                   
                                                      (
                                                      v
                                                      )
                                                   
                                                
                                                
                                                   
                                                      D
                                                      T
                                                   
                                                   
                                                      (
                                                      u
                                                      )
                                                   
                                                
                                             
                                          
                                       
                                    
                                 where

                                    
                                       (22)
                                       
                                          
                                             
                                                D
                                                T
                                             
                                             
                                                (
                                                u
                                                )
                                             
                                             =
                                             
                                                ∑
                                                
                                                   v
                                                   ∈
                                                   V
                                                
                                             
                                             
                                                p
                                                0
                                             
                                             
                                                (
                                                u
                                                ,
                                                v
                                                )
                                             
                                             
                                                N
                                                T
                                             
                                             
                                                (
                                                v
                                                )
                                             
                                          
                                       
                                    
                                 
                                 p*(v) is a distribution which represents the prior preference of visiting vertex v. p
                                 0(u, v) is the transition probability prior to any reinforcement and NT
                                 (v) is the number of times the walk has visited v up to time T.


                                 ClusterHITS (Wan & Yang, 2008): This method considers the theme clusters as hubs and the sentences as authorities. It makes use of sentence-to-cluster relationships to obtain sentence’s ranking score and finally picks top-ranked sentences into summarization.


                                 DSDR (He et al., 2012): Document summarization is modeled as a data reconstruction problem. This approach generates a summarization which consists of sentences that can best reconstruct the original document. It assumes that every sentence in the original document is equally important.


                                 CSum (Hu, Sun, & Lim, 2007): The proposed method first derives representative words from citation sentences and then selects sentences containing representative words. The representativeness score of a word Rep(wk
                                 ) is the combination of author-, quotation- and topic- measures. Sentence selection is performed through density-based selection (CSum
                                    DBS
                                 ) and summarization-based selection (CSum
                                    SBS
                                 ).


                                 OpinInt (Lu & Zhai, 2008): This approach first clusters sentences by leveraging semi-PLSA, where each clusters corresponds to different aspects. Then representative sentences are selected based on the similarity between the sentence and the cluster centroid.

@&#EXPERIMENTAL RESULTS@&#


                           Tables 4
                            and 5
                            show the R-measure and F-measure of ROUGE metric on AAN dataset and Microsoft dataset, respectively. Obviously, our proposed approach(DWR) demonstrates better than other methods in most situations, which indicates reconstructing the document by leveraging sentence’s weight is beneficial to summarization’s quality. On average two percent can be improved by our proposed approach. The only poor performance of DWR occurs at Rouge-2 scores on Microsoft dataset, which points out the direction for improvement of DWR. The different performance between AAN dataset and Microsoft dataset is because the different average number of citation sentence in those two datasets. More citation sentences are the indispensable foundation to generate meaningful amplified abstract, but they will introduce more noise, which increases the task’s difficulty.

Besides DWR, Centrality obtains the best performance compared to other methods. Centrality selects the most similar sentences to the original document, so this operation ensures the summarization’s representativeness and coverage. But we analyze this method performs weakly on summarization’s diversity, because sentences similar to the original document are similar to each other probably, which results in redundancy.

LexRank, DivRank and ClusterHits make similar performance, which are better than random selection. Those three approaches are all graph-based algorithms, which perhaps the reason for similar performance to some extent. ClusterHits considers the theme clusters as hubs and the sentences as authorities. The hubs and the authorities are interrelated and interacted on each other, which can improve the quality of summarization. LexRank computes sentence importance based on the eigenvector centrality in a graph representation of sentences while DivRank introduces the rich-gets-richer mechanisms to PageRank style random walks with reinforcements on transition probabilities.

Seen from Tables 6
                            and 7
                           , the proposed DWR achieves the best performance compared to approaches with similar setting, which indicates reconstructing the document by leveraging sentence’s weight is beneficial to summarization’s quality.

OpinInt performs better than CSum, which confirms the effectiveness of exploiting semantic similar relationship between sentences. CSumSBS
                            achieves better performance than CSumDBS
                           , which is probably because CSumSBS
                            favors sentences containing more representative words, while the distance measure between words in CSumDBS
                            is not very effective.

In order to analyze the impact of sentence’s weight learning part and data reconstruction part for the quality of amplified scientific paper’s abstract respectively, we separately compare our complete proposed method(DWR) with other two incomplete methods: (a) DWRrank
                           , which justly selects salient sentences based on sentence’s weight learned through step(b) in Fig. 1; (b) DWRconstruct
                           , that is DSDR (He et al., 2012), which justly assumes sentence’s weight to be equal and selects salient sentences through data reconstruction.


                           Tables 8
                            and 9
                            show that the complete DWR significantly out-performed DWRrank
                            and DWRconstruct
                            other than Rouge-2 metric on Microsoft dataset. This result illustrates that sentence’s weight and data reconstruction are both indispensable for a good amplified scientific paper’s abstract. The sentence’s weight learned from the bibliographic network can heavily improve performance of the proposed approach steadily. Especially to generate amplified scientific paper’s abstract, each sentence’s authority is an important consideration other than the summary’s coverage. In contrast to the complete proposed approach, DWRconstruct
                            assumes each sentence to be equally important, which reduces the authority of the amplified abstract. On the other hand, DWRrank
                            cannot guarantee good performance on summary as well, which is hampered by the redundancy among sentences.

In DWR, the semantic relationship identification plays a critical role in evaluating the weight of sentences. We investigate three topic models, including semiPLSA, supervised LDA (sLDA) (Blei & McAuliffe, 2007) and labeled LDA (labeledLDA) (Ramage, Hall, Nallapati, & Manning, 2009), to identify the semantic relationship between sentences, and compare their influence to the performance of DWR.


                           Tables 10
                            and 11
                            shows that semiPLSA performs slightly better than supervised LDA and labeled LDA for both datasets. But the performance of these three topic models is similar, which illustrates the effectiveness of topic models for identifying semantic relationship.

In the experiment, the sentences from original abstract are deemed as observed or labeled documents for supervised LDA and labeled LDA. In semiPLSA, the original abstract is deemed as priors for topics, which can be seen as a weak supervision process. However, in supervised LDA and labeled LDA, the original abstract is utilized as training data, which can be seen as standard supervision. So the sparsity of training data probably hampers the performance of these two models.

To explore the contributions of different types of relations to the performance of DWR, we investigate the performance of DWR over four different subsets of relations. The first subset only contains the Sentences-similar-Sentences relations (i.e., E
                           4), which models the primary semantic relevance between sentences. The second subset contains Sentences-similar-Sentences relations and paper information (i.e., E
                           3, E
                           7). The third subset contains Sentences-similar-Sentences relations, paper information, and conference information (i.e., E
                           1, E
                           6). The fourth subset contains Sentences-similar-Sentences relations, paper information, and author information (i.e., E
                           2, E
                           5, E
                           8).


                           Tables 12
                            and 13
                            show that all types of relations make contributions to the performance of DWR, especially the Sentences-similar-Sentences relations, which lays the foundation of DWR. As can been seen, there is slight improvement by using conference information (i.e., E
                           1, E
                           6). Perhaps it’s because the conference information between scientific papers is coarse-grained and cannot well model the relevance between papers.

We further give parameter sensitivity analysis for our proposed method. We show how sensitive our results are with respect to the parameters: summarization length, μ and γ.

To evaluate the influence of summarization length to its quality, we generated summarization with different length. In Fig. 2
                           , we show how ROUGE-1 varies with respect to summary length. It illustrates that more words contained in the summary makes better performance. This can be easily explained that more words can convey more information from the original sentence set. But the speed of improvement is slowing. It’s worth noting that DWR can always outperform the other approaches with respect to different summarization length, which illustrates the robustness of DWR.

The parameter μ of Eq. (9) controls the balance of inter-similarity of vertices in the hypergraph and the difference between the final ranking score and the initial score. The inter-similarity of vertices plays the regularization role for vertices’ ranking on hypergraph. It enables vertices in the same hyperedge should be assigned close ranking scores. In our experiment, the target scientific paper’s vertices’ initial ranking scores are set to 1, and others are set to 0. In Fig. 3
                           , we can see that the performance fluctuates within a limited range as we vary μ and μ > 0.5 makes gently better performance. This means the initial ranking score is relatively important to summarization. We believe it is because the sentences from abstract are more well-structured and less noisy than the citation sentences.

The parameter γ of Eq. (13) controls the balance of reconstruction error and sparsity of variable 
                              β
                            controlling the candidate sentences selection. In Fig. 4
                           
                           
                           
                            we see that the performance is pretty stable as we vary γ.

We find the trend for ROUGE-2 and ROUGE-L is similar so we leave out the figures for them.

We test the coverage of points of interest in our automatically generated amplified abstract versus the other approaches. To this end, we first asked two workers to identify key-phrases based on the target scientific paper’s abstract. Considering the identified key-phrases are different from the standard named entity and concept, these key-phrases are often recombination or modified by different authors, we then split those key-phrases into key words as the points of interest. The number of identified key words in AAN dataset and Microsoft dataset is 26 and 12 for each paper on average. We then check the percentage of these key words covered in the generated text fragment and report these recall scores in Table 14
                           . We can see that our method performs better than other methods, showing that the amplified abstract dose embody more potential points of interest.

We randomly select 20 scientific papers from those two datasets and employ five workers to judge the quality of text fragments generated by different methods from different perspectives. Similar to Hu and Wan (2014), the judges were asked to read the target scientific paper’s abstract in detail first and then answer the following questions by giving a rating on a scale of 1 to 5 for the generated text fragment(5 means very good, 1 means very bad):

                              
                                 1.
                                 The level of the generated text fragment’s content consistence to the subject of the original abstract.

The level of the generated text fragment’s content extension to the subject of the original abstract.

The level of the generated text fragment’s overall quality.

From Table 15
                           , we can see DWR performs better than other baselines from all perspectives. Especially, in the content consistence and overall quality, the improvement is significant. In content extension, the improvement is indeed, but there is room for improvement.

In Table 16
                           , we give the relative frequency of sentences selected from the original abstract and citing papers. From the result, we can see citation sentences can effectively amplify the quality of original abstract, and extend the information content. On the other hand, considering the relative size between original abstract sentences and citation sentences, original abstract still plays primary role in the amplified abstract, which illustrates DWR can guarantee the content consistence.
                           
                           
                        

In this section, we report three case studies of the proposed approach. Three automatically generated amplified abstract are illustrated in Figs. 8–10. In those tables, the bold italic sentences are selected from the target scientific paper’s abstract, while the others are selected from citation sentences. The complete target scientific paper’s abstract can be found based on it’s title. The purple phrases labeled by authors are informative entities or concepts, which can be utilized to represent the sentences .

From those three amplified abstracts, four key facts can be concluded.

Firstly, the amplified abstract is not only consistent with the original abstract about the subject of interest, but also expand the related topics of the subject.

For Fig. 8, contour spectrum and user interface are the subject of interest in original abstract. Obviously, those two terms are argued several times in the amplified abstract. Furthermore, the additional phrases, for example:

                              
                                 •
                                 user interaction, surface area, visual abstraction, iso-contours, contour tree, contour attributes iso-surfaces, iso-value definition, iso-value selection, geometric properties, topological relationships

are also closely related to the subject of interest.

For Fig. 9, the text-to-text similarity is the main interested subject of the original abstract. The representative phrases word-to-word, text similarity, semantic similarity and knowledge-based are both frequently occurred in the original abstract and the amplified abstract. Other representative phrases, like:

                              
                                 •
                                 lexical matching, lexical similarity, WordNet-based metrics, ontological relations, concepts and words, text fragments, concept vector quantitative measurement, similarity function, distributional distance, heuristic formulas, WBOW, SVM classifier and BOW

are also favorite entities related to the main subject.

For Fig. 10, the dependency parsers and margin multi-class training are the central phrases of the original abstract, which are semantic similar to the representative phrases in the amplified abstract, such as:

                              
                                 •
                                 data-driven dependency parsing, discriminative parsing, Bohnet-Parser, graph-based parsing, BerkeleyParser, transition-based parsers, dependency accuracy, sequential labeling, chunking, inductive inference, dynamic programming(DP), MERT, greedy deterministic search, beam search

Secondly, through comparing to the original abstract, some sentences which take as background role are discarded. Take Figs. 8–10 as an example, the number of sentences in original abstract are 6, 3 and 2, while the number of sentences retained in the corresponding amplified abstract are 3, 3, 2.

Thirdly, the order of sentences selected from the original abstract will be adjusted based on their contained quantity of information. In Fig. 9, the order of the top ranked three sentence I, II and III in the original abstract is {3, 2, 1}. Through analysis of representative phrases in those three sentences, the quantity of information contained in sentence I is obviously more than that of sentence II and III.

Lastly, the selected abstract sentences and citation sentences are mostly salient sentences which contain informative phrases, except few noisy sentences, like sentence X in Fig. 10, which contains no meaningful phrases.

In short, the automatically generated amplified abstract is more well-structured, content-rich and comprehensive, which can provide a good guide for researchers.

@&#CONCLUSIONS@&#

This paper explores the impact of heterogeneous bibliographic network for scientific paper’s amplified abstract. The impact is embodied in the sentence’s weight through regularization framework for ranking on hypergraph. According to the learned sentence’s weight, a data-weighted reconstruction function is proposed to assign different priority to sentences when reconstructing the original sentence set. The experimental results on realistic datasets demonstrate the good effectiveness of our proposed approach. So this approach can be applied to enable users to discover the material they need in increasingly large collections with less effort.

@&#ACKNOWLEDGMENTS@&#

This work is supported by the National Natural Science Foundation of China (No. 61103099), the Fundamental Research Funds for the Central Universities (No. 2014QNA5008) and Chinese Knowledge Center of Engineering Science and Technology (No. CKCEST-2015-2-5).

We first introduce an auxiliary function as:

                        
                           (A.1)
                           
                              
                                 G
                                 
                                    (
                                    λ
                                    ,
                                    
                                       a
                                       i
                                    
                                    )
                                 
                                 =
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    n
                                 
                                 
                                    {
                                    
                                       
                                          
                                             (
                                             P
                                             
                                                a
                                                i
                                             
                                             )
                                          
                                          j
                                       
                                       
                                          a
                                          
                                             i
                                             j
                                          
                                       
                                    
                                    
                                       λ
                                       j
                                       2
                                    
                                    −
                                    2
                                    
                                       
                                          (
                                          U
                                          X
                                          
                                             X
                                             T
                                          
                                          )
                                       
                                       
                                          i
                                          j
                                       
                                    
                                    
                                       λ
                                       j
                                    
                                    }
                                 
                              
                           
                        
                     where 
                        
                           P
                           =
                           U
                           X
                           
                              X
                              T
                           
                           +
                           d
                           i
                           a
                           g
                           
                              
                                 (
                                 β
                                 )
                              
                              
                                 −
                                 1
                              
                           
                           ,
                        
                      and 
                        
                           λ
                           =
                           
                              
                                 [
                                 
                                    λ
                                    1
                                 
                                 ,
                                 …
                                 ,
                                 
                                    λ
                                    n
                                 
                                 ]
                              
                              T
                           
                        
                      is a positive vector. G(λ, ai
                     ) can also be identified as the sum of singular-variable functions:

                        
                           (A.2)
                           
                              
                                 G
                                 
                                    (
                                    λ
                                    ,
                                    
                                       a
                                       i
                                    
                                    )
                                 
                                 =
                                 
                                    ∑
                                    
                                       j
                                       =
                                       1
                                    
                                    n
                                 
                                 
                                    G
                                    j
                                 
                                 
                                    (
                                    
                                       λ
                                       j
                                    
                                    )
                                 
                              
                           
                        
                     Let 
                        
                           F
                           
                              (
                              
                                 a
                                 i
                              
                              )
                           
                           =
                           
                              a
                              i
                              T
                           
                           P
                           
                              a
                              i
                           
                           −
                           2
                           
                              
                                 (
                                 U
                                 X
                                 
                                    X
                                    T
                                 
                                 )
                              
                              
                                 i
                                 *
                              
                           
                           
                              a
                              i
                           
                           ,
                        
                      
                     Sha et al. (2007) have proved that if aij
                      update as:

                        
                           (A.3)
                           
                              
                                 
                                    a
                                    
                                       i
                                       j
                                    
                                 
                                 ←
                                 arg
                                 
                                    min
                                    
                                       λ
                                       j
                                    
                                 
                                 
                                    G
                                    j
                                 
                                 
                                    (
                                    
                                       λ
                                       j
                                    
                                    )
                                 
                              
                           
                        
                     
                     G(λ, ai
                     ) converges monotonically to the global minimum of F(ai
                     ).

Taking the derivation of Gj
                     (λj
                     ) with respect to λj
                      and setting it to be zero, we obtain the updating formulation as:

                        
                           (A.4)
                           
                              
                                 
                                    a
                                    
                                       i
                                       j
                                    
                                 
                                 ←
                                 
                                    
                                       
                                          a
                                          
                                             i
                                             j
                                          
                                       
                                       
                                          
                                             (
                                             U
                                             X
                                             
                                                X
                                                T
                                             
                                             )
                                          
                                          
                                             i
                                             j
                                          
                                       
                                    
                                    
                                       
                                          [
                                          U
                                          A
                                          X
                                          
                                             X
                                             T
                                          
                                          +
                                          A
                                          d
                                          i
                                          a
                                          g
                                          
                                             
                                                (
                                                β
                                                )
                                             
                                             
                                                −
                                                1
                                             
                                          
                                          ]
                                       
                                       
                                          i
                                          j
                                       
                                    
                                 
                              
                           
                        
                     which agrees with Eq. (18).

We can rewrite the objective function J as:

                        
                           (A.5)
                           
                              
                                 J
                                 =
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    n
                                 
                                 F
                                 
                                    (
                                    
                                       a
                                       i
                                    
                                    )
                                 
                                 +
                                 T
                                 r
                                 
                                    [
                                    U
                                    X
                                    
                                       X
                                       T
                                    
                                    ]
                                 
                                 +
                                 γ
                                 ∥
                                 β
                                 
                                    ∥
                                    1
                                 
                              
                           
                        
                     Fixing β, we can obtain the minimizer of J by minimizing each F(ai
                     ) separately. Since the objective function J is the sum of all the individual terms F(ai
                     ) plus a term independent of ai
                     , we have shown that J is non-increasing with fixed β under the updating rule as Eq. (18).

@&#REFERENCES@&#

