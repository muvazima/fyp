@&#MAIN-TITLE@&#Gene expression microarray classification using PCA–BEL

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           The computational complexity of BEL model is O(n).


                        
                        
                           
                           BEL is a suitable model for high dimensional feature vector classification.


                        
                        
                           
                           In this paper, BEL is applied for the classification tasks of gene-expression microarray data.


                        
                        
                           
                           Proposed method improves the detection accuracy of SRBCT, HGG and lung cancer.


                        
                        
                           
                           Our method has been able to effect a 30.18% improvement on HGG classification.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Amygdala

BEL

Emotional neural network

Cancer

BELBIC

Diagnosis

Diagnostic method

@&#ABSTRACT@&#


               
               
                  In this paper, a novel hybrid method is proposed based on Principal Component Analysis (PCA) and Brain Emotional Learning (BEL) network for the classification tasks of gene-expression microarray data. BEL network is a computational neural model of the emotional brain which simulates its neuropsychological features. The distinctive feature of BEL is its low computational complexity which makes it suitable for high dimensional feature vector classification. Thus BEL can be adopted in pattern recognition in order to overcome the curse of dimensionality problem. In the experimental studies, the proposed model is utilized for the classification problems of the small round blue cell tumors (SRBCTs), high grade gliomas (HGG), lung, colon and breast cancer datasets. According to the results based on 5-fold cross validation, the PCA–BEL provides an average accuracy of 100%, 96%, 98.32%, 87.40% and 88% in these datasets respectively. Therefore, they can be effectively used in gene-expression microarray classification tasks.
               
            

@&#INTRODUCTION@&#

Every cell in our body contains a number of genes that specify the unique features of different types of cells. The gene expression of cells can be obtained by DNA microarray technology which is capable of showing simultaneous expressions of tens of thousands of genes. This technology is widely used to distinguish between normal and cancerous tissue samples and support clinical cancer diagnosis [27]. There are certain challenges facing classification of gene expression in cancer diagnosis. The main challenge is the huge number of genes compared to the small number of available training samples [47]. Microarray learning data samples are typically gathered from often less than one hundred of patients, while the number of genes in each sample is usually more than thousands of genes. Furthermore, microarray data contain an abundance of redundancy, missing values [7] and noise due to biological and technical factors [25,75]. In the literature, there are two general approaches to these issues including feature selection and feature extraction. A feature selection method selects a feature subset from the original feature space and provides the marker and causal genes [9,4,1] which are able to identify cancers quickly and easily. However, feature extraction methods, normally transforms the original data to other spaces to generate a new set of features containing high information packing properties. In each of these two approaches, the reduced features are applied by a proper classifier to diagnosis. A proper classifier increases the accuracy of detection and can influence the feature reduction step.

This paper aims to review these approaches, investigate the recently developed methodology and propose a proper feature reduction-classification method for cancer detection. The organization of the paper is as follows: feature selection methods are reviewed in Section 1.1. Section 1.2 explains the feature extraction methods and Section 2 offers the proposed method. Experimental results on cancer classification are evaluated in Section 3. Finally, conclusions are made in Section 4.

Researchers have developed various feature selection methods for classification. Feature selection methods are categorized into three techniques including the filter model [62], wrapper model and embedded model [19]. The filter model considers feature selection and classifier׳s learning as two separate steps and utilizes the general characteristics of training data to select features. The filter model includes both traditional methods which often evaluate genes separately and new methods which consider gene-to-gene correlation. These methods rank the genes and select top ranked genes as input features for the learning step. The gene ranking methods need a threshold for the number of genes to be selected. For example Golub et al. [20] proposed the selection of the top 50 genes. Additionally the filter model needs a criterion to rank the genes. Liu et al. [35] and Golub et al. [20] have investigated some filter methods based on statistical tests and information gain. Examples of the filter criterion include Pearson correlation coefficient method [84], t-statistics method [2] and signal-to-noise ratio method [20]. The time complexity of these methods is O(N) where N shows the dimensionality. It is efficient but they cannot remove redundant genes, the issue studied in recent literature [83,78,14,26,37].

In the wrapper model, a subset is selected and then the accuracy of a predetermined learning algorithm is predicted to determine the properness of a selected subset. In the wrapper model of Xiong et al. [83], the selected subsets learn through three learning algorithms including; linear discriminant analysis, logistic regression and support vector machines. These classifiers should be run for every subset of genes selected from the search space. This procedure has a high computational complexity. Like the wrapper methods, in the embedded models, the genes are selected as part of the specific learning method but with lower computational complexity [19]. The subset selection methods of wrapper model can be categorized into the population-based methods [71,34,53] and backward selection methods. Recently Lee and Leu [34], and Tong and Schierz [69] shed light on the effectiveness of the hybrid model in feature selection. The elements of a hybrid method include Neural Network (NN), Fuzzy System, Genetic Algorithm (GA; [76,23]) and Ant Colony [79]. Lee and Leu [34] examined the GA׳s ability in the feature selection. Furthermore, the abilities of fuzzy theories have been successfully applied by many researchers [12,72,10]. Tong and Schierz [69] used a genetic algorithm-Neural Network approach (GANN) as a wrapper model. The feature subset extraction is performed by GA and then the extracted subset is applied to learn the NN. These processes are repeated until the best subset is determined. Because of the high dimension data, the GA looks to be a proper strategy for feature selection.

In the literature, there are two well-known methods for feature extraction including principal component analysis (PCA; [78]) and linear discriminant analysis (LDA; [48]). They normally transform the original feature space to a lower dimensional feature transformation methods. PCA transforms the original data to a set of reduced feature that best approximate the original data. In the first step, PCA calculates the data covariance matrix and then finds the eigenvalues and the eigenvectors of the matrix. Finally it goes through a dimensionality reduction step. According to the final step, the only terms corresponding to the K largest eigenvalues are kept.

In contrast to the PCA, first LDA calculates the scatter matrices including a within-class scatter matrix for each class, and the between-class scatter matrix. The within-class scatter matrix is measured by the respective class mean, and within-class scatter matrix measures the scatter of class means around the mixture mean. Then LDA transforms the data in a way that maximizes the between-class scatter and minimizes the within-class scatter. So the dimension is reduced and the class separability is maximized.

The feature extraction/selection method is the first step in gene expression microarray classification and cancer detection. The second step consists of a classifier learning the reduced features. In the literature, various classifiers have been investigated in order to find the best classifier. It seems that the NN and various types of NN [29,36,57,6,56,68,74,81,69,16], k nearest neighbors [61,13], k-means algorithms [32], Fuzzy c-means algorithm [11], bayesian networks [4], vector quantization based classifier [59], manifold methods [18,80], fuzzy approaches [54,58,30,60], complementary learning fuzzy neural network [64–67], ensemble learning [55,8,27,50], logistic regression, support vector machines [22,5,82,73,63,46,70], LSVM [44], wavelet transform [28] as well as radial basis-support vector machines [51] have been investigated successfully in classification and cancer detection. But the recently developed classifiers such as brain emotional learning (BEL) networks [42] have not been examined in this field.

BEL networks are recently developed methodologies that use simulated emotions to aid their learning process. BEL is motivated by the neurophysiological knowledge of the human׳s emotional brain. In contrast to the published models, the distinctive features of the BEL are low computational complexity and fast training which make it suitable for high dimensional feature vector classification. In this paper, BEL is developed and examined for gene expression microarray classification tasks. It is expected that a model with low computational complexity can be more successful in solving the challenges of high dimensional microarray classification.


                     
                     Fig. 1 shows the general view of the proposed methods and the final proposed algorithm is presented in 
                     Fig. 2. In the proposed framework, what׳s different from published diagnostic methods is the application of BEL model to cancer classification. There are various versions of BEL, including basic BEL [3], BELBIC (BEL based intelligent controller; [45]), BELPR (BEL based pattern recognizer; [39]), BELPIC (BEL based picture recognizer; [43]) and supervised BEL [38,40–42]. They are learning algorithms of emotional neural networks [42]. These models are inspired by the emotional brain. The description of the relationship between the main components of emotional brain is common among all these models. What differs from one model to another is how they formulate the reward signal in the learning process. For example in the model presented by Balkenius and Morén [3], it is not clarified how the reward is assigned. In the BELBIC, the reward signal is defined explicitly and the formulization of other equations is formed accordingly. However, the supervised BEL employs the target value of input pattern instead of the reward signal in the learning phase. So supervised BEL is model free and can be utilized in different applications and here, this version is developed for gene expression microarray classification task. Generally the computational complexity of BEL is very low [39–42]. It is O(n) that make it suitable to use in high dimensional feature vector classification. BEL [42] is inspired by the interactions of thalamus, amygdala (AMYG) [15,17,21,24,31,33,77], orbitofrontal cortex (OFC) and sensory cortex in the emotional brain [42].

The first step is associated with PCA dimension reduction (Fig. 1). Consider the first k-principle components p
                     1, p
                     2,…, p
                     
                        k
                     , they are the outputs of the first step and the inputs of second step. In the second step, this pattern should be normalized between [0 1]. The normalized k-principle components p
                     
                        1
                     , p
                     
                        2
                     ,…, p
                     
                        k
                      are outputs of the second step and the inputs of thirds step. Fig. 2 illustrates the details of the proposed method. The input pattern of BEL is illustrated by vector p
                     1, p
                     2,…, p
                     
                        k
                      and the E is the final output. The model consists of two main subsystems including AMYG and the OFC. The AMYG receives the input pattern including: p
                     1, p
                     2,…, p
                     
                        k
                      from the sensory cortex, and p
                     
                        k+1 from the thalamus. The OFC receives the input pattern including p
                     1, p
                     2,…, p
                     
                        k
                      from the sensory cortex only. The p
                     
                        k+1 calculated by following formula:
                        
                           (1)
                           
                              
                                 
                                    p
                                 
                                 
                                    k
                                    +
                                    1
                                 
                              
                              =
                              
                                 
                                    max
                                 
                                 
                                    j
                                    =
                                    1..
                                    .
                                    k
                                 
                              
                              (
                              
                                 
                                    p
                                 
                                 
                                    j
                                 
                              
                              )
                           
                        
                     The v
                     
                        k+1 is related to AMYG weight and the w
                     
                        k+1 is related to OFC weight. The E
                     
                        a
                      is the internal output of AMYG which is used to adjust the plastic connection weights v
                     1, v
                     2,…, v
                     
                        k+1 (Eq. (6)). The E
                     
                        o
                      is the output of OFC which is used to inhibit the AMYG output. This inhibitory task is implemented by subtraction of E
                     
                        o
                      from E
                     
                        a
                      (Eq. (5)). As the corrected AMYG response, E is the final output node. It is evaluated by monotonic increasing activation function tansig and used to adjust OFC connection weights including w
                     1, w
                     2,…, w
                     
                        k+1 (Eq. (7)). The activation function is as follows:
                        
                           (2)
                           
                              tan
                              
                              s
                              i
                              g
                              (
                              x
                              )
                              =
                              
                                 2
                                 
                                    1
                                    +
                                    
                                       
                                          e
                                       
                                       
                                          −
                                          2
                                          x
                                       
                                    
                                 
                              
                              −
                              1
                           
                        
                     
                  

The AMYG, OFC and the final output are simply calculated by following formulas respectively:
                        
                           (3)
                           
                              
                                 
                                    E
                                 
                                 
                                    a
                                 
                              
                              =
                              
                                 ∑
                                 
                                    j
                                    =
                                    1
                                 
                                 
                                    k
                                    +
                                    1
                                 
                              
                              
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       j
                                    
                                 
                                 ×
                                 
                                    
                                       p
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 +
                                 b
                                 a
                              
                           
                        
                     
                     
                        
                           (4)
                           
                              
                                 
                                    E
                                 
                                 
                                    o
                                 
                              
                              =
                              
                                 ∑
                                 
                                    j
                                    =
                                    1
                                 
                                 k
                              
                              
                                 (
                                 
                                    
                                       w
                                    
                                    
                                       j
                                    
                                 
                                 ×
                                 
                                    
                                       p
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 +
                                 b
                                 o
                              
                           
                        
                     
                     
                        
                           (5)
                           
                              E
                              =
                              
                              tan
                              
                              s
                              i
                              g
                              (
                              
                                 
                                    E
                                 
                                 
                                    a
                                 
                              
                              −
                              
                                 
                                    E
                                 
                                 
                                    o
                                 
                              
                              )
                           
                        
                     
                  

Let t be target value associated to nth pattern (p). The t should be binary encoded. So the supervised learning rules are as follows:
                        
                           (6)
                           
                              
                                 
                                    v
                                 
                                 
                                    j
                                 
                              
                              =
                              
                                 
                                    v
                                 
                                 
                                    j
                                 
                              
                              +
                              l
                              r
                              ×
                              
                              max
                              (
                              t
                              −
                              
                                 
                                    E
                                 
                                 
                                    a
                                 
                              
                              ,
                              0
                              )
                              ×
                              
                                 
                                    p
                                 
                                 
                                    j
                                 
                              
                              
                              f
                              o
                              r
                              
                              j
                              =
                              1
                              …
                              k
                              +
                              1
                           
                        
                     
                     
                        
                           (7)
                           
                              
                                 
                                    w
                                 
                                 
                                    j
                                 
                              
                              =
                              
                                 
                                    w
                                 
                                 
                                    j
                                 
                              
                              +
                              l
                              r
                              ×
                              (
                              
                                 
                                    E
                                 
                                 
                                    a
                                 
                              
                              −
                              
                                 
                                    E
                                 
                                 
                                    o
                                 
                              
                              −
                              t
                              )
                              ×
                              
                                 
                                    p
                                 
                                 
                                    j
                                 
                              
                              
                              f
                              o
                              r
                              
                              j
                              =
                              1
                              …
                              k
                              +
                              1
                           
                        
                     
                     
                        
                           (8)
                           
                              b
                              a
                              =
                              b
                              a
                              +
                              l
                              r
                              ×
                              
                              max
                              (
                              t
                              −
                              
                                 
                                    E
                                 
                                 
                                    a
                                 
                              
                              ,
                              0
                              )
                           
                        
                     
                     
                        
                           (9)
                           
                              b
                              o
                              =
                              b
                              o
                              +
                              l
                              r
                              ×
                              (
                              
                                 
                                    E
                                 
                                 
                                    a
                                 
                              
                              −
                              
                                 
                                    E
                                 
                                 
                                    o
                                 
                              
                              −
                              t
                              )
                           
                        
                     where lr is learning rate, t is binary target and 
                        t
                        −
                        
                           
                              E
                           
                           
                              a
                           
                        
                      is calculated error, ba is the bias of AMYG neuron and bo is the bias of OFC neuron. The v
                     1, v
                     2,…, v
                     
                        k+1 AMYG are learning weights and w
                     1, w
                     2,…, w
                     
                        k+1 are OFC learning weights. Eqs. (3)–(9) show the multiple-inputs single-output model. In 
                     Figs. 2 and 3, the equations are extended to multiple-inputs multiple-outputs usage. The input training microarray data in Fig. 2 includes the two matrices of P and T. The size of the matrix P is m×s where m is the number of patterns and s is the number of features in each pattern (s⪢k). The size of the matrix T is m×c where c is the number of classes. The targets are encoded binary. So each row of matrix T includes only one “1” and other columns are “0”. In the flowcharts, p
                     
                        i
                      denotes the ith pattern and t
                     
                        i
                      is related target.

The learning rate lr can be adaptively adjusted to increase the performance. The final flowchart, Fig. 2, shows this adaptation and the related parameters including the ratio to increase the learning rate (lr_inc) initialized with 1.05, the ratio to decrease learning rate (lr_dec) with the initial value 0.7, the maximum performance increase (minc) with initial the value 1.04, first performance (perf_f; in step 4 of the flowchart) and last performance (perf_l) which can be calculated as MSE. The initial lr=0.001, the learning weights are initialized randomly (step 3 in the flowchart.) and according to the algorithm, if (perf_l/perf_f)>minc then lr=lr×lr_dec, else if (perf_l<perf_f), lr=lr×lr_inc. In the Fig. 2, the stop criterion is to reach a determined learning epoch. The stop criterion can be the maximum epoch, which means the maximum number of epochs has been reached (for example 10,000 epochs). Fig. 2 presents the learning step and Fig. 3 shows the flowchart of the testing step. The inputs of the algorithm presented in Fig. 3 are a testing pattern, number of classes and the weights adjusted in the learning step. The last step of the algorithm is associated to the diagnosis where the index of the maximum E shows the class number of the pattern.

The source code of the proposed method is accessible from http://www.bitools.ir/tprojects.html and it is evaluated to classify the gene expression microarray data of 4-class complementary DNA (cDNA) microarray dataset of the small round blue cell tumors (SRBCTs), high grade gliomas (HGG), lung, colon and breast cancer datasets. The SRBCTs dataset is a 4-class cDNA microarray data and contains 2308 genes and 83 samples including 29 samples in Ewing׳s sarcoma (EWS), 25 in rhabdomyosarcoma (RMS), 18 in neuroblastoma (NB) and 11 in Burkitt lymphoma (BL). This data set can be obtained from http://research.nghri.nih.gov/microarray/Supplement/. In the proposed algorithm, the maximum learning epoch=10,000, k=100 and initial lr is set at 0.001, 0.000001 and 0.001 for SRBCT, HGG and lung cancer datasets respectively. These parameters are picked empirically. The value k=100 and lr=0.001 and 0.000001 can show better results for these datasets. However, in other applications these parameters should be optimized.

The HGG dataset applied here, consist of 50 samples with 12,625 genes including 14 classic glioblastomas, 14 non-classic glioblastomas, 7 classic anaplastic oligodendrogliomas and 15 non-classic anaplastic oligodendrogliomas. HGG dataset is accessible from http://www.broadinstitute.org. In this dataset, the number of patterns much less than the number of the features in each sample and it may be difficult for classification methods to classify the data.

In the lung cancer dataset, there are 181 tissue samples in two classes: 31 points are malignant pleural mesothelioma and 150 points are adenocarcinoma. Each sample is described by 12,533 genes. This data set is also accessible from http://datam.i2r.a-star.edu.sg/datasets/. Other datasets, applied here, are colon and breast cancer datasets that are accessible from http://genomics-pubs.princeton.edu/oncology/affydata/index.html and http://datam.i2r.a-star.edu.sg/datasets/krbd/BreastCancer/BreastCancer.html, respectively. Colon dataset includes 62 tissue samples with 2000 genes and the breast cancer dataset consist of 97 samples and 24,481 genes.

Here and prior to entering comparative numerical studies, let us analyze the computational complexity of the proposed BEL. Regarding the learning step, the algorithm adjust O(2n) weights for each pattern-target sample, where n is the number of input attributes (for example for HGG database n=12,625). Let us compare the computational complexity with traditional neural networks and a supervised orthogonal discriminant projection classifier (SODP; [80]) applied in cancer detection. As mentioned above, the computational complexity of the proposed classifier is O(n). In contrast, computational time is O(cn) for neural network and it is O(n
                     2) for SODP. In NN architecture, c is the number of hidden neurons (generally c=10) and SODP uses a Lagrangian multiplier that imposes the complexity of O(n
                     2). So the proposed method has a lower computational complexity. This improved computing efficiency can be important for high dimensional feature vector classification and cancer detection. The key to the proposed method is the fast processing resulting from low computational complexity that makes it suitable for cancer detection.

Another important point which is observed across in the experimental implementations is that the results of the proposed model can change by changing the initial lr and k values. lr indicates the learning rate and k specifies the number of PCA׳s initial k component in the algorithm. In other words, the value of lr and k should be optimized in each problem. Here, the optimum values of 0.001, 0.000001, 0.001, 0.00001 and 0.000001 are assigned to lr and 100 to k for SRBCT, HGG, lung, colon and breast cancers respectively. The values assigned to lr are obtained from 0.1, 0.001, 0.0001, 0.00001… 0.0000000001 and in the case of k from the values of 10, 50 and 100 through implementation and observation.

The proposed method is compared with the results of the methods which have been reported by Zhang and Zhang [80]. They have reported the results based on the 5-fold cross validation method. This implementation can result in the assessment of accuracy and repeatability and it can be used to validate the proposed method [46]. The compared methods include supervised locally linear embedding (SLLE), probability-based locally linear embedding (PLLE), locally linear discriminant embedding (LLDE), constrained maximum variance mapping (CMVU), orthogonal discriminant projection (ODP) and supervised orthogonal discriminant projection (SODP).

These methods are extended manifold approaches that have been successfully used in tumor classification. SLLE, PLLE and LLDE are extended versions of the locally linear embedding (LLE) that is a classical manifold method. SODP is an extended version of ODP and CMVU is a linear approximation of multi-manifolds learning method.


                     
                     
                     
                     
                     
                     Figs. 4–8 show the comparative results based on average accuracy of 5-fold cross validation. As illustrated in the figures, the proposed model shows consistent results and provides higher performance in SRBCT, HGG and Lung cancer (Figs. 4–6). 
                     Table 1 presents the percentage improvement of PCA–BEL with respect to the best compared method reported by [80]. The best method in SRBCT and HDD detection is a supervised orthogonal discriminant projection (SODP) algorithm with 96.56% and 73.74% average accuracy while in lung cancer classification the best method is a locally linear discriminant embedding (LLDE) with average accuracy 93.18%. The proposed method improves these results.

It seems that SRBCT and lung cancer are rather simple challenges for the classifiers in terms of complexity, since the best compared classifiers i.e. SODP and LLDE (Table 1 and Figs. 8 and 6) have been able to exhibit a detection precision of 96.56% and 93.18%. The proposed model improves these numbers by 3.56% and 5.52% turning the accuracy into 100% and 98.32% for SRBCT and lung cancer, respectively (Table 1)

At any rate the detection precision of the proposed model is very significant for HGG. It seems that this dataset is too complex for other classifiers, because the best detection precision achieved for HGG is 73.74% using the SODP method (refer to Table 1 and Fig. 5). The proposed PCA–BEL has been able to effect a 30.18% improvement which results in 96% precision rate. However, the results of colon and breast cancers, obtained from PCA–BEL, are 87.40% and 88% accuracy which does not show any significant improvement compared to the existing methods (Figs. 7 and 8). The percentage improvement of the proposed PCA–BEL is summarized in Table 1 and calculated by the following formulas:
                        
                           (10)
                           
                              Percentage
                              
                              improvement
                              =
                              100
                              ×
                              (
                              propose
                              
                              method
                              
                              result
                              –
                              compared
                              
                              result
                              )
                              /
                              (
                              compared
                              
                              result
                              )
                           
                        
                     
                  

As illustrated in Table 1, the average accuracy of SRBCT, HGG and lung cancer classification are 100%, 96% and 98.32% respectively obtained from proposed PCA–BEL. 
                     Table 2 shows the statistical details of the improved results. The confidence level (confiLevel) in Table 2 is the Student׳s t-test with 95% confidence.

Finally 
                     Fig. 9 shows the averaged confusion matrix including accuracy, precision and recall of improved results obtained from proposed PCA–BEL in 5-fold. In Fig. 9a, the class numbers 1, 2, 3 and 4 belong to EWS, RMS, BL and NB respectively. In the experimental results, 10,000 cycles is considered as the maximum number of learning cycles in every run. However this parameter can change for different problems. The maximum number of cycles for the model is 220 in order to reach convergence and 100% accuracy while there is a need for more than 8000 or even the whole 10,000 cycles to reach convergence in some folds of HGG and lung cancer datasets. This parameter should preferably have the maximum value and considering the low calculation complexity of the method, increasing the number of learning cycles even to 100,000 will result in an acceptable calculation time in modern computers.

@&#CONCLUSIONS@&#

In this paper, a novel gene-expression microarray classification method is proposed based on PCA and BEL network. In contrast to the many other classifiers, the proposed method shows lower computational complexity. Thus BEL can be considered as an alternative approach to overcome the curse of dimensionality problem. The proposed model is accessible from http://www.bitools.ir/projects.html and is utilized for classification tasks of SRBCT, HGG, lung, colon and breast cancer datasets. According to the experimental results, the proposed method is more accurate than traditional methods in SRBCT, HGG and lung datasets. PCA–BEL improves the detection accuracy about 3.56%, 30.18% and 5.52% obtained respectively from SRBCT, HGG and lung cancer. The results indicate the superiority of the approach in terms of higher accuracy and lower computational complexity. Hence, it is expected that the proposed approach can be generally applicable to high dimensional feature vector classification problems.

However, the proposed approach has a drawback. Like many other methods that used PCA, this method has not just extract the informative gens. As mentioned in Section 1, PCA is a feature extraction method and cannot select the features. For future improvements the informative genes should be determined. To determine the informative gens, the proposed method should apply a feature selection step. This issue can be considered as the next step of this research effort i.e. a proper feature selection method should be found and replaced by PCA step of the proposed method. Furthermore, in order for the proposed method to provide a proper response in other cancer classification problems, lr and k parameters should be specifically optimized for each problem. This issue can also be considered for the future works and on the other datasets such as prostate cancer.

There is no conflict of interest.

@&#REFERENCES@&#

