@&#MAIN-TITLE@&#Applying latent semantic analysis to large-scale medical image databases

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           The proposed LSA approach overcomes the complexity barriers of SVD.


                        
                        
                           
                           Early data fusion with LSA outperforms late fusion methods.


                        
                        
                           
                           LSA combines effectively visual and textual information with an integrated approach.


                        
                        
                           
                           The proposed “bypass” solution to SVD makes it attractive for large scale medical image databases.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

SVD

LSA

CBIR

Feature selection

Data fusion

Text retrieval

Image retrieval

Classification

@&#ABSTRACT@&#


               
               
                  Latent Semantic Analysis (LSA) although has been used successfully in text retrieval when applied to CBIR induces scalability issues with large image collections. The method so far has been used with small collections due to the high cost of storage and computational time for solving the SVD problem for a large and dense feature matrix. Here we present an effective and efficient approach of applying LSA skipping the SVD solution of the feature matrix and overcoming in this way the deficiencies of the method with large scale datasets. Early and late fusion techniques are tested and their performance is calculated. The study demonstrates that early fusion of several composite descriptors with visual words increase retrieval effectiveness. It also combines well in a late fusion for mixed (textual and visual) ad hoc and modality classification. The results reported are comparable to state of the art algorithms without including additional knowledge from the medical domain.
               
            

@&#INTRODUCTION@&#

During the recent years, there is a continuous increase in digital medical imaging and the development of methods and tools for acquiring, archiving and communicating all forms of medical information. Moreover, images from many diagnostic modalities, such as Radiology, Visible light photography, Microscopy, etc., are actively used to support clinical decisions, medical research and education. The management of those very large heterogeneous collections is a challenging task. As a consequence, this has led to creating systems for storage retrieval management and distribution of medical images known as PACS (Picture Archiving and Communications Systems) [1]. However, according to [2], the users of PAC systems seem to consider the image manipulation, retrieval and comparison an important missing feature.

Meanwhile, immense amounts of medical image data are continuously produced during the daily clinical practice and research, especially from CT, MRI and PET modalities. It is evident that CBIR techniques must be able to process data of this scale efficiently.

Although text retrieval methods seem to provide acceptable results, it is evident from [3], that performance in image retrieval is still very far from being effective for several reasons: computational cost, scalability and performance. The use of textual information although is an effective way for describing semantic content, when used in image retrieval there is an additional overhead of manually annotating every image in the database. Images in the medical domain, however, are always associated with a short text (the radiologist report). Thus we are seeking for methods that fuse efficiently several low level visual features together with textual information to improve retrieval results. However, depending on the current context, such short descriptions of the images could be noisy due to the language ambiguity, such as polysemy and synonymy.

Latent semantic analysis [4] is a mathematical technique for extracting hidden relations of contextual usages of words in documents. Over the years, LSA has been used successfully in text retrieval with many applications [5], and seems to be an effective method to remove redundant information and noise from data. Although the computational complexity is high for large collections it is not prohibitive, mainly due to the sparsity of the textual data.

However, this is not the case in image retrieval, where images are represented with vectors in a compact and dense form. To our knowledge, the method has so far been used only with small size collections due to the high complexity of the approach. This article presents an effective and efficient approach of applying LSA by skipping the SVD analysis of the feature matrix. We show that our approach provides a much faster, stable and scalable solution. Also we show experimentally that our method fuses well several low level visual features together with textual information.

The rest of the paper is organized as follows. Section 2 presents a brief overview of previous relevant work and Section 3 describes our approach of applying LSA for retrieval in large-scale image collections. Section 4 describes the methods for representing images and the retrieval models used in conjunction with LSA. The following Section 5 provides a brief description of the datasets used for our experiments.

Section 6 demonstrates the effects of applying LSA to retrieval and classification. For the evaluation a wide range of experiments were applied using local and global descriptors in combination with state of the art methods, like Bag of Visual Words models (BoVW) based on SIFT and SURF features and Bag of Colors models (BoC) similar with the works in [6,7].

Finally, in Sections 7 and 8, results are presented from fusion of several different low-level visual descriptors separately an in combination with textual information. For comparison both late and early fusion methods were tested.

From all our experiments it is evident that LSA is superior to direct matching the image vectors and very fast in both, off-line pre-processing of the data and on-line at query evaluation.

@&#RELATED WORK@&#

LSA approach covers a wide area of applications, from text retrieval, cross language retrieval and classification to image annotation and retrieval. One of the earlier use of LSA for image retrieval, was in [8]. More work on fusing visual and textual information using LSA for multimedia retrieval and automatic annotation extraction shown promising results are presented in [9–12] to mention a few. However, in all these works, the size of the image collections used for experimentation were very limited, the largest contained 20,000 images, a rather small size for a search method to show its potentials.

In [13] we have proposed a “bypass” solution to the SVD problem which overcomes all its deficiencies, concerning time and space requirements and makes the method attractive for content-based image retrieval. From our experience of using the method in the ImageCLEF medical tasks over the past years [14,15], we perform here a systematic evaluation on the potentials of this approach testing several combinations of visual and textual information, into a semantic representation that is suitable for fast retrieval and classification on large scale image collections.

LSA is based on the SVD factorization of a feature matrix, say C, (m
                     ×
                     n) into a product C
                     =
                     USV
                     
                        T
                     , where U and V are orthogonal matrices, with the columns of U forming the right eigenvectors of the matrix CC
                     
                        T
                     , V
                     
                        T
                      the left eigenvectors of C
                     
                        T
                     
                     C and S a diagonal matrix with singular values σ
                     
                        i
                      in the diagonal, (σ
                     1
                     ≥
                     σ
                     2
                     ≥⋯≥
                     σ
                     
                        r
                     ) and r
                     =
                     rank(C) [16]. According to Eckart–Young theorem [17] a low rank approximation, 
                        
                           C
                           k
                        
                        =
                        
                           U
                           k
                        
                        
                           S
                           k
                        
                        
                           V
                           k
                           T
                        
                     , for k
                     ≪
                     r, is the best approximation of matrix C.

In LSA the columns of the matrix SV
                     
                        T
                     (=
                     U
                     
                        T
                     
                     C), are interpreted as the projections of the initial vectors (columns of C) into a space of k latent dimensions. For retrieval purposes, a user's query, q, is projected into the same kth dimensional space by 
                        
                           
                              q
                              ˆ
                           
                        
                        =
                        
                           U
                           k
                           T
                        
                        q
                     . The query 
                        
                           q
                           ˆ
                        
                      is evaluated using as scoring function the cosine similarity and results are presented in descending order. The SVD solution has a complexity of O(m
                     2
                     n). In text retrieval we can take advantage of sparsity that significantly reduces the complexity to O(cmn), where c is the average number of non zero elements per document (c
                     ≪
                     m). In the case of CBIR, matrix C is dense and that makes the problem not feasible to solve for large collections. For example in one of our experiments matrix C, has a size of 4176×306,000≈10GB (in double precision) which makes SVD impossible to solve with our computer resources.

From our image representation we observe that the number, m, of visual features is of moderate size which lead us to an SVD-bypass solution and solving the eigenvalue problem (EVD) of the real symmetric matrix, (m
                     ×
                     m) CC
                     
                        T
                      instead of the SVD(C). Thus our SVD-alternative algorithm is summarized as follows:
                        
                           1
                           Solve the m
                              ×
                              m eigenproblem CC
                              
                                 T
                              
                              U
                              =
                              US
                              2 for the k largest eigenvectors and corresponding eigenvalues.

Calculate the projections of the original images into the kth dimensional space 
                                 
                                    
                                       
                                          d
                                          ˆ
                                       
                                    
                                    j
                                 
                                 =
                                 
                                    U
                                    k
                                    T
                                 
                                 
                                    d
                                    j
                                 
                              .

For a query q, calculate the similarity 
                                 score
                                 (
                                 
                                    
                                       q
                                       ˆ
                                    
                                 
                                 ,
                                 
                                    
                                       
                                          d
                                          ˆ
                                       
                                    
                                    j
                                 
                                 )
                              , by the cosine function, where 
                                 
                                    
                                       q
                                       ˆ
                                    
                                 
                                 =
                                 
                                    U
                                    k
                                    T
                                 
                                 q
                              .

Matrices derived from the visual features of images are of integer type. Thus CC
                     
                        T
                      can be calculated block-wise with integer arithmetic (
                        
                           CC
                           T
                        
                        =
                        
                           C
                           1
                        
                        
                           C
                           1
                           T
                        
                        +
                        ⋯
                        +
                        
                           C
                           p
                        
                        
                           C
                           p
                           T
                        
                     ). We split C into p blocks, C
                     =(C
                     1, …, C
                     
                        p
                     ) such that each C
                     
                        i
                      can be accommodated in memory and their multiplication is performed in integer arithmetic before any normalization. The complexity of the EVD approach O(m
                     2
                     n
                     +
                     m
                     3
                     +
                     kmn), where the first factor corresponds to the calculation of CC
                     
                        T
                     , the second factor to the solution of EVD and the third factor to the projection of the initial vectors to a k dimensional space.

In the following we shall argument on the stability of the EVD solution.

As it is known from Linear Algebra [16] for a symmetric matrix, A, we have a stable backwards algorithm which computes the solution of a slightly perturbed problem (A
                        +
                        E). In this case the calculated eigenvalues 
                           
                              λ
                              i
                              ′
                           
                         approximate the exact ones λ
                        
                           i
                         within machine accuracy. Furthermore the corresponding eigenvectors 
                           
                              u
                              i
                              ′
                           
                         approximate u
                        
                           i
                         with full accuracy, O(ϵ), if σ
                        
                           i
                         are well separated no matter how small σ
                        
                           i
                         are. Thus if we could ensure that the eigenvalues are well separated then the approach offers a stable and very fast solution. In [5] it was found that, in the case of text retrieval the statistical significance of LSI dimensions which, are related to the squared of the singular values, follow a Zipf-law, indicating that LSI dimensions represent latent concepts in the same way as words in texts. Following the work in [5] it was found that the same result holds in the case of image retrieval with the eigenvalues of the matrix CC
                        
                           T
                        . The eigenvalues obey the relation 
                           
                              σ
                              i
                              2
                           
                           =
                           α
                           ×
                           
                              i
                              β
                           
                        , where α and β are constants depending on the collection. This means that there are a few large and well separated eigenvalues and a large number of small and close together ones. However, in LSA we are interested only on the largest eigenvalues and corresponding eigenvectors. In text retrieval this is a number between 200 and 300. In CBIR the number which gave the best performance over our collections was 50 over all combinations of visual features we used. An example of the distribution of the singular values for the CEDD descriptor is shown in Fig. 1
                        . Thus our approach provides a stable solution for finding the largest eigenvalues and corresponding eigenvectors within machine accuracy.

In [13] we observed that the image representation has an impact on the performance of LSA. To study the effect of LSA with various image representations visual features were separated into two categories based on their extraction method.
                        
                           •
                           Partition based features, extracted by splitting images into square patches and extracting low-level features from each patch and

Dictionary based features, representing images with visual words inspired from the Bag of Words model in text retrieval.

Localized features are extracted from 3×3 fixed sized, non-overlapping blocks from each image. For this purpose images are first resized into 256×256 pixels. The final feature vector of an image is constructed by concatenating the local feature vectors from each block in a row-wise order. In our experiments, the vector representation was based on the following descriptors:
                           
                              1
                              Gray color histogram (GCH) (vector size=2304 features).

Color layout (CL) which, represents effectively the spatial distribution of the dominant colors on a grid imposed on an image (vector size=1080 features).

Color and Edge Directivity Descriptor (CEDD) [18], a compact low-level visual feature that combines color and edge information (vector size=1296 features).

Fuzzy Color and Texture Histogram (FCTH) [19], a feature that combines, in one histogram, color and texture information (vector size=1728 features).

Color Correlogram (CORR) describes the global distribution of local spatial correlation of colors [20] (vector size=9216 features).

The use of visual dictionaries is an effective technique for indexing image collections, which has been used successfully with classification and retrieval. The main approaches followed are the Bag of Visual Words (BoVW) model based on SIFT [22] and SURF [23] features, and Bag of Colors (BoC) model similar with the works in [6,7].

Inspired from text retrieval, the bag of visual words model has been used extensively in computer vision and multimedia retrieval problems [24,25].

With this model, a visual vocabulary is first constructed, which is used to represent each image. The vocabulary is constructed selecting randomly a sample of images from the original dataset. The Scale-Invariant Feature Transform (SIFT) was used to extract key-points that are invariant to rotation, scaling and translation [22]. Each key-point from an image is represented by a feature vector of size 128. Those key-points, are then clustered into k clusters using the k-Means algorithm. The centroids of the clusters in the key-point space are the visual words of the dictionary. In our experiments, the size of the dictionary was 256. This size is indeed small, mainly for two reasons: first due to the high complexity of the method and the second reason is that we are seeking on the impact of LSA to fusion of several modalities rather than to demonstrate the effectiveness of the BoVW model of retrieval.

Subsequently, localized features from each image are represented by the closest cluster-centroid, quantizing in this way the descriptor's vector. Finally, images are represented by a global vector which encodes the frequency of occurrence of each visual word. A similar procedure was followed using the SURF descriptor as a base feature.

The BoC model proposed in [7] constructs, in a similar manner, a color codebook and represents images with a color signature which encodes the color histogram of the image. As in BoVW model, a sample of images is first selected from the collection. The images are partitioned into fixed sized blocks and the most occurring color in each block is calculated. Gray intensities, RGB, HSV and CIELab, color models have been tested, with CIELab giving the best results. The returned dominating colors are then clustered, using k-means, producing the color codebook with the cluster-centroids as its colors. For our experiments a vocabulary of 256 colors was created.

The images of a collection are represented with a histogram of the codebook colors as follows. Each image is resized into a fixed number of 256×256 pixels and for each pixel the closest color from the codebook is selected using the L2 distance.

As we mentioned, the effectiveness of LSA in CBIR so far has been demonstrated with very small collections. It has not come to our attention an application of LSA with a real life application. Medical is an appropriate field for testing our approach since as it is known images in the medical domain are always associated with a short description (radiologist's report). In this vein we experimented with a collection provided at ImageCLEF's 2012 & 2013 Medical Image Retrieval and Classification Tasks [3,15].

The data provided for these tasks, are a subset of 74,655 medical articles from PubMed Central,
                        1
                     
                     
                        1
                        
                           http://www.ncbi.nlm.nih.gov/pmc/.
                      with a total of 306,500 images. For each article, the title, abstract, and a list of its figures with their corresponding caption is provided. A link is also given to the full text of the articles. For the ad-hoc image retrieval task, a set of 35 query topics was provided, with 3–7 example images for each topic. In total there are 128 images in all 35 topics. It should be noted that of the queries given for this dataset, only a small part (8/35) is marked as purely visual queries. The rest are marked as either mixed or semantic, intended for approaches that utilize textual and semantic information. The evaluation was based on the gold standard provided by the organizers using the trec_eval
                        2
                     
                     
                        2
                        
                           http://trec.nist.gov/trec_eval/.
                      tool developed for this purpose within TREC.

The modalities of medical images are a strong feature in medical retrieval. Indeed such a knowledge can limit the search space and improve the results. The modality of an image can be extracted from the caption or the image itself using visual features. For the modality classification task, a training set of 2957 medical figures classified in 31 modalities was provided. Furthermore a test set of 2582 figures was given with their corresponding classes released at the end of the task. The train and test figures were a subset of the original dataset given for the retrieval sub-task. As a measure of performance, the accuracy metric was used defined by the proportion of successes in the test set.

However, as one can see in Fig. 2
                     , the distribution of the images in each category was quite unbalanced for both training and test set. Specifically, the first class (COMP), contained approximately half of the images. As a consequence, the accuracy measure is influenced by the largest category. Therefore, the macro-averaged F1 score [26], a measure influenced from small classes, was also used.

From early experiments, the performance of each low-level feature, resulted in very low MAP scores (<0.01). Making the comparison of results more difficult. In addition, as stated in the previous section, only 8 out of 35 queries were marked as visual. To address this, the experiments with visual features in Tables 2 and 3, were performed only for the 8 queries marked as visual.

The final score of an image to a multi-example query was calculated from
                           
                              (1)
                              
                                 SCORE
                                 (
                                 q
                                 ,
                                 d
                                 )
                                 =
                                 
                                    max
                                    
                                       
                                          q
                                          i
                                       
                                       ∈
                                       q
                                    
                                 
                                 score
                                 (
                                 
                                    q
                                    i
                                 
                                 ,
                                 d
                                 )
                              
                           
                        where q
                        
                           i
                         is the ith image-example in the query q (i
                        =1, …, p).

In all our experiments results are evaluated with the value of Mean Average Precision (MAP), the precision at the top retrieved images and the value of bpref, a measure which computes a preference relation of whether judged relevant documents are retrieved ahead of judged irrelevant.

For the modality classification, we employed the LibSVM
                           3
                        
                        
                           3
                           
                              http://www.csie.ntu.edu.tw/cjlin/libsvm.
                         implementation of Support Vector Machines (SVMs) expanded to the case of multi-class classification, using C-Support Vector Classification (C-SVC) with a radial kernel function. A 5-fold cross-validation parameter optimization was performed on the training data.

In Table 1
                         baseline results are presented from global features and from LSA with partitioning. It seems that LSA with a low rank approximation k
                        =50 on single descriptors is marginally superior in terms of accuracy, but certainly it is much faster in processing the queries.

As stated in the previous section, visual information extracted from single descriptors, performs poorly. Thus we proceed applying fusion techniques to combine data from multiple sources in order to achieve inferences, which will be more efficient and accurate than the results achieved from a single source. In this vein we proceed in the following, with fusion of visual data and textual data. Fusion methods can be separated into two main categories, late fusion and early fusion. Late fusion refers to methods that integrate results from several resources after a user request is submitted, while with early fusion, integration from several data representations is performed off line prior to a user request.

The most common methods used in late fusion are based on “merging” the ranked lists returned from different document representations. Many fusion approaches have been proposed and studied over the years, the most popular ones being CombMAX and CombSUM [27] due to their simplicity. Both methods have advantages and drawbacks.

For competence in evaluating the performance of LSA in fusing visual features, we performed a series of late fusion experiments using the same visual descriptors. A weighted version of CombSUM was used that resulted in improved results:


                        
                           
                              (2)
                              
                                 WeightedSUM
                                 (
                                 q
                                 ,
                                 d
                                 )
                                 =
                                 
                                    ∑
                                    i
                                 
                                 
                                    w
                                    i
                                 
                                 
                                    NormScore
                                    i
                                 
                                 (
                                 q
                                 ,
                                 d
                                 )
                              
                           
                        where 
                           
                              w
                              i
                           
                         is a weight proportional to the performance of the ith retrieval component. Therefore, to each 
                           
                              w
                              i
                           
                         was assigned the value of MAP obtained for each individual feature, using the queries of the 2012 ImageCLEF medical data collection. Table 2
                         presents the results obtained from late data fusion.

We note here the high cost of late fusion, which in a retrieval system, must be applied on-line, at query time. Late fusion requires to evaluate the similarity of the query with each document in the collection for each descriptor separately before finally merging all the retrieved lists. This cost is further increased in the case of multi-example queries, as it happens in our case.

In this section an integrated representation of images is constructed based on composite visual descriptors and visual patterns extracted from BoVW models. The aim is to take advantage of the implicit relationships among several multi-visual image representations using LSA. The vectors are concatenated into a single vector as it is shown in Fig. 3
                        . Early fusion with BoC, CORR, CEDD and FCTH forms a feature matrix C
                        =[BoC
                        ;
                        CORR
                        ;
                        CEDD
                        ;
                        FCTH] (in matlab notation) of size 12,496×306,000≈30GB. Although such a combination can improve the performance of retrieval, the large size of the final fused matrix can induce scalability problems. Indeed, it is not feasible to solve this particular experiment using SVD with our computer resources.
                           4
                        
                        
                           4
                           All experiments were performed with an Intel Core i7 3.40GHz 8GB RAM.
                         However, we can store in memory the cross product matrix of size 12,496×12,496≈1.2GB and solve the problem, EVD(C), for the k largest eigenvalues and corresponding eigenvectors. Indicatively, we mention that the EVD approach for this experiment, without any programming optimization, took 82.45min CPU time for k
                        =50. This value includes the CPU time to calculate the product CCT
                        , plus the time to solve the eigenproblem, [Uk, Dk
                        ]=eigs(CCT
                        , k) and the time to calculate the 
                           
                              
                                 
                                    
                                       V
                                       k
                                    
                                 
                              
                           
                           =
                           
                              
                                 
                                    
                                       U
                                       k
                                       T
                                    
                                    C
                                 
                              
                           
                        . It should be noted that this time also includes I/O calls, since matrix C and V are stored on disk.


                        Tables 3 and 4
                        
                         present the results of early fusion for image retrieval and modality classification respectively. In Fig. 4
                         one can see a comparison of both fusion approaches in terms of execution times to evaluate all topics for the experiments in Table 3.

In Table 3, we observe that values of MAP are very low, which makes it hard for someone to judge on the effectiveness of the algorithms used. Thus to show the effectiveness and efficiency of the algorithms we have used Corel-10k image collection, which has been used in the literature for CBIR. Corel-10k is a subset of the Corel Photo Gallery with 10,800 images separated into 80 thematic categories. The collection is divided into a training set of 10,000 images and a test set of 800 images (10 images from each thematic category). Performance is evaluated using the thematic category information of each query.

Moreover, with this dataset, the C matrix is small enough to accommodate into memory which, enables the comparison of the EVD efficiency against direct matching and SVD. The results in terms of MAP and execution times are presented in Table 5
                        . The EVD pre-processing time includes the time to calculate CC
                        
                           T
                        , the time to solve EVD ([U,D]=eigs(CC
                        
                           T
                        ,k) and the time to project the initial vectors to the lower dimensionality space 
                           
                              U
                              k
                              T
                           
                           C
                        . For SVD, the time refers to the solution of SVD(C) ([U,D,V]=svds(C,k). In both cases k was equal to 50. The values of MAP and query time for the SVD are the same as in EVD so are omitted from the table. We note here that, as the size of a collection increases, the difference in runtime between EVD and SVD will grow quadratically in favor of the EVD approach.

From our results so far it is clear that visual techniques alone are not capable to fulfill the semantic information needs of users. In this section we include experiments for testing whether, when mixing visual and textual information can obtain a better performance compared to what we can get from each case separately.

For each image in the ImageCLEF 2013 dataset, textual information for the title, abstract, and caption are provided. In our text based retrieval each record is identified by a unique FigureID and includes: the title of the corresponding article of the image, the figure's caption, and all the sentences in the article with a reference to the image. Our evaluation of textual retrieval on the imageCLEF 2013 data collection was based on 35 textual queries. As a base for our retrieval system, we have used the Lucene
                           5
                        
                        
                           5
                           
                              http://lucene.apache.org/.
                         search engine. Stop-words were removed and suffix stripping was applied with Porter's stemmer. Similarity was calculated using Lucene's default scoring function which shown a better performance in all our experiments compared to BM25 weighting scheme.

Two main schemes for retrieval were examined: first, by ignoring the structure of documents and merging all information associated with an image into a single document, and second, by using a weighted multi-field approach. For the second case, the performance from each component (title, caption, references) is evaluated independently, in terms of MAP, calculated using the queries of the imageCLEF 2012. Both schemes gave very satisfactory and comparative results so we present only the results from the first approach in Table 6
                        . In the first row of Table 8 we present the results of modality classification on textual data only. We have also tested the impact of performing LSA on those vectors prior to classification in order to reduce their size to a low dimensional space of k
                        =200. Although performance is marginally dropped, the classification speed is significantly improved.

For the ad hoc image retrieval, a weighted late data fusion was performed with textual and visual features resulting using LSA in the same manner as presented in Section 7.2. This was the fastest way for fusion these two representations of the images. In Table 7
                         it is shown that the method further improves the performance obtained from a text only approach. Results of modality classification with early fusion on mixed features are presented in table (Table 8
                        ).

We have investigated the application of LSA for content based image retrieval and classification in large databases. In this experimental work we show through a variety of examples the effectiveness of our approach which overcomes the barriers of SVD due to its high complexity on large scale data with. In Section 3 we argue on the stability of the EVD on the cross product CCT
                      matrix. Two image representations were evaluated based on partitioning and on dictionaries of visual patterns. It is shown that LSA, in most cases, is superior over the baseline performance obtained from direct match on the original low-level visual features. Additionally, it is shown that LSA combines effectively several image visual and textual information into an integrated and fast mixed data fusion approach. Our evaluation on fusion techniques, indicate that early fusion outperforms late fusion methods. It should be noted here the high computational cost of late fusion which increases the query response time to impractical levels with multi example queries.

For the modality classification, we observed a marginal drop of accuracy compared to using the original vectors. However it should be noted, that the dimensionality reduction attained with LSA reduces radically the computational complexity for training the classifier. In concluding the proposed “bypass” solution to the SVD problem, overcomes the issues that arise concerning time and memory space requirements, making the method attractive for use in large scale datasets.

Finally, for a comparison with other state of the art CBIR techniques on the same dataset we cite here the results published in the Web,
                        6
                     
                     
                        6
                        
                           http://www.imageclef.org/2013/medical.
                      from the top performing groups which participated in the imageCLEF 2013 medical retrieval task. From these, it is clear that our results lie together with the top performing for both tasks investigated: ad hoc retrieval and modality classification. It should be noted that our approach is a general purpose CBIR and uses no domain knowledge. Thus an interesting application of our approach is with fusion of visual and image annotations extracted from medical resources which is currently under investigation.

@&#REFERENCES@&#

