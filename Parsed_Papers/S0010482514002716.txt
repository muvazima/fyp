@&#MAIN-TITLE@&#Semi-automatic motion compensation of contrast-enhanced ultrasound images from abdominal organs for perfusion analysis

@&#HIGHLIGHTS@&#


               
                  
                  
                     
                        
                           
                           This paper proposes a semi-automatic system to motion compensated CEUS perfusion data.


                        
                        
                           
                           Evaluation showed equal or improved performance compared to manual processing.


                        
                        
                           
                           It saves 41% of the time required for manual processing of the datasets.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Ultrasonography

Motion analysis

Motion compensation

Registration

CEUS

Contrast-enhanced ultrasound

Perfusion

Perfusion modeling

@&#ABSTRACT@&#


               
               
                  This paper presents a system for correcting motion influences in time-dependent 2D contrast-enhanced ultrasound (CEUS) images to assess tissue perfusion characteristics. The system consists of a semi-automatic frame selection method to find images with out-of-plane motion as well as a method for automatic motion compensation. Translational and non-rigid motion compensation is applied by introducing a temporal continuity assumption. A study consisting of 40 clinical datasets was conducted to compare the perfusion with simulated perfusion using pharmacokinetic modeling. Overall, the proposed approach decreased the mean average difference between the measured perfusion and the pharmacokinetic model estimation. It was non-inferior for three out of four patient cohorts to a manual approach and reduced the analysis time by 41% compared to manual processing.
               
            

@&#INTRODUCTION@&#

Ultrasound (US) imaging is one of the most commonly available medical imaging techniques. It is cost-efficient compared to other imaging modalities, radiation free and portable. In the last decade, significant research has been conducted in the field of US contrast agents (CA) and associated acquisition protocols [1,2].

Contrast-enhanced ultrasound (CEUS) is commonly used to assess perfusion in various organs such as the heart, liver, kidney, spleen, pancreas or bowel [3–5]. Gas-filled microbubbles serve as true intra-vascular tracers as they do not leak into extra-vascular space and their non-linear echo can be separated from the tissue echo (see Fig. 1
                     ). Thus, they enable the measurement of absolute perfusion parameters such as blood volume, mean transit time and blood flow in an organ per unit time [6,7]. It is crucial to know the actual perfusion of an organ in many processes such as assessing ischemia, characterizing tumors and separating inflammatory from fibrotic processes [8–10]. Absolute perfusion can be quantified using pharmacokinetic modeling where the CA-concentration time curve is approximated by an appropriate model [6,11].

CEUS image sequences are subject to limitations interfering with correct analysis of the data. The most noticeable interference in addition to noise and speckle artifacts is motion [12]. Motion originates from various sources and can be subdivided into intrinsic and extrinsic sources [13]. Intrinsic sources are breathing, perfusion or digestive activity resulting in moving organs or tissue. Extrinsic sources include the tilting or moving of the US probe. This happens unintentionally due to breathing or patient motion. However, this also occurs intentionally when the examiner tries to adapt the field of view of the transducer to image a region of interest (ROI). This adaption is often necessary at the time of CA arrival when enhancement becomes visible. While breathing or tilting of the probe mostly causes linear motion shifts, organ motion such as aorta pulsation or the heartbeat results in non-linear deformation effects.

Valid spatial correspondences of tissue over time cannot be established if the images of the sequences are influenced by such motion [14]. A specific problem of 2D image acquisition is that motion can emerge in directions perpendicular to the imaging plane (3D motion) so that frames are acquired out-of-plane. Whereas motion occurring within the imaging plane can be compensated by image processing methods, frames with out-of-plane motion yield erroneous information and should be removed.

Motion analysis in US imaging can be performed through speckle decorrelation analysis [15]. However, reliable estimates of the motion extent are only obtained from an ideal medium with known and uniform speckle characteristics. In addition, this only works for in-plane motion as out-of-plane motion results in a changed speckle pattern. To overcome the problem of unknown speckle characteristics, Laporte and Arbel [16] proposed to use learning methods and synthetic US images with known ground truth. However, for learning methods, a sufficiently large database has to be available. Additionally, learning has to be performed for a specific application scenario.

Only few approaches in the literature discuss the detection of out-of-plane motion when applying motion compensation. Renault et al. [17] proposed a method for hepatic CEUS by using frame selection of a specific point in the respiration cycle through factor analysis of independent components. According to the authors, this is necessary to avoid mixing information of different imaging planes. The approach requires that the liver occupies the same position at a particular point in the respiration cycle. This cannot be generalized for observations of abdominal organs. Frouin et al. [18] examined the quality of motion compensation. If residual motion exceeds a certain threshold, the affected images are excluded. In other publications, the problem of out-of-plane motion is mentioned and accounted for in a motion compensation approach, although it does not lead to the exclusion of data [19,20].

In most scenarios of motion compensation, image registration is used [21]. In contrast to standard registration techniques where two images are aligned, motion compensation in CEUS requires the alignment of multiple images. Formalizing the problem for all images leads to the introduction of constraints in spatial and temporal direction. Temporal constraints, i.e. restrictions on motion depending on preceding or successive frames, can be incorporated in a number of different ways to stabilize the search process for correct motion compensation. In particular, this holds true if dealing with low signal-to-noise ratio (SNR) and low resolution [22].

Original work was done by Szeliski and Coughlan [23] who use splines to regularize the motion field for the registration of image series. This has been adapted by many researchers to solve specific application problems. Registration of US image sequences of the heart allows for quantification of the contractility of the myocardium by cardiac motion estimation. Carbayo et al. [24,19] treated this registration problem as a global one, searching for the correct motion within a single optimization scheme with spatial and temporal smoothness constraints by using spline functions. This compensates for the deficiencies in US datasets, specifically the low SNR and decorrelated speckle patterns due to out-of-plane motion. In addition to the smoothness assumption, the velocity of the motion change can be assumed to be continuous [25].

A new approach for the detection of out-of-plane motion and motion compensation in US is required that is not specialized for an organ class. We propose the first system to target semi-automatic motion analysis and compensation for CEUS images of abdominal organs. For the correct perfusion analysis of CEUS data it is inevitable to perform a motion analysis to exclude out-of-plane frames. We use a similarity correlation analysis based on the measured intensities in the B-mode data to help the user identify areas of out-of-plane motion. Subsequently, a two-step strategy for motion compensation combining the compensation of linear shifts and non-linear motion deformations is applied. Both steps are achieved using image registration with optimization through Markov random fields (MRF) [26]. This can be efficiently calculated with graph cut-based approaches [27–29]. MRF have been successfully used to solve problems of optical flow, non-rigid image registration and combined registration and segmentation [30–32].

The aim of our evaluation study was to examine if our CEUS analysis system performed equally well as manual correction and reduced the analysis time. Since we did not know the ground truth, the quality of both approaches was assessed by measuring the difference to a pharmacokinetic model [6]. We assume that a considerable amount of time is saved by using the proposed method and that the accuracy of the computer assisted motion compensation improves compared to manual analysis according to the pharmacokinetic model analysis.

In previous publications, we proposed the motion analysis for out-of-plane motion detection [13] and the MRF-based motion compensation system [33]. The difference to the motion compensation method in this work is the introduction of B-spline based transformation functions instead of direct pixel translations and a larger patient data evaluation.

The study was performed on 40 datasets. Twenty datasets were acquired from patients at the clinic at Haukeland University Hospital of which 10 had cystic fibrosis with exocrine pancreatic failure and 10 patients had an acute exacerbation of Crohn׳s disease. The other 20 recordings were from healthy volunteers of which 10 where acquired from the pancreas and 10 from the small or large bowel. Bowel and pancreas were chosen as target organs in this study as they represent different challenges in perfusion quantification. The bowel is relatively stable, superficially located and the healthy bowel wall is very thin. The pancreas is severely affected by both respiration and pulsation of the abdominal aorta and deeply located. The Regional Ethics Committee West approved the study and all patients and healthy volunteers gave their informed consent to participate in the study.

@&#METHODS@&#

Clinical CEUS datasets were acquired using a Logiq® E9 ultrasound scanner (GE Healthcare, Milwaukee, WI, USA). A bolus and burst regime was used during the acquisition [10]. A high frequency linear probe (L9) was used for scanning the bowel and a curvilinear probe (C1-5) was used for the pancreas examination. The dual view containing both a B-mode and contrast image was used for the acquisition. Recordings for both pancreas and bowel were acquired for about 90s with a frame rate of 9 frames per second (FPS) for pancreatic datasets and 11 FPS for bowel, respectively. For each examination an intravenous bolus of SonoVue® (Bracco, Milan, Italy) was given over 2s followed by an intravenous bolus of 10ml saline over four seconds. For the bowel examination 4.4ml of contrast agent was used while 1.5ml was used for the pancreas examination. After the arterial phase when the contrast intensity in the organ reached a pseudo-steady state the bubbles are burst and the replenishment is recorded. Pseudo-steady state in the bowel is reached after 60s and in the pancreas after 45s.

In order to reduce the size of data and thus computational load, the sequences were subsampled resulting in three FPS for pancreas and two FPS for bowel leading to about 270 and 190 frames, respectively. According to the dynamics of the CA distribution, this temporal resolution is sufficient to perform perfusion analysis. Temporal correlation between the frames is still high enough to use it as constraint in motion compensation. On the other hand, using original frame resolution would require to expand temporal constraints over multiple neighbor frames in order to impact motion compensation results.

For manual motion correction a software module in the perfusion analysis software (DCE-US) was used [34]. This module works with a simple point-and-click feature. The examiner defines a reference point representing a specific feature in each frame with the mouse. For every left mouse click, the video shifts to the next frame and each frame is translated to the same point. With a right mouse click a frame can be excluded. For each excluded and thus missing frame in the video the intensity value of the preceding frame was used in the perfusion calculation. The manual correction was performed in two steps. First, out-of-plane frames were removed. Second, the images were motion corrected. The time used for each of these procedures was recorded. The manual motion correction was performed by a clinician experienced in clinical ultrasound examinations and the use of contrast-enhanced ultrasound.

The proposed system consists of two components (Fig. 2
                        ). First, the image data are processed interactively by the frame selection module. The sequence is analyzed for out-of-plane motion. After frames have been selected, they are processed by the 2D registration module to compensate for linear motion. Further residual motion is corrected by non-linear 2D registration. Finally, the data is exported to be analyzed in other software tools for thorough perfusion analysis.


                        Frame selection module: Before the frame selection process can be performed, the US image sequence has to be analyzed for motion effects. The user has to define a ROI for analysis. The ROI is specified as a polygonal shape defining the organ or tissue area of interest. It will be used to calculate frame similarities based on image intensities from the B-mode images. Similarities in terms of the normalized cross-correlation are compared between all n frames in the sequence resulting in an 
                           n
                           ×
                           n
                         correlation matrix [13]. The normalization allows for comparison between different datasets. The dissimilarity in the correlation matrix is interpreted as an indicator for the amount of motion (see Fig. 3
                        ).

The matrix reveals different types of motion effects within a sequence. Non-recurring motion shifts, e.g. caused by patient motion or probe motion, are characterized by dark transitions between quadratic areas in the matrix view (cf. Fig. 3a). Recurring motion, e.g. caused by breathing or heart beat motion, is characterized by small regular patterns (cf. Fig. 3b). Most often, out-of-plane motion is not present in sequences characterized by this motion effect, as the sonographer chooses a recording angle so that the recurring motion occurs mostly in-plane and can be compensated by registration. Fig. 3c displays a dataset where both effects occur at the same time.

The matrix can be clicked by the user to navigate within the image sequence to analyze the motion and explore critical areas. Interaction is required to draw temporal regions to group frames which show the same area of interest (cf. Fig. 3d–f). All frames which are not assigned to a temporal region are automatically declared out-of-plane and will not be included in further steps. Finally, the result is processed by the next module addressing the motion compensation.


                        Motion compensation module: The problem of motion compensation of time-dependent CEUS data is approached by incorporating prior knowledge about the motion characteristics. This is the temporal continuity within a neighborhood of frames and the spatial continuity for non-rigid motion compensation. Additionally, assumptions about the amount of motion are made to reduce the search space or vice versa. The amount is intentionally restricted to prevent large deformations which introduce a higher risk of imprecision.

The MRF-based system is able to model these assumptions and account for the special restrictions mentioned above [26,35]. Each temporal region defined in the previous step will be represented by an MRF.

The temporal regions found by the frame selection module are processed independently by an MRF-based motion compensation. The MRF is an undirected graph 
                           G
                           =
                           (
                           V
                           ,
                           E
                           )
                         with nodes 
                           
                              
                                 v
                              
                              
                                 i
                              
                           
                           ∈
                           V
                         representing the variables to be optimized (motion parameters) and with edges 
                           e
                           =
                           〈
                           
                              
                                 v
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 v
                              
                              
                                 j
                              
                           
                           〉
                           ∈
                           E
                         expressing dependencies between the two variables. Spatial dependencies are represented by edges in 
                           
                              
                                 E
                              
                              
                                 s
                              
                           
                         and temporal dependencies are represented by edges in 
                           
                              
                                 E
                              
                              
                                 t
                              
                           
                        . Each node is assigned a label l
                        
                           k
                         from a discrete set 
                           L
                         representing a search space for the solution. The optimization problem is solved by finding labels minimizing the global energy E
                        
                           g
                         of the MRF consisting of the sum of the unary potentials 
                           
                              
                                 ψ
                              
                              
                                 u
                              
                           
                           (
                           ·
                           )
                         and the pairwise potentials 
                           
                              
                                 ψ
                              
                              
                                 p
                              
                           
                           (
                           ·
                           ,
                           ·
                           )
                        
                        
                           
                              (1)
                              
                                 
                                    
                                       E
                                    
                                    
                                       g
                                    
                                 
                                 =
                                 
                                    
                                       ∑
                                    
                                    
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                       ∈
                                       V
                                    
                                 
                                 
                                    
                                       ψ
                                    
                                    
                                       u
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 +
                                 
                                    
                                       ∑
                                    
                                    
                                       〈
                                       
                                          
                                             v
                                          
                                          
                                             i
                                          
                                       
                                       ,
                                       
                                          
                                             v
                                          
                                          
                                             j
                                          
                                       
                                       〉
                                       ∈
                                       E
                                    
                                 
                                 
                                    
                                       ψ
                                    
                                    
                                       p
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       i
                                    
                                 
                                 ,
                                 
                                    
                                       v
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 ,
                              
                           
                        The unary potentials represent the similarity evaluation achieved by the assigned labels to the nodes. The pairwise potentials (a priori term) are defined over the edges and represent a priori constraints. The minimum energy of the MRF is found by the α-expansion algorithm [27].

We use a two step registration strategy, starting with a translation transformation followed by a non-rigid B-spline-based transformation. For the former transformation the MRF is regarded as the Markov chain with only a single node for each frame. In this simplified case, there are no spatial restrictions required, but the motion shift is constrained in temporal direction. This was the straight forward solution, as we are able to use the same solver method for the translational step. For the non-rigid B-spline-based transformation, each B-spline control point contributes to the transformation and has to be represented through a node in the MRF.

For both transformation strategies, the translation and the non-rigid B-spline-based transformation, a discrete set of labels representing the transformation is required. This is a translation in 2D defined by a vector 
                           
                              
                                 l
                              
                              
                                 k
                              
                           
                           =
                           (
                           
                              
                                 t
                              
                              
                                 x
                              
                           
                           ,
                           
                              
                                 t
                              
                              
                                 y
                              
                           
                           )
                        , t
                        
                           x
                         and t
                        
                           y
                         representing the displacement in x and y directions.

Unary potential functions 
                           
                              
                                 ψ
                              
                              
                                 u
                              
                           
                           (
                           ·
                           )
                         evaluate the contribution of labels (transformation parameters) to the mean squared distance (MSD) in B-mode images [13]. For the non-rigid step we use a 7×7 pixel neighborhood M
                        
                           v
                         around a B-spline control point v in the image frame I
                        
                           v
                         for similarity calculation
                           
                              (2)
                              
                                 
                                    
                                       ψ
                                    
                                    
                                       u
                                    
                                 
                                 (
                                 v
                                 )
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                          
                                          
                                             p
                                             ∈
                                             
                                                
                                                   M
                                                
                                                
                                                   v
                                                
                                             
                                          
                                       
                                       
                                          
                                             (
                                             
                                                
                                                   
                                                      I
                                                   
                                                   
                                                      r
                                                   
                                                
                                                (
                                                p
                                                )
                                                −
                                                
                                                   
                                                      I
                                                   
                                                   
                                                      v
                                                   
                                                
                                                (
                                                p
                                                +
                                                
                                                   
                                                      l
                                                   
                                                   
                                                      v
                                                   
                                                
                                                )
                                             
                                             )
                                          
                                          
                                             2
                                          
                                       
                                    
                                    
                                       |
                                       
                                          
                                             M
                                          
                                          
                                             v
                                          
                                       
                                       |
                                    
                                 
                                 .
                              
                           
                        
                        p represents pixel positions in the form (x,y) and 
                           
                              
                                 l
                              
                              
                                 v
                              
                           
                           =
                           (
                           
                              
                                 t
                              
                              
                                 x
                              
                           
                           ,
                           
                              
                                 t
                              
                              
                                 y
                              
                           
                           )
                         is the label of v representing a translation. I
                        
                           r
                         is the predefined reference frame. For the linear step, the whole image (ROI covering the organ of interest) is used for similarity calculation instead of the local area 
                           |
                           
                              
                                 M
                              
                              
                                 v
                              
                           
                           |
                        .

The pairwise potential function reflecting the continuity assumptions is calculated by the Euclidean distance between labels. The pairwise energy ψ
                        
                           p
                         of the global MRF energy E
                        
                           g
                         in Eq. (1) is defined as the weighted sum of spatial energy ψ
                        
                           ps
                         and temporal energy ψ
                        
                           pt
                        
                        
                           
                              (3)
                              
                                 
                                    
                                       ψ
                                    
                                    
                                       p
                                    
                                 
                                 =
                                 
                                    
                                       ψ
                                    
                                    
                                       ps
                                    
                                 
                                 +
                                 β
                                 ·
                                 
                                    
                                       ψ
                                    
                                    
                                       pt
                                    
                                 
                              
                           
                        
                        
                           
                              (4)
                              
                                 
                                    
                                       ψ
                                    
                                    
                                       ps
                                    
                                 
                                 (
                                 v
                                 ,
                                 u
                                 )
                                 =
                                 d
                                 (
                                 
                                    
                                       l
                                    
                                    
                                       v
                                    
                                 
                                 ,
                                 
                                    
                                       l
                                    
                                    
                                       u
                                    
                                 
                                 )
                                 ,
                                 〈
                                 v
                                 ,
                                 u
                                 〉
                                 ∈
                                 
                                    
                                       E
                                    
                                    
                                       s
                                    
                                 
                              
                           
                        
                        
                           
                              (5)
                              
                                 
                                    
                                       ψ
                                    
                                    
                                       pt
                                    
                                 
                                 (
                                 v
                                 ,
                                 w
                                 )
                                 =
                                 d
                                 (
                                 
                                    
                                       l
                                    
                                    
                                       v
                                    
                                 
                                 ,
                                 
                                    
                                       l
                                    
                                    
                                       w
                                    
                                 
                                 )
                                 ,
                                 〈
                                 v
                                 ,
                                 w
                                 〉
                                 ∈
                                 
                                    
                                       E
                                    
                                    
                                       t
                                    
                                 
                                 .
                              
                           
                        
                        
                           d
                           (
                           ·
                           ,
                           ·
                           )
                         is the Euclidean distance of the vectors of the assigned labels 
                           
                              
                                 l
                              
                              
                                 v
                              
                           
                           ,
                           
                              
                                 l
                              
                              
                                 u
                              
                           
                         and l
                        
                           w
                        . The weighting parameter β is set to 0.5 and has been empirically chosen. For the linear step, the pairwise potential function ψ
                        
                           p
                         does only reflect the temporal continuity assumption ψ
                        
                           pt
                        .

The discrete set of labels for the first compensation step (linear motion) is chosen to cover a translation of up to 15 pixels in both directions. This results in an absolute number of 961 translation labels (all combinations of possible translation in x and y directions). According to empirical observation, this is enough to correct for large motion shifts in bowel and pancreas datasets due to breathing artifacts.

The discrete set of labels for the second compensation step (non-linear motion) is chosen such that the transformation function is a diffeomorphism. Using a B-spline control point grid of 20 by 20 pixels this results in an absolute number of 121 (11 ×11) labels.

@&#EVALUATION@&#

The development of intensity, i.e. of contrast agent concentration in specific tissue areas, is considered for the analysis of CEUS. The values are collected in temporal direction to establish a time intensity curve (TIC). The TIC is also used in evaluation.

The evaluation of the motion compensation module was divided into two experiments. The first one was a proof of concept and examined the contribution of translation transformation and B-spline based transformation as well as the contribution of the temporal constraints. Therefore, 12 randomly chosen datasets were motion compensated using different setups:
                           
                              •
                              linear registration only,

linear registration without temporal continuity constraint,

linear and B-spline-based registration,

linear and B-spline-based registration without temporal continuity constraint.

In addition, the performance was compared to a state-of-the-art approach (elastix toolkit) by performing pair-wise image registration on the sequences [36]. For evaluation, the standard deviation (SD) of intensities in a predefined ROI was analyzed before and after the registration process in B-mode data as well as the smoothness of the TIC, i.e., the CA concentration time development within the ROI. The variation of intensities in B-mode data should decrease as a result of registration using MSD as a measure for similarity. The TIC is expected to change slowly over time [37]. We calculated the TIC smoothness in terms of the mean absolute differences (MAD) between adjacent time points. The MAD is expected to decrease with improved registration quality.

The second experiment is to examine if the motion compensation module reduced the time required for evaluation and if the accuracy of the results is increased. We applied a perfusion model proposed by Jiřík et al. [6] for calculating absolute values for mean transit time, blood flow and blood volume in CEUS imaging. The model is based on bolus administration of CA and the burst-replenishment technique. It approximates the TIC by convolution of the arterial input function and the tissue residual function. This approximation is formulated as blind deconvolution. The quality of this approximation can be expressed as the residual error of the regression analysis in terms of the MAD.

The accuracy of the approximation to the model was calculated with the original data after manual frame selection and manual motion compensation and after the proposed semi-automatic CEUS analysis system. During the analysis of a dataset the examiner was blinded to the results from the other correction approach. The same ROI was used for each approach.

For results where significance information is provided in the Results section a non-parametric Wilcoxon matched-pairs signed-rank test with a significance level of 5% was used. The study was performed with a 64-bit 
                           
                              
                                 Intel
                              
                              
                                 ®
                              
                           
                         Core i7™ 2720QM CPU, 8 GB RAM system.

@&#RESULTS@&#

For the 40 datasets in the study, the manual definition of temporal regions took 204±66s on average per dataset by assessing the sequence manually frame-by-frame. The proposed technique using the similarity matrix to find areas where motion occurs took only 165±78s. The manual motion compensation took 294±95s on average for all 40 datasets. Automatic motion compensation was performed in 131±91s on average. A human operator is not required during automatic motion compensation. The processing time was significantly improved by our approach for both bowel and pancreas datasets. Overall, the time consumption was reduced by 41% compared to the time required for manual processing.

The manual definition of frames for datasets of the bowel excluded 5.6% of the frames. With the matrix-based technique 6.9% of the frames were excluded. The overlap of excluded frames between both methods was 92.9%±5.6 percentage points. The manual definition of frames for datasets of the pancreas excluded 24.4% of the frames. With the matrix-based technique 19.7% of the frames were excluded. The overlap of excluded frames between both methods was 81.4%±5.4 percentage points.

The results for the proof-of-concept experiments are shown in Figs. 4 and 5
                     
                     . Motion compensation with consecutive translation and B-spline based transformation with temporal constraints yielded the best results. The temporal constraints of the transformation parameters had no improvement effect on the SD measurements in B-mode. The approach outperformed the state-of-the-art approach (elastix) for translation transformation and performed equally well when using B-spline transformation functions. For the MAD measurements in contrast data the results with temporal constraints were improved compared to the results produced without temporal constraints for the parameters. In case of the MAD measurements, the proposed motion compensation approach outperformed the state-of-the-art approach (elastix) for both translation and B-spline based transformation.

The quality of fit of the TIC to the pharmacokinetic model in terms of MAD for manual and automatic processing are shown as box plots in Figs. 6 and 7
                     
                      for pancreas and bowel datasets, respectively. Additionally, the results are shown separately for patients and healthy volunteers. The results of the proposed motion compensation approach for bowel and pancreas datasets (healthy volunteers and patient groups) were not significantly different compared to manual motion compensation except for the group of patients in pancreatic datasets. The manual approach for frame selection and motion compensation conducted by a medical expert achieved significantly better results.

There was no negative correlation between the number of excluded frames and the quality of fit to the pharmacokinetic model. There are datasets where no frame exclusion was necessary and an improvement of the quality of fit was observed. This confirms that both the frame exclusion and the motion compensation contribute to a better quality of fit in terms of the model evaluation.

@&#DISCUSSION@&#

The most important finding of the study is that automatic motion compensation is able to produce the same quality of fit to a pharmacokinetic model compared to manual processing. At the same time, the proposed approach is able to significantly save processing time. The results for bowel datasets as well as for the healthy control group of the pancreas datasets were non-inferior to the results of manual processing. Additionally, the automatic motion compensation does not require user interaction to determine the transformations.

However, a noticeable difference was observed in the results of the different control groups (healthy volunteers and patients) and organ recordings (pancreas and bowel). Automatic motion compensation results for the pancreas data have larger quartiles and no improvements can be observed. The results for bowel datasets could be improved by the automatic motion compensation although the improvement was not significant.

The bowel and pancreas recordings represent different challenges with motion correction of time intensity data. The bowel wall is thin and the bright interface between bowel wall and luminal air cause significant noise in the dataset if included in the analysis. The anterior bowel wall was imaged in the longitudinal direction and the motion can be kept mostly in plane. In the pancreas recordings, out of plane motion occurs during respiration and air in the stomach and duodenum causes shadowing. In addition, the intensity of motion influence in pancreas datasets is higher because of the proximity to the diaphragm.

The temporal frame selection using the similarity matrix to overview the sequence proves to be faster compared to a manual frame-by-frame analysis. For the bowel datasets the results of the different approaches, manual vs. matrix-driven, had a high overlap (92.9%). For the pancreas datasets the overlap was smaller (81.4%) which is also caused by a higher number of frames with out-of-plane motion. However, these results show that the approaches produce different frame selections.

In discussions, the medical experts stated that they particularly trust high contrast changes in the similarity matrix to give evidence for possible out-of-plane motion. The matrix provides a good overview and enables to go back and forth to correct the frame selection. The user learns to evaluate the effects of motion on the similarity matrix. If in doubt, the matrix areas showing homogeneity can be assessed thoroughly by looking at the image sequence in detail.

The proof-of-concept evaluation of the motion compensation primarily shows the impact of both registration steps (linear and non-linear) and the impact of the temporal constraints. It also gives proof that the MRF based approach outperforms the state-of-the-art registration approach for this particular application. The variability of intensities in the B-mode data could be reduced by our approach. The smoothness of the TIC has not been improved to a similar extent. This is also related to the contrast signal which is disturbed by noise and speckle. There are cases where the speckle pattern changes due to residual out-of-plane motion effects. However, the setups using temporal constraints are able to lead to better results compared to the setups without using temporal constraints in terms of the smoothness of the TIC. The proof-of-concept results underline the necessity of the integrated approach compared to an image-by-image registration approach and the use of temporal constraining.

Regarding the results of the evaluation with the pharmacokinetic model, a noticeable difference can be observed between the results for pancreas and bowel datasets (on average 36.8% improvement compared to 7.0%). This can be explained by the amount of motion, which is much higher in pancreatic datasets offering more room for improvement. The pancreas is close to both the aorta, causing pulsation, and the diaphragm, causing respiratory movement. This is also reflected by a higher percentage of frames declared out-of-plane in pancreatic datasets, which is 19.7% compared to only 6.9% for bowel datasets. Also, there are many large vessels with high intensity values around the pancreas. If these are included in an analysis region, the time intensity curve is distorted.

A second noticeable difference can be observed between the results for patients and healthy volunteers for bowel datasets. The fitting quality for patients slightly deteriorates (median of −5.0% compared to 4.4% for the healthy control group), although the box plot shows very small lower quantiles (Fig. 7). The difference between the results for the patient group and the healthy volunteers group can be explained by fundamental differences in the datasets. The wall in a healthy bowel measures just a few millimeters. The regions of interest to measure contrast uptake are small and thin. Motion will have a larger effect in this case and motion compensation helps to improve the plausibility of perfusion in terms of the perfusion model. In bowel data of the patients, the area of contrast uptake is larger and so is the region of interest for analysis. Almost no differences can be observed between the automatic and manual approach comparing the median values.

For pancreas datasets, the average values between patients and healthy volunteers are within the same range for the proposed automatic approach (median of 32.1% for patients compared to 22.1% for healthy volunteers; see Fig. 6). For pancreas datasets of the healthy control group both approaches performed equally with no significant differences. However, for the patients group the manual approach was able to achieve significantly better results in terms of the model fitting quality (median of 55.5% compared to 23.7% for the automatic approach). The pancreas appears brighter in the contrast sequence for the patients group. As manual motion compensation uses both the B-mode and contrast images, this helps the medical expert to find a better alignment compared to the automatic approach which only uses the B-mode sequence for motion correction. The automatic motion compensation improves the quality of fit to the pharmacokinetic model for the patients group of pancreas data compared to results produced without motion compensation.

The frame selection module can be used for any other CEUS application. The motion compensation module can be adapted to process different recordings. The parameterization depends on the temporal resolution of the image sequence. Initial tests for kidney and liver recordings showed results comparable to bowel and pancreas.

The study was conducted by a single observer, i.e., inter-observer variability was not investigated. Additionally, we did not know the ground truth about the perfusion. So even though the model is correct and the fitting quality to the model was improved by motion compensation, we have no guarantee that the resulting perfusion information reflects the truth or not. However, pharmacokinetic model analysis is common practice in CEUS analysis. High conformity with such a model will provide confidence in physiological parameters extracted from the model to perform diagnosis. In order to be clinically valuable, the quality of the automatic motion compensation needs to be further improved, e.g. by using the information from the contrast sequence by applying a pharmacokinetic model for measuring registration quality.

To assess the clinical use for the application to other abdominal organs, additional studies need to be conducted. Further studies also need to cover intra- and inter-observer variability.

@&#CONCLUSIONS@&#

The presented methods offer a thorough approach to process CEUS recordings for perfusion analysis. They are able to reduce variability and analysis time compared to a manual approach especially with regard to noisy datasets. Compared to a time consuming manual analysis, the proposed system is able to reach the same level of accuracy in terms of a pharmacokinetic model evaluation in bowel and pancreas datasets. This is an important aspect as such a model is also used to extract perfusion parameters for diagnosis. The system is generalizable in order to be used for different abdominal organs. However, additional studies need to be conducted to assess the performance improvement compared to manual analysis.

None declared.

@&#ACKNOWLEDGMENTS@&#

This work has been funded by the German Research Foundation (DFG) (No. TO166/13-1). It has also been supported by the project of the Czech Science Foundation (No. GA102/12/2380) and the European Regional Development Fund – Project FNUSA-ICRC (No. CZ.1.05/1.1.00/02.0123). The study was supported by MedViz (http://medviz.uib.no/), an interdisciplinary research cluster from Haukeland University Hospital, University of Bergen and Christian Michelsen Research AS.

@&#REFERENCES@&#

