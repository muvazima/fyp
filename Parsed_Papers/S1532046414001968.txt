@&#MAIN-TITLE@&#A natural language processing pipeline for pairing measurements uniquely across free-text CT reports

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Oncology guidelines standardize response assessment based on measurements.


                        
                        
                           
                           Regular expression-based techniques recognize measurements in reports.


                        
                        
                           
                           We introduced the task of pairing lesion measurements across consecutive reports.


                        
                        
                           
                           A natural language processing pipeline can be construed for the pairing task.


                        
                        
                           
                           A post-processor enforces that each measurement is matched with at most one other.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Natural language processing

RECIST

Radiology report

Oncologic measurement

Information correlation

@&#ABSTRACT@&#


               
               
                  Objective
                  To standardize and objectivize treatment response assessment in oncology, guidelines have been proposed that are driven by radiological measurements, which are typically communicated in free-text reports defying automated processing. We study through inter-annotator agreement and natural language processing (NLP) algorithm development the task of pairing measurements that quantify the same finding across consecutive radiology reports, such that each measurement is paired with at most one other (“partial uniqueness”).
               
               
                  Methods and materials
                  Ground truth is created based on 283 abdomen and 311 chest CT reports of 50 patients each. A pre-processing engine segments reports and extracts measurements. Thirteen features are developed based on volumetric similarity between measurements, semantic similarity between their respective narrative contexts and structural properties of their report positions. A Random Forest classifier (RF) integrates all features. A “mutual best match” (MBM) post-processor ensures partial uniqueness.
               
               
                  Results
                  In an end-to-end evaluation, RF has precision 0.841, recall 0.807, F-measure 0.824 and AUC 0.971; with MBM, which performs above chance level (P
                     <0.001), it has precision 0.899, recall 0.776, F-measure 0.833 and AUC 0.935. RF (RF+MBM) has error-free performance on 52.7% (57.4%) of report pairs.
               
               
                  Discussion
                  Inter-annotator agreement of three domain specialists with the ground truth (κ
                     >0.960) indicates that the task is well defined. Domain properties and inter-section differences are discussed to explain superior performance in abdomen. Enforcing partial uniqueness has mixed but minor effects on performance.
               
               
                  Conclusion
                  A combined machine learning–filtering approach is proposed for pairing measurements, which can support prospective (supporting treatment response assessment) and retrospective purposes (data mining).
               
            

@&#INTRODUCTION@&#

Health care enterprises are under continuous pressure to produce “predictable and reproducible outcomes” from clinical examinations and diagnostic tests, which “requires that diagnostic information be expressed in quantitative form” [1]. In oncology, guidelines have been proposed to standardize and objectivize treatment response assessment, such as the World Health Organization guidelines [2] and RECIST (Response Evaluation Criteria in Solid Tumors) [3]. These guidelines are primarily based on radiologic measurements of selected index lesions [4].

Lesion measurements are generally made by radiologists [5] after selecting a set of representative and/or previously measured lesions. They are subsequently communicated by means of free-text radiology reports [6,7]. The free-text nature of radiology reports prohibits automated processing of their information content in support of downstream consumers [7–10], such as oncologists and clinical research associates (CRAs). Oncologists rely on reported measurements and qualitative assessments to synthesize treatment response status and to determine an optimal care plan. CRAs parse radiology reports of cancer patients to transcribe their lesion measurements into clinical trial databases.

If lesion measurement data were available in structured [11] and digital form [12], as a supplement to the narrative radiology report, it could be leveraged to support downstream consumers. Properly grouped by lesion, structured measurement data could be used to effortlessly compute RECIST scores and could be inserted automatically into clinical trial databases. Oncology information systems that accomplish this have the potential to minimize transcription errors [13,14], improve efficiency and facilitate data-driven treatment response assessment for on- and off-trial cancer patients alike. They may further open up novel application areas such knowledge discovery through data mining [15], cohort selection using advanced queries [16], and multi-disciplinary collaboration in oncology [17].

Such oncology information systems face three technological challenges. (1) Data acquisition: Data elements are obtained from structured or narrative sources [18]. In the latter case, pertinent data elements can be disclosed by natural language processing (NLP) techniques [19], for instance, for automatically synthesizing treatment histories [20] or populating registries of cancer patients [21]. (2) Data integration: multi-source and longitudinal data elements are mapped into one coherent data structure [22,23]. (3) Data presentation: integrated data elements are presented graphically to the user [24,25].

Systems that address these challenges in isolation have been reported more frequently in the literature than systems that address them in combination. A recent system that exemplifies the latter category extracts neuro-oncologic findings from a history of radiology reports and normalizes it with respect to a controlled interval change vocabulary containing, e.g., “existing” and “improving” [26,27].

In this work, we introduce the task of extracting and pairing measurements across consecutive reports. A unique feature of the task is that across two consecutive reports, a measurement is paired with at most one other measurement. We call this the partial uniqueness condition. This condition is motivated by the observation that in clinical practice once measured the vast majority of lesions continues to be measured in subsequent follow-up exams, unless the lesion resolves or if the radiologist fails to report its measurement. The output of automated solvers of this task can be utilized by downstream modules, e.g., for visualization or automated treatment response assessment.

In this paper, we propose a natural language processing (NLP) pipeline that consumes a patient’s history of narrative radiology reports and segments [28] them in the pre-processing phase. Then, addressing challenge 1, measurements are extracted, normalized and labeled with respect to their temporal orientation [29]. Finally, addressing challenge 2, measurements are paired across reports and a filter is proposed that enforces the partial uniqueness condition, which, as we argue above, holds for the vast majority of lesions.

@&#RELATED WORK@&#

All components in the pipeline proposed in this work are home grown, leveraging the results of prior research projects. Third-party engines can, however, be used to achieve parts of the aimed measurement pairing functionality.

Report segmentation, i.e., the automated break down of a medical narrative document in its main components (e.g., sections, subsections and sentences) has been studied in the literature, either as component of a general-purpose system (e.g., MedLEE [30], Leximer [31] and cTAKES [32]) or as a dedicated engines [28]. A potential downside of general-purpose systems is that their respective output must be processed further to retrieve the additional radiology-specific structure that cannot be assumed to exist in narrative documents from other medical specialties (e.g., oncology notes) that are within the scope of the general-purpose system. MedLEE recognizes measurements, which constitute the core tokens in the measurement matching task. This engine can thus be used as an alternative to our measurement extraction engine.

In previous research, we developed a pipeline that extracts and normalizes measurements from radiology reports. In addition, a classification engine in this pipeline was developed that detects the “temporal orientation” of a given measurement, that is, if the measurement was made on the current or prior exam. This engine was deployed to estimate the number of measurements across radiology reports of different modalities and anatomies [33]. To the best of our knowledge such methods have not been researched before. Indeed, we are not aware of any information extraction system that produces an output from which a measurement’s temporal orientation can be derived with relatively lightweight logic.

The work presented in this paper is an extension of a conference paper [34] in the sense that it includes chest reports in its ground truth in addition to the initial abdomen reports. Further, we extended the pipeline with the aforementioned MBM engine and report on micro analysis results.

We explore two approaches to automatically pairing measurements, which we define as a binary classification problem of instances. In the context of two consecutive reports, an instance is a pair of measurements from the Findings sections of the prior and current report, respectively. An instance is positive or a match, if its measurements quantify the same clinical finding on their respective exams [34], see Fig. 1
                     , which will serve as a running example throughout this section. Measurements from non-Findings sections are excluded as they report slice thickness (Technique) or re-iterate measurements from the Findings sections as a means to support the overall impressions of the radiological examination (Conclusion).

The first approach uses machine-learning methods to integrate features that quantify volumetric similarity between measurements, semantic similarity between their respective narrative contexts and structural properties of the measurements’ report positions. The second extends the first approach by a novel post-processing technique based on mutual best matches, which enforces the partial uniqueness condition. The proposed pipeline, including the post-processing filter, is schematically displayed in Fig. 2
                     .

A ground truth is constructed based on the abdomen and chest CT reports of 50 patients each. The ground truth’s quality and reproducibility of the ground truth construction process are assessed in an inter-annotator agreement study with three CRA domain specialists. The performance of the entire pipeline is assessed in an end-to-end evaluation with and without the mutual best match filter against the ground truth.

A database of radiology reports was obtained from The University of Chicago Medical Center. The reports were authored using dictation software (PowerScribe, Nuance, current version 3.0.19.6) with in-house developed reporting templates with all-caps section headers and anatomical paragraph headers, see Fig. 1.

We de-identified our dataset using the following approach. All dates in the database, in the form of metadata as well as narrative references in the reports, were offset by randomly generated, patient-specific integers. All other types of HIPAA patient health information were removed using a homegrown engine driven by a collection of regular expressions. The database was accessed under waived IRB 13-0379.

We identified pairs of reports based on two match criteria: whether the reports are consecutive and pertinent. Two reports from the corpus are consecutive if the more recent report (the current report) references the older report (the prior report) in its comparison section. A pair of consecutive reports is pertinent if their respective Findings sections contain at least one measurement. Fifty patients were randomly selected that have at least one pair of pertinent abdomen CT reports, and another fifty patients were randomly selected that meet the same criteria with respect to chest CT reports. The reports of these one hundred patients were separated from the other reports in the database, which were used in the pre-test development phase.

The ground truth was created by one reader (MS) in a homegrown WPF (Windows Presentation Foundation) annotation tool that was specifically designed to designate matching measurements between consecutive reports (Fig. 3
                           ). The ground truth can thus be seen as a mapping from measurement pairs (instances) to a binary label (match yes/no).

We divided each of the two groups of 50 patients in two disjoint groups of 25 patients. The reports of each of the resulting four patient groups were obtained, named A1, A2, C1 and C2. Three University of Chicago Medical Center CRAs were instructed on how to use the annotation tool (Fig. 3) by means of two sample report pairs that were not included in the ground truth. During the annotation creation process, the study supervisor (MS) was present to answer questions. None of the CRAs reported significant difficulties using the annotation tool.

The report pairs were annotated by the following CRAs: A1 reports by author JB, A2 and C1 reports by author WT, and C2 reports by author AC. Thus every pair of consecutive reports was annotated in the ground truth construction phase and by precisely one CRA. The CRA annotations of A1, A2, C1 and C2 were combined and the result was called the “benchmark annotation.”

The ground truth was compared against the benchmark annotation using Cohen’s κ and F-measure to quantify inter-annotator agreement [35].

The proposed pipeline consists of off-the-shelve solutions and novel classification techniques.

The pre-processing engines were used as off-the-shelve solutions: they were developed before the work on the current pipeline ensued and were not altered in this process.

The reports are segmented [36] in sections, paragraphs and sentences by means of a sentence boundary algorithm that was implemented as a maximum entropy model [37] using the SharpEntropy library. Section headers were normalized with respect to Technique, Comparison, Clinical history, Findings, Impressions and N/A. Non-section header sentences are grouped in paragraphs. Each first sentence in a paragraph of a Findings section is compared against a list of anatomical paragraph headers. If successful, the matching sentence was marked as paragraph header and was normalized (e.g., “LIVER, BILIARY TRACT”→“liver biliary tract”). Normalization of section and paragraph headers was done automatically, using a list of headers present on the abdomen and chest reporting templates.

In the pre-processing phase, the measurement extraction engine 
                           [33] recognizes measurements in free text by means of regular expressions. In Fig. 1, it would recognize the highlighted strings as measurement strings.

The measurement normalization engine maps each recognized measurement string onto a measurement data structure. For instance, the string “2.1×2.3cm” (measurement A1 from Fig. 1) would be mapped onto a data structure representing it as a two-dimensional measurement of magnitude 23mm in the long axis and 21mm in the short axis. Normalization allows us to detect that two syntactically dissimilar measurements are volumetrically identical (e.g., “2.1×2.3cm” versus “23 by 21mm”).

The temporal orientation engine [29] determines if a measurement describes an entity on the current exam (e.g., A1 in: “A representative left supraclavicular lymph node measures 2.1×2.3cm {A1}”), on the prior exam (e.g., B5 in: “Reference retrocrural lymph node now measures 3.4×2.4cm {B4}; this is in comparison to 3×2.2cm {B5} on prior”), or on both (e.g., B6 in: “Stable reference mesenteric pelvic adenopathy now measuring 4.4×3.9cm {B6}”). This engine is based on a maximum entropy classifier that weighs selected keywords (e.g. “measures”, “stable”) and more complex narrative contexts (e.g. “this is in comparison to …”).

For development of the classification engine, its features more in particular, and the mutual best match filter a set of some 200 reports was used that was disjoint from the set of reports underlying the ground truth.

We define 13 features for characterizing an instance, i.e., a pair of measurements (A, B) in which A is a measurement from the prior report and B is a measurement from the current report. The features are grouped in four families.

Volumetric and dimensional similarity—(1) The first feature is the label assigned to measurement A by the temporal orientation engine, i.e., “current”, “prior” or “comparison”. (2) The next feature gives the temporal orientation label of measurement B.

The measurement normalization engine derives the dimension for each measurement, e.g., “2.1cm” has dimensionality 1, whereas “2.1×2.3cm” has dimensionality 2. (3) We use the difference between the dimensionalities of A and B.

To compare one, two and three-dimensional measurements, we introduce the notion of normalized length. The normalized length of a one-dimensional measurement is the measurement proper; the normalized length of a two-dimensional measurement (e.g., “1.2×3.4cm”) is the square root of its surface ([1.2×3.4][1/2]); the normalized length of a three-dimensional measurement (e.g., “1.2×3.4cm×5.6”) is the cube root of its volume (1.2×3.4×5.6][1/3]).

The spatial ratio between two measurements is the normalized length of the larger measurement divided by the normalized length of the smaller measurement. (4) We use the spatial ratio between A and B set to –1 if it exceeds 2, which is illustrated in Table 1
                               among other features. Note that the length, surface and volume computed from a measurement are geometric approximations of the real dimensions of an oncological lesion, which are typically irregularly shaped.

For measurement A from the prior report, let d be the minimum of its spatial ratios with the measurements B1, B2,… in the current report. (5) We use the spatial ratio between A and B divided by d. Finally, (6) we use the symmetrical feature for measurement B from the current report. We call features (5) and (6) the relativizations [38] of feature (4).

Decision rules—A list of key phrases was compiled that indicate lesion interval stability, including “stable” and “unchanged”. If the sentence in which current measurement B appears contains a stability phrase, B is more likely to match a prior measurement that is spatially similar. (7) We use the binary feature that is 1, precisely if the following conditions are met: Measurement B co-occurs with a stability phrase; B was labeled with “comparison” by the temporal orientation engine; and the spatial ratio between A and B is smaller than 1.1.

Radiologists may mention prior measurements to make the interval change documentation in the current report comprehensive and self contained, e.g., B5 in “Reference retrocrural lymph node now measures 3.4
                              ×
                              2.4 cm {B4}; this is in comparison to 3
                              ×
                              2.2 cm {B5} on prior”. We can exploit these prior measurements as pointers to establish a match between B4 and a measurement in the prior report with dimensions 3 and 2.2cm. (8) We use the binary feature that is 1 for measurement pair (A, B), precisely if the following conditions are met: Measurements A and B were labeled “current” by the temporal orientation engine; in B’s sentence sits another measurement C (the pointer) that was labeled “prior”; and the spatial ratio between A and C is 1.

Semantic similarity—The next feature family quantifies the semantic similarity of the narrative contexts of A and B. We let the narrative context of a measurement be given by the sentence in which it appears and the inverse-document frequency (idf)-weighted cosine similarity metric of its words (excluding stop words). More precisely, we regard each sentence as a word vector that has a cell for each word in the domain. For each word in a sentence, the corresponding cell in the sentence’s word vector carries the idf value log(500,000/[N
                              +1]), where N is the prevalence of the word in a 500,000-sentence corpus of radiology reports obtained from a representative sample of general radiology reports obtained from the same institute as the ground truth corpus. (9) The semantic similarity feature assigns to A and B the cosine of their sentences’ word vectors. Semantic similarity is proportional to the cosine similarity, which is in turn proportional to the idf-weights of the words that appear in the sentences of both A and B.

For measurement A from the prior report, let d be the maximum of its sentence’s cosine similarity with the respective sentences of measurements B1, B2,… in the current report. (10) We use as an additional feature the cosine similarity between A and B divided by d. Finally, (11) we use the symmetrical feature for measurement B from the current report. As before, features (10) and (11) are relativizations of feature (9).

Structural similarity—The order in which a patient’s findings are reported are grossly the same across reports. We exploit this by comparing the “ranks” of two measurements. If a measurement is the K-th measurement in a report with N measurements, we defined its rank as K/(N
                              +1). (12) We use the difference between the ranks of A and B.

Finally, (13) we use the binary feature that returns 1 if the paragraphs in which the measurements appear have matching anatomical headers.

A Random Forest [39] classifier (RF) integrates the 13 features to characterize instances. In the learning phase, RF classifiers create a number of standard decision trees (numTrees). Each decision tree is created using a randomly selected subset of features of fixed size (numFeatures). The RF classifier aggregates the votes of its individual decision trees by taking a majority vote as the predicted outcome. In the case of our (binary) problem, the RF classifier accepts a pair of measurements as a match precisely if the majority of decision trees predict the pair is a match.

We use the Weka [40] implementation of RF using the following parameter settings: maxDepth=unlimited; numFeatures=log2(13)+1=4.70; numTrees=10. The value of numFeatures is based on the number of features used to characterize each instance (13). We experimented with larger values for numTrees but this did not result in higher performance. For efficiency purposes we therefore selected numTrees=10. We also experimented with Naïve Bayes and Support Vector Machine [41] classifiers, but these consistently achieved inferior results on our task [34].

One of the primary purposes of follow-up oncological imaging is to assess the dimensions of previously identified lesions. The radiology reports pertaining to such a series of follow-up exams describe the measurements of the identified lesions. Per lesion thus measured, a string of its measurements (A, B, C, …) can be construed that has as a unique feature that matches within this string are unique: A only matches B, B only matches C, and so on. Occasionally, a lesion measurement is not reported, for instance, when the lesion is no longer measurable or when the radiologist failed to describe it. For such lesions with missing measurements only a partial string (A, B, C) can be construed even though there are reports succeeding the one in which C was reported. In this event, the matches in the string are partially unique since C is not matched in any subsequent existing report.

The above observation motivates the partial uniqueness condition, which can be defined as follows: every measurement can be matched with at most one measurement in the prior report and with at most one measurement in the next report.

The pairs produced by RF may violate the partial uniqueness condition, as there is no mechanism in place to prevent RF from matching one measurement A with two measurements B1 and B2.

We introduce a novel post-processing routine to ensure that the resulting labeling meets the partial uniqueness condition. The RF classifier assigns to each instance (A, B) a certainty level Pr(A, B) from the interval [01] representing the likelihood that it is a match. The pair (A, B) is a mutual best match if RF classifies (A, B) as a match and A is the most likely candidate for B and vice versa, i.e., if Pr(A, B)=max{C: Pr(A, C)} and Pr(A, B)=max{C: Pr(C, B)}. Instances that are not mutual best matches are removed in the post-processing step. If the resulting set of matches does not meet the partial uniqueness condition, we randomly remove matches until it does. This procedure is illustrated in Fig. 4
                              .

The combination of RF and the mutual best match (MBM) filter constitutes the second correlation engine and shall typically be referred to as RF+MBM.

Below we detail the evaluations conducted for assessing the performance of RF and MBM individually and combined (RF+MBM).

The pre-processing engines were used as off-the-shelve solutions: they were developed before the work on the current pipeline ensued and were altered in this process. The report segmentation engine was internally evaluated on one neuroradiology data set from one US-based radiology institute, a breast radiology data set from the University of Chicago Medical Center and a general radiology data set from multiple US-based radiology institutes. Both the measurement extraction engine (precision=0.994 and recall=0.991 [33]) and the temporal orientation engine (accuracy=0.960 [29]) were formally evaluated in previous work.

RF is evaluated in a 10-fold cross validation protocol. In each fold, nine tenth of the instances are used for training an RF model, the remaining one tenth is used for evaluating the trained model. In this manner, RF assigns a predicted class and certainty level to each instance. The predictions and certainty levels are subsequently consumed by the MBM post-processor, which outputs a second set of predictions that constitutes the output of RF+MBM. We assess the performance of MBM individually by assessing if it selects the right match significantly above chance level (binomial test; P
                        <0.05) if RF matched one measurement in the first report with multiple measurements in the second report.

A true positive is an instance that is correctly identified as match by either engine; a false positive is an instance that is incorrectly identified as match; and likewise for true and false negatives. MBM randomly removes instances in order to break a tie between two equally good match candidates. To avoid that our evaluation data would be affected by a random source, we count the randomly removed instance as a false negatives, which will only affect the reported results negatively.

We conduct a macro and averaged micro analysis. In the macro analysis, the instances are pooled across report pairs and precision, recall (or sensitivity), F-measure and area under the receiver–operator curve are computed as usual [42]. We further compute precision, recall and F-measure for each report pair individually. Then we take the mean of these report-specific scores yielding their averaged micro score. We use the report pair-level evaluation scores to evaluate the impact of the number of instances per report pair on the engines’ performance.

If the ground truth contains no positive instances for a report pair, the recall of any classifier on this report pair is undefined (since the sum of the true positives and false negatives is 0). In this case we use two recall estimators: default value 0.5 and the macro recall value of the appropriate engine on the data set at hand (e.g., RF on abdomen). Similarly, if a classification engine (RF or RF+MBM) labeled none of the instances in a report pair positive, its precision on this report pair is undefined (since the sum of true and false positives is 0). In this case, we use default value 0.5 and the macro precision value as precision estimators.

@&#RESULTS@&#

The abdomen data set contained 330 reports, 283 of which were involved in a pertinent pair of reports; versus 387 and 311 in the chest data set, see Table 2
                        .

Inter-annotator agreement (κ) ranges from 0.960 to 0.973, see Table 3
                        .

Macro scores of the RF and RF+MBM engines are given in Table 4
                        . All scores are higher than 0.5. Hence, the averaged micro scores of both classifiers, shown in Table 5
                        , is higher using the macro estimator than the default estimator.

Both in the macro and averaged micro analysis, the F-measures of both engines are highest on the abdomen set. In this data set, macro F-measure of RF and RF+MBM is 0.894 and 0.903, respectively, and averaged micro F-measure of RF and RF+MBM is 0.878 and 0.891 (both with macro estimator).

Precision consistently trumps recall under all conditions investigated (RF with or without MBM/data set/estimator), especially for RF+MBM. RF’s macro recall is higher than its averaged micro recall on all three data sets, regardless the estimator. For RF+MBM the converse pattern applies: its macro precision is higher than its averaged micro recall, regardless the estimator.

The averaged micro F-measure scores of both engines do not exceed their macro F-measure scores on all three data sets (except RF+MBM on chest with the macro estimator). This indicates that the incorrectly labeled instances are not equally distributed over the report pairs.

We explore if the number of instances in a report pair influences the pair’s micro scores. To this end, we separate the report pairs in four quartiles of increasing number of instances Q1,…,Q4. In this manner, Q1 contains the 25% of report pairs with the fewest instances, and so on. Averaged micro F-measures of the reports in these four subsets are given in Table 6
                        . Highest averaged micro F-measures are obtained in Q2, whereas lowest are obtained in Q1. In abdomen, unlike the other two data sets, the scores of Q4 are comparable to those in Q2 (0.848, 0.898 in Q2 versus 0.854, 0.854 in Q4), that is, the performance of the engines does not degrade on report pairs with relatively many instances.

RF has flawless performance in 52.7–58.4% of report pairs (micro F-measure 1), see Table 6. In these report pairs no human intervention would have been required to establish the correct measurement correlation. These ratios are higher when MBM is applied, with scores ranging from 57.4% (abdomen–chest) to 63.0% (abdomen) of report pairs. The portion of report pairs in which RF misclassified all instances (micro F-measure 0) ranges from 5.9% (abdomen) to 11.3% (chest). Again, this portion is higher for RB+MBM: from 6.8% (abdomen) to 13.9% (chest) of report pairs.

The receiver–operator curves (Fig. 5
                        ) show that AUC is consistently higher for RF than for RF+MBM, with highest difference in the chest data set (0.959 versus 0.916).

In abdomen, MBM removed 6.8% (25/369) non-mutual best matches, versus 11.7% (61/519) in chest and 9.4% (81/861) in abdomen–chest. Then, 0.5% (2/[369–25]) of the remaining positive instances were randomly removed in abdomen, versus 1.3% (10/[519–61]) in chest, and 0.8% (6/[861–81]) in abdomen–chest.

If RF paired a measurement with two or more other measurements, MBM singled out the correct measurement with success rate 71.7% (38/53) in abdomen, 62.2% (74/118) in chest, and 65.5% (112/171) in abdomen–chest. This is significantly above chance level (P
                        =0.002, P
                        =0.007, P
                        <0.001, respectively).

If RF matches one measurement with two measurements, one of the two matches thus established is false positive per the partial uniqueness condition. In abdomen–chest, 19.8% (171/862) of the instances classified as positive by RF shared a measurement. This set accounted for 65.6% (90/137) of RF’s false positives. MBM filtered out 64.4% (58/90) of these false positives (to become true negatives).

MBM is a filter in the sense that it can re-label positive instances but not negative instances. If MBM’s call is correct, it turns a false positive into a true negative. In this manner, MBM reduced the number of false positives in abdomen–chest by 43.1% (137→78). If MBM’s call is incorrect, it turns a true positive into a false negative. MBM increased the number of false negatives by 16.2% (173→201) in abdomen–chest.

@&#DISCUSSION@&#

The inter-annotator agreement is very high. This indicates that the ground truth creation process is reproducible and that the task of pairing measurements across reports is well defined. Most disagreements between the ground truth and the CRA-annotated reports were due to plain oversights. Other sources of disagreements were domain (e.g., lesions with different descriptions between reports: “gastrohepatic ligament node” and “paraceliac node”) and task specific (e.g., when a lesion was measured in a different view or manner, it was unclear if it should be matched to the original measurement).

Volumetric and dimensional similarity—We conducted an end-to-end analysis of our classification pipeline. In this manner, errors made in the pre-processing phase introduce noise for the correlation classifiers downstream. For instance, 20 of 897 positive instances (2.2%) in abdomen–chest involved a measurement that was incorrectly classified as pointing to the prior exam by the temporal orientation engine. Subsequently, RF negatively classified 19 of these instances, comprising 11.0% of its false negatives (19/173).

Decision rules—The two binary decision rules (features (7) and (8)) applied infrequently (7.3% of instances; data not shown). This was a consequence of the fact that oftentimes the prior measurement is reported in a separate sentence (this measured 1.1
                        ×
                        2.0 cm on prior exam) or that a stability qualifier is lacking. Both features were also distracted by prior measurements that were proportionally similar to the current measurement, causing false positives.

Semantic similarity—In abdomen–chest, the average semantic similarity of the narrative contexts was 0.366 (±0.223) for true instances and 0.284 (±0.197) for negative instances. Sentences with low cosine similarity may still produce positive instances. This is typically the case if measurements are described in a separate sentence, remote from other diagnostic and/or anatomical descriptors. For instance, 12.1% of RF’s false negatives (21/173) involved a sentence that contained only measurement information and not, e.g., diagnostic information. (This has increased in size compared to prior exam, now measuring 1.4
                        
                        ×
                        
                        1.7 cm.) We experimented with augmenting sentences with few words with their next and/or previous sentence, but this did not improve results.

Conversely, sentences with high cosine similarity are not always about the same finding. RF seems to be distracted by high cosine similarity scores: 67.1% (92/137) of RF’s false positives has cosine similarity larger than 0.366, compared to 28.8% (2205/7642) for false instances in general.

The cosine framework based on inverse-document frequency (idf) may not be optimal for our task, as it presumes that infrequent words are more salient than frequent words. The words “left” and “right” were respectively the 8th and 12th most frequent words in the abdomen–chest data set, yet highly salient. Generally, sentence pairs with conflicting laterality determiners (“left”/“right”) were less likely to contain matching measurements (0.7%) than arbitrary sentence pairs (9.8%) or sentence pairs with aligned laterality determiners (18.1%). Different techniques for determining salience, such as language models [42], may help to improve the impact of semantic features on the overall classifier performance.

A straightforward argument shows that MBM negatively affects both precision and recall if its call is incorrect and that it only positively affects precision if it was correct. This explains why MBM has a negative impact on recall. From the fact that MBM’s net effect on macro precision is positive on all data sets, we conclude that its performance was sufficiently robust to outweigh the double penalty for incorrectly re-labeled instances. Indeed, the macro precision gain compensates the loss of macro recall in the sense that MBM had a minimal positive effect on macro F-measure on all data sets.

In an effort to optimize macro F-measure of RF+MBM, one could lower RF’s threshold for labeling an instance as positive. Doing so may deteriorate RF’s precision, but may yield superior macro F-measure for the combined engine.

Both classifiers performed better in abdomen than in chest. This may be caused by the fact that the distribution pattern of measurements over paragraphs varies between the domains: in chest, 79.0% of measurements sit in two paragraphs (“lungs and pleura” and “mediastinum and hila”) versus 29.7% in abdomen (“liver and biliary tract” and “retroperitoneum, lymph nodes”). In a χ
                        2 analysis, feature (13), the feature that characterizes if two measurements sit in the same paragraph, ranks 2nd in abdomen (out of 13) and 10th in chest, reducing this feature’s discriminative power.

Considering that the inter-rater variability is comparable between the abdomen and chest components of the ground truth (Table 3), we argue that the task is not inherently more complex on chest than on abdomen.

We observed that averaged micro F-measures of the Q2 and Q4 segments are comparable in abdomen but not in chest. The aforementioned fine structure imposed by the anatomical paragraph headers in abdomen reporting templates may explain this difference.

Averaged micro F-measure is consistently lowest in Q1. In abdomen–chest, the report pairs in Q1 have 1.77 instances on average (versus 17.25 for the entire corpus). For all report pairs in Q1 it is the case that either report has precisely one measurement and for 37.1% of report pairs both reports have one measurement. The latter subset of report pairs gives rise to one instance. For this one instance, the relativized features (features (5), (6), (10) and (11)) will indicate maximal relative similarity between the instance’s measurements since there are no other measurements to compare with. As an example, suppose A and B are the single measurements from the prior and current report, respectively. Even if measurement A is 80% larger than B (spatial ratio 1.8) and their semantic contexts are quite dissimilar (their sentences having only frequent non-salient words in common), the relative spatial ratio between A and B and the relative semantic similarity of their sentences is maximal, simply because there are no alternative candidates for A and B, respectively.

This introduces a bias toward accepting instances as match, offering one explanation for the somewhat paradoxical result by which the classification task seems to become harder if there are fewer candidate measurements in the other report. This reasoning partially applies to report pairs in which only one report has precisely one measurement. In such report pairs, the measurements from the multi-measurement report have no alternative candidate measurements for comparison, biasing two of the four relativized features.

We observed that precision is consistently higher than recall in all data sets. When MBM is applied, this gap is even widened. Higher precision is useful for pre-populating and correction–suggestion functionality in RECIST-compliant lesion management tools. On the other hand, lower recall may produce incomplete search results in data interrogation applications that allow for longitudinal search queries.

Our averaged micro analysis showed that performance of the engines is weaker on reports with fewer instances. We argue that in the context of a lesion management tool, lower averaged micro scores on these reports is less of a concern than on reports with many instances. If there is only instance in a report and it is incorrectly pre-populated by the engine, the user will readily observe this and make the necessary correction. If the number of measurements and instances increases, we expect that detecting flaws in the pre-populated matches will be more tedious to the user.

We saw that no human intervention would have been required to correctly correlate the measurements in 60.2% (chest) to 63.1% (abdomen) of report pairs if RF+MBM annotations were adopted. This rate may be higher for patients with many follow-ups. Anecdotally, we noticed that the content and structure of the reports of such patients become increasingly homogeneous as the pathology of all abnormal findings has been established and differential diagnoses have been ruled out. It may thus be that our engines have better performance correlation measurements between late follow-up reports than between the reports of the baseline and first follow-up exam.

The MBM post-processor has minimal impact on F-measure (slightly positive) and area under the ROC (slightly negative) with respect to the RF base classifier. This shows that the partial uniqueness condition can be enforced with minimal impact on performance. If series of paired measurements are known to adhere to the partial uniqueness condition, this can be exploited by downstream applications, such as visualization engines that present each such thread as a non-branching time series.

@&#FUTURE WORK@&#

The classifiers can potentially be improved by incorporating more complex contextual information. One feature that we did not exploit, for instance, is the qualitative descriptors often describing the lesion’s interval change. If a lesion is described as increased in size, we can eliminate all potentially matches with prior measurement that have larger normalized length. Such qualitative descriptors can be found in the measurements’ immediate context or, as suggested by one of the reviewers, in the conclusion section of the report. Another potentially promising feature is the slice number on which the lesion was measured, which should not vary substantially if the same imaging protocol was used. Extraction of image reference information was researched in prior research [36].

The proposed pipeline can be leveraged to generate a problem list of measured findings. For a series of paired measurement findings, downstream modules could search and match in the respective narrative contexts other characteristic features, such as anatomical location, visual appearance and diagnostic information. In this manner, measurements could function as a characteristic “seed” that can be grown to fully represent the measured clinical finding.

The RECIST type of a lesion – tumor or lymph node – is a natural candidate for augmenting the representation of a series of paired measurements. This information could then be utilized to support fully automated RECIST computation, as the lesion type of a measured finding determines the pertinent axis: longest for tumors; smallest for lymph nodes.

Such augmentation-by-seeding techniques rely on the partial uniqueness condition, which is violated biologically if a measured finding splits or if two findings coalesce. In such cases, it is common practice at the institution from which the data is obtained to register the initial lesion(s) as resolved and the offspring lesion(s) as new entities. This registration procedure respects the partial uniqueness condition, but it may be difficult to pick up for an automated pairing algorithm. Our data set did not contain splitting or coalescing lesions.

Finally, an important direction for future work is integration of the measurement pairing pipeline into solutions in support of clinical and research workflows.

@&#LIMITATIONS@&#

The corpus was obtained from one academic institute, which may have an idiosyncratic reporting style. For instance, it cannot generally be assumed that a radiology department uses reporting templates, let alone templates that have an anatomical subsection structure. In addition, prior measurements were generally reported as a means to quantitatively report interval change. This reporting style produces more complete reports but is obviously less efficient than styles in which interval change is only described qualitatively, i.e., without explicit mention of prior measurements, which may be dominant in private practice. Not reporting prior measurements may impact the results of our classifiers as it reduces the total number of measurements per report, the effects of which have been studied, and eliminates pointer cues that can be picked up by one of our decision rules (feature (8)).

The ground truth was created by a clinical informatician (MS) and validated against the annotations of CRA domain experts. We elected this procedure to ensure internal consistency of the ground truth, which may not have been obtained had it been construed from multiple annotations, and to reduce the workload for the CRAs as each whom annotated one fourth to one half of the report pairs that underlies the ground truth. Thus, the ground truth is not the result of aggregating the annotations of one or more domain experts, which is a limitation of our study. Considering the high inter-rater agreement scores of the individual CRAs with the ground truth, however, we are inclined to believe that this limited ground truth construction process has had no substantial impact on the performance results reported in this work.

The performance of the classification pipeline (with or without MBM) was assessed in an end-to-end evaluation. We discussed that errors introduced by the pre-processing engines, which were used as off-the-shelves solutions and were not individually evaluated, introduce for the classifiers downstream. A so-called oracle evaluation in which the output of the pre-processing engines is manually corrected before it is presented to the classifiers, could be used to assess the performance of the classifiers individually and to assess the impact of the noise introduced by the pre-processing engines. We did not conduct such an oracle evaluation, which is another limitation of our work.

@&#CONCLUSIONS@&#

If lesion measurement data were available in structured and digital form, as a supplement to the narrative radiology report, it could be leveraged to support downstream consumers. We proposed a natural language processing pipeline for automatically extracting and pairing measurements across consecutive radiology reports. A unique feature of this task is that between consecutive reports, each measurement is paired with at most one other measurement (“partial uniqueness”). High inter-annotator variability suggests that the task is well defined.

Our main technical contribution is the development and evaluation of a Random Forest classifier that utilizes contextual, narrative and volumetric properties to decide if two measurements “match”. A novel post-processing technique is proposed that enforces partial uniqueness. This technique, when applied to the output of the base Random Forest classifier, performs significantly above chance level and has a small positive effect on F-measure and small negative effect on area under ROC.

The output of the pipeline is a set of series of paired measurements that are readily visualized or used for in workflows such as treatments response assessment and advanced patient cohort selection. Our work shows that natural language processing techniques can be valuable component in an oncology information management system.

@&#REFERENCES@&#

