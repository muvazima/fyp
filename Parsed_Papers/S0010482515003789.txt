@&#MAIN-TITLE@&#Multilayer descriptors for medical image classification

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           Method presented for building an n-layer image using preprocessing methods.


                        
                        
                           
                           Performance of 2D descriptors improved using n-layer images.


                        
                        
                           
                           Multilayers and texture descriptors can be combined to enhance performance.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Texture descriptors

Ensemble

Local binary patterns

Local phase quantization

Support vector machine

Multilayer descriptors

@&#ABSTRACT@&#


               
               
                  In this paper, we propose a new method for improving the performance of 2D descriptors by building an n-layer image using different preprocessing approaches from which multilayer descriptors are extracted and used as feature vectors for training a Support Vector Machine. The different preprocessing approaches are used to build different n-layer images (n=3, n=5, etc.). We test both color and gray-level images, two well-known texture descriptors (Local Phase Quantization and Local Binary Pattern), and three of their variants suited for n-layer images (Volume Local Phase Quantization, Local Phase Quantization Three-Orthogonal-Planes, and Volume Local Binary Patterns). Our results show that multilayers and texture descriptors can be combined to outperform the standard single-layer approaches. Experiments on 10 datasets demonstrate the generalizability of the proposed descriptors. Most of these datasets are medical, but in each case the images are very different. Two datasets are completely unrelated to medicine and are included to demonstrate the discriminative power of the proposed descriptors across very different image recognition tasks.
                  A MATLAB version of the complete system developed in this paper will be made available at https://www.dei.unipd.it/node/2357.
               
            

@&#INTRODUCTION@&#

The amount of multidimensional visual information (e.g., 2D images, videos, 3D surface models of objects, and 3D tomographic images) uploaded to the internet on a daily basis has increased enormously in the past few years. The video hosting website YouTube, for instance, received more than 100h of new video every minute in 2014. In the field of medical imaging, individual radiology departments routinely produce an enormous amount of multidimensional information that is warehoused in private and public medical databases. Such large quantities of data are difficult to manually label for further access and reuse. There is, as a result, an urgent need to develop sophisticated, efficient, and accurate image classification algorithms that are able to provide best matches for specific classification tasks. Many applications require the ability of discriminating among classes of images, to distinguish, for instance, pedestrians from objects for advanced driver assistance, for disease recognition from medical images, for landmark recognition for touristic purposes, and many other applications. In these cases, computer vision plays a central role. Key to successful classification is the ability of representing images based on visual characteristics such as texture, color, and shape [1].

Automated visual tasks such as detection, localization, categorization, and recognition by visual features are important subjects of study in computer vision and image analysis. These tasks often prove difficult, however, due to high within-class variability that can be caused by a variety of factors: noise, distortions, illumination, scale changes, occlusion, and so on. Extracting features within visual databases is one of the most important steps in the classification process as success depends on the method adopted for extracting features from a given set of images. Particularly important are invariant image descriptors since they extract information from images which is invariant to noise, illumination, distortion, etc.

One class of invariant descriptors is color composition, which in a visual scene is robust to noise, image degradations, changes in size, resolution, and orientation. Most existing systems use various color descriptors [2,3] in order to retrieve relevant samples. Unfortunately, the classification performance of color descriptors is negatively affected by their lack of discriminative power.

Descriptors based on shape are considered more generic than color and are the most widely used descriptors in many application areas such as object detection and action recognition. There are many ways to represent shapes. Some examples include axial representation [4], primitive-based representation [5], histograms of oriented gradients [6], contour-based representation [7], and probability density function [8]. Most shape descriptors are robust to rigid transformations, noise, occlusions, and missing data.

Another class of descriptors is texture. Texture has received considerable attention [9–11] and has proven valuable in medical imaging, image retrieval, object recognition, and industrial product identification (e.g., classifying types of marble, ceramic tiles, parquet slabs, etc.). Various methods have been introduced to analyse texture in digital images. The most well-known methods are based on statistical approaches, such as histograms of gray-level pixel values and the Gray-Level Co-occurrence (GLC) matrix [12], wavelet transforms and Gabor filters [13], and the Local Binary Pattern (LBP) operator [14,15]. The LBP operator, in particular, has proven an efficient method for describing texture in 2D. It has received much attention from the scientific community, and many powerful variants have recently been proposed [16].

Much work has also been done exploring the integration of more than one feature to improve classification accuracy [17]; however, the performance of these combinations are strictly application dependent. In some cases less efficient features can degrade more efficient features with a loss of overall system accuracy.

Most recently, there has been a move to explore color in three dimensions. In [18] the RGB space is considered as three planes in a 3D image, and a 3D version of the Local Ternary Pattern (LTP) is then proposed that extracts features directly from the 3D image, instead of from each color space (i.e., separately from R, G, and B). Moreover, they show that it is possible to generate a multilayer image starting from gray value images using a 2D circular symmetric Gaussian filter. Please note that in some works (e.g. [18]) the authors use the term “volumetric descriptors” to denote descriptors extracted from a RGB image or any kind of images represented by more than a layer. In this work we use the term multilayers to denote such a concept to avoid misunderstanding with the the notion of “volumetric descriptor” that in the context of medical image analysis refers to some measure of volume in anatomically meaningful regions on a medical image.

In this paper we start from the idea in [18] that considered the RGB image as a three-layer image and extend this representation to preprocessed images, showing that it is possible to improve the performance of single-layer descriptors by building an n-layer image (n=3, n=5, etc.) using different preprocessing approaches.

Formally speaking if a single layer image can be considered as mappings from a spatial coordinate system into a value space, i.e. a mapping from [1,2,…,m]×[1,2,…,l]→ ℜ for an m×l image, a n-layer image is a mapping to a n dimensional space: [1,2,…,m]×[1,2,…,l]→ ℜ
                        n
                     . An RGB color image is a particular case of a multilayer image where n=3. This definition is different from that used in the literature for existing multilayer descriptors (e.g. [18,22]) since they were proposed for dynamical textures.

The n-layer images are here described using multilayer descriptors, and these feature vectors are fed into a Support Vector Machine (SVM) [19]. We test both color and gray-level images, two well-known texture descriptors (LBP and Local Phase Quantization), and three of their variants suited for multilayer images (viz., Volume Local Phase Quantization, Local Phase Quantization Three-Orthogonal-Planes, Volume Local Binary Patterns). Our results show that multilayer and texture descriptors can be combined to outperform standard single-layer approaches. The proposed approach is applied to ten datasets to demonstrate the generalizability of this approach. In this work, we do not deal with the segmentation problem, since all the datasets considered in the experiments contain only segmented image. The segmentation of large images that can contain parts of different textures is out of the scope of this paper.

The remainder of this paper is organized as follows. In Section 2 we describe the base approaches used in our system, and in Section 3 we present our system. We conclude in Section 4 by summarizing the significance of our work and by highlighting some future directions of exploration.

In this section we provide an overview of the methods used for building the ensemble of descriptors. We describe the multilayer descriptors in Section 2.1. A very brief description of the preprocessing methods employed to generate the n-layer images is available in Section 2.2.

In this subsection we briefly explain the methods used for building the ensemble of multilayer descriptors.

VLBP, introduced by [22], is an extension of LBP, with the notion of self-similarity, central to conventional image texture, extended to the spatiotemporal domain. VLBP deals with dynamic texture analysis on the 2D time series and not on the full 3D data; therefore, it only provides rotation invariance towards rotations around the z-axis.

VLBP extends standard LBP to handle dynamic texture analysis by considering the joint distribution v of the gray-levels of 3P+3 pixels (P>1 is the number of local neighboring points around the center in one frame):
                              
                                 
                                    
                                       V
                                    
                                    
                                       =
                                    
                                    
                                       v
                                    
                                    
                                       (
                                       
                                          
                                             
                                                g
                                             
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      c
                                                   
                                                
                                                
                                                   −
                                                
                                                
                                                   L
                                                
                                                
                                                   ,
                                                
                                                
                                                   c
                                                
                                             
                                          
                                          
                                             ,
                                          
                                          
                                             
                                                g
                                             
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      c
                                                   
                                                
                                                
                                                   −
                                                
                                                
                                                   L
                                                
                                                
                                                   ,
                                                   0
                                                
                                             
                                          
                                          
                                             ,
                                             …
                                             ,
                                          
                                          
                                             
                                                g
                                             
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      c
                                                   
                                                
                                                
                                                   −
                                                
                                                
                                                   L
                                                
                                                
                                                   ,
                                                
                                                
                                                   P
                                                
                                                
                                                   −
                                                   1
                                                
                                             
                                          
                                          
                                             ,
                                          
                                          
                                             
                                                g
                                             
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      c
                                                   
                                                
                                                
                                                   ,
                                                
                                                
                                                   c
                                                
                                             
                                          
                                          
                                             ,
                                          
                                          
                                             
                                                g
                                             
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      c
                                                   
                                                
                                                
                                                   ,
                                                   0
                                                
                                             
                                          
                                          
                                             ,
                                             …
                                             ,
                                          
                                          
                                             
                                                g
                                             
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      c
                                                   
                                                
                                                
                                                   ,
                                                
                                                
                                                   P
                                                
                                                
                                                   −
                                                   1
                                                
                                             
                                          
                                          
                                             ,
                                          
                                          
                                             
                                                g
                                             
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      c
                                                   
                                                
                                                
                                                   +
                                                
                                                
                                                   L
                                                
                                                
                                                   ,
                                                
                                                
                                                   c
                                                
                                             
                                          
                                          
                                             ,
                                          
                                          
                                             
                                                g
                                             
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      c
                                                   
                                                
                                                
                                                   +
                                                
                                                
                                                   L
                                                
                                                
                                                   ,
                                                   0
                                                
                                             
                                          
                                          
                                             ,
                                             …
                                             ,
                                          
                                          
                                             
                                                g
                                             
                                             
                                                
                                                   
                                                      t
                                                   
                                                   
                                                      c
                                                   
                                                
                                                
                                                   +
                                                
                                                
                                                   L
                                                
                                                
                                                   ,
                                                
                                                
                                                   P
                                                
                                                
                                                   −
                                                   1
                                                
                                             
                                          
                                       
                                       )
                                    
                                 
                              
                           where 
                              
                                 
                                    g
                                 
                                 
                                    t
                                    ,
                                    p
                                 
                              
                            
                           
                              
                                 t
                                 ∈
                              
                              
                                 [
                                 
                                    
                                       
                                          t
                                       
                                       
                                          c
                                       
                                    
                                    
                                       −
                                       L
                                       ,
                                    
                                    
                                       
                                          t
                                       
                                       
                                          c
                                       
                                    
                                    
                                       ,
                                    
                                    
                                       
                                          t
                                       
                                       
                                          c
                                       
                                    
                                    
                                       +
                                       L
                                    
                                 
                                 ]
                              
                              
                                 ,
                                 p
                                 ∈
                              
                              
                                 [
                                 
                                    0
                                    ,
                                    …
                                    ,
                                    P
                                    −
                                    1
                                 
                                 ]
                              
                            corresponds to the gray value of P equally spaced pixels on a circularly symmetric neighbor of the center pixel c, in the center frame 
                              
                                 
                                    t
                                 
                                 
                                    c
                                 
                              
                            and its previous/posterior frames with time interval L.

To obtain the gray-scale invariance as in LBP, the gray value of the center 
                              
                                 
                                    g
                                 
                                 
                                    
                                       
                                          t
                                       
                                       
                                          c
                                       
                                    
                                    
                                       ,
                                       c
                                    
                                 
                              
                            is subtracted from the gray values of the circularly symmetric neighborhood of each frame. To achieve invariance with respect to the scaling, the resulting code is binarized by considering only the signs of the differences. Finally, a unique VLBP
                              L,P,R
                            code is obtained by binary factorization (i.e., by weighting each pixel difference by a different binomial factor). The histogram obtained from the VLBP
                              L,P,R
                            code is normalized with respect to size variations by setting the sum of its bins to unity. Rotation invariance is obtained by rotating the neighbor set in three separate frames clockwise synchronously so that a minimal value is selected. Here we have used the following parameters R=1; P=4; L=1.

VLPQ, introduced in [23], is based on the binary encoding of the phase information of the local Fourier transform at low frequency points and is an extension to the LPQ operator [21] used for spatial texture analysis. Since dynamic texture consists of a sequence of texture in the spatio-temporal domain, the Fourier transform estimation is performed locally using Short-Term Fourier Transform (STFT). Given a sequence f(x), STFT is computed over an M-by-M-by-N neighborhood 
                              
                                 
                                    N
                                 
                                 
                                    x
                                 
                              
                            of x (where M is the spatial size and N the size in the temporal domain):
                              
                                 
                                    
                                       F
                                    
                                    
                                       (
                                       
                                          
                                             u
                                          
                                          
                                             ,
                                          
                                          
                                             x
                                          
                                       
                                       )
                                    
                                    
                                       =
                                    
                                    
                                       
                                          w
                                       
                                       
                                          u
                                       
                                       
                                          T
                                       
                                    
                                    
                                       
                                          f
                                       
                                       
                                          x
                                       
                                    
                                    
                                       ,
                                    
                                 
                              
                           where w
                           
                              u
                            is the basis of the 3D DFT at frequency u, and f
                           
                              x
                            is a vector containing all pixels from the neighborhood 
                              
                                 
                                    N
                                 
                                 
                                    x
                                 
                              
                           . The computation is efficient since STFT can be evaluated for each pixel using 1D convolutions for each dimension, due to the separability of the basis functions.

The transform matrix is obtained using the thirteen lowest non-zero frequency points (see [23] for details) and by separating the real and imaginary parts of each component. This results in a 26-by-M
                           2
                           N transform matrix:
                              
                                 
                                    
                                       W
                                    
                                    
                                       =
                                    
                                    
                                       [
                                       
                                          
                                             Re
                                          
                                          
                                             {
                                             
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      
                                                         
                                                            u
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                   
                                                
                                             
                                             }
                                          
                                          
                                             ,
                                          
                                          
                                             Im
                                          
                                          
                                             {
                                             
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      
                                                         
                                                            u
                                                         
                                                         
                                                            1
                                                         
                                                      
                                                   
                                                
                                             
                                             }
                                          
                                          
                                             ,
                                             …
                                             ,
                                          
                                          
                                             Re
                                          
                                          
                                             {
                                             
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      
                                                         
                                                            u
                                                         
                                                         
                                                            13
                                                         
                                                      
                                                   
                                                
                                             
                                             }
                                          
                                          
                                             ,
                                          
                                          
                                             Im
                                          
                                          
                                             {
                                             
                                                
                                                   
                                                      w
                                                   
                                                   
                                                      
                                                         
                                                            u
                                                         
                                                         
                                                            13
                                                         
                                                      
                                                   
                                                
                                             
                                             }
                                          
                                       
                                       ]
                                    
                                    .
                                 
                              
                           
                        

Hence, the vector form of the STFT for all frequencies u
                           1, …, u
                           13 can be written as
                              
                                 
                                    
                                       
                                          
                                             
                                                F
                                             
                                             
                                                x
                                             
                                          
                                          =
                                          
                                             W
                                          
                                          
                                             
                                                f
                                             
                                             
                                                x
                                             
                                          
                                       
                                       .
                                    
                                 
                              
                           
                        

In order to reduce the length of the resulting descriptor, dimension reduction by PCA and scalar quantization to a binary value is performed. Finally, the quantized coefficients are represented as integer values, using simple binary coding, and codified as a histogram of size L (where L is the dimension retained by PCA). Here we use windows of size=3 and L=10.

LPQ-TOP, proposed by Zhao and Pietikainen [24], is an extension of LPQ that, similarly to VLBP [22], takes into consideration dynamic texture. With LPQ-TOP the basic LPQ features are extracted independently from three orthogonal planes: XY, XT and YT (the XY plane provides the spatial domain information while the other two planes provide temporal information). The phase information is computed locally in a window for every image position in the three directions. The final descriptor, LPQ-TOP
                              Wx,Wy,Wt
                           , is the concatenation of the three normalized histograms obtained by accumulating the occurrence of quantized phase code in each direction (XY, XT and YT). Wx
                           , Wy
                            ,Wt
                            are the window size parameters (i.e., the dimension of the neighborhood at each pixel position for computing LPQ). These parameters are set to different values in the spatial and temporal planes. Following [24], we employed LPQ-TOP3,3,3 and LPQ-TOP5,5,5; the two feature vectors are then concatenated and fed into a SVM.

One of the aims of this work is to test the idea of improving the performance of texture descriptors by building a multilayer image using different preprocessing approaches. In this way, multilayer descriptors could describe the different nlayer images, and each feature set produced by each descriptor could then be fed into a SVM. In exploring this approach, we test the following preprocessing methods: decomposition by wavelets (Wave); Multi-Resolution by Gaussian filters (MRG); Gradient image (GR); color space transform for color images (RGB to HSV and RGB to YUV); image enhancement using exposure based subimage histogram equalization for gray-level images (ESI); and contrast enhancement based on layered difference representations of 2D histograms for color images (LDR). In some cases, when the approach (e.g., Wave, MRG, and GR) is applied to color images, it is applied separately to each space; thus, three processed images are obtained using these preprocessing methods. The remainder of this section describes each of aforementioned preprocessing methods in more detail.


                           WAV works well in many computer vision methods that detect and recognize objects of interest. In order to use WAV 
                           [25] for 2D decomposition, a 2D scaling function, 
                              
                                 
                                 φ
                                 
                                    (
                                    
                                       x
                                       ,
                                       y
                                    
                                    )
                                 
                              
                            and three 2D wavelets functions, 
                              
                                 
                                    
                                       ψ
                                    
                                    i
                                 
                                 (
                                 x
                                 ,
                                 y
                                 )
                              
                            are needed, where 
                              
                                 i
                              
                            represents the three possible intensity variations along horizontal, vertical, and diagonal edges 
                              
                                 i
                                 =
                                 
                                    {
                                    
                                       H
                                       ,
                                       V
                                       ,
                                       D
                                    
                                    }
                                 
                              
                           .

In this paper, we use the Daubechies [26] wavelet family (Wa) with four vanishing moments and a single scale decomposition. The four preprocessed images are thus the approximation coefficients matrix and the three details coefficients matrices (horizontal, vertical and diagonal). The processed images are resized to the size of the original image. In the experimental section, we label each of these as follows:
                              
                                 •
                                 Wave1: the approximation coefficients matrix;

Wave2: the horizontal details;

Wave3: the vertical details;

Wave4: the diagonal details.

In this paper, we test the Gaussian scale-space representation, where the original image is filtered to obtain two smoothed versions. This is accomplished using a 2D symmetric Gaussian lowpass filter of size k pixels (k=3 and k=5 in this work) with standard deviation 1.

Gradient is an image operation that is commonly used to detect edges. GR calculates the magnitude of the gradients of each pixel in the x and y directions based on its neighbors. The processed image at coordinates 
                              
                                 
                                    (
                                 
                                 
                                    x
                                 
                                 
                                    ,
                                 
                                 
                                    y
                                 
                                 
                                    )
                                 
                              
                            is given by the magnitude of the gradient vector at the same coordinates. Thus, each pixel of a gradient image measures the change in intensity in a given direction of the corresponding point in the original image.

A color space transform converts images from one color space representation to another. The input images in the tested databases are given in the RGB color space. In this paper, the original images are transformed into the following two color spaces: YUV and HSV.

The type of image enhancement applied depends on whether the images are colored or gray-level:
                              
                                 •
                                 
                                    ESI: For gray-level images, we apply a novel Exposure-based Sub-Image Histogram Equalization method, proposed in [27], that enhances the contrast of low exposure gray-scale images. Exposure thresholds are computed that divide the original image into subimages of different intensity levels, and the histogram is clipped using a threshold value (as an average number of gray-level occurrences) to control enhancement rate. The individual histograms of the subimages are then equalized independently, with all subimages finally integrated into one complete image that is used for analysis.


                                    LDR: For color images, we apply a novel contrast enhancement algorithm, proposed in [28], that is based on the layered difference representation of 2D histograms. The basic idea behind this method is to enhance image contrast by amplifying the gray-level differences between adjacent pixels. Enhancement is viewed as a constrained optimization problem that is based on the observation that the gray-level differences occurring more frequently in the input image should be emphasized more in the output image. This is accomplished by first taking a given RGB image and transforming it into the YUV space, where the Y band is enhanced. The YUV image is then transformed back to the RGB space.

Below is a simple description of our proposed algorithm:
                        
                           1.
                           INPUT IMAGE

The proposed method accepts both gray-level and color images.

DO COLOR SPACE CONVERSION

This step is performed only on color images and is aimed at analyzing the image from different color perspectives (see Section 2). The original RGB image is converted into HSV and YUV, thus obtaining six additional layers.

PREPROCESS IMAGES

Image preprocessing techniques are usually used to improve the quality of an image before processing into an application. In this approach we use some well-known preprocessing methods to highlight distinctive aspects of the images before performing feature extraction. The preprocessing techniques are outlined in Section 2.

MAKE n-LAYER IMAGE

In the case of a gray-level input image, the output of the preprocessing step is a 9-layer image (the original image plus eight preprocessed layers), otherwise the output is a 33-layer image (resulting from the concatenation of 24 layers produced from the preprocessing step, i.e. 8 approaches × 3 color channels, plus three layers from the original RGB image, plus six color layers from step 1).

EXTRACT FEATURES

The feature extraction is performed on both the input image and the n-layer image:
                                 
                                    41.
                                    TEXTURE DESCRIPTOR EXTRACTION

The following two texture descriptors are extracted from the gray-level image (note: RGB images are converted to gray-level images) and their processed images: canonical LBP and LPQ.


                                       n-LAYER DESCRIPTOR EXTRACTION

The following descriptors (which are the n-layer versions of the previous ones) are extracted from the n-layer image (or k-layer images, if a reduced number of layers is considered): VLBP, VLPQ, and LPQ-TOP.

CLASSIFY

Classification is performed separately for each descriptor using SVMs as the base classifier, with both linear and radial basis function kernels. For each dataset, the best kernel and the best set of parameters are chosen using a 5-fold cross-validation approach on the training data (note: we are only determining here which descriptors to combine). The SVM is implemented using the widely used tool LibSVM (available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/). The features used to train and test the SVMs are normalized to [0, 1] using the training data (the testing set is always completely blind).

NORMALIZE AND FUSE;

Before fusion, the scores obtained by each classifier are normalized to mean 0 and standard deviation 1. The final decision is obtained by combining the resulting scores from each n-layer descriptor and its related texture descriptor (VLPQ+LPQ, LPQTop+LPQ, and VLBP+LBP) using the weighted sum rule with the same settings applied in all datasets (see Section 4).


                     
                     Fig. 1 illustrates the steps 1–6 above. The steps of an input image vary depending on its type as indicated by the color of the arrows: gray for gray-level images and white for colored images. It should also be noted that the dimension of the layer image is different in these two cases.

@&#EXPERIMENTAL RESULTS@&#

In order to generalize the proposed variants of texture descriptors, we run several comparisons on ten datasets. Most of these datasets are medical, but in each case the images are very different. Two datasets, SM and PI, are completely unrelated to medicine and are included to demonstrate the discriminative power of the proposed descriptors across very different image recognition tasks.
                           
                              •
                              
                                 
                                    PA
                                  
                                 [29]: this is a pap-smear cell dataset containing 917 images unevenly distributed among seven classes acquired during Pap-smear tests for the diagnosis of cervical cancer. The dataset is divided into 242 normal cells (superficial and intermediate squamous and columnar epithelial) and 675 abnormal cells (mild, moderate, and severe squamous non-keratinizing dysplasia and squamous cell carcinoma in situ intermediate).

The 
                                    Vi
                                  dataset in [20] contains 1500 TEM virus images divided into 15 classes: Denovirus, Astrovirus, CCHF, Cowpox, Dengue, Ebola, Influenza, Lassa, Marburg, Norovirus, Orf, Papilloma, Rift Valley, Rotavirus, and West Nile. Each image is 41×41 pixels and is saved as a lossless compressed 16-bit PNG. For a more detailed description of the dataset, see [20]. In our comparisons, the same 10-fold cross-validation testing protocol in [20] was applied. The dataset, masks, and indices used in the 10-fold cross-validation procedure are available at http://www.cb.uu.se/~gustaf/virustexture/index.html
                              


                                 
                                    HI
                                  
                                 [30]
                                 : this dataset is composed of 2828 histology images captured from different organs and representing the four fundamental tissues, unevenly distributed (484 connective, 804 epithelial, 514 muscular, and 1026 nervous images). This dataset facilitates the study of different tissues and systems with specific markers for composing cells.


                                 
                                    CH
                                  
                                 [31]: this cell dataset contains 327 fluorescent microscopy images of Chinese Hamster Ovary cells that are distributed in five classes representative of different cell markers (anti-giantin, Hoechst 33258, anti-nop4, anti-tubulin, and anti-lamp2).


                                 
                                    TR
                                  
                                 [32, 33]: the LOCATE TRANSFECTED dataset contains 553 images of fluorescence-tagged or epitope-tagged protein transiently expressed in specific mouse subcellular organelles. The images are unevenly distributed in eleven classes: the same ten classes in LE and the additional cytoplasm class.


                                 
                                    RN
                                  this dataset contains 200 fluorescence fly cells microscopy images from D. melanogaster stained with DAPI (4׳, 6-diamidino-2-phenylindole) to visualize the nucleus. Images are evenly distributed among ten classes, each defined by the type of RNAi gene-knockdowns used.


                                 
                                    PR
                                  the protein dataset in [34] contains 118 DNA-binding Proteins and 231 Non-DNA-binding proteins, with texture descriptors extracted from the 2D distance matrix that represents each protein. The 2D matrix is obtained from the 3D tertiary structure of a given protein by considering only atoms that belong to the protein backbone (see [34] for details).


                                 
                                    BR
                                  the dataset in [35], containing 273 malignant and 311 benign breast cancer images.


                                 
                                    SM
                                  the dataset in [36], contains images extract from video-based smoke detection surveillance systems. The same division of the dataset into training/testing sets in [36] is used in all experiments reported in this paper.


                                 
                                    PI
                                  the dataset in [37] containing pictures extracted from digitalized pages of the Holy Bible of Borso d׳Este, duke of Ferrara (Italy) from 1450 A.D. to 1471 A.D. PI is composed of 13 classes, characterized by a clear semantic meaning and significant search relevance.

A descriptive summary of each dataset along with the URL where each dataset can be downloaded is reported in 
                        Table 1. The testing protocol was the fivefold cross-validation method, except for the VI and SM datasets, where the specific protocols and testing/training sets obtained from the creators of these two datasets were used.

For the performance indicator, the area under the ROC curve (AUC)
                           1
                        
                        
                           1
                           AUC is implemented as in dd_tools 0.95 davidt@ph.tn.tudelft.nl.
                         is used since it provides a better overview of classification results. AUC is a scalar measure that can be interpreted as the probability that the classifier will assign a higher score to a randomly picked positive sample than to a randomly picked negative sample [38]. In the multiclass problem, AUC is calculated using the one-versus-all approach (where a given class is considered “positive” and all the other classes are considered “negative”). The average AUC is reported.

The first experiment, presented in 
                        
                        Tables 2 and 3, examines the performance of the two texture descriptors, LBP (Table 1) and LPQ (Table 2), that form the base of the variants that are studied in this work. The performance of these descriptors provide a baseline for the other approaches. In order to reduce the production of data, the performance of the texture descriptors for color images is obtained by fusing the scores of the classifiers for each color band. In other words, Enh denotes the fusion by sum rule of an SVM trained on the enhanced Red image + an SVM trained on the enhanced Green image + an SVM trained on the enhanced Blue image. In addition, for color images, all 12 SVMs trained with the images obtained by the wavelet preprocessing method are combined by sum rule.

The second experiment, reported in 
                        
                        Tables 4 and 5, evaluates the performance obtained by the texture descriptors LBP and LPQ as a fusion of classifiers trained on different layers. This experiment is useful for a fair comparison with descriptors that are extracted on the same subset of K layers that are used to obtain the multilayer image. In Tables 4 and 5, the average AUC is reported.

For gray-level images, the following combination of layers are considered for fusion:
                           
                              KD1: Orig+ Gaus3+ Gaus5;

KD2: Orig+ Enh +Gaus3+ Gaus5;

KD3: Orig+ Wave1+ Wave2+ Wave3+ Wave4

KD4: Orig+ Enh +Gaus3+ Gaus5;

KD5: all the preprocessed images are considered.

For color images, the following combinations are considered:
                           
                              KD1: Orig+ Enh +Hsv+ Yuv;

KD2: Orig+ Enh +Hsv+ Yuv +Gaus3+ Gaus5;

KD3: Orig+Gaus3+ Gaus5+ Wave;

KD4: Orig;

KD5: all the preprocessed images are considered.

From the results reported in Tables 4 and 5, it is clear that combining different preprocessing methods boosts the performance of the descriptors extracted only from the original image. KD5 provides the best performance for both the color and gray-level images.

The third experiment, reported in 
                        
                        
                        Tables 6–8, compares the three descriptors studied in this paper: VLBP, VLPQ, and LPQTop. The experiments are related to the same subsets KD1-KD5 described above. In Tables 6–8, the average AUC is reported.

From the results reported in Tables 6–8, it is clear that multilayer descriptors can be used for extracting features from nD images built using different preprocessing approaches. KD2 obtains the best performance, on average, among the three descriptors.

The forth experiment, described in 
                        Table 9, evaluates the performance of a weighed fusion among a base texture descriptor and its nD version. Fusions have been performed considering the best combination of layers, both for the texture and the multilayer descriptors, which are KD5 and KD2, respectively (see the experiments reported in Sections 4.2 and 4.3). The same configuration has been maintained for all the datasets, and the average AUC is reported.

For a sake of easy comparison, some results obtained from the first and second experiments are again reported here: LPQ(Orig), LBP(Orig), LPQ(KD5), LBP(KD5) and LPQ(Best) and LBP(Best) which denote the best result among different preprocessing.

Also reported in Table 9 are the performance of two recent state-of-the-art descriptors:
                           
                              •
                              Rotation invariant co-occurrence local binary patterns (RICLBP) [41].

Completed local binary patterns (CLBP) [40].

To statistically validate our experiments and evaluate which algorithm is better over different data sets, we can use the Wilcoxon signed rank test [39], a nonparametric test that compares the performance of classifiers by considering the number of wins and losses and the difference in performance for wins and losses on different datasets [42]. We select the Wilcoxon signed rank test because it is one of the most used comparison measures in the literature. Demšar in [39] claims that the Wilcoxon signed rank test is one of the best statistical methods for comparing distributions.

The Wilcoxon signed rank test confirms (p-value 0.1) that:
                           
                              •
                              VLBP+5×LBP outperforms LBP(KD5);

LPQTOP+LPQ outperforms LPQ(KD5);

VLPQ+LPQ outperforms LPQ(KD5);

LPQ (KD5) outperforms LPQ(Orig);

LBP (KD5) outperforms LBP(Orig);
                              

LPQ (KD5) outperforms LPQ(Best);

LBP (KD5) outperforms LBP(Best);
                              

Even though the improvements shown in Table 9 sometimes appear modest, Wilcoxon signed rank test confirms the statistical superiority of some of the proposed approaches. Of particular interest are the following:
                           
                              •
                              Our new approach LPQ(KD5), which outperforms the 2D texture descriptors LPQ(Orig) and LPQ(Best);
                              

LBP(KD5), which likewise outperforms the 2D texture descriptors LPQ(Orig) and LPQ(Best);
                              

The combination of volumetric and standard descriptor LPQTOP+LPQ that we suggest in this paper as the approach with the best balance between computational cost and performance; it outperforms LPQ(KD5);

VLPQ, LPQTOP, and LPQ, each of which obtains a statistically similar performance (p-value 0.1); however, their fusion outperforms all the three stand-alone approaches

Although the performance of VLBP is not impressive, LPQTOP+LPQ works well. It is important to note that our results have been obtained without any ad hoc parameter optimization for each dataset. It should also be noted that this is the first attempt to combine sets of preprocessed images to obtain a multilayer image: this idea can be exploited in the future to rank preprocessing approaches and study their correlation. In many works the aim of the research is simply to find the best preprocessing method to improve the performance, here we are interested in finding the best ensemble.

As far as computational effort is concerned, we report in this section the execution time required for the main steps of the method: image preprocessing and feature extraction. The results reported in 
                        Table 10 have been obtained considering a sample gray scale image of 100×100 pixels.

The reported execution time (in seconds) has been obtained without parallelizing the MATLAB code on a PC (CPU i5-3470 3.2GHz – 8GB Ram). Almost all the approaches require computational times that are acceptable for most of applications; the only exception is VLBP, which is very computationally expensive. Using a multicore computer can offset some of the computational time considerations as the different preprocessing and feature extraction methods in our proposed approach can be parallelized.

Finally, in 
                        Table 11 the length of each descriptor is reported: the size of the texture descriptors is related to a single-layer image, while for nD descriptors the size is invariant with respect to the number of layers.

@&#CONCLUSIONS@&#

The aim of this work is to show the feasibility of building an n-layer image using different preprocessing approaches. These images are then described using multilayer descriptors, which form feature vectors that are used to train a set of SVMs. For assessing the performance of this approach, we run tests on several datasets, on both color and gray-level images, using both LPQ and LBP texture descriptors, and three multilayer descriptors based on LPQ and LBP: viz. VLBQ, VLBP, and VLB-TOP. Our results show that n-layer and single-layer descriptors can be combined for outperforming the standard single-layer approaches. This conclusion is confirmed using the Wilcoxon signed rank test with a p-value 0.1.

Since the performance of n-layer descriptors is strictly related to the order of the considered layers, we plan in the future to study how this order influence performance with the aim of designing a selection/ranking approach for selecting/ranking the layers most useful for classification purposes. Another feasible way to improve classification accuracy is to cluster layers into several groups, extract descriptors from each group, and then exploit fusions to improve classification performance.

None declared.

@&#REFERENCES@&#

