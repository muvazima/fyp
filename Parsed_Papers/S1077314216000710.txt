@&#MAIN-TITLE@&#Hybrid macro–micro visual analysis for city-scale state estimation

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We estimate large-scale land surface conditions using aerial and street images.


                        
                        
                           
                           These two types of images are captured from orthogonal viewpoints.


                        
                        
                           
                           Aerial image is used to learn the correspondence of land conditions.


                        
                        
                           
                           Street image is used to acquire high-resolution statistics of land conditions.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

City-scale

Vehicular imagery

Aerial imagery

@&#ABSTRACT@&#


               
               
                  We address the task of estimating large-scale land surface conditions using overhead aerial (macro-level) images and street view (micro-level) images. These two types of images are captured from orthogonal viewpoints and have different resolutions, thus conveying very different types of information that can be used in a complementary way. Moreover, their integration is necessary to enable an accurate understanding of changes in natural phenomena over massive city-scale landscapes. The key technical challenge is devising a method to integrate these two disparate types of image data in an effective manner, to leverage the wide coverage capabilities of macro-level images and detailed resolution of micro-level images. The strategy proposed in this work uses macro-level imaging to learn the extent to which the land condition corresponds between land regions that share similar visual characteristics (e.g., mountains, streets, buildings, rivers), whereas micro-level images are used to acquire high resolution statistics of land conditions (e.g., the amount of debris on the ground). By combining macro- and micro-level information about regional correspondences and surface conditions, our proposed method is capable of generating detailed estimates of land surface conditions over an entire city.
               
            

@&#INTRODUCTION@&#

We address the task of estimating large-scale land surface conditions using overhead aerial (macro-level) images and street view (micro-level) images. These two types of images are captured from orthogonal viewpoints and have different resolutions, thus conveying very different types of information that can be used in a complementary way. Moreover, their integration is necessary to enable an accurate understanding of the changes in natural phenomena over massive city-scale landscapes.

Aerial images are an excellent source for collecting wide-area information of land surface conditions. However, it may come at the cost of a lower resolution (i.e., number of pixels per square meter) and visibility may drastically change depending on the weather. For example, clouds may obscure the extent to which the land surface is visible (Fig. 1
                     ). A more important limitation of aerial images is that they are limited to a vertical (top-down) perspective of the ground surface, such that areas occluded by a roof or highway overpass are not visible to the camera (first and second row of Fig. 2
                     ) making it difficult to estimate land conditions in covered areas.

Street-view images, on the other hand, are captured at ground level and can provide higher resolution images of vertical structures and have unobstructed access to visual information in covered areas. They are also less affected by weather conditions. However, street-view images are constrained to the ground plane and a single image has limited physical range. Moreover, it is also labor intensive to acquire street-level images of large land surface areas (i.e., millions of square meters).

The key technical challenge is devising a method to integrate these two disparate types of image data in an effective manner, to leverage both the wide coverage capabilities of macro-level images and detailed resolution of micro-level images. The strategy proposed in this work uses macro-level imaging to learn the extent to which the land condition corresponds between land regions that share similar visual characteristics (e.g., mountains, streets, buildings, rivers), whereas micro-level images are used to acquire high-resolution statistics of land conditions (e.g., the amount of debris on the ground). By combining the macro- and micro-level information about regional correspondences and surface conditions, our proposed method generates detailed estimates of land surface conditions over an entire city.

The technical contribution of this paper is a novel procedure for generalizing from a sparse set of visual recognition results to a large-scale land condition regression estimate. The proposed system carefully brings together state-of-the-art algorithms for semantic scene understanding, structure-from-motion and non-parametric regression to generate a massive city-scale land condition probability map (Fig. 3
                     ). To the best of our knowledge, this is the first work of its kind to use sparse image-based street-level object recognition results to extrapolate the surface conditions of an entire city (over 4 million square meters).

Although our method can be generalized to accommodate different types of large-scale phenomena, we ground our proposed approach in a real-world application of post-tsunami city-scale damage estimation. In regions affected by such disasters, it is extremely hard to efficiently assess the large-scale impact of a natural disaster. Technologies that enable efficient city-scale estimates of damage can be extremely helpful for expediting aid to seriously damages areas. The approach described in this paper can also be used for long-term analysis by monitoring and tracking recovery efforts.

@&#RELATED WORK@&#

There have been significant advances in state-of-the-art techniques for quantitative geometric interpretations of large-scale city scenes. Methods for city-scale 3D reconstruction have been proposed using thousands of images gathered from the Internet [1,2]. Similar techniques have been proposed for images captured by a vehicle-mounted camera [3,4] or aerial images [5–7]. Street-view images have also been combined with aerial images for the purpose of improving 3D reconstruction, where 3D point clouds have been projected to the ground plane and aligned with edges of buildings detected from aerial images [8] or building maps [9]. There has also been work using aerial and street view images taken several months or decades apart [10–14] to understand temporal changes in a scene. The focus of these previous approaches was on a quantitative geometric interpretation of the scene where local visual features are matched directly to estimate camera pose using epipolar geometry [15]. In this work we aim to proceed beyond a purely geometric understanding of the scene towards a more qualitative understanding of city conditions. For instance, we are not only interested in the 3D geometry of a building but would also like to establish the condition of a building or the condition of the ground surrounding the building.

Other work focused on the qualitative estimation of land conditions over large-scale environments. In the field of remote sensing, coarse land surface conditions have been estimated using aerial color images, aerial infrared light, and aerial microwave sensing [16–25]. Color aerial images have been applied to land condition estimation for vegetation monitoring [26–28], land cover mapping, and flood risk and damage assessment [29,30]. For example, forest maps [31–33] are an important source of information for monitoring and reducing deforestation, allowing environmental scientists to determine the extent to which forested areas increase or decrease over the entire earth.

Apart from aerial imaging using color cameras, many other modes of sensing have been proposed for estimating coarse large-scale land surface conditions. Digital elevation map (DEM) [31], spectroradiometer (MODIS), high-resolution radiometer (AVHRR), and synthetic aperture radar (SAR) have been proposed to improve the accuracy of estimating large-scale land surface conditions. However, satellite-mounted MODIS and AVHRR only measure surface conditions at a very course resolution – typically with a cell size of several hundred meters. As such, this work did not utilize street-level sensing, as this would be too detailed to estimate. However, in this work we are interested in a more high-resolution estimate of land conditions on a cell size closer to 20 m wide.

Our proposed work therefore fills the gap between detailed geometric reconstructions of city-scale structures and coarse qualitative estimation of land conditions. We use known techniques to provide an accurate geometric model of the city and use state-of-the-art object recognition results carefully registered to the scene geometry to understand the qualitative conditions of the entire city.

Our framework integrates aerial and street-view images to estimate land surface conditions. In this section, we explain the details of the proposed method contextualized for post-tsunami debris detection. Although the following explanation takes debris as an example, the method is generally applicable to other types of land surface conditions. The proposed method consists of the following three steps:
                        
                           (i)
                           Detection of debris on perspective street-view image. (Section 3.1)

Projection of debris probabilities on street-view images to the ground using building contours. (Section 3.2)

Estimation of debris over an entire city by integrating the projection result with all other data (e.g., aerial image, DEM) using a Gaussian process. (Section 3.3)

The first step calculates the probability map of debris for each street-view image. Then, using the camera parameters for the street-view image, the probability map is projected onto the ground plane registered to a corresponding part of the aerial image. This projection method takes the existence of building walls into consideration. Finally, the estimation results obtained from street-view images are complemented by integrating the projected probability map with the information obtained from aerial images and DEM using a Gaussian process regression model.

We developed a method to calculate the probability map of debris (Fig. 4
                        ) according to a debris model, which is learned from a hand-labeled training image. The debris in the images consists of matter with irregular, complicated shapes and appearance. Therefore, we exploit Geometric Context [34] as a geometric feature and pixel-wise object probability [35] as a texture/appearance feature. Geometric Context estimates the probability of a super-pixel belonging to one of seven classes. We chose four of the seven classes, namely “ground plane”, “sky”, “porous non-planar”, and “solid non-planar”, and used the probabilities as debris classification features. The pixel-wise appearance probability that the pixel belongs to the estimation target p
                        appearance is calculated using an existing method [35]. Lab, HOG[36], BRIEF[37], and ORB[38] are used as each pixel feature of texture/appearance and random forest is used as regressor. The feature vector of debris is as follows.
                           
                              (1)
                              
                                 
                                    x
                                    =
                                    
                                       
                                          (
                                          
                                             p
                                             
                                                ground
                                             
                                          
                                          ,
                                          
                                             p
                                             
                                                sky
                                             
                                          
                                          ,
                                          
                                             p
                                             
                                                porous
                                             
                                          
                                          ,
                                          
                                             p
                                             
                                                solid
                                             
                                          
                                          ,
                                          
                                             p
                                             
                                                appearance
                                             
                                          
                                          ,
                                          
                                             m
                                             
                                                patch
                                             
                                          
                                          ,
                                          
                                             v
                                             
                                                patch
                                             
                                          
                                          )
                                       
                                       
                                          T
                                       
                                    
                                    ,
                                 
                              
                           
                        where p
                        ground, p
                        sky, p
                        porous, and p
                        solid are the probabilities of “ground plane”, “sky”, “porous non-planar”, and “solid non-planar”, respectively. In addition to these probabilities, the mean m
                        patch and variance v
                        patch of the grayscale patch (5 × 5) are added to the features.

We evaluated the accuracy of our debris detector by creating two datasets. Fig. 5
                         shows an example of the datasets and detection results. Each dataset consists of fifty images of debris. The images in the two data sets were taken on different dates and times. We compared random forest [39], logistic regression [40], and support vector machine [41]. Fig. 6
                         shows the F
                        1-scores of the debris detections. We chose the random forest as our debris detector for all experiments because the score of the random forest regressor was the best.

Furthermore, we evaluated the feature importance of the debris detector. Table 1
                         lists the evaluation result of the feature importance values of the debris detector. This result indicates that the average and variance values of local-patch intensities are discriminative features for debris; hence, this result can help to define debris.

The debris probability explained in the previous section is the probability map on the street-view image. This probability map is integrated with the aerial image by projecting the debris probability onto the ground plane. Fig. 7
                         shows the data flow diagram of the projection of the street-view image onto the coordinates of the aerial image. The projection requires the camera parameters of each street-view image. First, we performed Structure from Motion (SfM) to acquire the camera trajectories. We employ a standard SfM method [15,42,43] with extensions to process with omni-directional images [4]. The estimated camera trajectories are fitted to the GPS trajectory by similarity transformations in a least-squares sense.

Dividing the ground plane into a grid, we project the debris probability to the grid using the projection matrix of each image. In this projection, we use the 3D models of the buildings that are generated from a 2D map of the city (Section 4.2). To be specific, the debris probability is projected to a building wall if the wall is on the projection path, and otherwise it is directly projected to the ground, as shown in Fig 8
                        . Some of the buildings could have been demolished by the tsunami or during subsequent recovery efforts, and we assume here that it is already known which buildings still exist. We are developing, in other work, methods to automatically determine whether a building exists using a sequence of street images and a 2D map of the city which are recorded and created at different time points. [44].

The projected debris probability map obtained to date does not contain any information for some areas because of occlusions or the lack of street-level images, as shown in Fig. 8. Estimating a debris probability map from only an aerial image is difficult due to its low-resolution, occlusion, or weather conditions. To mutually complement the street-view images and the aerial image, we used a Gaussian process regression model [45]. The main idea here is that similar geographical locations tend to have similar debris probability. In the case of disaster resulting from a tsunami, the area from the seashore to the hillside is continuously affected, which means the damage caused by a tsunami has a strong correlation with the location, especially the elevation.

As described in the previous section, the debris probability of each grid p
                        s, i
                        
                        
                           
                              (
                              i
                              =
                              1
                              ,
                              …
                              ,
                              n
                              )
                           
                         is estimated from the street-view images. For each grid, its feature vector x
                        
                           i
                         is defined as follows.
                           
                              (2)
                              
                                 
                                    
                                       x
                                       i
                                    
                                    =
                                    
                                       
                                          (
                                          
                                             x
                                             i
                                          
                                          ,
                                          
                                             y
                                             i
                                          
                                          ,
                                          
                                             z
                                             i
                                          
                                          ,
                                          
                                             p
                                             
                                                
                                                   a
                                                
                                                ,
                                                i
                                             
                                          
                                          )
                                       
                                       
                                          T
                                       
                                    
                                 
                              
                           
                        where (xi, yi
                        ) is a center position of each grid, zi
                         is an elevation of each grid calculated from DEM, and p
                        a, i
                         is the debris probability of each grid estimated from the aerial image using pixel-wise object recognition[35], in which Lab, HOG, BRIEF, and ORB are used as each pixel feature of texture/appearance, and random forest is used as regressor. The feature vector x
                        
                           i
                         for all n grid are aggregated in the 4 × n training input matrix X, and the training outputs p
                        s, i
                         are collected in the vector y.

We estimate the debris probability of each grid fi
                         using the correlation between the feature vector of each grid x
                        
                           i
                        . The relationship between test input x
                        * and test output 
                           
                              
                                 f
                                 ¯
                              
                              *
                           
                         is as follows [45]
                        
                           
                              (3)
                              
                                 
                                    
                                       
                                          
                                             
                                                f
                                                ¯
                                             
                                             *
                                          
                                       
                                       
                                          =
                                       
                                       
                                          
                                             
                                                
                                                   
                                                      k
                                                      *
                                                   
                                                
                                                
                                                   T
                                                
                                             
                                             
                                                
                                                   (
                                                   K
                                                   +
                                                   
                                                      
                                                         
                                                            σ
                                                            n
                                                         
                                                      
                                                      2
                                                   
                                                   I
                                                   )
                                                
                                                
                                                   −
                                                   1
                                                
                                             
                                             y
                                             ,
                                          
                                       
                                    
                                 
                              
                           
                        where K is the covariance matrix, k(x
                        
                           p
                        , x
                        
                           q
                        ) is the element of K in row p, column q, 
                           
                              k
                              
                                 (
                                 
                                    x
                                    *
                                 
                                 )
                              
                              =
                              
                                 k
                                 *
                              
                           
                         is the vector of covariances between the test point and the training points, and I is the identity matrix. We solve Eq. (3) using a Cholesky decomposition. The feature vector x
                        
                           i
                         contains p
                        a, i
                         as the visual feature of the ith grid. Although p
                        a, i
                         is a scalar, due to pixel-wise object recognition[35], p
                        a, i
                         summarizes the visual information of the ith grid. Compared to using general visual feature descriptors, such as SIFT [42], directly in the feature vector x
                        
                           i
                        , p
                        a, i
                         saves computational resources required in the following calculation of covariance function. The covariance function for Gaussian process regression in the proposed method is as follows.
                           
                              (4)
                              
                                 
                                    k
                                    
                                       (
                                       
                                          x
                                          p
                                       
                                       ,
                                       
                                          x
                                          q
                                       
                                       )
                                    
                                    =
                                    
                                       
                                          
                                             σ
                                             f
                                          
                                       
                                       2
                                    
                                    exp
                                    
                                       (
                                       −
                                       
                                          1
                                          
                                             2
                                             
                                                l
                                                2
                                             
                                          
                                       
                                       
                                          
                                             
                                                |
                                                W
                                             
                                             
                                                (
                                                
                                                   x
                                                   p
                                                
                                                −
                                                
                                                   x
                                                   q
                                                
                                                )
                                             
                                             
                                                |
                                             
                                          
                                          2
                                       
                                       )
                                    
                                    +
                                    
                                       
                                          
                                             σ
                                             n
                                          
                                       
                                       2
                                    
                                    
                                       δ
                                       
                                          p
                                          q
                                       
                                    
                                 
                              
                           
                        where W is the weight diagonal matrix, l is the length-scale, σf
                        
                        2 is the signal variance, σn
                        
                        2 is the noise variance and δpq
                         is the Kronecker delta which is one if 
                           
                              p
                              =
                              q
                           
                         and zero otherwise.

The key insight to note here is that the output of the aerial image regressor p
                        a, i
                         enforces a correlation between parts of the scene that look similar. If two parts of the scene belong to an open field, the per-pixel response of the aerial object detection regressor will produce a similar response. The DEM also works in a similar manner to draw correlations between regions with similar elevation. The location feature enforces local smoothness over the final estimate of debris over the city. When the feature vectors x
                        
                           i
                         are used to compute the covariance function, regions that are similar in appearance and elevation will be constrained to have similar target values (debris estimates generated by the high-resolution debris regressor computed on the street images). In this way, the Gaussian process regression model is able to propagate local estimates of debris to the entire map. This regression mechanism allows our model to effectively estimate debris over the entire city from only a sparse set of street-view debris estimation results.

@&#EXPERIMENTAL RESULTS@&#

We evaluated the effectiveness of our proposed approach for estimating large-scale land conditions by performing two experiments. Our first experiment is a comprehensive ablative analysis to examine the benefit of integrating micro and macro-level imagery for city-scale land condition estimation. In addition to color imaging, we also evaluate the contributions of two other modes of data, namely, a digital elevation map (DEM) and building occupancy maps (BOM). In our second experiment, we focus on estimating the amount of greenery and vegetation across the entire city of Kamaishi. We use the exact same approach as for the debris estimation described in this paper and apply it to greenery estimation. Our results show that our approach is not limited to post-disaster analysis but can easily be applied to other modes of land condition analysis.

We created the ground truth labels used for the following evaluation by many hours of manual labeling of regions on the aerial images. Ground truth data for debris and greenery were generated by visual inspection by comparing the aerial image against the street-view images available on Google Earth. Many hours of ground truth labeling confirm that the manual inspection of large-scale land conditions is not a practical solution for real-world applications.

Our experiment includes two image-based input modalities and two sources of city-scale meta-data, which are described below.


                        Street images: We have been creating image archives of urban and residential areas damaged by the Great East Japan Earthquake in 2011. The target area stretches 500 km along the northeastern coastline of Japan. The RGB images were captured every three to four months by a vehicle equipped with an omnidirectional camera (Ladybug 3 and 5 of Point Grey Research Inc.) on its roof. The image data accumulated to date amount to about 40 TB. The target of this experiment is the entire city of Kamaishi, Japan (over 4 million square meters). For the experiments, we chose the two image sequences captured on April 26, 2011 (one month after the tsunami) and August 17, 2013 (two years and five months after the tsunami). The debris can often be seen in the earlier images, whereas they tend to disappear from the later images as the recovery operation proceeds.

The street images are used for appearance-based recognition of ‘stuff’ [46] described in Section 3.1. The results of pixel-wise regression are then projected onto the ground plane as an input feature for our city-scale GP regressor. The resolution of the sensors of Ladybug 3 and 5 are 1200 × 1600 and 2048 × 2448, respectively. We obtained the undistorted perspective images of the areas to the left and the right of the measurement vehicle from the omnidirectional camera, and resized the perspective images into 240 × 320 to reduce the computational time required for the pixel-wise regression.


                        Aerial images: We downloaded the RGB aerial images of the entire city using Google Earth. The resolution of the aerial images is 4200 × 4200 (0.2268 [m2/pixel]). Because the number of time instances recorded in Google Earth is sparse for Kamaishi, we selected the images from March 31, 2011 and May 13, 2012 which were the closest to the acquisition time stamps of the street-view dataset.

We used the aerial images for appearance-based recognition of ‘stuff’ categories using the same method described in Section 3.1 but applied it to the entire aerial image as a comparative baseline. We used the aerial images of May 13, 2012 as the labeled training data and test on the March 31, 2011 aerial image. Fig. 9
                         shows an example of the hand-labeled ground truth of the debris area on the aerial images.


                        Digital Elevation Map (DEM): The DEM information is freely available from the Geospatial Information Authority, under the Ministry of Land, Infrastructure, Transportation and Tourism in Japan. The mesh resolution of the DEM is 5 × 5 square meters and contains the elevation level for each grid location. The elevation is used directly as a feature for the city-scale GP regression.


                        Building Occupancy Map (BOM): The BOM provides building contours. We obtained the data from Zenrin Co., Ltd. The building contour data used for this experiment was produced before the earthquake. We used the BOM to prevent ‘stuff’ from being projected onto the ground over building location.

We examine the effects of each input data type on the overall performance of our proposed approach. Fig. 10
                         shows the estimation results of the debris amounts for the entire city on April 26, 2011 and August 17, 2013, respectively. The lines on the aerial images are the camera trajectories. Fig. 11
                         shows the performance of our debris detection by PR-plot and F1-score using different combinations of input data. The results indicate that the use of aerial images alone yields a low performance because the appearance of land conditions can change significantly over time due to changes in imaging conditions. When compared to the independent use of aerial images, our results indicate that street images are more accurate for estimating city-scale debris. Furthermore, when both aerial and street images are combined we obtain improved performance as the aerial information helps the city-scale GP regression to generalize across similar-looking city regions.

Additionally, we evaluated the effects of each input data type and different number of street-view images in three different streets. Figs. 12
                        –14
                        
                         show (a) an input aerial image, (b) the hand-labeled ground truth which we produced from aerial and street-view images, (c) projection results of debris probabilities of street-view images, (d) final debris estimation results integrating both street-view and aerial images along with the digital elevation map (DEM) and building occupancy map (BOM) of each street.


                        Fig. 15
                         shows the precision-recall curve (Left) and F1-scores (Right, recall=0.5) of the debris area detection. We examined the detection performance for a specific area. Street-view images basically provide detailed and accurate information of land-surface conditions. The effect of different input types differs depending on the area condition. For example, an aerial image could produce errors in occluded areas as we mentioned in the introduction. We implicitly assume that debris would not typically occur below a roofing structure. However, in Fig. 13, “Street-view + Aerial” is worse than “Street-view,” because there are too many areas with debris below the roofing structure, such as A and B in Fig. 10. It is necessary to consider the inconsistency between aerial and street-view images for the improvement. Furthermore, DEM information could cause errors because its elevation includes the height of buildings.

We also tested the effect of street coverage. Fig. 16
                         shows the F1-score of a different number of street-view images. In this experiment, we randomly sampled street-view images. The accuracy improves as we add more images, but it quickly saturates. This indicates that our algorithm only requires sparse street-view images. This sparse sampling requirement of our algorithm is beneficial for many other applications, for example, large-scale citizen science or journalism in which images captured at the scene are sent to cloud computers to analyze the condition on a city scale.

Our method has relatively low absolute precision because (i) the grid size is too large due to limitations in terms of computational resources and (ii) the estimated camera poses have errors due to GPS errors. We believe that we can solve the first problem using a large-scale Gaussian process [45]. The GPS issue can be addressed with [8,9,47] by taking temporal changes into account. Furthermore, in the case of extreme calamities, methods will be developed to take into consideration the complete disappearance of buildings due to disasters.


                        Fig. 10 shows the estimation results of the amounts of debris in the entire city on April 26, 2011 and August 17, 2013, respectively. The lines on the aerial images are the camera trajectories. The locations A–E correspond to A’–E’, respectively. The heat map color scale shows the probability of debris existing (red indicates a greater likelihood for debris and blue a reduced likelihood).

As mentioned in the introduction, there are certain types of debris which cannot be observed using an aerial image. The locations A and B in Fig. 10 show areas where debris is covered by the roof of the building and are not observed from the aerial image. However, since our method integrates both street and aerial images this region has been estimated as a high debris area (red and yellow green). In the same region observed on August 17, 2013 (A′ and B′), all the debris (including the roof structure!) has been removed.

At location C, there are stacked mounds of debris and the area is estimated as a moderate debris area (yellow) but is completely restored (C′) by 2013.

Location D is estimated as having a moderate level of debris (yellow) which is confirmed by the street image. By 2013 region D’ had been restored. The debris, building, and car were removed and a field of weeds had since grown there.

Location E shows an example of a case in which our approach failed, because the particular type of debris was not encountered in our debris training data and therefore not detected in the street image. The post-restoration image shows that the area is now under construction.

We applied our method to vegetation detection, to show how our approach can be generalized to estimate other modes of land condition. Fig. 17
                      shows an example of vegetation estimation in street-level images. The green vegetation detected in the street-view images is estimated using the same pixel-wise object recognition method [35] in which Lab, HOG, BRIEF, and ORB are used, because, in practice, most of the green vegetation can be detected using only the appearance features. The vegetation estimation of the whole city is based on extrapolation of the aerial images to estimate the street-view images.


                     Fig. 18
                      shows the results of vegetation estimation for the entire city similar to Fig. 10. (The locations A–E and A′–E′ in Fig. 18 correspond to A–E and A’–E’ in Fig. 10, respectively.) By observing the vegetation heat map for the entire city, it is clear that most of the vegetation had been washed away by the tsunami. There is also a sharp contrast between the widespread distribution of debris and the lack of vegetation in the time period directly after the tsunami. By 2013 however, we can see a large increase in the number of regions covered by vegetation. Our successful ability to detect vegetation indicates that our proposed method can indeed be generalized to estimate city-scale land conditions for different types of targets.

@&#CONCLUSION@&#

We presented a unified framework for integrating image data acquired at vastly different viewpoints to generate large-scale estimates of land surface conditions. The proposed strategy uses macro-level imaging to learn the correspondence of land conditions between land regions that share similar visual characteristics, whereas micro-level images are used to acquire high-resolution statistics of land conditions. Our approach was validated by conducting experiments to estimate the amount of post-tsunami damage over the entire city of Kamaishi, Japan. The experimental results show that our approach is capable of integrating both macro- (aerial) and micro-level (street-view) images, along with other forms of meta-data, to estimate city-scale phenomena.

Furthermore, we showed that our detection method can be applied to vegetation estimation. Our method has good general applicability for estimating city-scale phenomena when the detector target is replaced, for example, to estimate human flow, real estate, and dirt quality. These types of image data can be acquired by many kinds of data sources, such as camera-equipped mobile devices, surveillance cameras, car-mounted video recorders, or aerial-vehicle-mounted cameras. Our approach provides an effective and robust method for integrating different kinds of data to estimate city-scale phenomena.

@&#ACKNOWLEDGMENT@&#

This work was partly supported by CREST, JST and JSPS KAKENHI grant number 25280054.

@&#REFERENCES@&#

