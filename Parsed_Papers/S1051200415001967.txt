@&#MAIN-TITLE@&#Verification of hidden speaker behind transformation disguised voices

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           We investigate the effects of voice transformation disguise on ASV performance.


                        
                        
                           
                           We propose an algorithm to estimate transformation factor.


                        
                        
                           
                           We propose an algorithm to restore the genuine acoustic characteristics.


                        
                        
                           
                           We integrate the above algorithms into GMM–UBM based ASV system.


                        
                        
                           
                           The proposed approach achieves 3–4% EER.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Voice transformation

Disguise

Automatic speaker verification

Security

@&#ABSTRACT@&#


               
               
                  Voice transformation, which has been integrated in many audio (speech) processing tools, is a common operation to change a person's voice and to conceal his or her identity. It can deceive human beings and automatic speaker verification (ASV) systems easily, and thus it presents threats to security. Until now, few efforts have been reported on the recognition of hidden speakers from such disguised voices. In this paper, we propose concrete countermeasures to erase the disguise effects and verify the speaker's identity from voice transformation disguised voices. The proposed system is tested by commonly used audio editors and voice transformation algorithms. The experimental results show that the performances of baseline ASV system without our proposed countermeasures are entirely destroyed by voice transformation disguise with equal error rates (EERs) higher than 40%; while with our proposed countermeasures, the verification performances are improved significantly with EERs lowered to 3%–4%.
               
            

@&#INTRODUCTION@&#

Voice disguise can be classified into two categories: voice conversion (VC) and voice transformation (VT) [1,2]. VC intends to transform one's voice to imitate a target person provided with the target's acoustic information, while VT intends to change the sound without any target. It is apparent that VC is to change one's voice in order “to be recognized as another person” while VT is to change one's voice in order “not to be recognized”. Both present threats to human identification. However, since no target information is needed, VT is much easier to implement than VC, leading to the fact that VT has been incorporated in many prevailing audio editors while VC has never been.

Besides electronic disguise by audio editors, VT can be performed by non-electronic means of a mechanic system [3–8] like a mask over the mouth, a pen in the mouth or pinching the nostril. However, by using sophisticated algorithms the output by editors generally sounds much more natural than the mechanic one [1], and thus it presents greater confusion as people tend to be deceived more by voices that sound natural.

With high disguise quality and ease of implementation by abundant tools, digital VT disguise has been used in more and more criminal cases, and has presented threats to security. However, research efforts on this topic is still insufficient. In this paper we will examine automatic speaker verification (ASV) of digital VT disguised voices.

Though few researches on ASV of VT disguise were reported, there have been some related efforts in the past 15 years.

Tan [5], Zhang et al. [6], Perrot et al. [7] and Künzel et al. [8] investigated the effect of several mechanic VT disguises including hand over mouth, pinched nostril, high pitch and low pitch, and conducted experiments using Gaussian Mixture Model (GMM) [9], Vector Quantization (VQ) and Support Vector Machine (SVM) classifiers for ASV. The results indicated that the transformations make the recognition system fail totally. However, no detailed solution has been presented. Jin et al. [10,11], Bonastre et al. [12–14], Wu et al. [15] and Kinnunen et al. [16] studied the effect of VC on ASV systems to find that recognition performance was damaged. Unfortunately, no solution was proposed either. Alegre et al. [17] presented an algorithm based on the reduction in pair-wise distances between consecutive feature vectors, and integrated it into ASV systems. The experimental results showed that with this countermeasure, false acceptance rates (FARs) and equal error rates (EERs) are both significantly lowered compared to without it. However, the authors admitted that this is completely unrealistic of a practical spoofing scenario because the proposed countermeasure exploits prior knowledge of only one limited single spoofing attack. Kons et al. [21] reported an analysis of the vulnerability of text dependent ASV systems to simple VC spoofing attacks. They revealed that training using genuine voices as well as converted voices can improve the verification performance. Like the one in [17], this method also exploits prior full knowledge of only one spoofing attack which is unrealistic in practice; and the method aims at text dependent ASV, which is far more limited than text independent ASV.

From the above reports, it can be concluded that most of the efforts are focused on VC and mechanic VT disguise, while digital VT disguise has received less attention. More importantly, the major problem of the current work is that most of the researchers only investigate the effects of disguise on ASV systems. They do not present concrete robust and universal solutions to erase the negative effects for revealing genuine speaker's identity, which is a far more challenging issue.

Recently, Wang et al. [18] and Wu et al. [19,20] proposed algorithms to detect VT disguised voices from genuine voices. Cross-disguise-method and cross-corpus were included in the experiments. The results showed that detection rate can reach over 90% in all test situations. These works are significant contributions to VT disguise forensics. But again, a further research is needed on verification of hidden speaker VT disguised voices.

Aiming at the above problems, we will investigate the effects of electronic VT disguise on ASV performances. More importantly, we will propose robust countermeasures for ASV to recognize speaker's identity from disguised voices. We propose a new algorithm for estimating VT parameter estimation by using fundamental frequencies [22,23], and a modified MFCC (Mel Frequency Cepstral Coefficient) extraction algorithm [9] which is effective to recover the original MFCCs from VT disguised voices. The proposed countermeasures are integrated into the GMM–UBM ASV system to test its efforts on the most prevailing and dominant audio editors and algorithms. The proposed system is demonstrated to outperform the baseline system by achieving a low error rate.

The remainder of this paper is organized as follows. In Section 2, we introduce the principles of electronic voice transformations. In Section 3, we propose countermeasures to compensate the deformation of acoustic feature and to erase the disguise effects. Experimental results are given in Section 4. Finally, we summarize conclusions and future works in Section 5.

VT can be divided into two categories: frequency-domain based and time-domain based techniques. In this section we discuss the principle of VT techniques and their deformation effects on acoustic features.

The principle of frequency domain based VT is to raise or lower voice pitch by stretching or compressing the frequency spectrum [24]. Being related by Fourier transform, time and frequency characteristics of a signal are not independent but are of a duality relationship. It is essential to break this traditional tie between them to keep the tempo unchanged.

The most frequently used tool for analysis in frequency-domain methods is based on the quasi-stationary sinusoidal model [25], in which speech signal 
                           x
                           (
                           t
                           )
                         is represented as a sum of sinusoids whose instantaneous frequency and amplitude vary slowly with time. In most applications, this model is represented by short-time Fourier transform (STFT), which starts with dividing a signal into short segments. Fast Fourier transform (FFT) is then applied to each segment and the resulting spectral components can be manipulated in a variety of ways. However, due to the resolution limitation, FFT bin frequencies generally do not represent true or instantaneous frequencies. For example using a window of size 2048 and a sampling rate of 44.1 kHz, the resolution in frequency domain is only 21.5 Hz, which is far too coarse in the lower frequency band.

In order to solve this problem, a phase-vocoder is introduced which, by insight of relationship between phase and frequency, employs phase information that STFT ignores to improve frequency estimation. The kernel of the phase vocoder is to compute deviation from FFT bin frequencies to instantaneous frequencies by using phase information. Instantaneous frequency can then be computed by adding the deviation and the FFT bin frequency. Finally, three numbers obtained from the FFT analysis for each sinusoid, namely bin magnitude, bin frequency and bin phase, are reduced to only magnitude and transient frequency. We now present it in a simple form in Eqs. (1)–(3).

Suppose 
                           
                              
                                 x
                              
                              
                                 t
                              
                           
                           (
                           n
                           )
                         is a frame of length N from the input speech signal at time t. Firstly, it is windowed by 
                           w
                           (
                           n
                           )
                        , and then an FFT is performed on the windowed signal, using Eq. (1), where 
                           w
                           (
                           n
                           )
                         is a Hamming or Hanning window and k is the bin frequency index.
                           
                              (1)
                              
                                 F
                                 (
                                 k
                                 )
                                 =
                                 
                                    ∑
                                    
                                       n
                                       =
                                       0
                                    
                                    
                                       N
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       x
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 n
                                 )
                                 ⋅
                                 w
                                 (
                                 n
                                 )
                                 
                                    
                                       e
                                    
                                    
                                       −
                                       i
                                       
                                          
                                             2
                                             π
                                             k
                                             n
                                          
                                          N
                                       
                                    
                                 
                                 
                                 0
                                 ⩽
                                 k
                                 <
                                 N
                              
                           
                        
                     

Then, instantaneous magnitude 
                           |
                           F
                           (
                           k
                           )
                           |
                         and instantaneous frequency 
                           ω
                           (
                           k
                           )
                         are calculated by Eq. (2) and Eq. (3) respectively,
                           
                              (2)
                              
                                 |
                                 F
                                 (
                                 k
                                 )
                                 |
                                 =
                                 |
                                 
                                    ∑
                                    
                                       n
                                       =
                                       0
                                    
                                    
                                       N
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       x
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 n
                                 )
                                 ⋅
                                 w
                                 (
                                 n
                                 )
                                 
                                    
                                       e
                                    
                                    
                                       −
                                       i
                                       
                                          
                                             2
                                             π
                                             k
                                             n
                                          
                                          N
                                       
                                    
                                 
                                 |
                                 
                                 0
                                 ⩽
                                 k
                                 <
                                 N
                              
                           
                        
                        
                           
                              (3)
                              
                                 ω
                                 (
                                 k
                                 )
                                 =
                                 (
                                 k
                                 +
                                 Δ
                                 )
                                 ⋅
                                 
                                    
                                       F
                                    
                                    
                                       s
                                    
                                 
                                 /
                                 N
                                 
                                 0
                                 ⩽
                                 k
                                 <
                                 N
                              
                           
                         where 
                           
                              
                                 F
                              
                              
                                 s
                              
                           
                         is the sampling frequency and Δ is the deviation from the kth bin frequency.

For voice transformation, transient frequency 
                           ω
                           (
                           k
                           )
                         is modified by Eq. (4), where α is the scale factor.
                           
                              (4)
                              
                                 
                                    
                                       ω
                                    
                                    
                                       ′
                                    
                                 
                                 (
                                 ⌊
                                 k
                                 ⋅
                                 α
                                 ⌋
                                 )
                                 =
                                 ω
                                 (
                                 k
                                 )
                                 ⋅
                                 α
                                 
                                 0
                                 ⩽
                                 k
                                 ,
                                 k
                                 ⋅
                                 α
                                 <
                                 N
                                 /
                                 2
                              
                           
                        
                     

There are several ways to modify the instantaneous magnitude. The commonest method is linear interpolation, as seen in Eq. (5) 
                        [24], where 
                           0
                           ⩽
                           k
                           ,
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                           <
                           N
                           /
                           2
                        , 
                           k
                           =
                           ⌈
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                           /
                           α
                           ⌉
                        , and 
                           μ
                           =
                           
                              
                                 k
                              
                              
                                 ′
                              
                           
                           /
                           α
                           −
                           k
                        .
                           
                              (5)
                              
                                 |
                                 
                                    
                                       F
                                    
                                    
                                       ′
                                    
                                 
                                 (
                                 ⌊
                                 k
                                 ⋅
                                 α
                                 ⌋
                                 )
                                 |
                                 =
                                 μ
                                 |
                                 F
                                 (
                                 k
                                 )
                                 |
                                 +
                                 (
                                 1
                                 −
                                 μ
                                 )
                                 |
                                 F
                                 (
                                 k
                                 +
                                 1
                                 )
                                 |
                              
                           
                        
                     

Another commonly used method is energy-preserving modification by Eq. (6).
                           
                              (6)
                              
                                 |
                                 
                                    
                                       F
                                    
                                    
                                       ′
                                    
                                 
                                 (
                                 ⌊
                                 k
                                 ⋅
                                 α
                                 ⌋
                                 )
                                 |
                                 =
                                 
                                    ∑
                                    
                                       ⌊
                                       k
                                       ⋅
                                       α
                                       ⌋
                                       ⩽
                                       k
                                       ⋅
                                       α
                                       <
                                       ⌊
                                       k
                                       ⋅
                                       α
                                       ⌋
                                       +
                                       1
                                    
                                 
                                 |
                                 F
                                 (
                                 k
                                 )
                                 |
                              
                           
                        
                     

For simplicity, we still use k as the index of the modified instantaneous frequency 
                           
                              
                                 ω
                              
                              
                                 ′
                              
                           
                         and the instantaneous magnitude 
                           
                              
                                 F
                              
                              
                                 ′
                              
                           
                        .

The instantaneous phase 
                           
                              
                                 ϕ
                              
                              
                                 ′
                              
                           
                           (
                           k
                           )
                         is then calculated via the instantaneous frequency 
                           
                              
                                 ω
                              
                              
                                 ′
                              
                           
                           (
                           k
                           )
                         and the transformed FFT coefficients is obtained by Eq. (7).
                           
                              (7)
                              
                                 
                                    
                                       F
                                    
                                    
                                       ′
                                    
                                 
                                 (
                                 k
                                 )
                                 =
                                 |
                                 
                                    
                                       F
                                    
                                    
                                       ′
                                    
                                 
                                 (
                                 k
                                 )
                                 |
                                 
                                    
                                       e
                                    
                                    
                                       i
                                       
                                          
                                             ϕ
                                          
                                          
                                             ′
                                          
                                       
                                       (
                                       k
                                       )
                                    
                                 
                              
                           
                         An inverse FFT (IFFT) is performed on 
                           
                              
                                 F
                              
                              
                                 ′
                              
                           
                           (
                           k
                           )
                         and the transformed signal can thus be obtained.

By phase-vocoder based transformation, all frequency components are adjusted by the scaling factor that includes fundamental frequencies and formants [24]. Considering that fundamental frequencies are more stable and easier to extract than formants, we will use fundamental frequencies to estimate scaling factor.

A phase-vocoder is not universal, however. In some applications, STFT phases are either lost or not applicable with STFT magnitude (STFTM) as the only available information. Hence, algorithms [26–30] have been explored to reconstruct time-domain signal from STFTM without phase information. Among them the latest and most effective algorithms are real-time iterative spectrogram inversion (RTISI) [26] and RTISI with look-ahead (RTISI-LA) [27,28]. In RTISI or RTISI-LA, real phases are not needed for signal reconstruction. Instead, they are estimated by previous and (or) future frames. The estimated phase and the given magnitude spectrum are combined to construct the current frame. RTISI and RTISI-LA can be used for voice transformation, illustrated in Fig. 1
                        . If the normal analysis frame length is L, and transformation factor is α, then for each frame we use a block of 
                           
                              
                                 L
                              
                              
                                 ′
                              
                           
                           =
                           α
                           L
                         samples, as shown in Fig. 1(b). Resampling is done in the time-domain to generate a frame of length as shown in Fig. 1(c). The target STFTM is then computed on the resulting frame. RTISI or RTISI-LA is then run with the given STFTM and the estimated phases to reconstruct the transformed frame.

From the above principles, we can learn that the scaling factor α is the key factor in VT. α must be estimated in order to retrieve the original features from disguised voices. In phonetics, transformation is always measured by a 12-semitones division [31], which leads the scaling factor α to the following form.
                           
                              (8)
                              
                                 α
                                 (
                                 s
                                 )
                                 =
                                 
                                    
                                       2
                                    
                                    
                                       s
                                       /
                                       12
                                    
                                 
                              
                           
                        
                     

Theoretically, VT can be from 
                           s
                           =
                           −
                           12
                         to 
                           s
                           =
                           12
                         semitones. In practice, audio editors generally provide VT from −11 to 11 semitones.

VT has been incorporated in many audio editing tools, including Adobe Audition (formerly Cool Edit Pro) [32], a commercial standard editor for over ten years, Audacity [33], the winner of SourceForge 2007 and 2009 Community Choice Award for Best Project for Multimedia with 76.5 millions downloads [34], and GoldWave [35], a popular commercial digital audio editing software product. The quasi-stationary sinusoidal model and phase vocoder are used by Audacity for VT. Adobe Audition and GoldWave use frequency domain based techniques for VT, but the detailed algorithms are not open to public. In this paper, we will test voices disguised by these three leading editors and RTISI-LA.

Frequency-domain based VT can modify a voice to a large extent while preserving its tempo and natural audible quality, while time-domain based VT can do it only when the modification is small [24]. This leads to the fact that the latter has rarely been adopted by today's audio editors. But in some limited applications, time-domain VT with small scaling factors still proves useful for its simplicity.

Most time-domain techniques for VT are based on the same fundamental method, best illustrated by the Pitch-Synchronous Overlap and Add method (PSOLA) [36,37]. There are several variants of PSOLA, among which time-domain PSOLA (TD-PSOLA) [36,37] is the most popular and commonly used one.

In TD-PSOLA, a preliminary analysis stage is carried out to detect locations and contours of pitches. Then short-time signals are extracted at a pitch-synchronous rate (denoted as 
                           P
                           (
                           t
                           )
                         in Fig. 2
                        ) by use of a weighting window. The short-time signals are then overlap/added at a modified rate 
                           
                              
                                 P
                              
                              
                                 ′
                              
                           
                           (
                           t
                           )
                           =
                           P
                           (
                           t
                           )
                           /
                           α
                           (
                           t
                           )
                        , and are repeated (or discarded) to compensate for the corresponding modification of duration, where 
                           α
                           (
                           t
                           )
                         is the scaling factor. The periodicity of the glottal pulses is modified, which corresponds to an alteration of the fundamental frequency. Hence fundamental frequencies can be used to estimate scaling factor.

In our proposed ASV approach, we use MFCC, which incorporates psychoacoustics mode of human auditory system and has become the dominant acoustic feature in today's ASV systems. The normal MFCC extraction algorithm is described as follows.

Suppose 
                           
                              
                                 x
                              
                              
                                 t
                              
                           
                           (
                           n
                           )
                         is a frame of length N from the input speech signal at time t. Firstly, it is windowed by a Hanning or Hamming window 
                           w
                           (
                           n
                           )
                        , and then an FFT is applied on the windowed signal as shown in Eq. (9).
                           
                              (9)
                              
                                 F
                                 (
                                 k
                                 )
                                 =
                                 
                                    ∑
                                    
                                       n
                                       =
                                       0
                                    
                                    
                                       N
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       x
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 n
                                 )
                                 ⋅
                                 w
                                 (
                                 n
                                 )
                                 
                                    
                                       e
                                    
                                    
                                       −
                                       i
                                       
                                          
                                             2
                                             π
                                             k
                                             n
                                          
                                          N
                                       
                                    
                                 
                                 
                                 0
                                 ⩽
                                 k
                                 <
                                 N
                              
                           
                        
                     


                        
                           F
                           (
                           k
                           )
                         is then divided into M subbands according to the mel scale to simulate the human auditory system (HAS) critical subbands [38]. Energy of each FFT line within a subband is then weighted by a window 
                           
                              
                                 H
                              
                              
                                 m
                              
                           
                         to combine the total contribution of this subband to the HAS, and then a log operation is performed by Eq. (10).
                           
                              (10)
                              
                                 Y
                                 (
                                 m
                                 )
                                 =
                                 log
                                 ⁡
                                 [
                                 
                                    ∑
                                    
                                       k
                                       =
                                       0
                                    
                                    
                                       N
                                       −
                                       1
                                    
                                 
                                 
                                    
                                       |
                                       F
                                       (
                                       k
                                       )
                                       |
                                    
                                    
                                       2
                                    
                                 
                                 ⋅
                                 
                                    
                                       H
                                    
                                    
                                       m
                                    
                                 
                                 (
                                 k
                                 )
                                 ]
                                 
                                 1
                                 ⩽
                                 m
                                 ⩽
                                 M
                              
                           
                        
                     

Finally inverse discrete cosine transform (IDCT) is performed and a D-dimension MFCC vector at time t, 
                           
                              
                                 v
                              
                              
                                 t
                              
                           
                        , is obtained by Eq. (11).
                           
                              (11)
                              
                                 
                                    
                                       v
                                    
                                    
                                       t
                                    
                                 
                                 (
                                 d
                                 )
                                 =
                                 
                                    1
                                    M
                                 
                                 
                                    ∑
                                    
                                       m
                                       =
                                       1
                                    
                                    M
                                 
                                 Y
                                 (
                                 m
                                 )
                                 cos
                                 ⁡
                                 (
                                 
                                    
                                       d
                                       (
                                       m
                                       −
                                       0.5
                                       )
                                       π
                                    
                                    M
                                 
                                 )
                                 
                                 0
                                 ⩽
                                 d
                                 ⩽
                                 D
                              
                           
                         where 
                           
                              
                                 v
                              
                              
                                 t
                              
                           
                           (
                           d
                           )
                         is the dth MFCC component. Derived coefficients (ΔMFCC) reflecting dynamic cepstral features are computed from the MFCC vector. In this paper, 12-dimension MFCC and 12-dimension ΔMFCC are combined to formed the feature vector. For simplicity, we still use 
                           
                              
                                 v
                              
                              
                                 t
                              
                           
                         to represent the feature vector at time t, in which the first 12 coefficients are of MFCC components, and the next 12 coefficients are of ΔMFCC components.

From the computation of MFCC and frequency-domain based VT principles, we can learn that MFCCs are modified by frequency-domain based VT because STFTMs 
                           |
                           F
                           (
                           k
                           )
                           |
                         are used to compute MFCCs and STFTMs are changed by VT. We have conducted extensive experiments to study the effect of frequency-domain based VT on MFCCs. As an example, we present 3 24-dimension feature vectors from a 1024-sample frame of an original speech signal and from its VT disguised versions respectively in Fig. 3
                        . We can see that deformation is introduced to the features by VT which damages ASV performances in their entirety. Please refer to Section 4 for the details. Hence, countermeasures are needed to erase the deformation for ASV.

TD-PSOLA is different from the frequency based VT. In TD-PSOLA, because the short-time signals are not re-sampled or altered but are merely copied back, the FFT spectrums of those signal segments remain unchanged, and thus the MFCCs extracted from those signal segments remain unchanged. TD-PSOLA does not introduce deformation to all the MFCCs, which leads to a different consequence. We will discuss the disguise effect of TD-PSOLA by extensive experiments in Section 4.

In this section we first introduce the GMM–UBM based baseline ASV system. Then we introduce our proposed ASV system which can recognize a speaker's identity from frequency-domain based VT disguised voices. Finally, we discuss the countermeasures of ASV for TD-PSOLA disguise.

Speaker verification can be viewed as a hypothesis test between:


                        
                           
                              
                                 H
                              
                              
                                 0
                              
                           
                        : Y is from the hypothesized speaker S
                     

and


                        
                           
                              
                                 H
                              
                              
                                 1
                              
                           
                        : Y is not from the hypothesized speaker S.

Mathematically 
                           
                              
                                 H
                              
                              
                                 0
                              
                           
                         is represented by a model 
                           
                              
                                 λ
                              
                              
                                 hyp
                              
                           
                         which characterizes the hypothesized speaker S. 
                           
                              
                                 λ
                              
                              
                                 hyp
                              
                           
                         is derived from a sequence of feature vectors (i.e., MFCC vectors in this paper) 
                           V
                           =
                           {
                           
                              
                                 v
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 v
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 v
                              
                              
                                 T
                              
                           
                           }
                        , where 
                           
                              
                                 v
                              
                              
                                 t
                              
                           
                         is an MFCC vector at time 
                           t
                           ∈
                           [
                           1
                           ,
                           2
                           ,
                           …
                           ,
                           T
                           ]
                        . 
                           
                              
                                 H
                              
                              
                                 1
                              
                           
                         is represented by a model 
                           
                              
                                 λ
                              
                              
                                 
                                    
                                       hyp
                                    
                                    ‾
                                 
                              
                           
                        , which represents the entire space of possible alternatives to the hypothesized speaker. For this alternative hypothesis modeling, the most common procedure is to pool recordings of several speakers to train a single model. This is also referred to as universal background model (UBM). For clarity we will denote 
                           
                              
                                 λ
                              
                              
                                 
                                    
                                       hyp
                                    
                                    ‾
                                 
                              
                           
                         as 
                           
                              
                                 λ
                              
                              
                                 bkg
                              
                           
                         in the following context.

Decision between 
                           
                              
                                 H
                              
                              
                                 0
                              
                           
                         and 
                           
                              
                                 H
                              
                              
                                 1
                              
                           
                         is made by a logarithmic likelihood ratio given by Eq. (12), where 
                           p
                           (
                           V
                           |
                           
                              
                                 H
                              
                              
                                 i
                              
                           
                           )
                         is the probability density function for the hypothesis 
                           
                              
                                 H
                              
                              
                                 i
                              
                           
                        .
                           
                              (12)
                              
                                 Λ
                                 (
                                 V
                                 )
                                 =
                                 log
                                 ⁡
                                 p
                                 (
                                 Y
                                 |
                                 
                                    
                                       H
                                    
                                    
                                       0
                                    
                                 
                                 )
                                 −
                                 log
                                 ⁡
                                 p
                                 (
                                 Y
                                 |
                                 
                                    
                                       H
                                    
                                    
                                       1
                                    
                                 
                                 )
                                 =
                                 log
                                 ⁡
                                 p
                                 (
                                 V
                                 |
                                 
                                    
                                       λ
                                    
                                    
                                       hyp
                                    
                                 
                                 )
                                 −
                                 log
                                 ⁡
                                 p
                                 (
                                 V
                                 |
                                 
                                    
                                       λ
                                    
                                    
                                       bkg
                                    
                                 
                                 )
                                 
                                    {
                                    
                                       
                                          
                                             ⩾
                                             θ
                                          
                                          
                                             accept
                                             
                                             
                                                
                                                   H
                                                
                                                
                                                   0
                                                
                                             
                                          
                                       
                                       
                                          
                                             <
                                             θ
                                          
                                          
                                             reject
                                             
                                             
                                                
                                                   H
                                                
                                                
                                                   0
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        
                     

For text-independent ASV, the most successful likelihood function has been GMM [9].

For a D-dimensional MFCC vector 
                           
                              
                                 v
                              
                              
                                 t
                              
                           
                        , the likelihood function is a weighted linear combination of R unimodal Gaussian densities defined as:
                           
                              (13)
                              
                                 p
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       t
                                    
                                 
                                 |
                                 λ
                                 )
                                 =
                                 
                                    ∑
                                    
                                       i
                                       =
                                       1
                                    
                                    R
                                 
                                 
                                    
                                       w
                                    
                                    
                                       i
                                    
                                 
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 ,
                              
                           
                         where the mixture weights 
                           
                              
                                 w
                              
                              
                                 i
                              
                           
                         must satisfy 
                           
                              
                                 ∑
                              
                              
                                 i
                                 =
                                 1
                              
                              
                                 R
                              
                           
                           
                              
                                 w
                              
                              
                                 i
                              
                           
                           =
                           1
                        .


                        
                           
                              
                                 p
                              
                              
                                 i
                              
                           
                           (
                           
                              
                                 v
                              
                              
                                 t
                              
                           
                           )
                         is computed by a 
                           D
                           ×
                           1
                         mean vector, 
                           
                              
                                 μ
                              
                              
                                 i
                              
                           
                        , and a 
                           D
                           ×
                           D
                         covariance matrix, 
                           
                              
                                 Σ
                              
                              
                                 i
                              
                           
                        , as follows.
                           
                              (14)
                              
                                 
                                    
                                       p
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       t
                                    
                                 
                                 )
                                 =
                                 
                                    1
                                    
                                       
                                          
                                             (
                                             2
                                             π
                                             )
                                          
                                          
                                             D
                                             /
                                             2
                                          
                                       
                                       
                                          
                                             |
                                             
                                                
                                                   Σ
                                                
                                                
                                                   i
                                                
                                             
                                             |
                                          
                                          
                                             1
                                             /
                                             2
                                          
                                       
                                    
                                 
                                 exp
                                 ⁡
                                 {
                                 −
                                 
                                    1
                                    2
                                 
                                 
                                    
                                       (
                                       
                                          
                                             v
                                          
                                          
                                             t
                                          
                                       
                                       −
                                       
                                          
                                             μ
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       ′
                                    
                                 
                                 
                                    
                                       (
                                       
                                          
                                             Σ
                                          
                                          
                                             i
                                          
                                       
                                       )
                                    
                                    
                                       −
                                       1
                                    
                                 
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       t
                                    
                                 
                                 −
                                 
                                    
                                       μ
                                    
                                    
                                       i
                                    
                                 
                                 )
                                 }
                              
                           
                        
                     

Collectively a GMM λ is denoted as 
                           λ
                           =
                           {
                           
                              
                                 w
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 μ
                              
                              
                                 i
                              
                           
                           ,
                           
                              
                                 Σ
                              
                              
                                 i
                              
                           
                           }
                         
                        
                           i
                           =
                           1
                           ,
                           2
                           ,
                           …
                           R
                        .

Finally, the average log likelihood value of a model λ for a sequence of MFCC vectors 
                           V
                           =
                           {
                           
                              
                                 v
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 v
                              
                              
                                 2
                              
                           
                           ,
                           …
                           ,
                           
                              
                                 v
                              
                              
                                 T
                              
                           
                           }
                         is computed by Eq. (15)
                        
                           
                              (15)
                              
                                 log
                                 ⁡
                                 
                                 p
                                 (
                                 V
                                 |
                                 λ
                                 )
                                 =
                                 
                                    1
                                    T
                                 
                                 
                                    ∑
                                    
                                       t
                                       =
                                       1
                                    
                                    T
                                 
                                 log
                                 ⁡
                                 
                                 p
                                 (
                                 
                                    
                                       v
                                    
                                    
                                       t
                                    
                                 
                                 |
                                 λ
                                 )
                                 .
                              
                           
                        
                     

Expectation–maximization (EM) [39] is used to estimate the model λ, which iteratively adjusts the GMM parameters to monotonically increase the likelihood function, i.e., for iterations k and 
                           k
                           +
                           1
                        , 
                           p
                           (
                           V
                           |
                           
                              
                                 λ
                              
                              
                                 (
                                 k
                                 +
                                 1
                                 )
                              
                           
                           )
                           >
                           p
                           (
                           V
                           |
                           
                              
                                 λ
                              
                              
                                 (
                                 k
                                 )
                              
                           
                           )
                        . In our experiments, UBM is built by the EM algorithm. For training a specific speaker's model, it has been strongly indicated that adapting parameters from UBM outperforms a decoupled system where the speaker model is trained independently of the UBM [40]. Hence we use Maximum A posteriori (MAP) estimation algorithm [41] for modeling a speaker in which model parameters are iteratively adapted from those of UBM.

The proposed system is shown in Fig. 4
                        . The UBM training, the speaker's model (
                           
                              
                                 λ
                              
                              
                                 j
                              
                           
                        ) training and the probability computation are the same as the baseline system. It differs from the baseline in three aspects.

1) We add a module ‘F0 Extraction’ in the training phase, where the fundamental frequency [22,23] sequence of speaker 
                           
                              
                                 S
                              
                              
                                 j
                              
                           
                         is extracted, and the average of this sequence, 
                           
                              
                                 f
                              
                              
                                 j
                              
                           
                        , is computed by the mean operation. The model of speaker 
                           
                              
                                 S
                              
                              
                                 j
                              
                           
                         consists of its GMM 
                           
                              
                                 λ
                              
                              
                                 j
                              
                           
                         and 
                           
                              
                                 f
                              
                              
                                 j
                              
                           
                        .

2) In the testing phase, Y is a speech signal that may have been modified by VT. Its fundamental frequency sequence is extracted by the module ‘F0 Extraction’, and the average of this sequence, 
                           
                              
                                 f
                              
                              
                                 Y
                              
                           
                        , is computed by the mean operation. 
                           
                              
                                 f
                              
                              
                                 j
                              
                           
                         and 
                           
                              
                                 f
                              
                              
                                 Y
                              
                           
                         are used to compute the estimated scaling factor 
                           
                              
                                 α
                              
                              
                                 ′
                              
                           
                         by the division (/) operation.

3) In the testing phase, 
                           
                              
                                 α
                              
                              
                                 ′
                              
                           
                         is used in the module Modified MFCC Extraction, whose output V is the recovered MFCCs from Y.

Now let's introduce the estimation of scaling factors and the modified MFCC extraction algorithm in details.

In our approach, estimated scaling factors are essential to the recovery of MFCCs from a disguised voice. Here we turn to fundamental frequency for the estimation of scaling factors.

Fundamental frequency, often denoted as F0, is the lowest frequency in a harmonic series such as speech or music [22,23]. F0 of speech is produced by glottal excitation, best illustrated by a model [24,42] described in Fig. 5
                           . A volume velocity glottal excitation function 
                              
                                 
                                    u
                                 
                                 
                                    g
                                 
                              
                              (
                              t
                              )
                            excites a passive linear system. The glottal wave is modeled as a transfer function 
                              
                                 
                                    u
                                 
                                 
                                    g
                                 
                              
                              (
                              t
                              )
                           , which is considered to be the sum of a pulse train 
                              
                                 
                                    p
                                 
                                 
                                    g
                                 
                              
                              (
                              t
                              )
                            and a slowly varying function 
                              
                                 
                                    v
                                 
                                 
                                    g
                                 
                              
                              (
                              t
                              )
                           . The function 
                              
                                 
                                    p
                                 
                                 
                                    g
                                 
                              
                              (
                              t
                              )
                            is called the excitation pulse function. Each individual excitation pulse has an associated time of occurrence, or its excitation pulse time. The excitation time is defined to occur at the time when the excitation pulse function reaches a zero value at the end of each glottal cycle. This time is also the instant of glottal closure. Fundamental frequency F0 is defined as:
                              
                                 (16)
                                 
                                    F
                                    0
                                    =
                                    1
                                    /
                                    T
                                    0
                                 
                              
                           
                        

From the model we know that F0 is produced by the open and closure of glottis. It is one biometrical feature that reflects movements (structures) of one's glottis, and it is less related to the uttered contents.

Voice transformation in FFT spectrum is to scale F0 by the same scaling factor [24,25].

Hence, in training phase, the average of F0 of speaker 
                              
                                 
                                    S
                                 
                                 
                                    j
                                 
                              
                           , 
                              
                                 
                                    f
                                 
                                 
                                    j
                                 
                              
                           , is extracted and saved into the model of 
                              
                                 
                                    S
                                 
                                 
                                    j
                                 
                              
                           , as shown in Fig. 4. In testing, the average of F0 of testing speech Y, 
                              
                                 
                                    f
                                 
                                 
                                    Y
                                 
                              
                           , is computed. The estimated scaling factor, 
                              
                                 
                                    α
                                 
                                 
                                    ′
                                 
                              
                           , is computed by Eq. (17).
                              
                                 (17)
                                 
                                    
                                       
                                          α
                                       
                                       
                                          ′
                                       
                                    
                                    =
                                    
                                       
                                          f
                                       
                                       
                                          Y
                                       
                                    
                                    /
                                    
                                       
                                          f
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                           
                        

We use Praat [23] to extract the fundamental frequency. The proposed estimation method has demonstrated to be very effective by extensive experiments which will be presented in Section 4.

VT changes MFCCs, and thus damages ASV performances. In order to recognize speaker's identity from the disguised voice, we need to recover the original MFCCs from them. A straightforward way is to reconvert modified voice by scaling factor 
                              1
                              /
                              
                                 
                                    α
                                 
                                 
                                    ′
                                 
                              
                            before MFCC extraction. However, VT is not a lossless procedure. To have a transformed voice be transformed for a second time will result in a further loss of substantial frequency components, especially for a large modification. Besides, as prior knowledge of transformation types and details is unknown in most cases, this straightforward way is not universal.

Here, we propose a universal modified MFCC extraction algorithm which derives the original MFCCs from transformed voices directly for all the frequency domain based VT methods we consider in this paper.

From Eq. (4) or Eq. (6) in phase vocoder, we can learn that during transformation, instantaneous magnitudes (i.e., STFTMs or FFT spectrum) are modified, and from Eq. (10) and Eq. (11), we can also learn that the modified STFTMs are used to calculate MFCCs. Hence, once the original FFT spectrum is recovered, the original MFCCs can be retrieved. Inspired by Eq. (4) and Eq. (6), one of the two recovery algorithms can be adopted.

1) The STFTMs are linearly interpolated by estimated scaling factor 
                              1
                              /
                              
                                 
                                    α
                                 
                                 
                                    ′
                                 
                              
                            using Eq. (4).

2) The STFTMs are relocated by estimated scaling factor 
                              1
                              /
                              
                                 
                                    α
                                 
                                 
                                    ′
                                 
                              
                            using Eq. (6). Through extensive experiments, the first algorithm outperforms the second one for all VT methods. It is used in our proposed system to retrieve original MFCCs from disguised voices.

Some practical issues must be taken into consideration. Let us denote the Nyquist Frequency as 
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                           . In training, a speaker's model in database is derived from MFCCs, and each MFCC is computed from spectrum ranging from 0 Hz to 
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              /
                              2
                           . Accordingly, the spectrum of a testing speech clip must cover from 0 Hz to 
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              /
                              2
                           . We discuss it below.

1) If 
                              α
                              <
                              1
                           , FFT spectrum is condensed. To recover original MFCCs, FFT spectrum is scaled by 
                              1
                              /
                              
                                 
                                    α
                                 
                                 
                                    ′
                                 
                              
                            to expand and reaches 
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              /
                              2
                           , which satisfies the above requirement, provided that the estimation of scaling factor is accurate.

2) If 
                              α
                              >
                              1
                           , FFT spectrum is expanded and those components exceeding 
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              /
                              2
                            are discarded. To recover the original MFCCs, this spectrum is scaled by 
                              1
                              /
                              
                                 
                                    α
                                 
                                 
                                    ′
                                 
                              
                           . The resulting compressed spectrum covers from 0 Hz to 
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              /
                              2
                              /
                              
                                 
                                    α
                                 
                                 
                                    ′
                                 
                              
                           . An example is shown in Fig. 6
                           (a). Consequently, spectrum from 
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              /
                              2
                              /
                              
                                 
                                    α
                                 
                                 
                                    ′
                                 
                              
                            to 
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              /
                              2
                            must be compensated. Several compensation methods have been tried in our experiments, and the best one is to copy the spectrums from 
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              /
                              2
                              /
                              
                                 
                                    α
                                 
                                 
                                    ′
                                 
                              
                            to 
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              /
                              2
                              /
                              
                                 
                                    α
                                 
                                 
                                    ′
                                 
                              
                              −
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              /
                              2
                            into the spectrum region from 
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              /
                              2
                              /
                              
                                 
                                    α
                                 
                                 
                                    ′
                                 
                              
                            to 
                              
                                 
                                    F
                                 
                                 
                                    n
                                 
                              
                              /
                              2
                           , as illustrated in Fig. 6(b).

In Section 4 we will show the detailed performances.

As mentioned in Section 2.3, in TD-PSOLA, because the short-time signals are not re-sampled or altered but merely copied back, the MFCCs extracted from those signal segments remain unchanged. Hence, our proposed modified MFCC extraction algorithm in Section 3.2 for frequency-domain based methods is not applicable for TD-PSLOA. The straightforward way to reconvert the modified signal with scaling factor 
                           1
                           /
                           
                              
                                 α
                              
                              
                                 ′
                              
                           
                         could be considered. This is reasonable because TD-PSOLA can preserve content and natural audibility only when the scaling factors are small. To have a transformed voice signal be transformed for a second time with a small factor will not result in a loss of abundant signal components. However, since the MFCCs of those unchanged short segments remain unchanged, it is apparent that an ASV system without countermeasures can resist TD-PSOLA to some extent. The extent of needing to adopt countermeasures becomes a question. In Section 4, we will investigate this problem by experiments.

@&#EXPERIMENTS@&#


                        Transformations. Five types of transformation tools are tested, including Adobe Audition, Audacity, GoldWave, RSITI-LA and TD-PSOLA. Transformations from −11 to 11 semitones are taken into consideration.


                        Corpus and GMM–UBM configuration. TIMIT [43] is used as the corpus for the experiments. TIMIT is a commonly used corpus designed for the development of automatic speech/speaker recognition systems. It contains 630 speakers, namely 192 females and 438 males from 8 major dialect regions of America. Each speaker reads 10 sentences to obtain a total number of 6300 recordings in it. All the sentences are recorded as RIFF/WAV files using 16 kHz sampling rate, 16-bit quantization and mono channel. The duration of each recording lasts around 3 seconds. In our work, we divide TIMIT into 3 subsets according to the GMM–UBM architecture.

1) a world subset: a subset containing 60 female and 60 male speakers. All the recordings are pooled to train the single, gender independent and 2048-dimension UBM.

2) a normalization subset: a subset containing 40 female and 90 male speakers for score normalization (T-normalization).

3) a training-test subset: a subset containing 92 female speakers and 288 male speakers. For speaker j, five recordings are pooled to train the 2048-dimension model 
                           
                              
                                 λ
                              
                              
                                 j
                              
                           
                        , and to compute the average of fundamental frequencies 
                           
                              
                                 f
                              
                              
                                 j
                              
                           
                        ; these recordings form the training set. The other five recordings are pooled and disguised by voice transformations with different factors; the disguised recordings form the test set.

Consequently the models derived from development set are stored in database, and denoted as 
                           (
                           
                              
                                 λ
                              
                              
                                 j
                              
                           
                           ,
                           
                              
                                 f
                              
                              
                                 j
                              
                           
                           )
                        , 
                           1
                           ⩽
                           j
                           ⩽
                           380
                        . Disguised recordings in the evaluation set are denoted as 
                           
                              
                                 Y
                              
                              
                                 i
                              
                           
                           (
                           s
                           )
                        , 
                           1
                           ⩽
                           i
                           ⩽
                           380
                        , 
                           −
                           11
                           ⩽
                           s
                           ⩽
                           11
                        . The average of fundamental frequency sequence of 
                           
                              
                                 Y
                              
                              
                                 i
                              
                           
                           (
                           s
                           )
                         is denoted as 
                           
                              
                                 f
                              
                              
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                              
                           
                           (
                           s
                           )
                        . The likelihood ratio, i.e., score, of 
                           
                              
                                 Y
                              
                              
                                 i
                              
                           
                           (
                           s
                           )
                         against GMM 
                           
                              
                                 λ
                              
                              
                                 j
                              
                           
                         are computed by Eq. (12), rewritten in Eq. (18).
                           1
                        
                        
                           1
                           Subscript i and j are added to λ to associate with speakers' IDs. In addition, we are to study the effect of transformation strengths. Hence, λ is written as a function of s.
                        
                        
                           
                              (18)
                              
                                 
                                    
                                       Λ
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                 
                                 (
                                 s
                                 )
                                 =
                                 log
                                 ⁡
                                 p
                                 (
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 s
                                 )
                                 |
                                 
                                    
                                       λ
                                    
                                    
                                       j
                                    
                                 
                                 )
                                 −
                                 log
                                 ⁡
                                 p
                                 (
                                 
                                    
                                       Y
                                    
                                    
                                       i
                                    
                                 
                                 (
                                 s
                                 )
                                 |
                                 
                                    
                                       λ
                                    
                                    
                                       b
                                       k
                                       g
                                    
                                 
                                 )
                                 .
                              
                           
                        
                     

When 
                           i
                           =
                           j
                        , it is called a client score and when 
                           i
                           ≠
                           j
                        , it is called an imposter score. An ASV system is designed to separate distributions of these two scores.


                        Pre-processing and MFCC extraction. To lift the energy of the high frequency components, speech signal is pre-emphasized before MFCC extraction by transfer function 
                           H
                           (
                           z
                           )
                           =
                           1
                           −
                           0.97
                           
                              
                                 z
                              
                              
                                 −
                                 1
                              
                           
                        . Silence removal is also adopted for more accurate performance. Then from each 1024-sample frame,
                           2
                        
                        
                           2
                           In the following experiments, we will consider 23 disguise strengths for 5 disguise methods under 3 conditions. That is, we will conduct 
                                 23
                                 ×
                                 5
                                 ×
                                 3
                                 =
                                 345
                               times of an ASV procedure, which are of tremendously huge computation. The experiments are highly restricted by the computation ability and the memory resources. In order to shorten the time for computation and to reduce the memory occupation, we use a frame length of 64 ms instead of 20–30 ms which is usually adopted in speaker recognition.
                         a 24-dimension feature vector is extracted including a 12-dimension MFCC and a 12-dimension ΔMFCC.

Due to the vicinity of formant and fundamental frequency, and the discontinuous transitions between some concatenation points, singularities occurs in the extracted fundamental frequency sequence. An example is shown in Fig. 7
                        . We apply a filter which eliminates the top 15% and the bottom 15% fundamental frequencies. The remaining 70% fundamental frequencies are to compute the average. In order to associate estimated scaling factor with speakers' IDs, we written Eq. (17) as Eq. (19), by which we obtain the estimated scaling factor of recording 
                           
                              
                                 Y
                              
                              
                                 i
                              
                           
                         against model j.
                           
                              (19)
                              
                                 
                                    
                                       α
                                    
                                    
                                       i
                                       ,
                                       j
                                    
                                    
                                       ′
                                    
                                 
                                 (
                                 s
                                 )
                                 =
                                 
                                    
                                       f
                                    
                                    
                                       
                                          
                                             Y
                                          
                                          
                                             i
                                          
                                       
                                    
                                 
                                 (
                                 s
                                 )
                                 /
                                 
                                    
                                       f
                                    
                                    
                                       j
                                    
                                 
                              
                           
                        
                     

We present in Table 1
                         the following items of Adobe Audition when 
                           i
                           =
                           j
                        :

1) the true scaling factor 
                           α
                           (
                           s
                           )
                        ;

2) the estimated scaling factor with largest error, 
                           
                              
                                 α
                              
                              
                                 max
                              
                              
                                 ′
                              
                           
                           (
                           s
                           )
                        ;

3) the maximum error, 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                           (
                           s
                           )
                           =
                           |
                           α
                           (
                           s
                           )
                           −
                           
                              
                                 α
                              
                              
                                 max
                              
                              
                                 ′
                              
                           
                           (
                           s
                           )
                           |
                        ;

4) the maximum error rate, 
                           
                              
                                 E
                              
                              
                                 max
                              
                           
                           (
                           s
                           )
                           /
                           α
                           (
                           s
                           )
                        ;

5) the average of errors, 
                           
                              
                                 E
                              
                              
                                 mean
                              
                           
                           (
                           s
                           )
                        .

We can see that the maximum error rates range from 1.6% to only 7.7%, and the averages of error are also small, indicating the estimation is accurate.

The averages of the estimated scaling factors when 
                           i
                           =
                           j
                         in Fig. 8
                         also demonstrate that the estimated scaling factors are very close to the real ones. Similar results are obtained in other VT techniques.

Firstly, as an example, we present a 24-dimension MFCC vector from a 1024-sample VT disguised frame extracted by the baseline MFCC extraction algorithm and by the proposed modified algorithm in Fig. 9
                        . We can see that with our proposed countermeasure, the distance between the recovered MFCCs and the original MFCCs is smaller than the distance without our method.

Secondly, equal error rates (EERs) with different semitone transformations are shown in Fig. 10
                        . Global EERs are given in Table 2
                        . We present the performances of the baseline system and the proposed system. We show the best performances of our proposed system using true scaling factors and compare them with the performances using estimated scaling factors to demonstrate the effectiveness of our proposed system. Now we analyze the results for Adobe Audition in detail. Similar results are also obtained in Audacity, GoldWave and RTISI-LA.

When 
                           s
                           =
                           0
                        , i.e., when no disguise is performed on the speech recordings, the baseline performance of EER is 0.26%. When 
                           s
                           <
                           0
                        , the baseline EERs drastically increase from 0.26% to over 50%; when 
                           s
                           >
                           0
                        , the baseline EERs increase drastically from 0.53% to over 50%. The global baseline EER is 42.8% in Table 2. The above EERs are so high that the baseline ASV system has been destroyed by the disguise.

In our proposed system with estimated scaling factors, when 
                           s
                           =
                           0
                        , the proposed system achieves EER of 0.26% too as in the baseline. When 
                           s
                           <
                           0
                        , the EERs remain at 0.26% which are the same as the best result without disguise, indicating that with our proposed countermeasures speakers' features have been fully recovered. When 
                           s
                           >
                           0
                        , EERs increase from 0.26% to 10.5% in our proposed system. But they are still far lower than those of the baseline system. The global EER is 3.35% in Table 2. We can see that significant improvements have been achieved by our proposed countermeasures.

On the other side, when 
                           s
                           ⩽
                           5
                        , EERs of our proposed system using estimated scaling factors are the same as those using true scaling factors. When 
                           s
                           >
                           5
                        , these two kinds of EERs are very close. The biggest difference is only 
                           7.90
                           %
                           −
                           5.53
                           %
                           =
                           2.37
                           %
                         when 
                           s
                           =
                           9
                        . The global EERs of our proposed system using estimated and true scaling factors are 3.35% and 2.62%, respectively. The difference is only 0.73%. We can learn from these results that performances of our proposed system are close to the best performances, which demonstrates that the estimation error of scaling factor is small and has little influence on the ASV performance.

Thirdly, we plot global detection error tradeoff (DET) in Fig. 11
                        (a). Again, the proposed system outperforms the baseline system significantly, and are close to the best performances.

Empirically, when 
                           |
                           s
                           |
                           ⩽
                           3
                        , the disguise is too weak to deceive human beings; when 
                           9
                           ⩽
                           |
                           s
                           |
                           ⩽
                           11
                        , the modification is too large to preserve natural audibility. Here we take into account the disguise when 
                           4
                           ⩽
                           |
                           s
                           |
                           ⩽
                           8
                         that presents disguise confusion and natural audible quality. The EERs are shown in Table 3
                        . We can see that compared with Table 2, EERs of the proposed system in Table 3 decrease further for frequency based techniques.

We can see that verification performances are worst when 
                           s
                           >
                           0
                         than when 
                           s
                           ⩽
                           0
                        . This is because when 
                           s
                           >
                           0
                        , high frequency components are discarded by VT, and thus they have to be compensated for calculating MFCCs. A compensation method, however, affects the performances. Our proposed compensation method in Section 3.2 successfully lowers the error rates to an acceptable level.

In TD-PSOLA, as shown in Fig. 12
                         when 
                           s
                           ⩽
                           8
                         EERs remain at 0.26% both in proposed and baseline system. When 
                           s
                           ⩾
                           9
                        , EERs increase from 0.53% to 1.84% in the proposed system, and from 0.53% to 0.79% in the baseline system. Furthermore, global EERs (
                           |
                           s
                           |
                           ⩽
                           11
                        ) of the proposed system and the baseline system are 0.41% and 0.19%, respectively. Global EERs (
                           4
                           ⩽
                           |
                           s
                           |
                           ⩽
                           8
                        ) of the proposed system and the baseline system are 0.21% and 0.16%, respectively. DET is shown in Fig. 13
                        .

From the above results, we can see that, unlike the frequency domain based VT techniques, TD-PSOLA does NOT destroy the ASV performances. It is noted that the baseline system outperforms the proposed system slightly. As mentioned in Section 2.2, the short-time signals are not re-sampled or altered but merely copied back in TD-PSLOA, resulting in unchanged MFCCs from those segments. This is why the ASV system without countermeasures can resist TD-PSOLA.

For a further investigation, we compare the deformation effects on MFCCs introduced by TD-PSOLA and the four frequency-domain based techniques. We compute the distance between an original speech and its transformed version by dynamic time warping (DTW) [44]. We present the results of one speech recording in Table 4
                        . Similar results are obtained in other recordings. We can see that as 
                           |
                           s
                           |
                         increases, the DTW values increase dramatically in the four frequency-domain based techniques, while they remain steadily at a much lower level in TD-PSOLA. For those large modifications, like when 
                           s
                           =
                           −
                           11
                        , the DTW values reflet that the deformation introduced by TD-PSOLA is only around 5.1% to 11.3% of those by the other four.

From the above analysis and observation, we can expect TD-PSOLA to have less disguise ability and fewer threats to ASV.

@&#CONCLUSIONS@&#

Voice transformation, which has been integrated in many audio editing tools, can be used for disguise purposes to deceive human ears and ASV systems. Yet, little attention has been paid to this problem. In this paper, we give a very comprehensive study on ASV of VT disguises. The most significant contribution is that we propose concrete, practical and effective solutions for ASV to erase VT disguise and to recognize the identity of the hidden speaker behind VT disguised voices.

The key issue is to recover the speaker's original features from his or her disguised voice. For frequency-based VT techniques, we propose an algorithm for the estimation of scaling factors using fundamental frequency, and a modified MFCC extraction algorithm which uses the estimated scaling factors to recover the original MFCCs from the disguised voices. We integrate these countermeasures into a GMM–UBM based ASV system.

For a comprehensive investigation, we test our proposed system by frequency-domain based and time-domain based VT techniques. For frequency-domain VT, we take into account the leading commercial or free audio editors, including Adobe Audition, Audacity and GoldWave, and the latest and popular algorithm RTISI-LA. Experimental results show that the global EERs (
                        |
                        s
                        |
                        ⩽
                        11
                     ) are higher than 40% in the baseline system while they are around 2%–4% in our proposed systems. Global EERs (
                        4
                        ⩽
                        |
                        s
                        |
                        ⩽
                        8
                     ) are over 40% in the baseline system while they are around 1%–3% in our proposed system. The experiments demonstrate that 1) the performances of the baseline ASV system are entirely damaged by VT disguise, and 2) our proposed system outperforms the baseline so significantly that its error rates have been lowered to an acceptable level in many speech applications.

We also study the disguise effect of TD-PSOLA, the dominant time-domain VT technique, on ASV. Extensive experiments demonstrate that TD-PSOLA does not damage ASV performance. We then conduct analysis and comparison to explain the non-disguise effect of TD-PSOLA.

As mentioned in Section 1, besides VT, there are another type of voice disguise: voice conversion (VC). VC disguises have different principles and follow different technical routes from VT disguises. In future work, we will focus on speaker verification from VC disguised voices.

@&#ACKNOWLEDGEMENTS@&#

This work is supported by National Natural Science Foundation of China (NSFC) under Grant 61100168, Research Fund for the Doctoral Program of Higher Education of China under Grant 20110171120052, the Fundamental Research Funds for the Central Universities under Grant 12lgpy38 and the Natural Science Foundation of Guangdong Province under Grant 2014A030313623.

@&#REFERENCES@&#

