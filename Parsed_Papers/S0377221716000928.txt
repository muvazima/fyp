@&#MAIN-TITLE@&#Advanced Greedy Randomized Adaptive Search Procedure for the Obnoxious p-Median problem

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           GRASP is proposed with two constructive and two local search procedures.


                        
                        
                           
                           A memory structure is presented to allow incremental calculation of solutions.


                        
                        
                           
                           A mechanism to discard low quality solutions before the local search is proposed.


                        
                        
                           
                           A complete experimental experience is provided that studies the parameter values.


                        
                        
                           
                           The proposal is tested against the state of the art for the current problem.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Obnoxious location

Diversity problem

Metaheuristics

GRASP

Filter solutions

@&#ABSTRACT@&#


               
               
                  The Obnoxious p-Median problem consists in selecting a subset of p facilities from a given set of possible locations, in such a way that the sum of the distances between each customer and its nearest facility is maximized. The problem is 
                        NP
                     -hard and can be formulated as an integer linear program. It was introduced in the 1990s, and a branch and cut method coupled with a tabu search has been recently proposed. In this paper, we propose a heuristic method – based on the Greedy Randomized Adaptive Search Procedure, GRASP, methodology – for finding approximate solutions to this optimization problem. In particular, we consider an advanced GRASP design in which a filtering mechanism avoids applying the local search method to low quality constructed solutions. Empirical results indicate that the proposed implementation compares favorably to previous methods. This fact is confirmed with non-parametric statistical tests.
               
            

@&#INTRODUCTION@&#

Facility location has been of practical and theoretical interest for more than the half of a century (Cohen, 1973; Klose & Drexl, 2005). First linear programming (LP) formulations origin in the late fifties and were soon followed by solution techniques, which later on became well-known as Branch and Bound (B&B) or Mixed Integer Programing (MIP). The classical so-called warehouse location problem (WLP), also well-known as (simple) facility or plant location problem, is an uncapacitated optimization problem, which can be modeled as an LP (Balinski, 1965) or network problem (Drezner & Hamacher, 2004; Melkote & Daskin, 2001). It is a site-selecting location-allocation model with a min–sum objective, where from a number of potential facility or warehouse sites the set of costumers has to be serviced, while minimizing the total fixed site-costs (location) plus the total variable customer assignment costs (allocation). The WLP is 
                        NP
                     -hard (Garey & Johnson, 1979; Papadimitriou & Yannakakis, 1991) and, as such, has been in the focus of researchers interested in developing specialized LP-approaches (Körkel, 1999) or approximative constructive and local search or improvement algorithms, including standard add- or drop-heuristics, and Lagrangian approaches (Beasley, 1993; Kuehn & Hamburger, 1963), which then were followed by more advanced metaheuristics like genetic algorithms or tabu search and its derivatives (Greistorfer & Rego, 2006; Kratica, Tošić, Filipović, & Ljubic, 2001; Michel & Hentenryck, 2004). Comparing the WLP with the p-median problem, pMP (Hakimi, 1964; 1965), one identifies two differences: (1) there are no fixed site-costs involved and (2) the number of finally opened sites, p, is no longer a decision variable, but becomes included in the model. The pMP can be solved in polynomial time for fixed values of p, but is strongly 
                        NP
                     -hard for variable values of p (Current, Daskin, and Shilling, 2004, chap. 3; Garey & Johnson, 1979; Megiddo & Supowit, 1984). Consequently, any pMP is a special case of the general class of WLPs. Such as for the WLP, a variety of solution procedures, exact approaches, primal-dual approaches and metaheuristics, have been introduced for the pMP (Mladenović, Brimberg, Hansen, & Moreno-Pérez, 2007; Reese, 2006; Tansel, Francis, & Lowe, 1983). A most recent paper, Batta, Lejeuneb, and Prasad (2014), is recommended for a classical as well as modern view (dispersion, population, and equity criteria) on locational developments, offering an additional special focus on pMP.

This work relates to location type problems like the WLP and pMP, additionally taking account of the so-called obnoxious or semi-obnoxious effects. Such effects often occur when interesting services of some provider are based on unwanted, but inevitably necessary locations, which do not add additional value to the product. Quite contrary, pure obnoxiousness clearly devalues the production process. Obnoxious problems were firstly coined in Church and Garfinkel (1978) who located a facility in a network using an exact method and also introduced the term of the so-called bottleneck points for a network.


                     Erkut and Neuman (1989) used the terms disservice and service of the people in the vicinity of an (semi-)obnoxious facility that occurs during the processing of a product. In general, such services include a type of hazardous material, waste disposal, water treatment, nuclear power or chemical plants as well as big public facilities like airports. Regularly, these problems arise in the context of urban settings when the network may consist of noisy or polluting roads, transportation corridors, or rail lines (Segal, 2003).

The present work deals with the Obnoxious p-Median (OpM) problem. It can be formally defined as follows. Let I be a set of clients, J a set of facilities, and dij
                      the distance between the client i ∈ I and the facility j ∈ J. The OpM problem consists in finding a set S with p facilities (with S ⊆ J and p < |J|), such that the sum of the minimum distance between each client and the set of facilities is maximized. In mathematical terms:

                        
                           
                              
                                 
                                    
                                    
                                    
                                       
                                          
                                             max
                                          
                                          
                                          
                                          
                                          
                                             ∑
                                             
                                                i
                                                ∈
                                                I
                                             
                                          
                                          
                                             min
                                             {
                                             
                                                d
                                                
                                                   i
                                                   j
                                                
                                             
                                             :
                                             j
                                             ∈
                                             S
                                             }
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          subject
                                          
                                          to
                                          
                                          
                                          
                                       
                                    
                                 
                                 
                                    
                                    
                                    
                                       
                                          S
                                          ⊆
                                          J
                                          
                                          
                                          
                                          ,
                                          
                                          
                                          
                                          |
                                          S
                                          |
                                          =
                                          p
                                       
                                    
                                 
                              
                           
                        
                     
                  

Facilities in S are called open facilities, while facilities in J∖S are known as closed or unopened facilities.


                     Table 1
                      shows an example of pair-client distances, where there are 9 clients 
                        
                           I
                           =
                           {
                           
                              i
                              1
                           
                           ,
                           
                              i
                              2
                           
                           ,
                           
                              i
                              3
                           
                           ,
                           
                              i
                              4
                           
                           ,
                           
                              i
                              5
                           
                           ,
                           
                              i
                              6
                           
                           ,
                           
                              i
                              7
                           
                           ,
                           
                              i
                              8
                           
                           ,
                           
                              i
                              9
                           
                           }
                        
                      and 6 potential facilities 
                        
                           J
                           =
                           {
                           
                              j
                              1
                           
                           ,
                           
                              j
                              2
                           
                           ,
                           
                              j
                              3
                           
                           ,
                           
                              j
                              4
                           
                           ,
                           
                              j
                              5
                           
                           ,
                           
                              j
                              6
                           
                           }
                        
                     . Suppose that 
                        
                           p
                           =
                           3
                        
                     . Then, a solution of the OpM consists of selecting 3 facilities out of 6 in J. Table 2
                     a shows a solution with 
                        
                           S
                           =
                           {
                           
                              j
                              2
                           
                           ,
                           
                              j
                              5
                           
                           ,
                           
                              j
                              6
                           
                           }
                        
                     . The minimum distance between each client and the corresponding facility is highlighted with bold font. For example, the distances from client i
                     1 to each facility are: 
                        
                           d
                           (
                           
                              i
                              1
                           
                           ,
                           
                              j
                              2
                           
                           )
                           =
                           2
                           ,
                        
                     
                     
                        
                           d
                           (
                           
                              i
                              1
                           
                           ,
                           
                              j
                              5
                           
                           )
                           =
                           4
                           ,
                        
                      and 
                        
                           d
                           (
                           
                              i
                              1
                           
                           ,
                           
                              j
                              8
                           
                           )
                           =
                           8
                        
                     . Then, j
                     2 is the closest facility to client i
                     1. The value of the objective function of this solution, denoted as f(S), is the sum of the minimum distances from each client to the set of facilities: 
                        
                           f
                           (
                           S
                           )
                           =
                           2
                           +
                           12
                           +
                           4
                           +
                           1
                           +
                           4
                           +
                           4
                           +
                           2
                           +
                           2
                           +
                           4
                           =
                           35
                        
                     .


                     Table 2 b shows a different solution 
                        
                           
                              S
                              ′
                           
                           =
                           
                              {
                              
                                 j
                                 2
                              
                              ,
                              
                                 j
                                 3
                              
                              ,
                              
                                 j
                                 4
                              
                              }
                           
                           ,
                        
                      where the value of the objective function is 
                        
                           f
                           (
                           
                              S
                              ′
                           
                           )
                           =
                           23
                        
                     . Considering that the OpM is a maximization problem, solution S is better than solution S′. In other words, the minimum distances among the clients and facilities in S has a sum that is larger than the one in S′.

In this paper we explore the adaptation of the Greedy Randomized Adaptive Search Procedure methodology, GRASP, introduced in Feo and Resende (1989), to solve the OpM problem. Each GRASP iteration consists of constructing a trial solution and then applying an improvement procedure to find a local optimum. We explore different designs for both phases, construction and improvement. In particular, in Section 3 we propose two different constructive methods and in Section 4, we describe two local search algorithms. Additionally, we propose an efficient strategy to update the objective function value that considerably reduces the running time of GRASP. In Section 5, we present a filtering strategy intended to discard low-quality solutions and to selectively apply the local search only to promising solutions. This mechanism reduces the computing time without deteriorating the quality of the final solution. Section 6 reports on an extensive computational experience to validate the proposed algorithm by comparing its performance with those in the current state of the art. Finally, Section 7 summarizes the main conclusions of our research.

@&#LITERATURE REVIEW@&#

There exists an extensive literature treating obnoxious and semi-obnoxious situations, modeled on the plane or in a graph, using max–min, max–max and often bi-criteria objectives (Batta et al., 2014; Cappanera, 1999; Conceição Fonseca & Captivo, 2007; Erkut & Neuman, 1989; Farahani, SteadieSeifi, & Asgari, 2010; Segal, 2003). This goes along with the vast research on (obnoxious) p-median, p-center and similar location problems. Some of these sources are highlighted subsequently.

In Drezner and Wesolowsky (1980) a planar, Euclidean model of a 1-facility-problem is presented in which the shortest weighted distance to a point is maximized. Simultaneously, a side constraint must hold, i.e., the facility must be within a pre-specified distance from each point. The problem is solved using a graphical circles-based approach. A continuation of this work can be found in Drezner and Wesolowsky (1983), where a constrained obnoxious problem is considered for which a weighted rectangular-metric objective has to be maximized. The authors propose two algorithms, a so-called boundary and segment search and an LP-based approach. A semi-obnoxious scenario is solved in Melachrinoudis (1985). It uses a max–min model and seeks for a point on a convex two-dimensional bounded region, which maximizes the minimum weighted distance from that point to a given set of existing points in the region to be considered. As before, a circles-based, straightforward geometrical approach as well as an exact algorithm, using Kuhn–Tucker boundary points, is introduced. Complexity issues regarding the placement of several facilities in an obnoxious setting, a p max–min problem, were considered by Tamir (1991). It is shown that even the finding of an approximate solution is 
                        NP
                     -hard. Again, Drezner and Wesolowsky (1996) deal with an Euclidean network and an obnoxious urban situation in which a location has to be determined inside the convex hull of a number of nodes in order to maximize the minimum weighted distance between this point and the nodes and arcs of the network. Among others, the authors offer an ϵ-approximation scheme, which was later improved in Segal (2003). Welch and Salhi (1997) present three heuristics to solve the max–min formulation for siting p facilities on a network considering pollution dispersion equation and additionally facility interaction. Starting with 
                        
                           p
                           =
                           1
                           ,
                        
                      a graphical method approximates the pollution dispersion by the use of polygons. Afterwards, the general case for p and the combination of both the p max–min and p max–sum objectives, using a lexicographic approach, are investigated. Moreover, a simulated annealing algorithm is used to evaluate the base-algorithms.


                     Ohsawa and Tamura (2003) study the placement of a semi-obnoxious facility in a continuous plane underlying the bi-objective of maximizing the distance to the nearest inhabitant and minimizing the sum of distances to all users. In doing so, an elliptic max–min criterion and the rectangular min–sum criterion are used to model the push- and pull-aspect, respectively. For determining an efficient set and according trade-off curves, polynomial-time algorithms are presented. The obnoxious topic is considerably extended in Cappanera, Gallo, and Maffioli (2004), who additionally consider a routing component, in which sites and transportation links may be affected by a hazardous single commodity. The resulting flow model of this obnoxious facility location problem is then solved by a Lagrangian heuristic and possibly improved by a subsequent B&B-algorithm. A multi-objective model for the location of landfills is discussed in Rakas, Teodorović, and Kim (2004). They use a combined weighted objective and solve the overall problem based on a composition of several MIP-solutions. Highlighted are the use of fuzzy arithmetic rules with triangular fuzzy numbers to cover the aspect of uncertainty and the coverage of a real-case-study. Tamir (2006) focuses on the problem of finding the location of two new and obnoxious facilities in the plane. The objective is to maximize the minimum of all weighted distances between the existing customer facilities and the two new facilities, and the weighted distance between the pair of new facilities. For the Euclidean and rectilinear versions of this 2-obnoxious facility location problem efficient logarithmic running time algorithms are presented, and the rectilinear case is extended to also handle general (non-convex) planar and compact polygonal sets. Yapicioglu, Smith, and Dozier (2007) use a single- and a bi-objective particle swarm optimizer, well-known from evolutionary computation for their semi-obnoxious facility location problem. The idea is to simultaneously minimize transportation costs and undesirable effects, represented by a piecewise distance function depicting the degree of obnoxiousness. A computational analysis approximates the linear complexity in effort with increasing problem size. Berman and Wang (2007; 2008) consider the problem of locating single and multiple semi-obnoxious facilities while expropriating the nearest demand nodes under the restriction of a given total budget hold by the developer. The objective is to maximize the minimum weighted distance to the non-expropriated demand nodes. The algorithms introduced are based on dominating sets, Lagrangian relaxation and B&B. In Berman, Drezner, Wang, and Wesolowsky (2008) the base topic is extended to the possible expropriation of population centers in the vicinity of selected routes of a transportation model, thus introducing a routing effect, e.g., when avoiding the transportation of hazardous material in populated resident zones. Solution approaches for the single- and multiple-flow-case are based on a greedy heuristic and on column generation or branch and price, respectively.

A multi-objective obnoxious facility location model is treated in Bhattacharya (2011), in which an obnoxious facility interacts with a given set of existing costomers on the plane and has to be located according to a max–min criterion, while at the same time maximizing the number of existing facility points covered. The algorithm proposed is a rectangle decomposition, a step-wise analytical approach that embeds linear as well as non-linear programming sub-procedures. Ortigosa, Hendrix, and Redondo (2011) describe heuristic methods, based on Pareto-approaches, for generating semi-obnoxious locations. In doing so, they use a bi-objective convex/non-convex approach and several randomized sampling methods to optimize the desirable and the non-desirable aspect of the model, i.e. the min–sum pull-objective and the obnoxious push-objective. Coutinho-Rodrigues, Tralhão, and Alçada-Almeida (2012) deal with an empiric case of an urban waste collection problem in which a number of waste containers has to be located in a way that combines push and pull aspects of being serviced by a location too near (visual aesthetics, smell, nuisance, attractiveness) and by a location too far away (walking effort to deposit waste), respectively. Again, a bi-objective model approach is used, this time to minimize both, the container investment cost and a so-called weighted average customer dissatisfaction. The solution approach is a MIP-model, letting the decision-maker the final choice from non-dominated locations. Lozano, Duarte, Gortázar, and Martí (2012) study the so-called separation or dual bandwidth problem. This 
                        NP
                     -hard problem uses a max–min objective function to optimize transmitters in radio frequency assignment by assigning different frequencies in such a way that physically neighboring transmitters have as different frequencies as possible. To solve this type of layout problem, different metaheuristic approaches are studied, e.g., ejection chains, tabu search and variable neighborhood search. The interesting point in Heydari and Melachrinoudis (2012), who describe a semi-obnoxious facility with elliptic max–min and network min–sum objectives, is the use of two different metric measures. To find the efficient set of this bi-objective problem three phases are implemented: a network redefinition (of the transportation network due to bottleneck points and Voronoi-clustering), elimination (of inefficient edges) and construction (of the efficient set and its non-dominated trade-off curve). In Plastria, Gordillo, and Carrizosa (2013) a continuous, i.e., Euclidean, semi-obnoxious facility location problem is in the focus of interest. The analytical method offered, based on machine learning, gives the necessary conditions for optimality, and, using the latter, develops a polynomial enumeration on the set of dominating solutions.

In this paper, we target the Obnoxious p-Median (OpM) problem, which consists of selecting a subset of p facilities from a given set of possible locations, in such a way that the sum over all customers of the distances between each customer and its nearest facility is maximized. OpMP was introduced in the 1990s (Cappanera, 1999; Eiselt & Laporte, 1995; Welch & Salhi, 1997) and, as of today, has only gained relatively few attention. It is 
                        NP
                     -hard (Tamir, 1991) and can be formulated as a binary LP. Note that Burkard, Fathali, and Taghizadeh Kakhki (2007) proved that the special case of the OpM problem on a tree can be solved in linear time. A branch and cut method coupled with a tabu search has been recently suggested by Belotti, Labbé, Maffioli, and Ndiaye (2007). We include both methods, the branch and cut and the tabu search in our computational experimentation reported in Section 6.

The GRASP construction phase is an iterative, greedy, randomized and adaptive procedure that constructs solutions from scratch. It is iterative since the solution is built by considering one element at a time. It is greedy because the addition of each element to the solution under construction is guided by a greedy function. It is randomized because it performs a random selection from a list of good candidates. Finally, the method is adaptive in the sense of updating relevant information from a construction step to the next one.

A complete solution of the OpM is a set S ⊂ J with exactly p elements (
                        
                           |
                           S
                           |
                           =
                           p
                        
                     ). Let S ⊂ J be a partial solution to the OpM problem (i.e., 0 ≤ |S| < p). For each client i ∈ I, we define the minimum distance between i and the facilities in S, denoted as δi
                     , as follows:

                        
                           (1)
                           
                              
                                 ∀
                                 i
                                 ∈
                                 I
                                 →
                                 
                                    δ
                                    i
                                 
                                 =
                                 
                                    min
                                    
                                       j
                                       ∈
                                       S
                                    
                                 
                                 
                                    d
                                    
                                       i
                                       j
                                    
                                 
                              
                           
                        
                     
                  

The value of this partial solution, f(S), can be directly computed as:

                        
                           (2)
                           
                              
                                 f
                                 
                                    (
                                    S
                                    )
                                 
                                 =
                                 
                                    ∑
                                    
                                       i
                                       ∈
                                       I
                                    
                                 
                                 
                                    δ
                                    i
                                 
                              
                           
                        
                     
                  

We propose a greedy function that calculates the change in the objective function value when a new facility is added to a partial solution. In particular, if the facility j is included in the partial solution S, producing a new solution 
                        
                           
                              S
                              ′
                           
                           =
                           S
                           ∪
                           
                              {
                              j
                              }
                           
                           ,
                        
                      the corresponding δ′-value for each client is updated as follows:

                        
                           
                              
                                 ∀
                                 i
                                 ∈
                                 I
                                 →
                                 
                                    δ
                                    i
                                    ′
                                 
                                 =
                                 min
                                 
                                    {
                                    
                                       δ
                                       i
                                    
                                    ,
                                    
                                       d
                                       
                                          i
                                          j
                                       
                                    
                                    }
                                 
                              
                           
                        
                     
                  

Instead of directly computing the change of the objective function value, we determine the contribution of the new facility to the objective function. This is obtained with the greedy function g(S, j):

                        
                           (3)
                           
                              
                                 g
                                 
                                    (
                                    S
                                    ,
                                    j
                                    )
                                 
                                 =
                                 
                                    ∑
                                    
                                       i
                                       ∈
                                       I
                                    
                                 
                                 min
                                 
                                    {
                                    0
                                    ,
                                    
                                       (
                                       
                                          d
                                          
                                             i
                                             j
                                          
                                       
                                       −
                                       
                                          δ
                                          i
                                       
                                       )
                                    
                                    }
                                 
                              
                           
                        
                     where 
                        
                           (
                           
                              d
                              
                                 i
                                 j
                              
                           
                           −
                           
                              δ
                              i
                           
                           )
                           <
                           0
                        
                      indicates that the facility j is closer to i than the remaining facilities in S. Then, the inclusion of j reduces δi
                     . On the other hand, if 
                        
                           (
                           
                              d
                              
                                 i
                                 j
                              
                           
                           −
                           
                              δ
                              i
                           
                           )
                           ≥
                           0
                           ,
                        
                      the inclusion of the facility j does not affect client i. We use g(S, j) to determine the best potential facility, j
                     ⋆, to be included in the partial solution. In mathematical terms:

                        
                           
                              
                                 
                                    j
                                    ☆
                                 
                                 =
                                 arg
                                 
                                    max
                                    
                                       j
                                       ∈
                                       J
                                       ∖
                                       S
                                    
                                 
                                 
                                    {
                                    g
                                    
                                       (
                                       S
                                       ,
                                       j
                                       )
                                    
                                    }
                                 
                              
                           
                        
                     
                  

Let us illustrate the computation of the proposed greedy function with the example shown in Section 1. Considering that the OpM problem consists of finding a subset of facilities distant from the clients, we first identify the facility with the largest sum of distances. In this example, the sum of distances from all clients to each facility is 72, 51, 55, 73, 82 and 69. Then, the most distant facility with respect to all clients is j
                     5 with a sum of distances equal to 82. Therefore, the greedy strategy selects this facility, obtaining the partial solution 
                        
                           S
                           =
                           {
                           
                              j
                              5
                           
                           }
                        
                      and the minimum distance from each client to j
                     5 is [4, 15, 14, 10, 9, 4, 4, 15, 7], respectively.


                     Table 3
                      calculates the objective function change when we try to include a new facility, showing for each client i, 
                        
                           
                              d
                              
                                 i
                                 j
                              
                           
                           −
                           
                              δ
                              i
                           
                        
                     . For example, if we selected j
                     1 to be included in the partial solution, the situation of clients i
                     1, i
                     2, i
                     3, i
                     4, and i
                     8 would be deteriorated since j
                     1 is closer to these clients than j
                     5. On the other hand, i
                     5, i
                     6, i
                     7, and i
                     9 are not affected for the inclusion of this new facility. The facility which presents the best (higher) evaluation of the greedy function is, in fact, j
                     1, with a value equal to 
                        −
                     21 according to Eq. (3). We then update the partial solution to 
                        
                           S
                           =
                           {
                           
                              j
                              1
                           
                           ,
                           
                              j
                              5
                           
                           }
                        
                      and the array of distances to [3, 14, 5, 3, 9, 4, 4, 12, 7], respectively.

Similarly, Table 4
                      shows the values of the greedy function when we try to add a new facility to the current solution. The best option now is to include j
                     6, which obtains a value of 
                        −
                     12. We then include j
                     6 in the partial solution, obtaining 
                        
                           S
                           =
                           {
                           
                              j
                              1
                           
                           ,
                           
                              j
                              5
                           
                           ,
                           
                              j
                              6
                           
                           }
                        
                      and the distances to elements: [3, 12, 5, 3, 4, 4, 2, 12, 4]. Considering that 
                        
                           p
                           =
                           3
                           ,
                        
                      
                     S is a feasible solution with an objective function value of 49 (i.e., the sum of minimum distances).

Now we describe our two constructive algorithms C1 and C2 for the OpM problem. C1 implements a typical GRASP construction where each candidate element is initially evaluated by a greedy function to construct a Restricted Candidate List (RCL), and one element is selected at random from the RCL. Algorithm 1
                      shows the pseudo-code for C1. It initially creates a list of candidates (CL) which contains the elements that can be added to the partial solution under construction (Step 2). In order to favor the diversity of the constructed solutions, the method randomly selects the first facility from CL (Step 3) and includes it in the partial solution (Step 4). The method thus iterates until it obtains a solution with p facilities (Steps 6–13). In each iteration, C1 calculates the minimum (gmin
                     ) and maximum (gmax
                     ) values of the greedy function g(S, j) (Steps 7 and 8). After that, C1 constructs a restricted candidate list (RCL) with all the elements (facilities) whose greedy value exceeds a percentage α of the best greedy value (Step 9). Finally, in Step 10, the method selects at random one facility from the RCL and adds it to the solution, updating CL (Steps 11–12).

We now describe C2, based on the construction strategy introduced in Resende and Werneck (2004), in which randomization takes place before the greedy selection in each construction step. This strategy has been successfully applied in Campos, Martí, Sánchez-Oro, and Duarte (2014), Duarte, Martí, Resende, and Silva (2011) and Duarte, Sánchez-Oro, Resende, Glover, and Martí (2015). In C2, we first randomly choose candidates and then evaluate them to make the greedy choice. C2 first constructs a restricted candidate list RCL with a fraction (with 0 ≤ α ≤ 1) of the elements in the CL selected at random. Then, it evaluates all of them, computing the greedy function for all elements in the RCL, and selects the best one. Algorithm 2
                      shows the pseudo-code for C2. As it can be seen, the initial steps are the very same than those in Algorithm 1. The main difference between both methods resides on the instructions in the main loop (Steps 6–12 in Algorithm 2). In particular, it randomly selects a number of facilities from the CL. This number is determined by α, the percentage of the current size of the CL. Notice that we ensure that the value of size ranges from 1 to |CL| (see Step 7). The restricted candidate list is built with size elements of the CL, selected at random (Step 8). Then, the method selects the element j
                     ⋆, which presents the best value according to the greedy function g (Step 9). Similarly to C1, the constructive procedure adds the selected element to S (Step 10) and removes it from CL (Step 11).

The α parameter controls the greediness/randomness of the GRASP constructive procedures. Specifically, if 
                        
                           α
                           =
                           0
                           ,
                        
                      the corresponding method would be totally random. On the other hand, if 
                        
                           α
                           =
                           1
                           ,
                        
                      the constructive procedure would be a pure greedy method. In Section 6 we investigate the influence of this parameter on the performance of C1 and C2, selecting the value that obtains the best result.

The second stage of a GRASP algorithm consists in improving the constructed solution by applying a local search method to obtain a local optimum. Given a solution S, a neighbor solution S′ can be obtained by applying a move, which can be viewed as a perturbation in the solution S. The set of all neighbor solutions of S, called neighborhood, is denoted as N(S). In particular, we define a move operator that interchanges a facility j ∈ S with a facility j′ ∈ J∖S, producing a new feasible solution 
                        
                           
                              S
                              ′
                           
                           =
                           S
                           ∖
                           
                              {
                              j
                              }
                           
                           ∪
                           
                              {
                              
                                 j
                                 ′
                              
                              }
                           
                        
                     . For the sake of simplicity, we denote this move as S′ ← exchange(S, j, j′). Therefore, the neighborhood of a solution can be defined as follows:

                        
                           
                              
                                 N
                                 
                                    (
                                    S
                                    )
                                 
                                 =
                                 {
                                 
                                    S
                                    ′
                                 
                                 ←
                                 e
                                 x
                                 c
                                 h
                                 a
                                 n
                                 g
                                 e
                                 
                                    (
                                    S
                                    ,
                                    j
                                    ,
                                    
                                       j
                                       ′
                                    
                                    )
                                 
                                 :
                                 j
                                 ∈
                                 S
                                 ,
                                 
                                    j
                                    ′
                                 
                                 ∈
                                 J
                                 ∖
                                 S
                                 }
                              
                           
                        
                     
                  

The neighborhood can be explored with two different strategies: best improvement, in which all the solutions are examined (i.e., the associated neighborhood is completely explored), to identify the best; and first improvement, that tries to avoid the time complexity of exploring the whole neighborhood by performing the first improving move encountered during the exploration of the corresponding neighborhood. Therefore, the order in which the neighbors are inspected has a significant influence on the search. Notice that the order of exploration in the best improvement strategy is irrelevant since the corresponding neighborhood is fully explored.

We propose two different local search procedures based on the first improvement strategy. The difference between them resides in the neighborhood scanning strategy. In particular, LS1 explores the neighborhood at random. Algorithm 3
                      shows the pseudo-code of this procedure. It receives as input argument a feasible solution S, which is improved (Steps 2–20). The local search uses working copies of the set of facilities J, which are given as Jc
                      (opened sites, Step 4) and 
                        
                           J
                           c
                           ′
                        
                      (unopened sites, Step 5), which are both randomly sorted. The neighborhood is, therefore, explored at random by selecting an opened facility (Step 7) and an unopened facility (Step 9). Specifically, in Step 10 a neighbor solution is visited. Then, LS1 tests whether S′ improves upon S (Step 11) or not (Step 14). If so, the exploration of the current neighborhood is abandoned, updating the incumbent solution (Steps 12 and 13); otherwise, the copied sets are updated (Steps 15 and 16). The local search ends when no further improvement is found.

The second local search, LS2, sorts the facilities in the solution in ascending order of its contribution to the solution. Here, the contribution of each facility is the sum of the δ-values for each client in the solution (see Section 3). For the sake of simplicity we do not include the pseudo-code of this method since is similar to the one presented in Algorithm 3. The only difference is that facilities j and j′ are selected according to the contribution to a solution (instead of selecting them at random). We will study the performance of both methods in the computational experience.

One of the key elements in designing an effective local search method is the definition of the move and the associated move value (change in the objective function value). A move produces a change in the objective function value, called 
                        
                           m
                           o
                           v
                           e
                           _
                           v
                           a
                           l
                           u
                           e
                        
                     . More precisely, let S be a solution whose associated cost is f(S), let j
                     1 ∈ S be an opened facility, and let j
                     2 ∈ J∖S be an unopened facility. Then, exchange(S, j
                     1, j
                     2) produces a new solution 
                        
                           
                              S
                              ′
                           
                           =
                           S
                           ∖
                           
                              {
                              
                                 j
                                 1
                              
                              }
                           
                           ∪
                           
                              {
                              
                                 j
                                 2
                              
                              }
                           
                        
                      with cost f(S′). The change in the objective function value (due to the move) can be computed as:

                        
                           
                              
                                 m
                                 o
                                 v
                                 e
                                 _
                                 v
                                 a
                                 l
                                 u
                                 e
                                 =
                                 f
                                 
                                    (
                                    
                                       S
                                       ′
                                    
                                    )
                                 
                                 −
                                 f
                                 
                                    (
                                    S
                                    )
                                 
                              
                           
                        
                     
                  

We say that exchange(S, j
                     1, j
                     2) is an improving move if 
                        
                           m
                           o
                           v
                           e
                           _
                           v
                           a
                           l
                           u
                           e
                           >
                           0
                           ,
                        
                      since the OpM is a maximization problem.

The local search performs a sequence of moves to reach the final solution (local optimum). To perform a single move, it evaluates many candidates in the neighborhood. Therefore, the move evaluation is a critical part when designing a GRASP, since it requires a considerable computational effort. In a straightforward implementation, once the improvement method executes a move, it has to be evaluated, computing its corresponding 
                        
                           m
                           o
                           v
                           e
                           _
                           v
                           a
                           l
                           u
                           e
                        
                     . Normally, this computation requires to scan all the clients and all the facilities (i.e., the computational complexity of such a straightforward move evaluation is Θ(|I||J|)). Therefore, the larger the number of clients or facilities, the longer the execution time. However, we can considerably improve and reduce this complexity by maintaining a list for each client, which stores the currently best link to the closest facility. We conveniently separate these distances in two independent data structures. The first one contains the distances from all clients to the opened facilities. In particular, given a solution S, we maintain for each client i the list ΔS
                     (i) with those distances from i to each facility j ∈ S. This list is sorted is ascending order. For example, given the matrix of distances shown in Table 1 and the solution 
                        
                           S
                           =
                           {
                           
                              j
                              2
                           
                           ,
                           
                              j
                              5
                           
                           ,
                           
                              j
                              6
                           
                           }
                           ,
                        
                      the corresponding data structure is depicted in Table 5
                     a, where each row contains the information of each client. For instance, the list of distances for client i
                     3 is 
                        
                           
                              Δ
                              S
                           
                           
                              (
                              
                                 i
                                 3
                              
                              )
                           
                           =
                           
                              {
                              
                                 (
                                 4
                                 ,
                                 
                                    j
                                    2
                                 
                                 )
                              
                              ,
                              
                                 (
                                 6
                                 ,
                                 
                                    j
                                    6
                                 
                                 )
                              
                              ,
                              
                                 (
                                 14
                                 ,
                                 
                                    j
                                    5
                                 
                                 )
                              
                              }
                           
                           ,
                        
                      which means that the closest facility is j
                     2 (where the distance from i
                     3 to j
                     2 is 4), the second closest facility is j
                     6 (distance 6), and the farthest facility is j
                     5 (distance 14).

We construct a similar structure with the distances from clients to unopened facilities (i.e., those in J∖S). Specifically, we store for each client i the ordered list Δ
                     
                        J∖S
                     (i) with the distances from i to each facility j ∈ J∖S. Table 5b shows the list of distances for each client considering again the example described above (i.e. 
                        
                           J
                           ∖
                           S
                           =
                           {
                           
                              j
                              1
                           
                           ,
                           
                              j
                              3
                           
                           ,
                           
                              j
                              4
                           
                           }
                        
                     ).

The evaluation of a move is considerably improved by using these two data structures. In particular, if we execute the move S′ ← exchange(S, j, j′), the value f(S′) can be obtained in 
                        
                           O
                           (
                           |
                           I
                           |
                           )
                           ,
                        
                      since it is only required to access the first element of each list. In addition, the aforementioned data structures can be updated in 
                        
                           O
                           (
                           |
                           I
                           |
                           log
                           (
                           |
                           J
                           |
                           )
                           )
                           ,
                        
                      since this operation requires to insert the elements j and j′ in 2*|I| ordered lists (that can be performed in 
                        
                           O
                           (
                           log
                           (
                           |
                           J
                           |
                           )
                           )
                        
                     ). Therefore, 
                        
                           m
                           o
                           v
                           e
                           _
                           v
                           a
                           l
                           u
                           e
                        
                      can be computed in 
                        
                           O
                           (
                           |
                           I
                           |
                           log
                           (
                           |
                           J
                           |
                           )
                           )
                           +
                           O
                           (
                           |
                           I
                           |
                           )
                           =
                           O
                           (
                           |
                           I
                           |
                           log
                           (
                           |
                           J
                           |
                           )
                           )
                           ,
                        
                      which is considerably better than the straightforward implementation. In Section 6 we will study the influence of this strategy that we denote as incremental evaluation.

GRASP repeatedly applies a construction followed by a local search. However, as mentioned in Section 4, the local search is time consuming, so we implement a filtering mechanism to discard low-quality constructed solutions and skip the local search. Filtering low-quality solutions was proposed in the early paper by Feo, Resende, and Smith (1994). We implement here the specific design proposed in Laguna and Martí (1999) and applied in Martí, Pantrigo, Duarte, Campos, and Glover (2011).

The percentage of improvement achieved by the application of the local search can be estimated as follows:

                        
                           
                              
                                 P
                                 
                                    (
                                    i
                                    )
                                 
                                 =
                                 
                                    
                                       f
                                       
                                          (
                                          
                                             S
                                             i
                                             ′
                                          
                                          )
                                       
                                       −
                                       f
                                       
                                          (
                                          
                                             S
                                             i
                                          
                                          )
                                       
                                    
                                    
                                       f
                                       (
                                       
                                          S
                                          i
                                          ′
                                       
                                       )
                                    
                                 
                              
                           
                        
                     where i indicates the ith iteration, Si
                      is the solution generated by the constructive algorithm at iteration i, and 
                        
                           S
                           i
                           ′
                        
                      is the solution obtained after applying the local search method to Si
                     . Considering that the OpM problem is a maximization problem the value of this percentage ranges from 
                        
                           P
                           (
                           i
                           )
                           =
                           0
                        
                      (i.e., the improvement method is not able to improve the constructed solution) to P(i) < 1.

The filtering strategy requires a “warming up” phase to compute the average improvement obtained by the local search, as well as the standard deviation of the improvement. This phase corresponds to the first k iterations of the GRASP algorithm, where k is an input parameter of the algorithm. After that phase, the mean μP
                      and standard deviation σP
                      of the percentage of improvement P can be estimated as:

                        
                           
                              
                                 
                                    
                                       μ
                                       ^
                                    
                                    P
                                 
                                 =
                                 
                                    
                                       
                                          ∑
                                          
                                             i
                                             =
                                             1
                                          
                                          k
                                       
                                       
                                          P
                                          (
                                          i
                                          )
                                       
                                    
                                    k
                                 
                              
                           
                        
                     
                     
                        
                           
                              
                                 
                                    
                                       σ
                                       ^
                                    
                                    P
                                 
                                 =
                                 
                                    
                                       
                                          
                                             ∑
                                             
                                                i
                                                =
                                                1
                                             
                                             k
                                          
                                          
                                             
                                                (
                                                P
                                                
                                                   (
                                                   i
                                                   )
                                                
                                                −
                                                
                                                   
                                                      μ
                                                      ^
                                                   
                                                   P
                                                
                                                )
                                             
                                             2
                                          
                                       
                                       
                                          k
                                          −
                                          1
                                       
                                    
                                 
                              
                           
                        
                     
                  

These two statistics can be used to determine whether it is “likely” that the constructed solution at iteration i (with i > k) can be improved enough to outperform the current best solution, S
                     ⋆. To do this, we calculate the minimum percentage improvement that a constructed solution Si
                      needs to be improved in order to be better than S
                     ⋆. This value, denoted as imp(i), is obtained with the following expression:

                        
                           
                              
                                 i
                                 m
                                 p
                                 
                                    (
                                    i
                                    )
                                 
                                 =
                                 
                                    
                                       f
                                       
                                          (
                                          
                                             S
                                             ☆
                                          
                                          )
                                       
                                       −
                                       f
                                       
                                          (
                                          
                                             S
                                             i
                                          
                                          )
                                       
                                    
                                    
                                       f
                                       (
                                       
                                          S
                                          i
                                       
                                       )
                                    
                                 
                              
                           
                        
                     
                  

If this value is close to 
                        
                           
                              
                                 μ
                                 ^
                              
                              P
                           
                           ,
                        
                      we can assume that the improvement method could improve the current best solution. On the other hand, if imp(i) is considerably different than the estimation of the mean, it is unlikely that the local search method is able to obtain a solution better than the current one. In particular we only apply the local search method if and only if imp(i) is smaller than the estimated mean plus q times the estimation of the standard deviation (to consider a conservative rule). In mathematical terms:

                        
                           
                              
                                 i
                                 m
                                 p
                                 
                                    (
                                    i
                                    )
                                 
                                 <
                                 
                                    
                                       μ
                                       ^
                                    
                                    P
                                 
                                 +
                                 q
                                 ·
                                 
                                    
                                       σ
                                       ^
                                    
                                    P
                                 
                              
                           
                        
                     where q is a search parameter representing a threshold on the number of standard deviations away from the estimated mean percentage improvement. It is well-known that in a GRASP implementation the local search method consumes most of the running time. Therefore, we can considerably reduce it if we only improve promising constructed solutions. In Section 6, we will study the effect of different q values on solution quality and running time.

The pseudo-code of the GRASP approach with the filtering strategy is shown in Algorithm 4
                     . It performs N independent iterations as it is customary in GRASP. The algorithm starts each iteration by constructing a solution (Step 3) with any of the procedures described in Section 3 (C1 or C2). If the number of iterations i is lower than k, the constructed solution is improved (Step 5) with any of the local search procedures introduced in Section 4 (LS1 or LS2). Additionally, the percentage of improvement with respect to the best solution found is also updated (Steps 6 and 7). After k iterations (out of N), the method decides whether to improve a new constructed solution or not. In particular, the average mean, standard deviation and the percentage of improvement are computed in Steps 9–11. Notice that these computations are performed in an incremental way. Additionally, these three values are updated during the entire execution of the procedure. This strategy has the effect of being more restrictive in the application of the local search procedure as soon as the number of iterations increases.

@&#EXPERIMENTAL RESULTS@&#

In this section we first describe the preliminary experiments that guided us in the selection of the values of the different key search parameters and strategies of both constructive and local search algorithms. Then, we show the results of our GRASP method compared with the state-of-the-art methods, i.e., branch & cut and tabu search procedures described in Belotti et al. (2007). All the experiments, including the algorithms reported in Belotti et al. (2007), were executed on the same computer, an Intel i5 660 processor running at 3.3 gigahertz with 8 gigabytes of RAM using GNU/Linux. In addition, all the results related to execution times will be displayed in seconds.

We have used a set of 72 instances in our experimentation. In particular, they were generated by considering 24 instances (from pmed17 to pmed40) of the well-known p-median problem,
                        1
                     
                     
                        1
                        
                           http://people.brunel.ac.uk/~mastjjb/jeb/orlib/pmedinfo.html .
                      where the number of nodes ranges from 400 to 900. In order to transform a p-median instance into an obnoxious p-median one, Belotti et al. (2007) described the following procedure. Given the original instance with n nodes, this method selects n/2 nodes at random to be the set of clients. The remaining n/2 become the set of facilities. Additionally, for each original instance, Belotti et al. (2007) derived three new instances for the OpM by considering three values of p: ⌊n/2⌋, ⌊n/4⌋ and ⌊n/8⌋. Table 6
                      reports the main characteristics of the new set of 72 instances, where n indicates the number of nodes, |I|/|J| represents the number of clients/facilities, and p the number of required facilities.

We have designed two constructive algorithms based on the same greedy function (see Section 3). These procedures, namely C1 and C2 are parameterized by α, which controls the trade-off between randomness and greediness. In the first experiment, we evaluate the influence of this parameter over the performance of C1 and C2 by considering four different values: 0.25, 0.5, 0.75, and random, where random indicates that the method randomly selects an α value in the range [0, 1] for each construction. In order to avoid the over-fitting of our methods, we consider a representative subset of 10 percent of them (i.e., 8 instances) from the whole set of instances. In this way, we select, at least, one instance for each considered size, and an intermediate value of p on each case. We do not consider additional features of the instances since the coordinates of the nodes (facilities and clients) in the corresponding instances were generated by a random uniform distribution (Beasley, 1990). Notice that the remaining 90 percent instances (64 out of 72) are reserved for the final comparison with the state-of-the-art procedures. Table 6 shows with bold font the 8 instances that form the representative subset.


                     Table 7
                     
                     
                      shows the corresponding results when generating 100 independent constructions averaged over the subset of 8 instances. We report the average best cost (Avg. cost); deviation with respect to the best result in this experiment (Dev. (percent)); average computing time (Avg. time); the diversity of the solutions (Div.), computed as the sum of the different facilities found in the constructions generated on each run, divided by the number of constructions; and the number of best results (#Best), computed as the number of times the algorithm is able to match the best solution in this experiment. Attending to these results, we can assert that C2 outperforms C1 in terms of effectiveness, because it obtains the best results regarding the average cost, deviation and number of best solutions found, emerging C2 with 
                        
                           α
                           =
                           0.75
                        
                      as the best variant. On the other hand, C1 presents better results in terms of diversity, being C1 with 
                        
                           α
                           =
                           0.25
                        
                      the best variant. Finally, C2 with random α is the fastest algorithm, although the average cost and deviation are worse than C2(0.75).

At this point, we thought that it could be interesting to select both, the most effective constructive algorithm, C2(0.75), and the algorithm that produces the most diverse solutions, C1(0.25). The first one feeds the local search with high-quality solutions, although they share most of their facilities. On the contrary, the second one provides diverse solutions to the local search to drive the search to different areas in the search space, but the quality of this solutions is considerably worse. It is well-known that the design of efficient metaheuristics mainly relies on a balance between search intensification and diversification. Therefore, we cannot anticipate which would result in better outcomes when coupled with the local search method.

In the next experiment we analyzed this combination by considering the two local search procedures defined in Section 4, namely LS1 and LS2 with the best two constructive procedures aforementioned (i.e., C1(0.25) and C2(0.75)). Table 8 shows the results of the four derived GRASP variants in terms of cost, deviation, time, and number of best solutions. We do not include in this experiment the diversity computation since it is not useful for further improvements.

As shown in Table 8, the GRASP variants that use C2(0.75) as constructive procedure obtain better results than the ones that consider C1(0.25). We can then conclude that in this problem it is more relevant to produce high quality solutions, although it requires to sacrifice the diversity (at least in the set of instances considered in this experiment). Additionally, LS1 obtains slightly better results both in average cost and number of best results, spending longer computing times than LS2. This fact can be partially explained because LS2 sorts the elements in the solution according to the δ-values (see Section 4), while LS1 scans the elements in a random way. Given that the local search is based on the first improvement strategy, LS2 reaches improved solutions faster than LS1.

In the next experiment, we study the computing time of the best GRASP method when (1) using a direct cost computation (DCC) and (2) using the incremental cost computation (ICC) presented in Section 4. Fig. 1 depicts a bar diagram where the X-axis represents the name of the 8 instances considered in the preliminary experiments and the Y-axis gives the computing time required to obtain a local optimum for both methods, respectively, (DCC and ICC) in the corresponding instance. The figure clearly shows that the saving in computing time is significant for ICC. Specifically, for these 8 instances we have calculated the geometric mean of the times required to obtain a local optimum in both methods. For DCC, the geometric mean value is 775.36 seconds, while the value for ICC is 139.72 seconds to obtain the same optimum value, which corresponds to a reduction of a 81 percent.

In the next preliminary experiment we test the influence of the filter strategy described in Section 5. This mechanism skips the improvement method when the value of the constructed solution does not reach a minimum quality. The parameter q controls the number of standard deviations away from the estimated mean percentage improvement. In order to determine the effectiveness of this strategy and to select the best value of q, we configure the GRASP algorithm with 
                        
                           k
                           =
                           20
                        
                      (warming up phase) and 
                        
                           N
                           =
                           100
                           ,
                        
                      applying the filter strategy from iteration 21 to 100 (see Algorithm 4).


                     Table 9
                      shows the results of this experiment, where five values of q, from 0.5 to 2.5, are studied. In order to analyze the improvement with respect to traditional GRASP implementations, we include two versions without the filter strategy (last two rows of the table). The number between parentheses indicates the number of iterations of each variant.

We additionally include the percentage time reduction with respect to the slowest algorithm (percent vs max), and the average number of skipped improvements (Avg. #skip). As expected, the lower the value of q, the larger the number of skipped improvements, and the larger the reduction in the computing time. It is important to remark that the quality of the obtained results (in terms of the average percentage deviation and the number of best solutions found) does not present significant changes across different values of q, thus indicating the robustness of the filter (i.e. solutions skipped for improvement hardly modify the final result).

The reduction in the execution time is also remarkable. In particular, the slowest algorithm, GRASP(100), employs 198.99 seconds on average, while the filter variants need less than 2 seconds to find similar results. The filter strategy described in Section 5 reduces the computing time in more than 99 percent. We select the variant with q = 2 since it presents the best results in terms of quality and with a really competitive computing time (1.70 seconds). Notice that a meaningful consequence of the filter approach is the fact that it allows us to run the GRASP algorithm for a larger number of iterations.

As described in Section 2, the best identified procedures in the related literature are the branch & cut (B&C) and the tabu search method (XTS) described in Belotti et al. (2007).
                        2
                     
                     
                        2
                        The authors kindly provided us with the source code of their algorithms.
                      As it was aforementioned, the B&C is a finite algorithm that obtains the optimal solution of a given problem. However, when solving a large instance, it usually requires very long computational times, which can even make it unpractical. Considering that we are proposing heuristic procedures, we set the maximum computing time of this exact method to 3600 seconds. If after that time the B&C has not found the optimal solution, we interrupt its execution, returning the best solution found. The XTS is configured with the best parameters described in Belotti et al. (2007). We compare these methods with our best GRASP variant (constructive C2 with 
                        
                           α
                           =
                           0.75
                           ,
                        
                      local search LS1, incremental computation of the cost, and filter strategy with 
                        
                           q
                           =
                           2
                        
                     ) executed for two time horizons (1000 and 5000 iterations).


                     Table 10
                      presents the performance of each algorithm over the whole group of 72 instances. We report the same statistics previously used. These results show the merit of both GRASP approaches. In particular, the fastest version (1000 iterations) clearly outperforms B&C and XTS in all the considered metrics. In particular, it obtains the best results in 34 instances (out of 72), while the competitors obtain 11 and 25, respectively. In the same line, GRASP(1000) presents a remarkable average percentage deviation of 0.76 percent, which compares favorably to the 3.27 percent and 1.57 percent, achieved by B&C and XTS. Notice that OpM instances used in this experimentation present a large value of the objective function (see the column Avg. cost in Table 10). Therefore, improving the value of the cost in hundreds barely affects to the value of the average deviation (this problem is even aggravated in large instances). Then, the reduction of 0.81 percent in this set of instances can be considered as a success. GRASP(1000) obtains these results in considerably shorter computing time. It is important to remark that all algorithms were executed in the same computer.

In order to show how the performance of the GRASP procedure is affected by the number of iterations, we include a version executed for 5000 iterations. It improves upon the results of the fastest version, obtaining the best solution in 47 instances (out of 72) with an average percentage deviation of 0.73 percent. In this case, GRASP(5000) is slower than XTS but faster than B&C. Notice that the inclusion of the B&C solution in this table has the objective of providing a baseline in our heuristic comparison, and it is not meant to compare the B&C itself with other heuristics.

We finally compare our best method, GRASP(1000), with the best previous heuristic, XTS, with two well-known non-parametric tests for pairwise comparisons: the Wilcoxon test and the Sign test. The Wilcoxon test answers the question: Do the two samples (solutions obtained with both methods in our case) represent two different populations? The resulting p-value of 0.017 clearly indicates that the values compared come from different methods (using a typical significance level 
                        
                           α
                           =
                           0.05
                        
                      as the threshold for reject or not the null hypothesis). On the other hand, the Sign test computes the number of instances on which an algorithm supersedes another one. The resulting p-value of 0.013 indicates that there are significant differences between both algorithms, confirming the superiority of our GRASP method.

@&#CONCLUSIONS@&#

In this work we have studied the application of a GRASP approach to solve the Obnoxious p-Median problem. In particular, we have developed two constructive algorithms based on a greedy function that calculates the contribution of a facility to the current solution. Our experimentation showed that one of them emphasizes search intensification, while the other one mainly diversifies the search into different regions.

We also proposed two different local search procedures to be executed after the constructive phase. Knowing that the local search changes only one facility in a given solution, an incremental move evaluation of the solutions was proposed, which geometrically averages an 81 percent reduction in the execution time in relation to a full evaluation of the solutions. In line with our objective of designing an efficient method, we implemented a filtering mechanism that is able to skip the application of the improvement phase in not promising constructed solutions. In this way, this technique saves an average of 91.4 percent of the computation time for the GRASP with 100 constructions, obtaining similar results in terms of cost than the approach with no filter.

Finally, before engaging in competitive testing, we performed a series of scientific preliminary tests to determine the contribution of the various elements that we have designed. We believe that the reader can find them very useful since valuable lessons can be learned from them, and applied to other problems.

The experimental results show that our GRASP algorithm is able to outperform the current state-of-the-art methods in both short and long time horizons.

As future work, we will incorporate additional cost functions to enhance the quality of the solutions. The idea is to deal with a multi-objective optimization scenario for the OpM problem.

In order to contribute to a further research in this problem, we detail in this appendix the results of our experiments breaking down them to the instance level. Table 11 shows the best cost found for each one of the 72 instances we have dealt with. In addition, it is also indicated the algorithm/s that obtained that solution, as well as the time spent to reach the solution. In the cases where two or more algorithms found the same best solution, the reported time corresponds to the faster one, which will be shown in bold font.

@&#REFERENCES@&#

