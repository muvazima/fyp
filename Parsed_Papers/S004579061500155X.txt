@&#MAIN-TITLE@&#Novel energy and SLA efficient resource management heuristics for consolidation of virtual machines in cloud data centers

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A multi criteria resource allocation policy is proposed.


                        
                        
                           
                           A multi criteria policy for determination of underloaded PMs is proposed.


                        
                        
                           
                           A novel holistic resource management procedure is proposed.


                        
                        
                           
                           The results show up to 45% reductions in energy consumption.


                        
                        
                           
                           The results show up to 99% reductions in SLA violation.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Cloud computing

Consolidation

Data center

Energy consumption

Resource allocation

@&#ABSTRACT@&#


               
               
                  Proliferation of IT services provided by cloud service delivery model as well as diverse range of cloud users have led to the establishment of huge energy hungry data centers all around the world. Therefore, cloud providers are confronted with great pressures to reduce their energy consumption as well as their CO2 emissions. In this direction, consolidation is proposed as an effective method of energy saving in cloud data centers. This paper proposes a new holistic cloud resource management procedure as well as novel heuristics based on multi-criteria decision making method for both determination of underloaded hosts and placement of the migrating VMs. The results of simulations using Cloudsim simulator validates the applicability of the proposed policies which shows up to 46%, 99%, and 95% reductions in energy consumption, SLA violation, and number of VM migrations, respectively in comparison with state of the arts.
               
            

@&#INTRODUCTION@&#

Cloud computing has recently been brought into focus in both academic and industrial communities due to the increasing pervasive applications and the economy of scale that cloud computing provides [1,2]. Cloud computing is an operational management model that brings some new modern technologies together to provide extensive services dynamically for all range of cloud users. The research and development community has quickly reached consensus on core concepts of cloud computing such as on-demand computing, elastic scaling, elimination of up-front capital and operational expenses, and establishing pay-as-you-go business model for information technology services [3]. Three main types of cloud computing services are Infrastructure as a Service (IaaS), Platform as a Service (PaaS) and Software as a Service (SaaS) [4]. Users can easily avail of these services on a pay-as-you-use basis without any geographical restrictions [5].

As a direct result of cloud computing’s increasing popularity, cloud computing service providers such as Amazon, Google, IBM and Microsoft have begun to establish increasing numbers of energy hungry data centers for satisfying the growing customers resource (e.g. computational and storage resources) demands [6]. Continuous increase in energy consumption of such huge data centers raises a great concern for both governments and service providers to consume energy more effectively. Apart from the overwhelming operating costs and the total cost of acquisition (TCA) caused by high energy consumption, another rising concern is the environmental impact in terms of carbon dioxide (CO2) emissions [7]. The main portion of energy waste in cloud data centers is in their hardware infrastructure including servers, storage, and network devices. Since hardware devices consume their near maximum power level when they are idle, not fully utilizing them leads to enormous energy wastage. Forrester Research states that servers use nearly 30% of their peak power consumption while sitting idle 70% of time [8]. So, the basic reason of energy waste in data centers’ infrastructure is underutilization. Cloud provides scalability using virtualization and host applications which suffer high load at certain times [9].

Server consolidation using virtualization is an effective approach to achieve better energy efficiency of cloud data center [1,10,11]. The reason is that at times of low load, VMs are consolidated on a limited subset of the available physical resources, so that the remaining (idle) computing nodes can be switched to low power consumption modes or turned off [6]. Virtualization is an important feature of cloud computing that allows providing multiple VMs on a single physical machine as well as migration of VMs [12]. Due to the heterogeneity of cloud resources, and also the fact that cloud users may have sporadic and dynamic resource consumption, the cloud environment is highly dynamic. On the other hand, considering various goals that sometimes are contradicted with each other makes the resource management problem in cloud data center a challenging issue which needs tuning some trade-offs between targets. Cloud computing infrastructure controller has to guarantee pre-established contracts despite all the dynamism of workload changes and also it has to efficiently utilize resources and reduce resource wastage [13]. The basic online consolidation problem in cloud data centers is divided into four parts [10]: (1) determining when a host is considered as being overloaded; (2) determining when a host is considered as being underloaded; (3) selection of VMs that should be migrated from an overloaded host; and (4) finding a new placement of the VMs selected for migration from the overloaded and underloaded hosts. This paper focuses on the second and the fourth phases and proposes novel heuristics for them.

According to [14], solving the resource allocation problem using a vector packing algorithm is the best approach for static workloads. However, the key fact that workloads in cloud environments are dynamic makes this conclusion weak for cloud environments. Moreover, the vector packing problem is NP-hard [10]. So, heuristic algorithms such as Best Fit Decreasing (BFD) algorithm have been developed by researchers to solve it. BFD is shown to use no more than 11/9.OPT+1 bins (where OPT is the number of bins provided by the optimal solution) [15]. For instance, [16,10] model the problem of resource allocation as a bin packing problem with variable bin sizes and prices and solve it by applying Modified Best Fit Decreasing (MBFD) algorithm and Power Aware Best Fit Decreasing (PABFD) algorithm, respectively. The major drawback of current approaches for resource allocation problem in virtualized cloud data center is that they only consider one target such as power consumption in the core of their solutions. However, this paper proposes multi-criteria algorithms based on the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) [17] for both resource allocation and underloaded PM determination phases. The proposed policies simultaneously optimize energy consumption, SLA violation, as well as the number of VM migrations. Another major contribution of this paper is proposing novel procedure for the whole process of resource management in virtualized cloud data centers. The main idea behind this policy is solving the resource allocation problem for the VMs that are selected to be migrated from either overloaded or underloaded PMs in one step rather than in separate steps for each one. By doing so, a holistic view of resource allocation can be applied to the aggregated VMs and consequently a more precise solution can be found for the problem.

The main contributions of this paper are:
                        
                           •
                           Proposing a novel multi criteria resource allocation method namely TOPSIS Power and SLA Aware Allocation (TPSA) policy that simultaneously optimizes energy consumption, number of VM migrations and SLA violations.

Proposing a novel multi criteria method for determination of underloaded PMs including Available Capacity (AC), Migration Delay (MDL), and TOPSIS-Available Capacity-Number of VMs-Migration Delay (TACND) Policies.

Proposing Enhanced Optimization (EO) policy for the whole process of resource management in cloud data centers that aggregates the resource allocation phases for the VMs selected to be migrated from either overloaded or underloaded PMs in one single phase.

This paper begins by reviewing related works in Section 2. Section 3 presents system models including data center model and the metrics used to evaluate the efficiency of the proposed policies. Section 4 presents our proposed EO policy for the whole process of resource management. Sections 5 and 6 present our proposed resource allocation policy and the policies proposed for determination of underloaded PMs, respectively. Section 7 assesses the applicability of our proposed solutions using Cloudsim simulator. Finally, concluding remarks and future directions are presented in Section 8.

As stated in [2], there is a wide area of research in resource management field in cloud computing including resource provisioning, resource allocation, resource adaptation, resource mapping, resource modeling, resource estimation and resource brokering.

The authors in [18] have investigated power management techniques in the context of large-scale virtualized systems for the first time. In addition to the hardware scaling and VMs consolidation, they have proposed a new power management method for virtualized systems called “soft resource scaling.” Also, they have suggested dividing the resource management problem into local and global levels. In the local level, the algorithms monitor power management of guest VMs. On the other hand, global policies coordinate multiple physical machines. In this paper, the goal of the proposed model is minimizing energy consumption as well as satisfying performance requirements.

The authors in [1] have proposed a number of VM consolidation algorithms for cloud data center energy reduction considering structural features such as racks and network topology of the data center underlying the cloud. More precisely, they have taken into account the cooling and network structure of the data center hosting the physical machines when consolidating the VMs. By doing so, fewer racks and routers are employed, without compromising the service-level agreements, so that idle routing and cooling equipment can be turned off in order to reduce the energy consumption.

The authors in [19] have addressed the problem of power management by incorporating five different policies for power management. They have evaluated power consumptions of physical machines individually and distributed power dynamically across them to fulfill the group power budget. They have modeled VM allocation as an integer programming problem. Also, the aims of the proposed algorithms are to minimize power consumption, minimize performance loss, and meet power budget. However, the authors do not provide specific solutions to guarantee service level agreement (SLA) in cases of workload fluctuations.

The authors in [12] have proposed efficient consolidation algorithms which can reduce energy consumption and at the same time the SLA violations in some cases. They have introduced an efficient SLA-aware resource allocation algorithm that considers the trade-off between energy consumption and performance. Their proposed resource allocation algorithm takes into account both host utilization and correlation between the resources of a VM with the VMs present on the host. Moreover, they have proposed a novel algorithm for determination of underloaded PMs in the process of resource management in cloud data centers considering host CPU utilization and number of VMs on the host.

The authors in [20] have investigated the problem of power- and performance-efficient resource management in virtualized data center environments. The goal of this paper is to maximize the resource provider’s revenue by minimizing power consumption and SLA violation simultaneously. They have addressed the resource management problem using a sequential optimization model and proposed solutions using a limited look-ahead control to estimate future system states over a prediction slot by the help of Kalman filter. Decision goals to be optimized are the following: the number of VMs to be provisioned for each service; the CPU share allocated to each VM; the number of servers to switch on or off; and a fraction of the incoming workload to distribute across the servers hosting each service. However, the key problem with the proposed solution is its complexity such that the execution time of its optimization controller is approximately 30min even for 15 nodes, which is not appropriate for large scale cloud environments.

The authors in [5] have proposed a complete data center resource management scheme for the Infrastructure as a Service (IaaS) cloud environment. Their proposed scheme can guarantee user quality of service specified by SLAs and also tries to achieve maximum energy saving and green computing goals. They have achieved consolidation of resources by VM migrations technology and switching low-utilized or idle hosts to power saving mode while ensuring adherence to SLAs. Also, they have applied intelligent method of modified shuffled frog leaping algorithm based on improved extremal optimization to efficiently complete the dynamic allocation of VMs.

The authors in [21] have explored the problem of dynamic placement of applications in virtualized systems. Their goal is to minimize power consumption while meeting the requested SLA. The proposed solution contains three managers and an arbitrator. The arbiter coordinates managers’ actions and makes allocation decisions. Performance manager gathers applications information and resize VMs according to current resource requirements and the SLA. Power manager handles hardware power states and applies DVFS when it is necessary. Migration manager coordinates live migration of VMs. However, the proposed algorithms do not support SLAs and the performance of applications can be degraded due to the workload variability.

The authors in [22] have proposed a system that uses virtualization technology to allocate data center resources dynamically based on application demands and support green computing by optimizing the number of servers in use. Their aim is to achieve two goals in their algorithm: overload avoidance and green computing. To reach these goals, they have designed a load prediction algorithm that can capture the future resource usages of applications accurately without looking inside the VMs. Furthermore, they have defined a server as a hot spot if the utilization of any of its resources is above a static hot threshold and as a cold spot if the utilizations of all its resources are below a static cold threshold. However, fixed values of utilization thresholds are unsuitable for an environment with dynamic and unpredictable workloads, in which different types of applications can share a physical resource [10]. The system should be able to automatically adjust its behavior depending on the workload patterns exhibited by applications [10].

The authors in [16] have proposed an architectural framework and principle for energy-efficient cloud computing aimed at the development of energy-efficient provisioning of cloud resources, while meeting QoS requirements defined by SLA. They divided the VM allocation problem into two parts: the first part is the admission of new requests for VM provisioning and placing the VMs on hosts, whereas the second part is the optimization of the current VM allocations. They have modeled the first part as a bin packing problem and solved it by MBFD algorithm in which they first sort all VMs in decreasing order of their current CPU utilizations, and allocate each VM to a host that provides the least increase of power consumption due to this allocation. Moreover, they have stated that the optimization of the current VM allocations is carried out in two steps: at the first step they select VMs that need to be migrated, at the second step, the chosen VMs are placed on the hosts using the MBFD algorithm.

The authors in [6] have presented a dynamic resource management scheme that is able to automatically manage physical resources of a cloud infrastructure in such a way to maximize the profit of the cloud provider by minimizing SLA violations while reducing the energy consumed by the physical infrastructure. Their scheme utilizes both DVFS and server consolidation to minimize power consumption for cloud data centers while providing application-level performance guarantees. They provide each application with the minimum amount of physical resource capacity needed to meet its SLA, and dynamically relocate VMs according to the current resources requirements.

The authors in [10] have conducted competitive analysis and proved competitive ratios of optimal online deterministic algorithms for the single VM migration and dynamic VM consolidation problems. They have divided the problem of dynamic VM consolidation into four parts for the first time including: (1) determining when a host is considered as being overloaded; (2) determining when a host is considered as being underloaded; (3) selection of VMs that should be migrated from an overloaded host; and (4) finding a new placement of the VMs selected for migration from the overloaded and underloaded hosts. They have proposed novel adaptive heuristics for all parts. They have used PABFD algorithm to solve resource allocation problem in the fourth part which is similar to MBFD policy that they adopted in their previous work [16].

In sum, the main drawback of all the aforementioned studies is that they consider either energy consumption or SLA violation as their main objective and develop their solutions based on that. Another deficiency of the proposed algorithms is lack of holistic view in decision process. However, our study considers all targets including energy consumption, SLA violation, and number of VM migrations at the same time using novel multi-criteria algorithms which leads to notable improvements in output results.

The target system model consists of data centers with heterogeneous resources which host various users with different applications who run multiple heterogeneous VMs on data center nodes, resulting in a dynamic mixed workload on each PM. VMs and PMs are characterized with parameters including CPU computation power defined in Millions Instructions Per Second (MIPS), RAM, Disk capacity, and Network bandwidth. The target system model is depicted in Fig. 1
                      which is a modified version of the model described in [10]. This model includes two important parts: a central manager similar to global manager in [10] and the agents similar to local manager in [10]. The central manager is the resource manager of a specific data center which allocates virtual machines to available hosts in the data center based on a predefined goal. Also, it resizes VMs according to their resource needs, and decides when and which VMs should be migrated from PMs. The agents which are implemented in hypervisors are connected to central manager through network interfaces and have responsibility for monitoring PMs as well as sending gathered information to the central manager. Hypervisor performs actual resizing and migration of VMs as well as changes in power modes of the PMs. The main difference between our model and the one proposed in [10] is that both the decision on VMs resizing and the decision on when and which VMs should be migrated are made in central manager rather than in agents which results in having a more holistic view in decision making process. However, if the central manager runs on a single PM and that PM fails, there is no fault-tolerance policy. So, we propose running the central manager on a VM instead of a PM and use FT (fault tolerance) and HA (High Availability) capabilities which are possible; thanks to virtualization technology.

Traditionally, recent studies [16,20] have subscribed to the belief that power consumption by servers can be approximated by a linear relationship with CPU utilization. This approximation comes from the idea that CPU is the major power consumer in a data center. A serious weakness with this argument, however, is that by introducing multi-core CPUs with modern power management techniques, as well as utilization of virtualization technique, CPU is not the only major power consumer in data centers anymore [10]. This fact combined with the difficulty of modeling power consumption in modern data centers, makes building precise analytical models a complex research problem [10]. Hence, instead of using a complex analytical model for power consumption of a server, we utilize real data on power consumption provided by the results of the SPEC power benchmark [23]. Table 1
                         shows the power consumption of the servers used in this study which is provided in [10].

Moreover, energy consumption is modeled as the summation of power consumed during a period of time according to Eq. (1) which is widely used in the literature such as [16].
                           
                              (1)
                              
                                 E
                                 (
                                 t
                                 )
                                 =
                                 
                                    ∫
                                    
                                       t
                                    
                                 
                                 P
                                 (
                                 t
                                 )
                                 dt
                              
                           
                        
                     

QoS requirements are commonly formalized in the form of SLAs, which can be determined in terms of such characteristics as minimum throughput or maximum response time delivered by the deployed system [10]. As these characteristics can vary for different applications, it is necessary to define a workload independent metric that can be used to evaluate the SLA delivered to any VM deployed in an IaaS such as OTF (Overload Time Fraction) metric defined in [11]. In this study, we use the SLA Violation (SLAV) metric introduced in [10] as defined in Eq. (2) which is composed of multiplication of two metrics: the SLA violation time per active host (SLATAH) and performance degradation due to migration (PDM) as defined in Eq. (3).
                           
                              (2)
                              
                                 SLAV
                                 =
                                 SLATAH
                                 ×
                                 PDM
                              
                           
                        
                        
                           
                              (3)
                              
                                 SLATAH
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       N
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          i
                                          =
                                          1
                                       
                                       
                                          N
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             T
                                          
                                          
                                             
                                                
                                                   S
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             T
                                          
                                          
                                             
                                                
                                                   a
                                                
                                                
                                                   i
                                                
                                             
                                          
                                       
                                    
                                 
                                 ,
                                 
                                 PDM
                                 =
                                 
                                    
                                       1
                                    
                                    
                                       M
                                    
                                 
                                 
                                    
                                       
                                          ∑
                                       
                                       
                                          j
                                          =
                                          1
                                       
                                       
                                          M
                                       
                                    
                                 
                                 
                                    
                                       
                                          
                                             C
                                          
                                          
                                             
                                                
                                                   d
                                                
                                                
                                                   j
                                                
                                             
                                          
                                       
                                    
                                    
                                       
                                          
                                             C
                                          
                                          
                                             
                                                
                                                   r
                                                
                                                
                                                   j
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where 
                           
                              
                                 
                                    T
                                 
                                 
                                    
                                       
                                          s
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                           
                         is the total time during which the host i has experienced the utilization of 100%; 
                           
                              
                                 
                                    T
                                 
                                 
                                    
                                       
                                          a
                                       
                                       
                                          i
                                       
                                    
                                 
                              
                           
                         is the total time during which the host i has been in the active state; N is the number of PMs; 
                           
                              
                                 
                                    C
                                 
                                 
                                    
                                       
                                          d
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                           
                         is the estimate of the performance degradation of the VM
                           j
                         caused by migrations which is estimated as 10% of the average CPU utilization in MIPS during all migrations of the VM
                           j
                        ; 
                           
                              
                                 
                                    C
                                 
                                 
                                    
                                       
                                          r
                                       
                                       
                                          j
                                       
                                    
                                 
                              
                           
                         is the total CPU capacity requested by the VM
                           j
                         during its lifetime; and M is the number of VMs.

This study improves the on-line resource allocation process in two aspects. First, it proposes EO policy as a novel flowchart for on-line resource management procedure. EO suggests gathering all the VMs to be migrated from either overloaded or underloaded PMs in the VMs migration list and solving the on-line resource allocation problem at once using novel heuristics rather than in separate steps for them. Second, it proposes TPSA policy as a novel heuristic for off-line and on-line resource allocation problems which will be described in the next section. Solving resource allocation using EO policy has the benefit of applying a holistic view of the whole system and finding the best allocation of VMs on available PMs.

The proposed system flowchart based on EO policy is depicted in Fig. 2
                         in which the five boxes that make our flowchart different from state of the arts are highlighted by drawing dashed lines around them. The boxes numbered 2 and 4 emphasize that the VMs to be migrated from either overloaded or underloaded PMs are gathered in the migration list and the final resource allocation is not yet arranged to be executed for them. In box numbered 3, our proposed policies for detection of underloaded PMs are utilized. In boxes numbered 1 and 5, our proposed policies for resource allocation are utilized. A resource optimization cycle is repeated every 300s and PMs’ resource utilizations information are gathered during this management scheduling interval. At the beginning, a resource allocation procedure using TPSA policy is executed and newly arrived VMs are allocated to available PMs.

In the next step, PMs are searched one by one to find overloaded PMs until there is no more hotspot. Resource utilization values of each PM are predicted based on the resource utilization history of PMs, using Local Regression (LR) prediction algorithm [10]. If the prediction algorithm forecasts for a PM that its utilization will be more than 100%, then this PM is determined to be an overloaded PM. After that, VMs residing on overloaded PMs are selected for migration based on Minimum Migration Time (MMT) policy [10] until the elimination of hot spots.

In the following step, selected VMs are categorized based on their CPU utilization. Then, a resource allocation procedure is executed for the sorted VMs to find their probable migration destination using MBFD allocation policy proposed in [16]. MBFD policy finds the PM that both have enough resource to host the VM as well as the least power increase after allocation of a VM. If the control system finds a proper destination for a VM, then it is added to the migration list.

Following that, underloaded PMs are determined. In each searching step to find underloaded PMs, the defined policy for determination of underloaded PMs is executed and a PM is selected as a candidate of being underloaded. VMs from underloaded PMs are added to the migration list until the controlling system cannot find any underloaded PM. In the following step, selected VMs from underloaded PMs are sorted based on their CPU utilization. If the control system can find proper PMs as probable migration destinations for all the VMs residing on an underloaded PM using MBFD policy, then all its VMs are added to the VM’s migration list. Otherwise, none of the VMs are added to the VM’s migration list.

At the final step, a new placement is found for all the VMs in the migration list based on our proposed TPSA allocation policy and then migrations are initiated. Major advantage of our proposed flowchart is that the VM placement step is executed in the final step after finding the complete list of VMs to be migrated either from overloaded or underloaded PMs, rather than in separate steps for them. More precisely, other works such as [10] execute VM allocation for the VMs to be migrated from overloaded and underloaded PMs separately during the process of resource optimization. Consequently, our placement has a holistic view of the whole probable allocations rather than executing VM allocations one by one.

In this section we present our proposed policy for resource allocation problem in cloud data centers.

TPSA policy takes advantage of TOPSIS as a multi-criteria algorithm that considers five criteria depicted in Table 2
                         in its decision process. This policy computes the scores of all the PMs that are candidate for hosting a VM using the method that is described in this section and selects the PM with the highest score. Criteria considered in TPSA policy can have either benefit or cost type. The more the value of criteria with the benefit type, and the lower the value of criteria with the cost type, the closer is the answer to the optimum point. TPSA computes the score of PMs so that the following conditions exist in the answer: (1) the selected PM has the least power increase, (2) the selected PM has the most available resource, (3) the selected PM has the least number of VMs, (4) VMs on the selected PM have the least resource correlation with the VM to be allocated, and (5) the migration delay of the VM to be allocated to the selected PM is the least. In other words, TPSA is a multiple criteria method to identify solutions from a finite set of alternatives based upon simultaneous distance minimization from an ideal point and distance maximization from a nadir point [16].

It is important to note that by selecting a PM that has the least number of VMs, the probability that the VM has lower number of competent for shared resources is higher which leads to reduction in SLA violations. Moreover, selecting a PM with the highest available capacity ensures the allocation of resources that the VMs require with higher probability and consequently reduces the SLATAH metric. Besides, considering resource correlation is based on the idea given in [24] that the higher the correlation between applications that use the same resources on an oversubscribed server, the higher probability the server to become overloaded. According to this idea, we find allocation for a VM so that the VM has the least resource correlation with the VMs on a PM. Also, consideration of the migration delay of the VMs to be allocated reduces the SLA violation during migration process and consequently the PDM metric. Also, it reduces the number of VM migrations due to smart decisions and omission of migrations with long delays. More precisely, in TPSA method, the chosen PM has the shortest distance from the ideal positive point (PM+) and the farthest distance from the ideal negative point (PM−). PM+ and PM− are formed as composite of best and worst values of considered criteria for all PMs. Distance from each of these poles are measured in the Euclidean distance.

All the information assigned to the PMs in time slot t form a decision matrix 
                           
                              
                                 
                                    MCTV
                                 
                                 
                                    →
                                 
                              
                           
                         as shown in Eq. (4).
                           
                              (4)
                              
                                 
                                    
                                       MCTV
                                    
                                    
                                       →
                                    
                                 
                                 =
                                 
                                    
                                       
                                          
                                             
                                                
                                                   
                                                      
                                                         PI
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               1
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         AC
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               1
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         NV
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               1
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         RC
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               1
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         MD
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               1
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   …
                                                
                                                
                                                   …
                                                
                                                
                                                   …
                                                
                                                
                                                   …
                                                
                                                
                                                   …
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         PI
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         AC
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         NV
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         RC
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         MD
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               i
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                             
                                                
                                                   …
                                                
                                                
                                                   …
                                                
                                                
                                                   …
                                                
                                                
                                                   …
                                                
                                                
                                                   …
                                                
                                             
                                             
                                                
                                                   
                                                      
                                                         PI
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               N
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         AC
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               N
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         NV
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               N
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         RC
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               N
                                                            
                                                         
                                                      
                                                   
                                                
                                                
                                                   
                                                      
                                                         MD
                                                      
                                                      
                                                         
                                                            
                                                               PM
                                                            
                                                            
                                                               N
                                                            
                                                         
                                                      
                                                   
                                                
                                             
                                          
                                       
                                    
                                 
                              
                           
                        where PM1, PM2, …, PM
                           N
                         are the available PMs that are candidate of selection by TPSA; PI, AC, NV, RC, and MD are the criteria depicted in Table 2. In order to select the best PM we go through the following steps:


                        
                           
                              Step 1: First, we normalize the decision matrix 
                                    
                                       
                                          
                                             MCTV
                                          
                                          
                                             →
                                          
                                       
                                    
                                  to have dimensionless decision matrix 
                                    
                                       
                                          
                                             
                                                
                                                   MCTV
                                                
                                                
                                                   ̲
                                                
                                             
                                          
                                          
                                             →
                                          
                                       
                                    
                                 . The decision matrix is made dimensionless by dividing each entry by maximum value of each column according to Eq. (5).


                        
                           
                              Step 2: In the next step, PM+ and PM− are determined. Each attribute can be considered to have either benefit or cost type. Larger values for a benefit type attribute leads to less distance from PM+ and more distance from PM−, while the opposite condition is hold for a cost type variable. Before determining PM+ and PM−, type of each attribute should be defined which are shown in Table 2. We want to place a VM on a PM so that the PM has the least power increase, the highest available capacity, and the least number of VMs. Also, we want a solution in which the VM has the least resource correlation with the VMs on the PM, and also the delay for migration of the VM to the PM is the least. PM+ and PM− are defined in Eqs. (6) and (7), respectively.


                        
                           
                              Step 3: The relative distance for each criterion of a PM from PM+ and PM− are calculated using Eq. (8).


                        
                           
                              Step 4: Compute the total score of a PM using Eq. (9).


                        
                           
                              Step 5: Rank the PMs according to their score and select the one with the highest score. The PM with the highest score has the maximum distance from PM− and the minimum distance from PM+.

In this section we describe our proposed policies for determination of underloaded PMs which are AC, MDL, and TACND.

The main idea of this policy is considering available resource capacity instead of resource utilizations as a measure of determining underloaded PMs. This policy selects a PM as being underloaded when its available capacity is the least among all candidate PMs as depicted in Algorithm 1. Since CPU and memory consumptions are correlated in our target system, we compute the capacity of a PM based on its CPU capacity. Traditionally, the utilization percentage was considered to determine underloaded PMs. However, since the cloud environments are heterogeneous, the PMs with equal utilization percentages do not necessarily have equal available capacities. Therefore, AC suggests that between two PMs with equal utilization percentages the one with lower available capacity is a better candidate of being underutilized.
                           
                              
                                 
                                 
                                 
                                    
                                       
                                          Algorithm 1: AC policy for determination of underloaded PMs
                                    
                                    
                                       1
                                       
                                          Input: Candidate PMs to be underloaded.
                                    
                                    
                                       2
                                       
                                          Output: Underloaded PM.
                                    
                                    
                                       3
                                       Minimum Capacity = Max Value.
                                    
                                    
                                       4
                                       For all candidate PMs do
                                    
                                    
                                       5
                                       
                                          Available Capacity = CPU clock speed × Number of CPU cores.
                                    
                                    
                                       6
                                       
                                          If (Available Capacity of this PM < Minimum Capacity)
                                    
                                    
                                       7
                                       
                                          
                                          Minimum Capacity = Available Capacity of this PM.
                                    
                                    
                                       8
                                       
                                          
                                          Underloaded PM = This PM.
                                    
                                    
                                       9
                                       
                                          end
                                    
                                    
                                       10
                                       End
                                    
                                    
                                       11
                                       Return Underloaded PM
                                    
                                 
                              
                           
                        
                     

This policy is based on the idea of MMT policy proposed in [10] for VM selection from over-utilized PMs. This policy selects a VM that requires the minimum time to complete a migration relative to other VMs allocated to a host. Likewise, MDL selects a PM as being underloaded when the delay to migrate all the VMs running on it is the least among all candidate PMs. The migration delay for each VM is estimated as the RAM capacity of the VM divided by the available bandwidth of the PM. Taking migration delay into consideration reduces the SLA violations incurred due to migration process.
                           
                              
                                 
                                 
                                 
                                    
                                       
                                          Algorithm 2: MDL policy for determination of underloaded PMs
                                    
                                    
                                       1
                                       
                                          Input: Candidate PMs to be underloaded.
                                    
                                    
                                       2
                                       
                                          Output: Underloaded PM.
                                    
                                    
                                       3
                                       Minimum Delay = Max Value.
                                    
                                    
                                       4
                                       For all candidate PMs do
                                    
                                    
                                       5
                                       
                                          Migration Delay=
                                          
                                             
                                                
                                                   
                                                      ∑
                                                   
                                                   
                                                      i
                                                      =
                                                      1
                                                   
                                                   
                                                      #
                                                      VM
                                                      
                                                      on
                                                      
                                                      this
                                                      
                                                      PM
                                                   
                                                
                                                
                                                   
                                                      
                                                         
                                                            RAM
                                                         
                                                         
                                                            
                                                               
                                                                  VM
                                                               
                                                               
                                                                  i
                                                               
                                                            
                                                         
                                                      
                                                   
                                                   
                                                      Available Network Bandwidth of this PM
                                                   
                                                
                                             
                                          
                                       
                                    
                                    
                                       6
                                       
                                          If (Migration Delay of this PM < Minimum Delay)
                                    
                                    
                                       7
                                       
                                          
                                          Minimum Delay = Migration Delay of this PM.
                                    
                                    
                                       8
                                       
                                          
                                          Underloaded PM = This PM.
                                    
                                    
                                       9
                                       
                                          end
                                    
                                    
                                       10
                                       end
                                    
                                    
                                       11
                                       Return Underloaded PM
                                    
                                 
                              
                           
                        
                     

TACND takes advantage of TOPSIS as a multi criteria decision making method that takes three criteria depicted in Table 3
                         into consideration including available capacity of the PM, number of VMs on the PM, and the migration delays of VMs present on the PM. The process of determination of underloaded PM based on TACND policy is similar to the process described for TPSA policy in the previous section except that the three criteria depicted in Table 3 are considered in TACND. Moreover, the AC criterion is defined as a cost type criterion in TACND while it is defined as a benefit type criterion in TPSA. Therefore, in TACND lower values for AC criterion leads to less distance from ideal positive point and more distance from ideal negative point. Traditionally, the criterion that determines an underloaded PM is resource utilization. However, similar to Section 6.1, we have considered available resource capacity instead of resource utilization as one of important criterion for determination of underloaded PMs. It is important to note that current studies consider a PM with the least utilization as underloaded PM. However, if two PMs have identical utilizations, the PM with lower number of VMs is a better candidate to be underloaded. More precisely, the PM with lower number of VMs has lower probability of utilization increase and also it is more probable that it will remain underloaded. So, we consider number of VMs as one of input criterion in TACND algorithm. Besides, considering migration delay of the VMs present on an underloaded candidate PM results in reduction of migration delays and consequently reduction of SLA violations incurred due to migration process.

@&#PERFORMANCE EVALUATION@&#

In this section, we discuss a performance evaluation of the heuristics presented in this paper. We compare our solutions with recent energy aware resource allocation studies which are close to our study including [10,12] as benchmarks. Similar to our study, they consider the four-phased resource management process introduced in [10].

Since our target system is generic cloud computing environment, it is vital to analyze it on a large-scale virtualized data center infrastructure. However, implementing and evaluating the proposed algorithms on such infrastructure is very expensive and time-consuming. Moreover, executing repeatable large-scale experiments to analyze and compare the results of proposed algorithms is really hard. So, we have used simulation for performance evaluation. We have utilized an extension of Cloudsim toolkit [25] and its entire provided infrastructure as our simulation platform. Adopting Cloudsim toolkit enables us to perform repeatable experiments on large-scale virtualized data centers. Besides, it is a modular and extensible open source toolkit which has built-in capability to implement and compare energy aware algorithms in cloud environments.

In our infrastructure setup which has real configurations, we have simulated a cloud computing infrastructure comprising a data center with 800 installed heterogeneous physical machines including 200 HP ProLiant ML110 G4, 200 HP ProLiant ML110 G5, 200 IBM Server x3250, and 200 IBM Server x3550. Characteristics of these machines are depicted in Table 4
                        . Power consumptions of physical machines are computed based on the data described in Section 3.1. VMs are supposed to correspond to five Amazon EC2 VM types as shown in Table 5
                        . Since using real workload for simulation experiments is important, we consider 10days data of CoMon project [26]. This data contains CPU utilization in 5-min intervals of more than a thousand VMs that are located at more than 500 servers around the world (Table 6
                        ). During the simulations, each VM is randomly assigned a workload trace from one of the VMs from the corresponding day.

In order to assess the simultaneous minimization of energy, SLA violation, and number of VMs’ migrations, we use a new metric which we denote Energy-SLA-Migration (ESM) as defined in Eq. (10). Besides, in order to make our results comparable with the algorithms presented by Beloglazov and Buyya, we also consider ESV parameter defined in [10].
                           
                              (10)
                              
                                 ESM
                                 =
                                 Energy
                                 ×
                                 SLAV
                                 ×
                                 MigrationsCount
                              
                           
                        
                     

The process of on-line resource allocation inside data centers include four main phases: (1) determining when a host is considered as being overloaded; (2) determining when a host is considered as being underloaded; (3) selection of VMs that should be migrated from an overloaded host; and (4) finding the new placement of the VMs selected for migration [10]. In this section, a reference scenario consisting of a combination of the best policies reported in [10] for the aforementioned phases including Local Regression (LR) for the first phase, a simple method (SM) for the second phase, Minimum Migration Time for the third phase (MMT), and PABFD policy for the fourth phase is compared with the scenario described in [12] as well as with our proposed policies. In Section 7.3.1 the allocation policies are compared with each other; in Section 7.3.2 the policies for determination of underloaded PMs are compared; and in Section 7.3.3 the combination of best policies proposed in this study as well as in [10,12] are evaluated. In this study, there are three separate factors that make different total execution time for decision making process: (1) the policy that is used for resource allocation, (2) the policy that is used for determination of underutilized hosts, and (3) the flowchart adopted for the whole process of resource management. In order to compare the net effect of each section in the execution time, the execution time of each part is evaluated separately by changing only one of the aforementioned parts in Sections 7.3.1, 7.3.2, and 7.3.3, respectively.

In this section we compare our proposed TPSA policy with PABFD policy proposed in [10] as well as with Host Utilization and Minimum Correlation (UMC) policy proposed in [12]. It is important to note that in this section the default four-phased resource management flowchart proposed in [10] is utilized in resource management procedure. Ten experiments are executed separately for the 10days of workloads depicted in Table 6 and their median results for energy consumption, SLA violation, number of VM migrations, execution time as well as ESV and ESM metrics are reported in Table 7
                           . Fig. 3
                            shows the energy consumption in the data center for all aforementioned policies; Fig. 4
                            shows the value of SLA violation incurred to the system; Fig. 5
                            depicts the value of ESV metric which can be used to infer the simultaneous improvement of energy consumption and SLA violation; Fig. 6
                            shows the overall number of VM migrations executed in the system during simulation time; Fig. 7
                            depicts the ESM metric which can be used to measure simultaneous improvement of energy consumption, SLA violation, and number of VM migrations; and Fig. 8
                            shows the median value for average execution time of the whole resource management process.

As depicted in Figs. 3, 4 and 6, the results for TPSA policy regarding energy consumption, SLA violation, and number of VM migrations are prominently lower than PABFD and UMC. As a result, ESV and also ESM metrics for TPSA policy are much less in comparison with PABFD and UMC as shown in Figs. 5 and 7. More precisely, it can be inferred from Table 7 that adoption of TPSA policy leads to 46.7%, 99%, 91%, 99.47%, and 99.95% reductions in energy consumption, SLA violation, number of VM migrations, ESV metric, and ESM metric, respectively, in comparison with PABFD policy. This observation can be described by the fact that TPSA policy simultaneously considers multiple criteria depicted in Table 2 which notably improves the output results. Moreover, it can be deduced from Fig. 8 that adopting TPSA policy leads to the least execution time in comparison with PABFD and UMC. This observation can be described by the fact that TPSA policy takes advantage of simpler mathematical calculations with lower complexities in comparison with PABFD and UMC policies.

In this section we compare our proposed policies for determination of underloaded PMs including AC, MDL, and TACND with Simple Method (SM) policy proposed in [10] as well as with VM-based Dynamic Threshold (VDT) policy proposed in [12]. It is important to note that in this section the default four-phase resource management flowchart proposed in [10] is utilized in resource management procedure. Ten experiments are executed separately for the 10days of workloads depicted in Table 6 and their median results for energy consumption, SLA violation, number of VM migrations, execution time as well as ESV and ESM metrics are reported in Table 8
                           . Fig. 9
                            shows the energy consumption in the data center for all aforementioned policies; Fig. 10
                            shows the value of SLA violation incurred to the system; Fig. 11
                            depicts the value of ESV metric which can be used to infer the simultaneous improvement of energy consumption and SLA violation; Fig. 12
                            shows the overall number of VM migrations executed in the system during simulation time; Fig. 13
                            depicts the ESM metric which can be used to measure simultaneous improvement of energy consumption, SLA violation, and number of VM migrations; and Fig. 14
                            shows the median value for average execution time of the whole resource management process.

As depicted in Figs. 10 and 12, the results for TACND policy regarding SLA violation and number of VM migrations are prominently lower than other policies. As a result, ESV and also ESM metrics for TACND policy are much less in comparison with other policies as shown in Figs. 11 and 13, respectively. More precisely, it can be inferred from Table 8 that adoption of TACND policy leads to 99.79%, 94.31%, 99.59%, and 99.97% reductions in SLA violation, number of VM migrations, ESV metric, and ESM metric, respectively, in comparison with SM policy. This observation can be described by the fact that TACND policy simultaneously considers all the criteria depicted in Table 3 which notably improves the output results. As depicted in Fig. 9, energy consumption of TACND is about two times more than the other policies. This observation can be described by the existence of an intrinsic trade-off between energy consumption and SLA violation. However, TACND notably reduces the amount of SLA violation, as depicted in Fig. 10, while increasing the energy consumption in comparison with other policies. If this amount of energy consumption is not acceptable for cloud service providers, then MDL policy is the best among policies evaluated in this section; because, it leads to the least values for ESV and ESM metrics, in the next rank after TACND, as shown in Figs. 11 and 13, respectively. Moreover, it can be inferred from Fig. 14 that adopting TACND policy leads to the least execution time in comparison with SM, VDT, AC, and MDL policies. This observation can be described by the fact that TACND policy takes advantage of simpler mathematical calculations with lower complexities in comparison with other policies.

In this section we compare four scenarios consisting of combination of our best proposed policies for resource management process in cloud data centers with each other as well as with the ones proposed in [10,12]. Best combination of policies proposed in [10] include LR, MMT, SM, and PABFD for four phases of resource management process. So, other scenarios are compared with Lr/Mmt/Sm/Pabfd scenario as a reference scenario. Best combination of policies proposed in [12] is similar to the ones adopted in [10] except that it uses VDT policy for determination of underloaded PMs and UMC policy for resource allocation. The main difference between our scenarios and the one proposed in [10,12] is that all of our four scenarios take advantage of EO policy based on the flowchart depicted in Fig. 2. Moreover, EO/LR/MMT/MDL/TPSA, EO/LR/MMT/AC/TPSA, and EO/LR/MMT/TACND/TPSA scenarios adopt TPSA policy for resource allocation as well as MDL, AC, and TACND policies for determination of underloaded PMs, respectively. It is important to note that EO/LR/MMT/SM/PABFD scenario is considered in this section to evaluate the net performance of EO policy. More precisely, the only difference between EO/LR/MMT/SM/PABFD and the reference scenario is consideration of EO policy. Ten experiments are executed separately for the 10days of workloads depicted in Table 6 and their median results for energy consumption, SLA violation, number of VM migrations, execution time as well as ESV and ESM metrics are reported in Table 9
                           . Fig. 15
                            shows the energy consumption in the data center; Fig. 16
                            shows the value of SLA violation incurred to the system due to resource shortage as well as performance degradation due to migration; Fig. 17
                            depicts the value of ESV metric which can be used to infer the simultaneous improvement of energy consumption and SLA violation; Fig. 18
                            shows the overall number of VM migrations executed in the system during simulation time; Fig. 19
                            depicts the ESM metric which can be used to measure simultaneous improvement of energy consumption, SLA violation, and number of VM migrations; and Fig. 20
                            shows the median value for average execution time of the whole resource management process.

As depicted in Figs. 15–20, adopting EO policy leads to better performance regarding energy consumption, SLA violation, ESV metric, number of VM migrations, and ESM metric, respectively in comparison with the reference scenario. In sum, adopting EO policy results in 93.35% ESM improvement compared to LR/MMT/SM/PABFD scenario, as depicted in Table 9, which validates the applicability of EO policy in resource management procedure. This observation can be described by the fact that EO policy applies a holistic view to the resource management procedure and aggregates resource allocation for the VMs to be migrated from either overloaded or underloaded PMs in one single step. Moreover, it can be inferred from Figs. 16–20 that EO/LR/MMT/TACND/TPSA scenario prominently has the best performance regarding SLA violation, ESV, number of VM migrations, ESM, and execution time, respectively. Furthermore, as depicted in Table 9, adopting EO/LR/MMT/TACND/TPSA scenario leads to 99.99% ESM improvement compared to LR/MMT/SM/PABFD scenario. However, adopting this policy leads to about two times more energy consumption in comparison with other policies as depicted in Fig. 15. As depicted in Figs. 16–20, EO/LR/MMT/MDL/TPSA scenario has the next best performance regarding SLA violation, ESV, number of VM migrations, ESM, and execution time that also has acceptable energy consumption as depicted in Fig. 15. Besides, as depicted in Table 9, EO/LR/MMT/MDL/TPSA scenario leads to 99.89% ESM improvement compared to LR/MMT/SM/PABFD scenario.

The net effect of EO policy on execution time can be found by comparing the execution time of EO/LR/MMT/SM/PABFD and LR/MMT/SM/PABFD scenarios which unveils that utilizing EO policy has led to about one and a half more execution time as depicted in Fig. 20. This observation can be described by the fact that EO policy adds two steps to the default resource management procedure for aggregation of VMs to be migrated from either overloaded or underloaded PMs.

In this section a statistical analysis is presented for the best algorithm combinations and benchmark algorithms. Based on the Ryan–Joiners normality test, ESM values of all three type scenarios (LR/MMT/SM/PABFD, LR/MMT/VDT/UMC and EO/LR/MMT/TACND/TPSA) follow a normal distribution with the P value>0.1. Table 10
                         shows results based on paired t tests for all three aforementioned scenarios. Results show that there is statistically significant difference between these algorithms. The T-tests have shown that the usage of the EO/LR/MMT/TACND/TPSA scenario leads to a statistically significantly lower value of the ESM metric with the P-value<0.001. Table 11
                         compares the best algorithm combinations and benchmark algorithms regarding the mean values of the ESM metric along with 95% confidence intervals. From the observed results, we can conclude that EO/LR/MMT/TACND/TPSA scenario has the best performance regarding ESM metric.

Development of huge cloud data centers all around the world has led to enormous energy consumption and a steady increase in carbon emissions. This paper has concentrated on consolidation in virtualized cloud data centers as a solution to tackle with this problem. This paper has proposed EO policy as a novel resource management procedure in cloud data centers. Besides, this paper has introduced the central importance of optimizing different targets in cloud data centers at the same time including energy consumption, SLA violation, and number of VM migrations. Moreover, this paper has proposed novel multi-criteria algorithms for both phases of resource allocation and determination of underloaded PMs in cloud data centers. More precisely, this paper has proposed TPSA as a novel policy for resource allocation as well as MDL, AC, and TACND as novel policies for determination of underloaded PMs. The results of experiments obtained from an extensive evaluation of proposed policies using Cloudsim simulator have shown that proposed policies outperform existing resource management algorithms due to simultaneous optimization of important criteria in decision process as well as applying a holistic view thanks to adoption of EO policy. More precisely, this paper has evaluated performance of combination of best proposed policies for resource management process and has concluded that combination of EO policy for the whole process of consolidation procedure with TPSA policy for resource allocation and TACND policy for determination of underloaded PMs results in the best performance which shows a notable 99.99% reduction in ESM metric.

The research work is planned to be followed by implementing the proposed policies using real cloud infrastructure management products such as OpenStack. Another direction for future research is the investigation of algorithms for the other phases of resource management process in cloud data centers including determination of overloaded PMs as well as selection of VMs from overloaded PMs. The other research direction is looking into the effect of different weights assigned to the components of the utility functions of the multi-criteria optimization problems.

@&#REFERENCES@&#

