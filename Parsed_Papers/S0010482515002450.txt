@&#MAIN-TITLE@&#Referral system for hard exudates in eye fundus

@&#HIGHLIGHTS@&#


               
               
                  
                     
                        
                           
                           A referral system for diabetic retinopathy can decrease the load on ophthalmologists.


                        
                        
                           
                           A referral system is developed by using combination of mathematical techniques.


                        
                        
                           
                           Referral system makes treatment of patient cost and time effective.


                        
                        
                           
                           The referral system has been tested on four fundus databases.


                        
                        
                           
                           We have examined the referral system using in two different scenarios.


                        
                     
                  
               
            

@&#KEYPHRASES@&#

Exudates

Medical imaging

Eye fundus

Treatment

Vision

@&#ABSTRACT@&#


               
               
                  Hard exudates are one of the most common anomalies/artifacts found in the eye fundus of patients suffering from diabetic retinopathy. These exudates are the major cause of loss of sight or blindness in people having diabetic retinopathy. Diagnosis of hard exudates requires considerable time and effort of an ophthalmologist. The ophthalmologists have become overloaded, so that there is a need for an automated diagnostic/referral system. In this paper a referral system for the hard exudates in the eye-fundus images has been presented. The proposed referral system works by combining different techniques like Scale Invariant Feature Transform (SIFT), K-means Clustering, Visual Dictionaries and Support Vector Machine (SVM). The system was also tested with Back Propagation Neural Network as a classifier. To test the performance of the system four fundus image databases were used. One publicly available image database was used to compare the performance of the system to the existing systems. To test the general performance of the system when the images are taken under different conditions and come from different sources, three other fundus image databases were mixed. The evaluation of the system was also performed on different sizes of the visual dictionaries. When using only one fundus image database the area under the curve (AUC) of maximum 0.9702 (97.02%) was achieved with accuracy of 95.02%. In case of mixed image databases an AUC of 0.9349 (93.49%) was recorded having accuracy of 87.23%. The results were compared to the existing systems and were found better/comparable.
               
            

@&#INTRODUCTION@&#

Diabetes is becoming one of the rapidly increasing health threats in the recent years [1,2]. WHO has suggested that around 347 million diabetes affected people are present in the world today. Among them a large number of people are undiagnosed and untreated [3]. It has been estimated that 80% of these people belong to middle or low income countries which cannot afford expensive treatments. Between 2005 to 2030 the death rate of diabetes affected people will double according to statistics of WHO [3]. The Diabetics׳ Institute of Pakistan has estimated that in Pakistan there are 12.9 million people suffering from this disease. It makes up about 10% of the total population. Around 3.5 million such patients remain undiagnosed due to poor medical conditions [4]. These figures are alarming.

Hard exudates are among the most common artifacts found in the eye fundus image in people suffering from Diabetic Retinopathy. In most of the cases the presence of hard exudates suggests that the patient requires immediate referral to an ophthalmologist or medical expert for proper treatment of the disease. Usually the presence of hard exudates in the eye fundus also indicates the existence of other vision threatening anomalies. Manifestation of hard exudates can be exploited for diagnosis of Diabetic Retinopathy or alternatively Diabetes. If left untreated, the disease can seriously affect the patient, causing blurred vision and in the worst case scenario, blindness. The above mentioned statistics provided by WHO elucidates the rapid increase of the disease in the world, especially third world countries. The ophthalmologists are becoming more engaged and overloaded due to increasing number of patients. This problem establishes the need for an automated system for diagnosis/referral. This system will reduce the load on the ophthalmologists and medical experts resulting in more effective utilization of their energies. Digital imaging medical diagnostic tools have proven to be very effective when it comes to providing better medical facilities to low income or poor countries. The digital imaging diagnostic procedures are non-invasive, painless and patient-friendly. Such diagnostic/referral systems have already been launched in The Netherlands [5], United kingdom [6] and Australia [7]. A study conducted in United Kingdom suggests that using automated systems, for screening fundus images, at clinics reduces 36.6% load from the medical experts [8].

In this paper a referral system for the condition of hard exudates in diabetic retinopathy has been proposed. The proposed referral system uses SIFT [9] to extract features/descriptors from the images, K-means clustering [10] for making Visual Dictionaries (VD) [11–14] and a simple binary class support vector machine (SVM) [15] for classification. Section 2 gives a brief introduction about hard exudates. In Section 3 the proposed technique consisting of training and testing phases has been discussed. Section 4 explains how the dataset is constructed and experiments are designed. Section 5 gives the details of obtained results. In Section 6 conclusions have been drawn.

The hard exudates visible in the eye fundus images are usually of yellow color but may also be found in white color [16]. Hard exudates are formed due to the lipid break down materials which are usually left behind when the localized edema resolves. Usually they have sharp margins, as seen in 
                     Fig. 1(a)–(d). Often they appear as waxy and shiny structures [17]. Few important works for the detection of hard exudates have been very briefly discussed.

Sopharak et al. [18] tried to develop a system using basic image processing algorithms like filtering and contrast enhancement. In the work it has been assumed that the pixels close to exudates can be separated from the normal pixels by using only their intensities. Garcia et al. [19] introduced a extrude detection system by using classifiers and machine learning. In this system the candidate regions for hard exudates were separated. Properties like average and standard deviation of RGB values of candidate regions and normal regions were taken as features. The classifiers were used to find the hard exudates based on these features. In another work by Sopharak et al. [20] a new approach using fuzzy clustering and data analysis algorithms for the detection of hard exudates was suggested. The features used in the system like pixel intensities, standard deviation of pixel intensities and hue etc. were carefully chosen by the medical experts. Another pixel based approach was introduced by Dupas et al. [21]. Sanchez et al. [22] addressed the same problem by using dynamic thresholding and mixture of statistical methods. In addition various algorithms for pre-processing and post-processing were also used. Another system for the detection of hard exudates was introduced by Welfer et al. [23]. This system uses morphological operations and watershed transforms in LUV color space for the detection process. Sanchez et al. [24] introduced another hard exudates detection system by exploiting the patient׳s contextual information. Chen et al. [25] proposed an algorithm which used a combination of histogram operations and morphological operations. The classification was done by using SVM. Garcia et al. [26] made use of logistic regression in combination with multilayer perceptron classifier and radial basis function classifier for the detection of hard exudates from fundus images. His method required proper pre-processing. Kayal et al. [27] employed various basic images basic techniques for the purpose of hard exudates detection. The summary of the stated works and their performance has been given in 
                     Table 1.

The proposed referral system for hard exudates works by using different techniques like formation of VD, k-means Clustering, SIFT and SVM. SIFT has already proved its superior performance when it comes to recognition systems, object detection systems and target detection systems [9]. This superiority of SIFT comes with a price. SIFT is used for exact matching of data but when it comes to classification in broad or even constrained domain its performance is very poor. SIFT performs well in image retrieval systems where discriminative power is extremely important but when searching in complex categories, generalization power is vital. To solve the problem a method has been employed which uses visual dictionaries [11–14]. The visual dictionary comprises of visual words. Visual words are obtained through K-Means clustering. A single visual word represents a single cluster. A cluster is the collection of a number of points of interest (POI), close to each other, obtained through SIFT. As a consequence a visual word is the center of the cluster and represents the generalized form of many neighboring POIs. The detail description of the training and testing phase of the proposed system is described in the following sections.

In images different POIs can be detected. These POIs can be used in recognition, retrieval or referral systems [28]. It has been observed that the features found around these POIs are more useful and robust as compared to global features [28–34]. In the first step of training phase the POIs were detected in the training images. A digital fundus image containing hard exudates is shown in 
                        Fig. 2(a), whereas Fig. 2(b) indicates few POIs detected in the same image. Three medical experts were engaged to annotate the images. In training phase the optic disc region is also annotated by the experts. The annotations indicated the regions of the images containing artifacts and normal regions. SIFT was used to detect a large number of descriptors around the POIs which acted as low level features. The features obtained from SIFT are termed as low level feature because they require further processing before feeding them to the SVM. Consider an arbitrary training image 
                           
                              
                                 I
                              
                              
                                 i
                              
                           
                         where 
                           i
                           ∈
                           {
                           1
                           ,
                           2
                           ,
                           3
                           ,
                           ..
                           .
                           ,
                           m
                           }
                        . Here 
                           m
                         is the total number of training images. Using the SIFT descriptor the low levels features 
                           
                              
                                 d
                              
                              
                                 a
                              
                           
                         and 
                           
                              
                                 d
                              
                              
                                 n
                              
                           
                         are found from 
                           
                              
                                 I
                              
                              
                                 i
                              
                           
                        . Where 
                           
                              
                                 d
                              
                              
                                 a
                              
                           
                         are the low level features extracted from annotated regions of image 
                           
                              
                                 I
                              
                              
                                 i
                              
                           
                         marked as containing artifact, and 
                           
                              
                                 d
                              
                              
                                 n
                              
                           
                         are the low level features extracted from annotated regions of image 
                           
                              
                                 I
                              
                              
                                 i
                              
                           
                         marked as normal. Also 
                           a
                           ∈
                           {
                           1
                           ,
                           2
                           ,
                           3
                           ,
                           ..
                           .
                           ,
                           q
                           }
                         and 
                           n
                           ∈
                           {
                           1
                           ,
                           2
                           ,
                           3
                           ,
                           ..
                           .
                           ,
                           p
                           }
                        , where 
                           q
                         and 
                           p
                         are total low level features extracted from annotated training images and 
                           
                              
                                 d
                              
                              
                                 a
                              
                           
                        ,
                           
                              
                                 d
                              
                              
                                 n
                              
                           
                        
                        
                           ∈
                           
                              
                                 ℝ
                              
                              y
                           
                         exists in 
                           y
                         -dimensional space. Using the low level features 
                           
                              
                                 d
                              
                              
                                 a
                              
                           
                         and 
                           
                              
                                 d
                              
                              
                                 n
                              
                           
                        , a visual dictionary 
                           V
                           =
                           {
                           
                              
                                 v
                              
                              
                                 1
                              
                           
                           ,
                           
                              
                                 v
                              
                              
                                 2
                              
                           
                           ,
                           
                              
                                 v
                              
                              
                                 3
                              
                           
                           ,
                           ..
                           .
                           ,
                           
                              
                                 v
                              
                              
                                 k
                              
                           
                           }
                         is created using a K-means clustering method, where 
                           
                              
                                 v
                              
                              
                                 k
                              
                           
                         represents an arbitrary visual code word from visual dictionary 
                           V
                        . In the next step of training phase the quantization and spooling of 
                           
                              
                                 I
                              
                              
                                 i
                              
                           
                         is done based on visual dictionary 
                           V
                        . For this, first each 
                           
                              
                                 d
                              
                              
                                 a
                              
                           
                        ,
                           
                              
                                 d
                              
                              
                                 n
                              
                           
                        
                        
                           ∈
                           
                              
                                 ℝ
                              
                              y
                           
                         is mapped onto the 
                           V
                         
                        [35]. This step transforms the low level 
                           
                              
                                 d
                              
                              
                                 a
                              
                           
                         and 
                           
                              
                                 d
                              
                              
                                 n
                              
                           
                         onto a representation bases upon visual codewords of 
                           V
                        . The step can be represented as 
                           f
                           :
                           
                              
                                 ℝ
                              
                              y
                           
                           →
                           
                              
                                 ℝ
                              
                              k
                           
                        , 
                           f
                           (
                           
                              
                                 d
                              
                              
                                 a
                              
                           
                           )
                           =
                           
                              
                                 μ
                              
                              
                                 a
                              
                           
                         and 
                           f
                           (
                           
                              
                                 d
                              
                              
                                 n
                              
                           
                           )
                           =
                           
                              
                                 μ
                              
                              
                                 n
                              
                           
                        , where the 
                           μ
                        ׳s can be obtained by using the ‘hard assignment’ [36] of low level feature to the closest code word of the visual dictionary 
                           V
                         i.e.
                           
                              
                                 
                                    
                                       μ
                                    
                                    
                                       q
                                       ,
                                       k
                                    
                                 
                                 =
                                 1
                                 
                                 if
                                 
                                 q
                                 =
                                 arg
                                 
                                    
                                       min
                                    
                                    k
                                 
                                 
                                    
                                       ‖
                                       
                                          
                                             
                                                v
                                             
                                             
                                                k
                                             
                                          
                                          −
                                          
                                             
                                                d
                                             
                                             
                                                l
                                             
                                          
                                       
                                       ‖
                                    
                                    2
                                 
                                 
                                 else
                                 
                                 q
                                 =
                                 0
                              
                           
                        where 
                           
                              
                                 μ
                              
                              
                                 q
                                 ,
                                 k
                              
                           
                         is the 
                           q
                           th
                         component of the newly obtained mid-level feature and 
                           d
                           =
                           {
                           
                              
                                 d
                              
                              
                                 a
                              
                           
                           ,
                           
                              
                                 d
                              
                              
                                 n
                              
                           
                           }
                           ,
                           μ
                           =
                           {
                           
                              
                                 μ
                              
                              
                                 a
                              
                           
                           ,
                           
                              
                                 μ
                              
                              
                                 n
                              
                           
                           }
                           ,
                           l
                           =
                           n
                           +
                           p
                        . These features still require spooling step to be used in SVM. Therefore they are named as mid-level features.

In spooling the high level feature vector 
                           τ
                         is found using the sum spooling technique, i.e.
                           
                              
                                 g
                                 (
                                 {
                                 
                                    
                                       μ
                                    
                                    
                                       k
                                    
                                 
                                 }
                                 )
                                 =
                                 τ
                                 :
                                 ∀
                                 q
                                 ,
                                 
                                    
                                       τ
                                    
                                    
                                       q
                                    
                                 
                                 =
                                 
                                    ∑
                                    
                                       k
                                       =
                                       1
                                    
                                    N
                                 
                                 
                                    
                                       
                                          μ
                                       
                                       
                                          q
                                          ,
                                          k
                                       
                                    
                                 
                                 
                                 where
                                 
                                 τ
                                 ∈
                                 
                                    
                                       ℝ
                                    
                                    k
                                 
                              
                           
                        
                     

The 
                           τ
                        ׳s obtained are considered as high level features which are ready to be fed in the linear binary SVM [15]. In the proposed method a simple linear kernel binary classifier is used due to its observed merits over other classifiers as described in [15]. It should be noted that when it is said that linear kernel is used, it means that no kernel has been employed by the SVM. However the experiments were also repeated using Back Propagation Neural Network [37] instead of SVM to have a comparison between the performances of both classifiers. In the case of Neural Network only one hidden layer was used apart from input and output layers. The learning rate of the network was set to 0.9 and the momentum was set to 0.5.

As already discussed that our system applies SIFT to the annotated hard exudates regions to obtain 
                              
                                 
                                    d
                                 
                                 
                                    a
                                 
                              
                            from a low level features of the image and regards other low level features belonging to 
                              
                                 
                                    d
                                 
                                 
                                    n
                                 
                              
                           . When POIs are found in the fundus images, many of them appear at the edges of the original fundus area and the area of the black filter placed while taking the image. The descriptors obtained by such POIs are useless and can even affect the performance of the system. Similarly during the process of finding POIs many of them may appear in the region of the optic disc. While training, the system may pick the features from these points and may confuse them with 
                              
                                 
                                    d
                                 
                                 
                                    a
                                 
                              
                           . To remove such POIs from the edges of the filter and optic disc two masks were used. Fig. 2(c) shows the filter used to eliminate POIs from edges and Fig. 2(d) shows the filter used to remove POIs from optic disc. The process flow diagram is shown in 
                           Fig. 3.

The testing phase repeats almost the same steps described in the training phase on test image 
                           
                              
                                 I
                              
                              
                                 t
                              
                           
                        . SIFT low level features 
                           
                              
                                 d
                              
                              
                                 t
                              
                           
                         are found in the test image, where 
                           
                              
                                 d
                              
                              
                                 t
                              
                           
                           ∈
                           
                              
                                 ℝ
                              
                              y
                           
                        . The low level features on the edges of the filter are removed by the same mask used in the training phase. However in the testing phase the method described in [38] has been used, instead of annotations provided by experts, to locate and remove the low level features present in the regions of optic disc. The test image 
                           
                              
                                 I
                              
                              
                                 t
                              
                           
                         is quantized by using the same visual dictionary 
                           V
                         created in the training phase. This step creates the mid-level feature 
                           
                              
                                 μ
                              
                              
                                 t
                              
                           
                           =
                           f
                           (
                           
                              
                                 d
                              
                              
                                 t
                              
                           
                           )
                         and maps 
                           
                              
                                 d
                              
                              
                                 t
                              
                           
                           ∈
                           
                              
                                 ℝ
                              
                              y
                           
                         to 
                           
                              
                                 μ
                              
                              
                                 t
                              
                           
                           ∈
                           
                              
                                 ℝ
                              
                              k
                           
                         using the method as described in Section 3.1. The final high level feature 
                           
                              
                                 τ
                              
                              
                                 t
                              
                           
                         for the test image is obtained by sum spooling, discussed in Section 3.1. This high level feature 
                           
                              
                                 τ
                              
                              
                                 t
                              
                           
                         is fed into the already trained SVM or NN to classify if the image requires referral or not. The process flow diagram of the testing phase is shown in 
                        Fig. 4.

In the presented work four fundus image databases were used namely STARE [39], DR1 [40], DR2 [40] and Diaretdb1 [41].

STARE database was created by University of California, San Diego. The images were provided by Shiley Eye Center at the University of California, San Diego, and by the Veterans Administration Medical Center in San Diego. Currently the database contains 400 images from which 78 images were identified to contain hard exudates. The resolution of the images in STARE fundus database is 
                           700
                           ×
                           605
                         pixels. The images were captured by using TopCon TRV-50 camera which has a 35° field of view.

DR1 is a database created by Department of Ophthalmology, Federal University of Sao Paulo (UNIFESP). The database contains 234 images containing hard exudates, which are of our interest. The average resolution of the images in DR1 database is 
                           640
                           ×
                           480
                         pixels. The images were captured by using TRX-50X, mydriatic camera. The maximum resolution of the camera is one mega-pixel and has a 45° field of view [40].

DR2 database has also been provided by Department of Ophthalmology, Federal University of Sao Paulo (UNIFESP). The database has 520 images but there are 79 images of our interest which contain hard exudates artifacts. The resolution of the images is 
                           867
                           ×
                           575
                         pixels. The images were captured using TRC-NW8 non-mydriatic retinal camera with a Nicon D90 camera [40].

The Diaretdb1 is the fourth database used in our experiments. The data base contains 89 fundus images. 46 fundus images were recognized by the medical experts of having hard exudates, which are sometimes very mild. The images were taken in Kuopio University hospital. The images were captured using 50° field of view digital fundus camera. The name and type of the camera has not been described in the dataset. The resolution of the images is 
                           1500
                           ×
                           1152
                         pixels [41]. Different research works which used DR1, DR2 and Diaretdb1 databases are given in 
                        Table 2.

@&#EXPERIMENTS@&#

There are two sets of experiments conducted in the presented work. Both sets of experiments were conducted using SVM and Back Propagation Neural Network in turns. The first set of experiments was conducted on STARE [39] fundus image database to assess the performance of the system. The results obtained from using the STARE image database reflect the performance of the system when all the fundus images come from the same source. In the second set of experiments images from three different datasets containing hard exudates were mixed. These databases include DR1, DR2 and Diaretdb1. The purpose of conducting the second set of experiments was to assess the performance of the system when the number of images is large and the images come from different sources. The images containing hard exudates were separated from the three databases and were mixed to perform the experiments. Therefore 359 (234+79+46) images containing hard exudates and 359 randomly selected normal images from three databases were separated. Overall 718 images were involved in the performance evaluation of the proposed system.

In first set of experiments 80 images were used for training of the system while the remaining images were utilized while performing testing phase. In the second set of experiments 200 images were used for training and the remaining images were used in the testing phase. Although the images are taken from three different datasets, the resolution of the images does not affect the performance. Finding the proper POIs from the images does not depend on the resolution of images. Considering that merging images from different datasets may cause biasing effect in the results, both sets of experiments have been performed three times using random subsampling [50]. Each time a different randomly selected set of training and testing images were used. The random sets were named as RS 1, RS 2 and RS 3. Random selection was done by the computer. In addition, the experiments were performed using different sizes of the visual word dictionary to estimate its effect on the performance of the system. The sizes of visual dictionary used in the experiments were named as VD 50, VD 100, VD 150, VD 200, VD 250, VD 300, VD 350 and VD 400. It should be noted that 50 VD means that the visual dictionary used in the experiment contains 50 code words (high level features) from images containing artifacts and 50 code words (high level features) from the images marked as normal. Same convention is followed in other VDs.

@&#RESULTS@&#


                     
                     Table 3 displays the sensitivity, specificity and accuracy obtained for different sizes of visual dictionary and the three combinations of the random sets (RS) for first set of experiments using SVM for classification. 
                     Table 4 shows the results when the first set of experiments was performed using NN as a classifier. The sensitivity and specificity are defined by the following formulas [51]:
                        
                           
                              sensitivity
                              =
                              
                                 
                                    TP
                                 
                                 
                                    TP
                                    +
                                    FN
                                 
                              
                              ,
                              
                              specificity
                              =
                              
                                 
                                    TN
                                 
                                 
                                    FP
                                    +
                                    TN
                                 
                              
                           
                        
                     where TP=True Positives, TN=True Negatives, FP=False Negatives, FP=False Positives

The area under the curve (AUC) of radio operating curve (ROC) has also been calculated. The AUC is considered as the probability that the classification system will rank a randomly chosen positive instance higher than a randomly chosen negative one [52]. In Tables 2 and 3 the average AUC of three tests with in the specific VDs have also been shown. In addition the standard deviation 
                        σ
                      of the set of experiments, with in mentioned VD, for three random sets (RS) has also been given.


                     Table 3 shows that when using SVM the maximum AUC of 0.9702(97.02%) is recorded in RS 3 when the visual dictionary size is set on VD 250. The highest accuracy obtained is 95.02% on VD 350 for RS 2. The maximum average AUC of three experiments within the visual dictionaries is 0.9651(96.51%). The standard deviation results form Table 3 show that there is little fluctuation in the results when the visual dictionary size is increased from VD 50 to VD 400. The maximum standard deviation recorded is 0.024 when VD 300 is used. 
                     Fig. 5(a)–(h) displays the ROCs of three RS within the specific size of VD. There are minor variations in ROCs due to the use of different RSs. As mentioned, Table 4 shows the results when NN is used as a classifier. Table 4 indicates that the maximum AUC i.e. 0.9392 (93.92%) has been achieved when VD 250 is used on RS1. The maximum accuracy noted in the case is 88.22% on VD 250 on RS1. Minimum standard deviation is observed at VD 50 i.e. 0.011. The ROCs of three RS have been shown in 
                     Fig. 6(a)–(h).


                     
                     Fig. 7(a)–(c) shows another view of the obtained results in the first set of experiments for SVM and 
                     Fig. 8(a)–(c) show the same results for NN.

As stated in Section 4.2 that second set of experiments is also conducted to see the effects on the performance of the system when the number of images used is relatively large, taken under different conditions and come from dissimilar sources. The procedure followed to perform the second set of experiments is similar to the procedure used for first set of experiments. 
                     Tables 4 and 5 show the obtained results.

It can be easily observed from Table 5 that when using SVM the maximum AUC 0.9349(93.49%) is achieved for RS 2 while keeping the visual dictionary size to VD 250. The maximum average AUC of three experiments within the visual dictionaries is 0.9125(91.25%), which is also attained by using the VD 250. From Table 5 it is clear that the value of standard deviation of experiments fluctuates minutely as the size of VD is increased from VD 50 to VD 400 i.e. it remains almost stable. The maximum standard deviation has been recorded as 0.0575 within VD 200. In case of the NN classifier the maximum AUC obtained is 0.8955 (89.55%) when VD 250 was used on RS 1(shown in 
                     Table 6). The maximum accuracy 84.80% is also acquired using VD 250 on RS 1. The minimum standard deviation of 0.0139 is obtained on VD 350. The 
                     Fig. 9(a)–(h) shows the ROCs of three RS within the specific size of VD when SVM is used and 
                     Fig. 10(a)–(h) shows the results when NN is used as classifier. Variations in ROCs due to the use of different RSs can be observed. RS 3 showed some clear variation from RS 1 and RS 2 but this is due to the random selection of training images by the computer.


                     
                     Fig. 11(a)–(c) is another view of the results by displaying the ROCs of different VDs while keeping the RS constant in SVM classifier. 
                     Fig. 12(a)–(c) shows the same case when NN is used.

Few works that used STARE fundus image database for detection of hard exudates are as shown in 
                     Table 7.

It should be noted that some authors have specified their results in the form of accuracy achieved while Diri et al. [54] has only mentioned the specificity achieved by their system. In our work Table 7 represents the maximum accuracy achieved and maximum sensitivity and specificity pair achieved in both sets of experiments.

@&#CONCLUSION@&#

It has been observed from 
                     Figs. 13(a), 
                     14(a), 
                     
                     15(a) and 16(a) that the first set of experiments has almost stable AUC for all RS, however sometimes RS 3 had overall a bit poor performance than RS 1 and RS 2. That is due to the mixing and random selection of images, obtained through different image databases, by the computer. From Figs. 13(b), 14(b), 15(b) and 16(b) it can be noticed that the value of average AUC remains almost stable from VD 50 to VD 400 for both set of experiments. Figs. 13(c), 14(c), 15(c) and 16(c) display the standard deviation within different VDs for first and second set of experiments. It can be seen from figures that there is minute fluctuation in standard deviation value.

It has been discussed that there has been a need of an automated referral system to diagnose hard exudates from the eye fundus images and to reduce the pressure of work from the medical experts. Keeping this in view an automated referral system for hard exudates has been suggested. The suggested system has used combination of different techniques like SIFT, K-means clustering, Visual Dictionary and SVM. For comparison the tests were also performed with NN as classifier. The tests were carried out for different trainings and testing sets randomly selected by computer. In addition, the performance of the system on various visual dictionary sizes was also tested and very little effect of size of VD on the system was found. The suggested system used SVM and showed promising performance by displaying maximum AUC to be 0.9702(97.02%) on VD 250 when using STARE fundus image database with accuracy 95.02% and an AUC of 0.9349(93.49%) on VD 250 with accuracy of 87.23% when using mixture of image databases. In the future work the same system will be tested with other variants of SIFT like ASIFT, PCA-SIFT and CSIFT. The experiments will also be performed using different kernels of SVM and their comparisons will be made based upon detailed results.

The authors whose names are listed immediately below certify that they have NO affiliations with or involvement in any organization or entity with any financial interest (such as honoraria; educational grants; participation in speakers׳ bureaus; membership, employment, consultancies, stock ownership, or other equity interest; and expert testimony or patent-licensing arrangements), or non-financial interest (such as personal or professional relationships, affiliations, knowledge or beliefs) in the subject matter or materials discussed in this manuscript.

@&#REFERENCES@&#

